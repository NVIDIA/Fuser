++ mktemp -d
+ d=/tmp/tmp.gzipQdC7JD
+ cd /tmp/tmp.gzipQdC7JD
+ counter=0
+ rv=0
+ find /opt/pytorch/nvfuser/bin/ -maxdepth 1 -perm -111 -name 'test_*' -type f,l
/opt/pytorch/nvfuser/bin/test_host_ir_jit
/opt/pytorch/nvfuser/bin/test_rng
/opt/pytorch/nvfuser/bin/test_nvfuser
/opt/pytorch/nvfuser/bin/test_multidevice_tutorial
/opt/pytorch/nvfuser/bin/test_topk
/opt/pytorch/nvfuser/bin/test_layout_op
/opt/pytorch/nvfuser/bin/test_moe
/opt/pytorch/nvfuser/bin/test_tutorial
/opt/pytorch/nvfuser/bin/test_argsort
/opt/pytorch/nvfuser/bin/test_host_ir
/opt/pytorch/nvfuser/bin/test_matmul
/opt/pytorch/nvfuser/bin/test_reshape
/opt/pytorch/nvfuser/bin/test_cluster
/opt/pytorch/nvfuser/bin/test_scan
/opt/pytorch/nvfuser/bin/test_mps
/opt/pytorch/nvfuser/bin/test_profiler
/opt/pytorch/nvfuser/bin/test_multidevice
/opt/pytorch/nvfuser/bin/test_greedy
/opt/pytorch/nvfuser/bin/test_external_src
+ [[ -n '' ]]
+ [[ -n '' ]]
++ find /opt/pytorch/nvfuser/bin/ -maxdepth 1 -perm -111 -name 'test_*' -type f,l
+ all_test_files='/opt/pytorch/nvfuser/bin/test_host_ir_jit
/opt/pytorch/nvfuser/bin/test_rng
/opt/pytorch/nvfuser/bin/test_nvfuser
/opt/pytorch/nvfuser/bin/test_multidevice_tutorial
/opt/pytorch/nvfuser/bin/test_topk
/opt/pytorch/nvfuser/bin/test_layout_op
/opt/pytorch/nvfuser/bin/test_moe
/opt/pytorch/nvfuser/bin/test_tutorial
/opt/pytorch/nvfuser/bin/test_argsort
/opt/pytorch/nvfuser/bin/test_host_ir
/opt/pytorch/nvfuser/bin/test_matmul
/opt/pytorch/nvfuser/bin/test_reshape
/opt/pytorch/nvfuser/bin/test_cluster
/opt/pytorch/nvfuser/bin/test_scan
/opt/pytorch/nvfuser/bin/test_mps
/opt/pytorch/nvfuser/bin/test_profiler
/opt/pytorch/nvfuser/bin/test_multidevice
/opt/pytorch/nvfuser/bin/test_greedy
/opt/pytorch/nvfuser/bin/test_external_src'
+ [[ '' == \0 ]]
+ [[ '' == \1 ]]
+ test_files='/opt/pytorch/nvfuser/bin/test_host_ir_jit
/opt/pytorch/nvfuser/bin/test_rng
/opt/pytorch/nvfuser/bin/test_nvfuser
/opt/pytorch/nvfuser/bin/test_multidevice_tutorial
/opt/pytorch/nvfuser/bin/test_topk
/opt/pytorch/nvfuser/bin/test_layout_op
/opt/pytorch/nvfuser/bin/test_moe
/opt/pytorch/nvfuser/bin/test_tutorial
/opt/pytorch/nvfuser/bin/test_argsort
/opt/pytorch/nvfuser/bin/test_host_ir
/opt/pytorch/nvfuser/bin/test_matmul
/opt/pytorch/nvfuser/bin/test_reshape
/opt/pytorch/nvfuser/bin/test_cluster
/opt/pytorch/nvfuser/bin/test_scan
/opt/pytorch/nvfuser/bin/test_mps
/opt/pytorch/nvfuser/bin/test_profiler
/opt/pytorch/nvfuser/bin/test_multidevice
/opt/pytorch/nvfuser/bin/test_greedy
/opt/pytorch/nvfuser/bin/test_external_src'
+ for f in $test_files
+ [[ /opt/pytorch/nvfuser/bin/test_host_ir_jit == *matmul* ]]
+ [[ /opt/pytorch/nvfuser/bin/test_host_ir_jit == *multidevice* ]]
+ executable=/opt/pytorch/nvfuser/bin/test_host_ir_jit
++ basename /opt/pytorch/nvfuser/bin/test_host_ir_jit
+ report_name=TEST-report-test_host_ir_jit.xml
+ sed_flag=s/__tmp_/__tmp_c0_/g
+ /opt/pytorch/nvfuser/bin/test_host_ir_jit --gtest_output=xml:TEST-report-test_host_ir_jit.xml
+ sed -u s/__tmp_/__tmp_c0_/g
Running main() from /opt/pytorch/nvfuser/third_party/googletest/googletest/src/gtest_main.cc
Note: This is test shard 2 of 3.
[==========] Running 4 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 4 tests from HostIrJitTest
[ RUN      ] HostIrJitTest.HostIrContainer
[       OK ] HostIrJitTest.HostIrContainer (190 ms)
[ RUN      ] HostIrJitTest.Reorder
[       OK ] HostIrJitTest.Reorder (107 ms)
[ RUN      ] HostIrJitTest.BroadcastTest
[       OK ] HostIrJitTest.BroadcastTest (4 ms)
[ RUN      ] HostIrJitTest.Linear
[       OK ] HostIrJitTest.Linear (106 ms)
[----------] 4 tests from HostIrJitTest (408 ms total)

[----------] Global test environment tear-down
[==========] 4 tests from 1 test suite ran. (408 ms total)
[  PASSED  ] 4 tests.
+ rv=0
+ for f in __tmp_*
+ [[ __tmp_* == __tmp_c* ]]
+ fs='__tmp_*'
++ echo '__tmp_*'
++ sed s/__tmp_/__tmp_c0_/g
+ ft='__tmp_c0_*'
+ mv '__tmp_*' '__tmp_c0_*'
mv: cannot stat '__tmp_*': No such file or directory
+ counter=1
+ for f in $test_files
+ [[ /opt/pytorch/nvfuser/bin/test_rng == *matmul* ]]
+ [[ /opt/pytorch/nvfuser/bin/test_rng == *multidevice* ]]
+ executable=/opt/pytorch/nvfuser/bin/test_rng
++ basename /opt/pytorch/nvfuser/bin/test_rng
+ report_name=TEST-report-test_rng.xml
+ sed_flag=s/__tmp_/__tmp_c1_/g
+ /opt/pytorch/nvfuser/bin/test_rng --gtest_output=xml:TEST-report-test_rng.xml
+ sed -u s/__tmp_/__tmp_c1_/g
Running main() from /opt/pytorch/nvfuser/third_party/googletest/googletest/src/gtest_main.cc
Note: This is test shard 2 of 3.
[==========] Running 5 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 5 tests from RNGTest
[ RUN      ] RNGTest.ManualScheduleValidateWithCURand
[       OK ] RNGTest.ManualScheduleValidateWithCURand (460 ms)
[ RUN      ] RNGTest.BroadcastingRNG2
[       OK ] RNGTest.BroadcastingRNG2 (1006 ms)
[ RUN      ] RNGTest.Uniform
[       OK ] RNGTest.Uniform (349 ms)
[ RUN      ] RNGTest.FunctionalUniform
[       OK ] RNGTest.FunctionalUniform (792 ms)
[ RUN      ] RNGTest.SameAsRNGOpDeterministic
[       OK ] RNGTest.SameAsRNGOpDeterministic (0 ms)
[----------] 5 tests from RNGTest (2609 ms total)

[----------] Global test environment tear-down
[==========] 5 tests from 1 test suite ran. (2609 ms total)
[  PASSED  ] 5 tests.
+ rv=0
+ for f in __tmp_*
+ [[ __tmp_* == __tmp_c* ]]
+ fs='__tmp_*'
++ echo '__tmp_*'
++ sed s/__tmp_/__tmp_c1_/g
+ ft='__tmp_c1_*'
+ mv '__tmp_*' '__tmp_c1_*'
mv: cannot stat '__tmp_*': No such file or directory
+ counter=2
+ for f in $test_files
+ [[ /opt/pytorch/nvfuser/bin/test_nvfuser == *matmul* ]]
+ [[ /opt/pytorch/nvfuser/bin/test_nvfuser == *multidevice* ]]
+ executable=/opt/pytorch/nvfuser/bin/test_nvfuser
++ basename /opt/pytorch/nvfuser/bin/test_nvfuser
+ report_name=TEST-report-test_nvfuser.xml
+ sed_flag=s/__tmp_/__tmp_c2_/g
+ /opt/pytorch/nvfuser/bin/test_nvfuser --gtest_output=xml:TEST-report-test_nvfuser.xml
+ sed -u s/__tmp_/__tmp_c2_/g
Running main() from /opt/pytorch/nvfuser/third_party/googletest/googletest/src/gtest_main.cc
Note: This is test shard 2 of 3.
[==========] Running 2134 tests from 137 test suites.
[----------] Global test environment set-up.
[----------] 15 tests from NVFuserTest
[ RUN      ] NVFuserTest.KernelDb_Query_CUDA
[       OK ] NVFuserTest.KernelDb_Query_CUDA (0 ms)
[ RUN      ] NVFuserTest.RegisterSharingCircularBufferingPointwiseCustom
[       OK ] NVFuserTest.RegisterSharingCircularBufferingPointwiseCustom (258 ms)
[ RUN      ] NVFuserTest.TmaRegisterSharingDynamicShapesExpectFail
[       OK ] NVFuserTest.TmaRegisterSharingDynamicShapesExpectFail (2 ms)
[ RUN      ] NVFuserTest.FusionHashDifferentDefinition
[       OK ] NVFuserTest.FusionHashDifferentDefinition (84 ms)
[ RUN      ] NVFuserTest.IndexSelectVectorizationLookupTensorCase0
[       OK ] NVFuserTest.IndexSelectVectorizationLookupTensorCase0 (98 ms)
[ RUN      ] NVFuserTest.IndexSelectVectorizationIndexTensorNoBroadcast
[       OK ] NVFuserTest.IndexSelectVectorizationIndexTensorNoBroadcast (92 ms)
[ RUN      ] NVFuserTest.OptOutMutatorMutatedOutput
[       OK ] NVFuserTest.OptOutMutatorMutatedOutput (70 ms)
[ RUN      ] NVFuserTest.AvoidProjectingToInputsIfRecomputeHasDropout
[       OK ] NVFuserTest.AvoidProjectingToInputsIfRecomputeHasDropout (26 ms)
[ RUN      ] NVFuserTest.InnerReductionUnrollVectorization
[       OK ] NVFuserTest.InnerReductionUnrollVectorization (115 ms)
[ RUN      ] NVFuserTest.ReductionSchedulerWithAdditionalIDOuterNormalization
[       OK ] NVFuserTest.ReductionSchedulerWithAdditionalIDOuterNormalization (198 ms)
[ RUN      ] NVFuserTest.VectorizeSimple
[       OK ] NVFuserTest.VectorizeSimple (117 ms)
[ RUN      ] NVFuserTest.Vectorization2
[       OK ] NVFuserTest.Vectorization2 (3 ms)
[ RUN      ] NVFuserTest.VectorizationStrideValidation
[       OK ] NVFuserTest.VectorizationStrideValidation (78 ms)
[ RUN      ] NVFuserTest.Translate1Welford
[       OK ] NVFuserTest.Translate1Welford (347 ms)
[ RUN      ] NVFuserTest.WelfordOuterPersistence
[       OK ] NVFuserTest.WelfordOuterPersistence (551 ms)
[----------] 15 tests from NVFuserTest (2047 ms total)

[----------] 9 tests from AbstractTensorTest
[ RUN      ] AbstractTensorTest.MergeIterDomainsLeftBroadcasting
[       OK ] AbstractTensorTest.MergeIterDomainsLeftBroadcasting (0 ms)
[ RUN      ] AbstractTensorTest.MergeValGroups
[       OK ] AbstractTensorTest.MergeValGroups (0 ms)
[ RUN      ] AbstractTensorTest.SplitSingleIterDomain
[       OK ] AbstractTensorTest.SplitSingleIterDomain (0 ms)
[ RUN      ] AbstractTensorTest.Reorder
[       OK ] AbstractTensorTest.Reorder (0 ms)
[ RUN      ] AbstractTensorTest.SwizzleIterDomainsLeftBroadcasting
[       OK ] AbstractTensorTest.SwizzleIterDomainsLeftBroadcasting (0 ms)
[ RUN      ] AbstractTensorTest.SwizzleValGroups
[       OK ] AbstractTensorTest.SwizzleValGroups (0 ms)
[ RUN      ] AbstractTensorTest.Unzip
[       OK ] AbstractTensorTest.Unzip (0 ms)
[ RUN      ] AbstractTensorTest.Parallelize
[       OK ] AbstractTensorTest.Parallelize (0 ms)
[ RUN      ] AbstractTensorTest.MergeTaggedTensor
[       OK ] AbstractTensorTest.MergeTaggedTensor (0 ms)
[----------] 9 tests from AbstractTensorTest (0 ms total)

[----------] 17 tests from AliasTest
[ RUN      ] AliasTest.View_AliasForSameLayout
[       OK ] AliasTest.View_AliasForSameLayout (2 ms)
[ RUN      ] AliasTest.ViewPermute
[       OK ] AliasTest.ViewPermute (1 ms)
[ RUN      ] AliasTest.SliceRightOfBroadcast
[       OK ] AliasTest.SliceRightOfBroadcast (1 ms)
[ RUN      ] AliasTest.NotAllOutputsAlias_Pointwise
[       OK ] AliasTest.NotAllOutputsAlias_Pointwise (98 ms)
[ RUN      ] AliasTest.AliasOutputBeforeNonAliasOutput
[       OK ] AliasTest.AliasOutputBeforeNonAliasOutput (80 ms)
[ RUN      ] AliasTest.DuplicateInputs
[       OK ] AliasTest.DuplicateInputs (0 ms)
[ RUN      ] AliasTest.TrivialInputForwarding_ScalarTensor
[       OK ] AliasTest.TrivialInputForwarding_ScalarTensor (0 ms)
[ RUN      ] AliasTest.ManyAliasesBetweenOutputs
[       OK ] AliasTest.ManyAliasesBetweenOutputs (96 ms)
[ RUN      ] AliasTest.Broadcast
[       OK ] AliasTest.Broadcast (1 ms)
[ RUN      ] AliasTest.MergeBroadcastsBetweenConcretes
[       OK ] AliasTest.MergeBroadcastsBetweenConcretes (91 ms)
[ RUN      ] AliasTest.ReuseBuffer
[       OK ] AliasTest.ReuseBuffer (57 ms)
[ RUN      ] AliasTest.AliasOnlyKernelsAreNotLaunched
[       OK ] AliasTest.AliasOnlyKernelsAreNotLaunched (89 ms)
[ RUN      ] AliasTest.KernelExecutor
[       OK ] AliasTest.KernelExecutor (0 ms)
[ RUN      ] AliasTest.Bookend_InputsAndOutputs
[       OK ] AliasTest.Bookend_InputsAndOutputs (95 ms)
[ RUN      ] AliasTest.Bookend_ReuseSegmentSet
[       OK ] AliasTest.Bookend_ReuseSegmentSet (70 ms)
[ RUN      ] AliasTest.Issue2664
[       OK ] AliasTest.Issue2664 (134 ms)
[ RUN      ] AliasTest.FusionEmpty
[       OK ] AliasTest.FusionEmpty (1 ms)
[----------] 17 tests from AliasTest (823 ms total)

[----------] 7 tests from AliasAnalysisTest
[ RUN      ] AliasAnalysisTest.View_SymbolicTensor
[       OK ] AliasAnalysisTest.View_SymbolicTensor (0 ms)
[ RUN      ] AliasAnalysisTest.View_MergeNonContiguous
[       OK ] AliasAnalysisTest.View_MergeNonContiguous (0 ms)
[ RUN      ] AliasAnalysisTest.View_SplitExpandedBroadcast
[       OK ] AliasAnalysisTest.View_SplitExpandedBroadcast (0 ms)
[ RUN      ] AliasAnalysisTest.TrivialSlice
[       OK ] AliasAnalysisTest.TrivialSlice (0 ms)
[ RUN      ] AliasAnalysisTest.BroadcastExpandDimensions
[       OK ] AliasAnalysisTest.BroadcastExpandDimensions (0 ms)
[ RUN      ] AliasAnalysisTest.View_DidInFront
[       OK ] AliasAnalysisTest.View_DidInFront (2 ms)
[ RUN      ] AliasAnalysisTest.View_DidOnSplit
[       OK ] AliasAnalysisTest.View_DidOnSplit (0 ms)
[----------] 7 tests from AliasAnalysisTest (2 ms total)

[----------] 13 tests from AllocationDomainTest
[ RUN      ] AllocationDomainTest.NCHW4d_To_NHWC4d
[       OK ] AllocationDomainTest.NCHW4d_To_NHWC4d (84 ms)
[ RUN      ] AllocationDomainTest.Tensor3d_To_NHWC3d
[       OK ] AllocationDomainTest.Tensor3d_To_NHWC3d (86 ms)
[ RUN      ] AllocationDomainTest.NHWC1d_To_NHWC4d
[       OK ] AllocationDomainTest.NHWC1d_To_NHWC4d (87 ms)
[ RUN      ] AllocationDomainTest.NHWC2d_To_NHWC2d
[       OK ] AllocationDomainTest.NHWC2d_To_NHWC2d (84 ms)
[ RUN      ] AllocationDomainTest.NHWC4d_To_NHWC4d_cacheAfter
[       OK ] AllocationDomainTest.NHWC4d_To_NHWC4d_cacheAfter (89 ms)
[ RUN      ] AllocationDomainTest.NHWC2d_To_NHWC2d_cacheFork
[       OK ] AllocationDomainTest.NHWC2d_To_NHWC2d_cacheFork (90 ms)
[ RUN      ] AllocationDomainTest.ContiguityIssue1021
[       OK ] AllocationDomainTest.ContiguityIssue1021 (90 ms)
[ RUN      ] AllocationDomainTest.VectorizeOverlappingTensor
[       OK ] AllocationDomainTest.VectorizeOverlappingTensor (102 ms)
[ RUN      ] AllocationDomainTest.TrivialStrideOrderTensorViewBuilder
[       OK ] AllocationDomainTest.TrivialStrideOrderTensorViewBuilder (0 ms)
[ RUN      ] AllocationDomainTest.ReductionSchedulerIssue1895
[       OK ] AllocationDomainTest.ReductionSchedulerIssue1895 (779 ms)
[ RUN      ] AllocationDomainTest.InputAllocationIsSplit_Concrete
[       OK ] AllocationDomainTest.InputAllocationIsSplit_Concrete (1 ms)
[ RUN      ] AllocationDomainTest.InputAllocationIsSplit_Symbolic
[       OK ] AllocationDomainTest.InputAllocationIsSplit_Symbolic (1 ms)
[ RUN      ] AllocationDomainTest.SmemAllocationDomainChanged
[       OK ] AllocationDomainTest.SmemAllocationDomainChanged (94 ms)
[----------] 13 tests from AllocationDomainTest (1594 ms total)

[----------] 4 tests from AllocationOrderInferenceTest
[ RUN      ] AllocationOrderInferenceTest.BinaryOpPropagationOneTV
[       OK ] AllocationOrderInferenceTest.BinaryOpPropagationOneTV (0 ms)
[ RUN      ] AllocationOrderInferenceTest.TensorFactoryBinaryOpPropagation
[       OK ] AllocationOrderInferenceTest.TensorFactoryBinaryOpPropagation (0 ms)
[ RUN      ] AllocationOrderInferenceTest.ReductionOpPropagation
[       OK ] AllocationOrderInferenceTest.ReductionOpPropagation (0 ms)
[ RUN      ] AllocationOrderInferenceTest.SdpaBackward
[       OK ] AllocationOrderInferenceTest.SdpaBackward (0 ms)
[----------] 4 tests from AllocationOrderInferenceTest (0 ms total)

[----------] 3 tests from BFSTest
[ RUN      ] BFSTest.ValGraphBFS3
[       OK ] BFSTest.ValGraphBFS3 (0 ms)
[ RUN      ] BFSTest.IRBFSGetValsBetween
[       OK ] BFSTest.IRBFSGetValsBetween (0 ms)
[ RUN      ] BFSTest.IRBFSPermissiveTraversal
[       OK ] BFSTest.IRBFSPermissiveTraversal (0 ms)
[----------] 3 tests from BFSTest (0 ms total)

[----------] 1 test from FindAllExprsTest
[ RUN      ] FindAllExprsTest.Test2
[       OK ] FindAllExprsTest.Test2 (0 ms)
[----------] 1 test from FindAllExprsTest (0 ms total)

[----------] 5 tests from CaLogicalDomainMapTest
[ RUN      ] CaLogicalDomainMapTest.FusionRootMappingBasic_CUDA
[       OK ] CaLogicalDomainMapTest.FusionRootMappingBasic_CUDA (0 ms)
[ RUN      ] CaLogicalDomainMapTest.FusionRootMappingReductionDependency2_CUDA
[       OK ] CaLogicalDomainMapTest.FusionRootMappingReductionDependency2_CUDA (0 ms)
[ RUN      ] CaLogicalDomainMapTest.FusionRootMappingReductionDependency5_CUDA_CUDA
[       OK ] CaLogicalDomainMapTest.FusionRootMappingReductionDependency5_CUDA_CUDA (0 ms)
[ RUN      ] CaLogicalDomainMapTest.FusionRootMappingBroadcastNonUniqueSize_CUDA
[       OK ] CaLogicalDomainMapTest.FusionRootMappingBroadcastNonUniqueSize_CUDA (0 ms)
[ RUN      ] CaLogicalDomainMapTest.FusionRootMappingConsumerMappedWithReductionInput
[       OK ] CaLogicalDomainMapTest.FusionRootMappingConsumerMappedWithReductionInput (0 ms)
[----------] 5 tests from CaLogicalDomainMapTest (1 ms total)

[----------] 3 tests from PingPongCircularBuffering
[ RUN      ] PingPongCircularBuffering.StageSlicePositionComputeAt/stage_slice_position_1
[       OK ] PingPongCircularBuffering.StageSlicePositionComputeAt/stage_slice_position_1 (2 ms)
[ RUN      ] PingPongCircularBuffering.StageSlicePositionComputeAt/stage_slice_position_4
[       OK ] PingPongCircularBuffering.StageSlicePositionComputeAt/stage_slice_position_4 (387 ms)
[ RUN      ] PingPongCircularBuffering.NonCircularBufferedTv
[       OK ] PingPongCircularBuffering.NonCircularBufferedTv (218 ms)
[----------] 3 tests from PingPongCircularBuffering (609 ms total)

[----------] 19 tests from CombinedSchedulerTest
[ RUN      ] CombinedSchedulerTest.LayerNormBackward/dtype_double_batch_216_hidden_96
[       OK ] CombinedSchedulerTest.LayerNormBackward/dtype_double_batch_216_hidden_96 (610 ms)
[ RUN      ] CombinedSchedulerTest.LayerNormBackward/dtype_double_batch_216_hidden_1024
[       OK ] CombinedSchedulerTest.LayerNormBackward/dtype_double_batch_216_hidden_1024 (611 ms)
[ RUN      ] CombinedSchedulerTest.LayerNormBackward/dtype_double_batch_216_hidden_1984
[       OK ] CombinedSchedulerTest.LayerNormBackward/dtype_double_batch_216_hidden_1984 (846 ms)
[ RUN      ] CombinedSchedulerTest.LayerNormBackward/dtype_float_batch_216_hidden_3
[       OK ] CombinedSchedulerTest.LayerNormBackward/dtype_float_batch_216_hidden_3 (384 ms)
[ RUN      ] CombinedSchedulerTest.LayerNormBackward/dtype_float_batch_216_hidden_576
[       OK ] CombinedSchedulerTest.LayerNormBackward/dtype_float_batch_216_hidden_576 (601 ms)
[ RUN      ] CombinedSchedulerTest.LayerNormBackward/dtype_float_batch_216_hidden_1280
[       OK ] CombinedSchedulerTest.LayerNormBackward/dtype_float_batch_216_hidden_1280 (669 ms)
[ RUN      ] CombinedSchedulerTest.LayerNormBackward/dtype_float_batch_216_hidden_1987
[       OK ] CombinedSchedulerTest.LayerNormBackward/dtype_float_batch_216_hidden_1987 (475 ms)
[ RUN      ] CombinedSchedulerTest.LayerNormBackward/dtype___half_batch_216_hidden_32
[       OK ] CombinedSchedulerTest.LayerNormBackward/dtype___half_batch_216_hidden_32 (697 ms)
[ RUN      ] CombinedSchedulerTest.LayerNormBackward/dtype___half_batch_216_hidden_768
[       OK ] CombinedSchedulerTest.LayerNormBackward/dtype___half_batch_216_hidden_768 (712 ms)
[ RUN      ] CombinedSchedulerTest.LayerNormBackward/dtype___half_batch_216_hidden_1600
[       OK ] CombinedSchedulerTest.LayerNormBackward/dtype___half_batch_216_hidden_1600 (739 ms)
[ RUN      ] CombinedSchedulerTest.LayerNormBackward/dtype___half_batch_216_hidden_65536
[       OK ] CombinedSchedulerTest.LayerNormBackward/dtype___half_batch_216_hidden_65536 (702 ms)
[ RUN      ] CombinedSchedulerTest.LayerNormBackward/dtype___bfloat_batch_216_hidden_96
[       OK ] CombinedSchedulerTest.LayerNormBackward/dtype___bfloat_batch_216_hidden_96 (696 ms)
[ RUN      ] CombinedSchedulerTest.LayerNormBackward/dtype___bfloat_batch_216_hidden_1024
[       OK ] CombinedSchedulerTest.LayerNormBackward/dtype___bfloat_batch_216_hidden_1024 (714 ms)
[ RUN      ] CombinedSchedulerTest.LayerNormBackward/dtype___bfloat_batch_216_hidden_1984
[       OK ] CombinedSchedulerTest.LayerNormBackward/dtype___bfloat_batch_216_hidden_1984 (740 ms)
[ RUN      ] CombinedSchedulerTest.SharedConsumer
[       OK ] CombinedSchedulerTest.SharedConsumer (2178 ms)
[ RUN      ] CombinedSchedulerTest.CombinedReductionMultiPerBlock
[       OK ] CombinedSchedulerTest.CombinedReductionMultiPerBlock (308 ms)
[ RUN      ] CombinedSchedulerTest.SharedMemoryPersistentVectFactor
[       OK ] CombinedSchedulerTest.SharedMemoryPersistentVectFactor (463 ms)
[ RUN      ] CombinedSchedulerTest.AllocationDomainBroadcast
[       OK ] CombinedSchedulerTest.AllocationDomainBroadcast (525 ms)
[ RUN      ] CombinedSchedulerTest.IllegalSizeToUseTMA
[       OK ] CombinedSchedulerTest.IllegalSizeToUseTMA (211 ms)
[----------] 19 tests from CombinedSchedulerTest (12892 ms total)

[----------] 1 test from StaticWarpReductionTest
[ RUN      ] StaticWarpReductionTest.StaticWarpReductionValidation
[       OK ] StaticWarpReductionTest.StaticWarpReductionValidation (474 ms)
[----------] 1 test from StaticWarpReductionTest (474 ms total)

[----------] 2 tests from ComputeWithTest
[ RUN      ] ComputeWithTest.ComputeWith1
[       OK ] ComputeWithTest.ComputeWith1 (150 ms)
[ RUN      ] ComputeWithTest.ComputeWith4
[       OK ] ComputeWithTest.ComputeWith4 (108 ms)
[----------] 2 tests from ComputeWithTest (258 ms total)

[----------] 1 test from ContigIDGroupsTest
[ RUN      ] ContigIDGroupsTest.BackwardSplitInputBecomesContig
[       OK ] ContigIDGroupsTest.BackwardSplitInputBecomesContig (0 ms)
[----------] 1 test from ContigIDGroupsTest (0 ms total)

[----------] 1 test from DriverApiTest
[ RUN      ] DriverApiTest.WriteValue
[       OK ] DriverApiTest.WriteValue (0 ms)
[----------] 1 test from DriverApiTest (0 ms total)

[----------] 8 tests from DynamicTransformTest
[ RUN      ] DynamicTransformTest.DynamicTransform3
[       OK ] DynamicTransformTest.DynamicTransform3 (94 ms)
[ RUN      ] DynamicTransformTest.DynamicTransform6
[       OK ] DynamicTransformTest.DynamicTransform6 (1 ms)
[ RUN      ] DynamicTransformTest.DynamicTransform9
[       OK ] DynamicTransformTest.DynamicTransform9 (0 ms)
[ RUN      ] DynamicTransformTest.DynamicTransformFusionExecutorCache
[       OK ] DynamicTransformTest.DynamicTransformFusionExecutorCache (182 ms)
[ RUN      ] DynamicTransformTest.FusionDynamicSliceToBroadcast
[       OK ] DynamicTransformTest.FusionDynamicSliceToBroadcast (2 ms)
[ RUN      ] DynamicTransformTest.DynamicTransformIssue418
[       OK ] DynamicTransformTest.DynamicTransformIssue418 (512 ms)
[ RUN      ] DynamicTransformTest.SymbolicSqueeze
[       OK ] DynamicTransformTest.SymbolicSqueeze (81 ms)
[ RUN      ] DynamicTransformTest.DynamicSqueezeTrivialReduction
[       OK ] DynamicTransformTest.DynamicSqueezeTrivialReduction (160 ms)
[----------] 8 tests from DynamicTransformTest (1035 ms total)

[----------] 1 test from EmbeddingTest
[ RUN      ] EmbeddingTest.EmbeddingFwdNode
[       OK ] EmbeddingTest.EmbeddingFwdNode (26 ms)
[----------] 1 test from EmbeddingTest (26 ms total)

[----------] 10 tests from ExprEvalTest
[ RUN      ] ExprEvalTest.Double
[       OK ] ExprEvalTest.Double (0 ms)
[ RUN      ] ExprEvalTest.KnownValUpdate
[       OK ] ExprEvalTest.KnownValUpdate (3 ms)
[ RUN      ] ExprEvalTest.PostLower
[       OK ] ExprEvalTest.PostLower (9 ms)
[ RUN      ] ExprEvalTest.Struct
[       OK ] ExprEvalTest.Struct (0 ms)
[ RUN      ] ExprEvalTest.Validation
[       OK ] ExprEvalTest.Validation (0 ms)
[ RUN      ] ExprEvalTest.Permute
[       OK ] ExprEvalTest.Permute (0 ms)
[ RUN      ] ExprEvalTest.Reshape_SplitBroadcast
[       OK ] ExprEvalTest.Reshape_SplitBroadcast (0 ms)
[ RUN      ] ExprEvalTest.CatOp
[       OK ] ExprEvalTest.CatOp (0 ms)
[ RUN      ] ExprEvalTest.BinaryOpFmod
[       OK ] ExprEvalTest.BinaryOpFmod (0 ms)
[ RUN      ] ExprEvalTest.TernaryOpsWhere
[       OK ] ExprEvalTest.TernaryOpsWhere (11 ms)
[----------] 10 tests from ExprEvalTest (24 ms total)

[----------] 1 test from ExceptionTest
[ RUN      ] ExceptionTest.MultipleArgCalls
[       OK ] ExceptionTest.MultipleArgCalls (0 ms)
[----------] 1 test from ExceptionTest (0 ms total)

[----------] 7 tests from ExprSimplifierTest
[ RUN      ] ExprSimplifierTest.EliminateTrivialComputation
[       OK ] ExprSimplifierTest.EliminateTrivialComputation (1 ms)
[ RUN      ] ExprSimplifierTest.SignProve
[       OK ] ExprSimplifierTest.SignProve (13 ms)
[ RUN      ] ExprSimplifierTest.CancelDivMod
[       OK ] ExprSimplifierTest.CancelDivMod (1 ms)
[ RUN      ] ExprSimplifierTest.DistributeMul
[       OK ] ExprSimplifierTest.DistributeMul (0 ms)
[ RUN      ] ExprSimplifierTest.ReducePredicateRegisterUsage
[       OK ] ExprSimplifierTest.ReducePredicateRegisterUsage (5 ms)
[ RUN      ] ExprSimplifierTest.FactorizeGcd
[       OK ] ExprSimplifierTest.FactorizeGcd (0 ms)
[ RUN      ] ExprSimplifierTest.NonZeroLoopIndexStart
[       OK ] ExprSimplifierTest.NonZeroLoopIndexStart (0 ms)
[----------] 7 tests from ExprSimplifierTest (23 ms total)

[----------] 1 test from ExprSortTest
[ RUN      ] ExprSortTest.SegmentedGroup_Unary
[       OK ] ExprSortTest.SegmentedGroup_Unary (111 ms)
[----------] 1 test from ExprSortTest (111 ms total)

[----------] 8 tests from GatherTest
[ RUN      ] GatherTest.GatherAddMul
[       OK ] GatherTest.GatherAddMul (2078 ms)
[ RUN      ] GatherTest.GatherAddMulHugeSize
[       OK ] GatherTest.GatherAddMulHugeSize (593 ms)
[ RUN      ] GatherTest.TakeAlongBroadcastIndex
[       OK ] GatherTest.TakeAlongBroadcastIndex (223 ms)
[ RUN      ] GatherTest.TakeAlongAxisIntermediateTensorPointwise2
[       OK ] GatherTest.TakeAlongAxisIntermediateTensorPointwise2 (112 ms)
[ RUN      ] GatherTest.TakeAlongAxisIntermediateTensorReduction3
[       OK ] GatherTest.TakeAlongAxisIntermediateTensorReduction3 (110 ms)
[ DISABLED ] GatherTest.DISABLED_TakeAlongAxisIntermediateTensorReduction4
[ DISABLED ] GatherTest.DISABLED_TakeAlongAxisIntermediateTensorNormalization2
[ RUN      ] GatherTest.TakeAlongAxisIntermediateTensorNormalizationAndReduction1
[       OK ] GatherTest.TakeAlongAxisIntermediateTensorNormalizationAndReduction1 (184 ms)
[ RUN      ] GatherTest.TakeAlongAxisIntermediateTensorTranspose2
[       OK ] GatherTest.TakeAlongAxisIntermediateTensorTranspose2 (118 ms)
[ RUN      ] GatherTest.GatherIterGoupedReduction
[       OK ] GatherTest.GatherIterGoupedReduction (354 ms)
[----------] 8 tests from GatherTest (3777 ms total)

[----------] 44 tests from Gpu1Test
[ RUN      ] Gpu1Test.FusionDispatch_CUDA
[       OK ] Gpu1Test.FusionDispatch_CUDA (0 ms)
[ RUN      ] Gpu1Test.FusionMove_CUDA
[       OK ] Gpu1Test.FusionMove_CUDA (10 ms)
[ RUN      ] Gpu1Test.FusionComplexAbsTypes_CUDA
[       OK ] Gpu1Test.FusionComplexAbsTypes_CUDA (0 ms)
[ RUN      ] Gpu1Test.FusionFilterVals_CUDA
[       OK ] Gpu1Test.FusionFilterVals_CUDA (0 ms)
[ RUN      ] Gpu1Test.FusionTVReorder_CUDA
[       OK ] Gpu1Test.FusionTVReorder_CUDA (0 ms)
[ RUN      ] Gpu1Test.FusionOuterSplit_CUDA
[       OK ] Gpu1Test.FusionOuterSplit_CUDA (269 ms)
[ RUN      ] Gpu1Test.FusionSimplePWise_CUDA
[       OK ] Gpu1Test.FusionSimplePWise_CUDA (748 ms)
[ RUN      ] Gpu1Test.FusionAdvancedComputeAt1_CUDA
[       OK ] Gpu1Test.FusionAdvancedComputeAt1_CUDA (122 ms)
[ RUN      ] Gpu1Test.FusionAdvancedComputeAt4_CUDA
[       OK ] Gpu1Test.FusionAdvancedComputeAt4_CUDA (235 ms)
[ RUN      ] Gpu1Test.FusionAdvancedComputeAt7_CUDA
[       OK ] Gpu1Test.FusionAdvancedComputeAt7_CUDA (131 ms)
[ RUN      ] Gpu1Test.FusionComputeAtCommonConsumer1_CUDA
[       OK ] Gpu1Test.FusionComputeAtCommonConsumer1_CUDA (77 ms)
[ RUN      ] Gpu1Test.FusionComputeAtNoCommonConsumer_CUDA
[       OK ] Gpu1Test.FusionComputeAtNoCommonConsumer_CUDA (78 ms)
[ RUN      ] Gpu1Test.FusionLoopUnroll_CUDA
[       OK ] Gpu1Test.FusionLoopUnroll_CUDA (138 ms)
[ RUN      ] Gpu1Test.TernaryOps
[       OK ] Gpu1Test.TernaryOps (633 ms)
[ RUN      ] Gpu1Test.Fp4CopyKernelFusionExecutorCache
[       OK ] Gpu1Test.Fp4CopyKernelFusionExecutorCache (1 ms)
[ RUN      ] Gpu1Test.BitCeilKernel
[       OK ] Gpu1Test.BitCeilKernel (77 ms)
[ RUN      ] Gpu1Test.FusionReduction2_CUDA
[       OK ] Gpu1Test.FusionReduction2_CUDA (113 ms)
[ RUN      ] Gpu1Test.FusionReduction5_CUDA
[       OK ] Gpu1Test.FusionReduction5_CUDA (95 ms)
[ RUN      ] Gpu1Test.FusionMultiGridReduction2_CUDA
[       OK ] Gpu1Test.FusionMultiGridReduction2_CUDA (125 ms)
[ RUN      ] Gpu1Test.FusionBranches_CUDA
[       OK ] Gpu1Test.FusionBranches_CUDA (149 ms)
[ RUN      ] Gpu1Test.FusionSimpleBCast3_CUDA
[       OK ] Gpu1Test.FusionSimpleBCast3_CUDA (77 ms)
[ RUN      ] Gpu1Test.FusionComplexBCast1_CUDA
[       OK ] Gpu1Test.FusionComplexBCast1_CUDA (119 ms)
[ RUN      ] Gpu1Test.FusionSoftmax1D_CUDA
[       OK ] Gpu1Test.FusionSoftmax1D_CUDA (104 ms)
[ RUN      ] Gpu1Test.FusionSoftmax3DNormalized_CUDA
[       OK ] Gpu1Test.FusionSoftmax3DNormalized_CUDA (118 ms)
[ RUN      ] Gpu1Test.FusionGridReduction2_CUDA
[       OK ] Gpu1Test.FusionGridReduction2_CUDA (112 ms)
[ RUN      ] Gpu1Test.FusionGridReduction4_CUDA
[       OK ] Gpu1Test.FusionGridReduction4_CUDA (122 ms)
[ RUN      ] Gpu1Test.FusionGridReduction7_CUDA
[       OK ] Gpu1Test.FusionGridReduction7_CUDA (97 ms)
[ RUN      ] Gpu1Test.FusionGridReduction10_CUDA
[       OK ] Gpu1Test.FusionGridReduction10_CUDA (117 ms)
[ RUN      ] Gpu1Test.FusionBCastInnerDim_CUDA
[       OK ] Gpu1Test.FusionBCastInnerDim_CUDA (0 ms)
[ RUN      ] Gpu1Test.FusionComputeAtExprOrder1_CUDA
[       OK ] Gpu1Test.FusionComputeAtExprOrder1_CUDA (148 ms)
[ RUN      ] Gpu1Test.FusionZeroDimComputeAt_CUDA
[       OK ] Gpu1Test.FusionZeroDimComputeAt_CUDA (70 ms)
[ RUN      ] Gpu1Test.FusionBCastAfterReduce_CUDA
[       OK ] Gpu1Test.FusionBCastAfterReduce_CUDA (92 ms)
[ RUN      ] Gpu1Test.FusionReductionKeepDimScheduler_CUDA
[       OK ] Gpu1Test.FusionReductionKeepDimScheduler_CUDA (99 ms)
[ RUN      ] Gpu1Test.FusionReductionScheduler_CUDA
[       OK ] Gpu1Test.FusionReductionScheduler_CUDA (100 ms)
[ RUN      ] Gpu1Test.FusionReductionSchedulerMultiDimNonFastest_CUDA
[       OK ] Gpu1Test.FusionReductionSchedulerMultiDimNonFastest_CUDA (261 ms)
[ RUN      ] Gpu1Test.FusionCacheAfter_CUDA
[       OK ] Gpu1Test.FusionCacheAfter_CUDA (77 ms)
[ RUN      ] Gpu1Test.FusionCacheBcast_CUDA
[       OK ] Gpu1Test.FusionCacheBcast_CUDA (307 ms)
[ RUN      ] Gpu1Test.FusionSmemReduce_CUDA
[       OK ] Gpu1Test.FusionSmemReduce_CUDA (506 ms)
[ RUN      ] Gpu1Test.FusionSmemDynamicPersistentSoftmax2D_CUDA
[       OK ] Gpu1Test.FusionSmemDynamicPersistentSoftmax2D_CUDA (126 ms)
[ RUN      ] Gpu1Test.FusionMagicSchedulerLayerNormBackward_CUDA
[       OK ] Gpu1Test.FusionMagicSchedulerLayerNormBackward_CUDA (513 ms)
[ RUN      ] Gpu1Test.FusionMagicSchedulerRMSNormalization_CUDA
[       OK ] Gpu1Test.FusionMagicSchedulerRMSNormalization_CUDA (172 ms)
[ RUN      ] Gpu1Test.FusionMagicSchedulerInstanceNormalizationBackward_CUDA
[       OK ] Gpu1Test.FusionMagicSchedulerInstanceNormalizationBackward_CUDA (1301 ms)
[ RUN      ] Gpu1Test.FusionSmemDynamicPersistentNorm_CUDA
[       OK ] Gpu1Test.FusionSmemDynamicPersistentNorm_CUDA (140 ms)
[ RUN      ] Gpu1Test.FusionSmemDynamicPwiseMulSymbolicArgWAR_CUDA
[       OK ] Gpu1Test.FusionSmemDynamicPwiseMulSymbolicArgWAR_CUDA (1618 ms)
[----------] 44 tests from Gpu1Test (9417 ms total)

[----------] 51 tests from Gpu2Test
[ RUN      ] Gpu2Test.FusionGlobalIntermediate_CUDA
[       OK ] Gpu2Test.FusionGlobalIntermediate_CUDA (91 ms)
[ RUN      ] Gpu2Test.FusionUnrollWithAlloc_CUDA
[       OK ] Gpu2Test.FusionUnrollWithAlloc_CUDA (100 ms)
[ RUN      ] Gpu2Test.FusionComputeAtNonterminatingOutput_CUDA
[       OK ] Gpu2Test.FusionComputeAtNonterminatingOutput_CUDA (73 ms)
[ RUN      ] Gpu2Test.FusionTraversalOrder3_CUDA
[       OK ] Gpu2Test.FusionTraversalOrder3_CUDA (326 ms)
[ RUN      ] Gpu2Test.FusionTraversalOrder6_CUDA
[       OK ] Gpu2Test.FusionTraversalOrder6_CUDA (149 ms)
[ RUN      ] Gpu2Test.FusionLSTMCell_CUDA
[       OK ] Gpu2Test.FusionLSTMCell_CUDA (520 ms)
[ RUN      ] Gpu2Test.FusionReduceImplicitBroadcast_CUDA
[       OK ] Gpu2Test.FusionReduceImplicitBroadcast_CUDA (109 ms)
[ RUN      ] Gpu2Test.FusionTrivialReduction_CUDA
[       OK ] Gpu2Test.FusionTrivialReduction_CUDA (260 ms)
[ RUN      ] Gpu2Test.FusionInputsIdLookup_CUDA
[       OK ] Gpu2Test.FusionInputsIdLookup_CUDA (0 ms)
[ RUN      ] Gpu2Test.FusionBiasGeluFwd_CUDA
[       OK ] Gpu2Test.FusionBiasGeluFwd_CUDA (171 ms)
[ RUN      ] Gpu2Test.FusionSmemIndexingSimple_CUDA
[       OK ] Gpu2Test.FusionSmemIndexingSimple_CUDA (76 ms)
[ RUN      ] Gpu2Test.FusionCacheBeforeReduction2_CUDA
[       OK ] Gpu2Test.FusionCacheBeforeReduction2_CUDA (81 ms)
[ RUN      ] Gpu2Test.FusionIssue363_CUDA
[       OK ] Gpu2Test.FusionIssue363_CUDA (109 ms)
[ RUN      ] Gpu2Test.FusionIssue382_CUDA
[       OK ] Gpu2Test.FusionIssue382_CUDA (85 ms)
[ RUN      ] Gpu2Test.FusionLoopUnswitch_CUDA
[       OK ] Gpu2Test.FusionLoopUnswitch_CUDA (83 ms)
[ RUN      ] Gpu2Test.FusionVarMean_CUDA
[       OK ] Gpu2Test.FusionVarMean_CUDA (2833 ms)
[ RUN      ] Gpu2Test.FusionSoftmax3DTransposed_CUDA
[       OK ] Gpu2Test.FusionSoftmax3DTransposed_CUDA (106 ms)
[ RUN      ] Gpu2Test.FusionAdvancedComputeAtTransposed3_CUDA
[       OK ] Gpu2Test.FusionAdvancedComputeAtTransposed3_CUDA (189 ms)
[ RUN      ] Gpu2Test.FusionAdvancedComputeAtTransposed6_CUDA
[       OK ] Gpu2Test.FusionAdvancedComputeAtTransposed6_CUDA (118 ms)
[ RUN      ] Gpu2Test.FusionGridPersistence_CUDA
[       OK ] Gpu2Test.FusionGridPersistence_CUDA (118 ms)
[ RUN      ] Gpu2Test.FusionBroadcastAcrossComputeAt_CUDA
[       OK ] Gpu2Test.FusionBroadcastAcrossComputeAt_CUDA (83 ms)
[ RUN      ] Gpu2Test.FusionValidateParallelize1_CUDA
[       OK ] Gpu2Test.FusionValidateParallelize1_CUDA (0 ms)
[ RUN      ] Gpu2Test.FusionValidateParallelize4_CUDA
[       OK ] Gpu2Test.FusionValidateParallelize4_CUDA (82 ms)
[ RUN      ] Gpu2Test.FusionValidateParallelize7_CUDA
[       OK ] Gpu2Test.FusionValidateParallelize7_CUDA (4 ms)
[ RUN      ] Gpu2Test.FusionValidateParallelize10_CUDA
[       OK ] Gpu2Test.FusionValidateParallelize10_CUDA (103 ms)
[ RUN      ] Gpu2Test.FusionDAGScalarMerging_CUDA
[       OK ] Gpu2Test.FusionDAGScalarMerging_CUDA (190 ms)
[ RUN      ] Gpu2Test.FusionIssue728_CUDA
[       OK ] Gpu2Test.FusionIssue728_CUDA (0 ms)
[ RUN      ] Gpu2Test.FusionSegmentVerticalMerge_CUDA
[       OK ] Gpu2Test.FusionSegmentVerticalMerge_CUDA (50 ms)
[ RUN      ] Gpu2Test.FusionSBAR_CUDA
[       OK ] Gpu2Test.FusionSBAR_CUDA (168 ms)
[ RUN      ] Gpu2Test.FusionBNBackwardRepro2_CUDA
[       OK ] Gpu2Test.FusionBNBackwardRepro2_CUDA (287 ms)
[ RUN      ] Gpu2Test.FusionZeroSizeTensorPW_CUDA
[       OK ] Gpu2Test.FusionZeroSizeTensorPW_CUDA (0 ms)
[ RUN      ] Gpu2Test.FusionSegmentIslands_CUDA
[       OK ] Gpu2Test.FusionSegmentIslands_CUDA (159 ms)
[ RUN      ] Gpu2Test.FusionBackOffInnerBroadcast3_CUDA
[       OK ] Gpu2Test.FusionBackOffInnerBroadcast3_CUDA (0 ms)
[ RUN      ] Gpu2Test.FusionWarpPadMergeSplit_CUDA
[       OK ] Gpu2Test.FusionWarpPadMergeSplit_CUDA (123 ms)
[ RUN      ] Gpu2Test.FusionMultipleDimBinding_CUDA
[       OK ] Gpu2Test.FusionMultipleDimBinding_CUDA (116 ms)
[ RUN      ] Gpu2Test.FusionWarpReduceUnrollOuterLoop_CUDA
[       OK ] Gpu2Test.FusionWarpReduceUnrollOuterLoop_CUDA (195 ms)
[ RUN      ] Gpu2Test.FusionBufferReuseBroadCastMultiVisit_CUDA
[       OK ] Gpu2Test.FusionBufferReuseBroadCastMultiVisit_CUDA (87 ms)
[ RUN      ] Gpu2Test.FusionBufferReuseNo2hop_CUDA
[       OK ] Gpu2Test.FusionBufferReuseNo2hop_CUDA (88 ms)
[ RUN      ] Gpu2Test.FusionBufferReuseNoAcrossBroadcast_CUDA
[       OK ] Gpu2Test.FusionBufferReuseNoAcrossBroadcast_CUDA (90 ms)
[ RUN      ] Gpu2Test.FusionIssue1021_CUDA
[       OK ] Gpu2Test.FusionIssue1021_CUDA (84 ms)
[ RUN      ] Gpu2Test.FusionParallelDimensionMap2_CUDA
[       OK ] Gpu2Test.FusionParallelDimensionMap2_CUDA (96 ms)
[ RUN      ] Gpu2Test.FusionParallelDimensionMap5_CUDA
[       OK ] Gpu2Test.FusionParallelDimensionMap5_CUDA (85 ms)
[ RUN      ] Gpu2Test.FusionWARSyncAliasedSmem_CUDA
[       OK ] Gpu2Test.FusionWARSyncAliasedSmem_CUDA (93 ms)
[ RUN      ] Gpu2Test.FusionIssue1189_CUDA
[       OK ] Gpu2Test.FusionIssue1189_CUDA (102 ms)
[ RUN      ] Gpu2Test.FusionPointwiseVectorize_CUDA
[       OK ] Gpu2Test.FusionPointwiseVectorize_CUDA (3 ms)
[ RUN      ] Gpu2Test.FusionGridWelfordWithNonExactParallelDimensions_CUDA
[       OK ] Gpu2Test.FusionGridWelfordWithNonExactParallelDimensions_CUDA (125 ms)
[ RUN      ] Gpu2Test.FusionPredicateParallelizedDomains_CUDA
[       OK ] Gpu2Test.FusionPredicateParallelizedDomains_CUDA (131 ms)
[ RUN      ] Gpu2Test.FusionIssue1127_CUDA
[       OK ] Gpu2Test.FusionIssue1127_CUDA (1 ms)
[ RUN      ] Gpu2Test.FusionTestWarpSoftMax_CUDA
[       OK ] Gpu2Test.FusionTestWarpSoftMax_CUDA (142 ms)
[ RUN      ] Gpu2Test.FusionIssue1223_CUDA
[       OK ] Gpu2Test.FusionIssue1223_CUDA (115 ms)
[ RUN      ] Gpu2Test.FusionRfactorIndirectRoot_CUDA
[       OK ] Gpu2Test.FusionRfactorIndirectRoot_CUDA (98 ms)
[----------] 51 tests from Gpu2Test (8522 ms total)

[----------] 64 tests from Gpu3Test
[ RUN      ] Gpu3Test.FusionNonDivisibleSplit2_CUDA
[       OK ] Gpu3Test.FusionNonDivisibleSplit2_CUDA (118 ms)
[ RUN      ] Gpu3Test.FusionNonDivisibleSplit5_CUDA
[       OK ] Gpu3Test.FusionNonDivisibleSplit5_CUDA (104 ms)
[ RUN      ] Gpu3Test.FusionIssue1305Repro_CUDA
[       OK ] Gpu3Test.FusionIssue1305Repro_CUDA (0 ms)
[ RUN      ] Gpu3Test.FusionBroadcastConcretization2_CUDA
[       OK ] Gpu3Test.FusionBroadcastConcretization2_CUDA (90 ms)
[ RUN      ] Gpu3Test.FusionIssue1430_CUDA
[       OK ] Gpu3Test.FusionIssue1430_CUDA (283 ms)
[ RUN      ] Gpu3Test.FusionTestGridComm2_CUDA
[       OK ] Gpu3Test.FusionTestGridComm2_CUDA (93 ms)
[ RUN      ] Gpu3Test.FusionSmemAlignment_CUDA
[       OK ] Gpu3Test.FusionSmemAlignment_CUDA (2711 ms)
[ RUN      ] Gpu3Test.FusionVectorizeContigIndexFail_CUDA
/opt/pytorch/nvfuser/tests/cpp/test_gpu3.cpp:1176: Skipped


[  SKIPPED ] Gpu3Test.FusionVectorizeContigIndexFail_CUDA (0 ms)
[ RUN      ] Gpu3Test.FusionVectorizeContigIndexValidationFail_CUDA
/opt/pytorch/nvfuser/tests/cpp/test_gpu3.cpp:1285: Skipped


[  SKIPPED ] Gpu3Test.FusionVectorizeContigIndexValidationFail_CUDA (0 ms)
[ RUN      ] Gpu3Test.FusionVectorizeContigIndexWithBroadcast_CUDA
[       OK ] Gpu3Test.FusionVectorizeContigIndexWithBroadcast_CUDA (86 ms)
[ RUN      ] Gpu3Test.FusionRAWSyncInsertionPlace1_CUDA
[       OK ] Gpu3Test.FusionRAWSyncInsertionPlace1_CUDA (95 ms)
[ RUN      ] Gpu3Test.FusionRAWSyncInsertionPlace4_CUDA
[       OK ] Gpu3Test.FusionRAWSyncInsertionPlace4_CUDA (10 ms)
[ RUN      ] Gpu3Test.FusionSimpleCpAsync_CUDA
[       OK ] Gpu3Test.FusionSimpleCpAsync_CUDA (171 ms)
[ RUN      ] Gpu3Test.FusionPropagateParallelTypesToSiblings_CUDA
[       OK ] Gpu3Test.FusionPropagateParallelTypesToSiblings_CUDA (137 ms)
[ RUN      ] Gpu3Test.FusionTestReEntrantGridWelford_CUDA
[       OK ] Gpu3Test.FusionTestReEntrantGridWelford_CUDA (477 ms)
[ RUN      ] Gpu3Test.FusionRedundantPredSync3_CUDA
[       OK ] Gpu3Test.FusionRedundantPredSync3_CUDA (1609 ms)
[ RUN      ] Gpu3Test.FusionSqueeze1_CUDA
[       OK ] Gpu3Test.FusionSqueeze1_CUDA (158 ms)
[ RUN      ] Gpu3Test.FusionRepro1713_CUDA
[       OK ] Gpu3Test.FusionRepro1713_CUDA (283 ms)
[ RUN      ] Gpu3Test.FusionExpandToConcrete_CUDA
[       OK ] Gpu3Test.FusionExpandToConcrete_CUDA (4 ms)
[ RUN      ] Gpu3Test.FusionTransformPropagateSelectorSibling_CUDA
[       OK ] Gpu3Test.FusionTransformPropagateSelectorSibling_CUDA (1 ms)
[ RUN      ] Gpu3Test.FusionIssue1770Repro_CUDA
[       OK ] Gpu3Test.FusionIssue1770Repro_CUDA (84 ms)
[ RUN      ] Gpu3Test.FusionMaxLogicalDomainInfoSpanningTreePrintTwice_CUDA
[       OK ] Gpu3Test.FusionMaxLogicalDomainInfoSpanningTreePrintTwice_CUDA (0 ms)
[ RUN      ] Gpu3Test.FusionSkipReplay_CUDA
[       OK ] Gpu3Test.FusionSkipReplay_CUDA (0 ms)
[ RUN      ] Gpu3Test.FusionIssueRepro1844_CUDA
[       OK ] Gpu3Test.FusionIssueRepro1844_CUDA (209 ms)
[ RUN      ] Gpu3Test.FusionExpandReduce_CUDA
[       OK ] Gpu3Test.FusionExpandReduce_CUDA (84 ms)
[ RUN      ] Gpu3Test.FusionPrint_CUDA
T3[0] = 0.000000 @ threadIdx=(0,0,0), blockIdx=(0,0,0)
T3[0] = 1.000000 @ threadIdx=(1,0,0), blockIdx=(0,0,0)
T3[0] = 0.000000 @ threadIdx=(0,0,0), blockIdx=(0,0,0)
T3[0] = 1.000000 @ threadIdx=(1,0,0), blockIdx=(0,0,0)
T4[0] = 0.000000 @ threadIdx=(0,0,0), blockIdx=(0,0,0)
T4[0] = 1.000000 @ threadIdx=(1,0,0), blockIdx=(0,0,0)
T4[0] = 0 @ threadIdx=(0,0,0), blockIdx=(0,0,0)
T4[0] = 1 @ threadIdx=(1,0,0), blockIdx=(0,0,0)
T4[0] = 0 @ threadIdx=(0,0,0), blockIdx=(0,0,0)
T4[0] = 1 @ threadIdx=(1,0,0), blockIdx=(0,0,0)
T4[0] = false @ threadIdx=(0,0,0), blockIdx=(0,0,0)
T4[0] = true @ threadIdx=(1,0,0), blockIdx=(0,0,0)
T4[0] = 0.000000 @ threadIdx=(0,0,0), blockIdx=(0,0,0)
T4[0] = 1.000000 @ threadIdx=(1,0,0), blockIdx=(0,0,0)
[       OK ] Gpu3Test.FusionPrint_CUDA (698 ms)
[ RUN      ] Gpu3Test.FusionDependencyCheck_CUDA
[       OK ] Gpu3Test.FusionDependencyCheck_CUDA (0 ms)
[ RUN      ] Gpu3Test.AsyncCompilation_CUDA
[       OK ] Gpu3Test.AsyncCompilation_CUDA (193 ms)
[ RUN      ] Gpu3Test.FusionReplayTrivialReductionAndBroadcast2_CUDA
[       OK ] Gpu3Test.FusionReplayTrivialReductionAndBroadcast2_CUDA (94 ms)
[ RUN      ] Gpu3Test.FusionVectorizeRepro1843_CUDA
[       OK ] Gpu3Test.FusionVectorizeRepro1843_CUDA (194 ms)
[ RUN      ] Gpu3Test.FusionIssue2068_CUDA
[       OK ] Gpu3Test.FusionIssue2068_CUDA (458 ms)
[ RUN      ] Gpu3Test.FusionSqueezeInlining_CUDA
[       OK ] Gpu3Test.FusionSqueezeInlining_CUDA (77 ms)
[ RUN      ] Gpu3Test.FusionIssue2074_CUDA
[       OK ] Gpu3Test.FusionIssue2074_CUDA (140 ms)
[ RUN      ] Gpu3Test.FusionIssue2075_CUDA
[       OK ] Gpu3Test.FusionIssue2075_CUDA (136 ms)
[ RUN      ] Gpu3Test.FusionIssue2163ReproInvalidAlias_CUDA
[       OK ] Gpu3Test.FusionIssue2163ReproInvalidAlias_CUDA (560 ms)
[ RUN      ] Gpu3Test.FusionVectorizeWelford1_CUDA
[       OK ] Gpu3Test.FusionVectorizeWelford1_CUDA (367 ms)
[ RUN      ] Gpu3Test.FusionExprSortMatmulLikeSchedule_CUDA
[       OK ] Gpu3Test.FusionExprSortMatmulLikeSchedule_CUDA (1288 ms)
[ RUN      ] Gpu3Test.FusionClearThreadPredicateByRAWSync_CUDA
[       OK ] Gpu3Test.FusionClearThreadPredicateByRAWSync_CUDA (122 ms)
[ RUN      ] Gpu3Test.FusionTypePromotionATenConsistency_CUDA
[       OK ] Gpu3Test.FusionTypePromotionATenConsistency_CUDA (0 ms)
[ RUN      ] Gpu3Test.FusionExecutorCacheIndexType2_CUDA
[       OK ] Gpu3Test.FusionExecutorCacheIndexType2_CUDA (389 ms)
[ RUN      ] Gpu3Test.FusionManagedData_CUDA
[       OK ] Gpu3Test.FusionManagedData_CUDA (5 ms)
[ RUN      ] Gpu3Test.FusionAvoidRedundantWriteDifferentConcretizedDomains_CUDA
[       OK ] Gpu3Test.FusionAvoidRedundantWriteDifferentConcretizedDomains_CUDA (328 ms)
[ RUN      ] Gpu3Test.FusionDomainEquivalence_CUDA
[       OK ] Gpu3Test.FusionDomainEquivalence_CUDA (0 ms)
[ RUN      ] Gpu3Test.CompareDomainWithReference2
[       OK ] Gpu3Test.CompareDomainWithReference2 (0 ms)
[ RUN      ] Gpu3Test.AllIDsWithExtraLoopIDs1
[       OK ] Gpu3Test.AllIDsWithExtraLoopIDs1 (0 ms)
[ RUN      ] Gpu3Test.FusionMinMaxNanPropagation_CUDA
[       OK ] Gpu3Test.FusionMinMaxNanPropagation_CUDA (1046 ms)
[ RUN      ] Gpu3Test.IntegerDivision_CUDA
[       OK ] Gpu3Test.IntegerDivision_CUDA (142 ms)
[ RUN      ] Gpu3Test.FusionOptionsGuard_CUDA
[       OK ] Gpu3Test.FusionOptionsGuard_CUDA (284 ms)
[ RUN      ] Gpu3Test.FusionDanglingUnaryOp_CUDA
[       OK ] Gpu3Test.FusionDanglingUnaryOp_CUDA (75 ms)
[ RUN      ] Gpu3Test.AllInputDtypes
[       OK ] Gpu3Test.AllInputDtypes (176 ms)
[ RUN      ] Gpu3Test.OpaqueTupleAsComplex
[       OK ] Gpu3Test.OpaqueTupleAsComplex (75 ms)
[ RUN      ] Gpu3Test.PredicateRNGOps
[       OK ] Gpu3Test.PredicateRNGOps (1745 ms)
[ RUN      ] Gpu3Test.Reduction3DConstantIterationDomain
[       OK ] Gpu3Test.Reduction3DConstantIterationDomain (120 ms)
[ RUN      ] Gpu3Test.BlockReduction3D
[       OK ] Gpu3Test.BlockReduction3D (5102 ms)
[ RUN      ] Gpu3Test.DecoupledDomains1
[       OK ] Gpu3Test.DecoupledDomains1 (0 ms)
[ RUN      ] Gpu3Test.BroadcastFromNowhereFusion
[       OK ] Gpu3Test.BroadcastFromNowhereFusion (591 ms)
[ RUN      ] Gpu3Test.MoveNonConcretizedBroadcastInNormalization
[       OK ] Gpu3Test.MoveNonConcretizedBroadcastInNormalization (114 ms)
[ RUN      ] Gpu3Test.Issue2685Repro
[       OK ] Gpu3Test.Issue2685Repro (355 ms)
[ RUN      ] Gpu3Test.RAWSync
[       OK ] Gpu3Test.RAWSync (1 ms)
[ RUN      ] Gpu3Test.AvoidReplacingWithDependentVal
[       OK ] Gpu3Test.AvoidReplacingWithDependentVal (0 ms)
[ RUN      ] Gpu3Test.AllIdsMultipleDependencies
[       OK ] Gpu3Test.AllIdsMultipleDependencies (0 ms)
[ RUN      ] Gpu3Test.RepeatBroadcastAndNonBroadcast
[       OK ] Gpu3Test.RepeatBroadcastAndNonBroadcast (117 ms)
[ RUN      ] Gpu3Test.DeviceSharedMemoryLimit
[       OK ] Gpu3Test.DeviceSharedMemoryLimit (0 ms)
[ RUN      ] Gpu3Test.InliningPosWithVectorizedCastOps
[       OK ] Gpu3Test.InliningPosWithVectorizedCastOps (109 ms)
[----------] 64 tests from Gpu3Test (21935 ms total)

[----------] 14 tests from IdModelTest
[ RUN      ] IdModelTest.DetectSelfMapping
Inputs:
  T0_g_float[iS0{2}, iS1{2}]
Outputs:
  T2_g_float[iS4{2}, iS5{2}]

%kernel {
T1_l_float[iS3{2}, iS2{2}]
   = Set.Permute( T0_g_float[iS0{2}, iS1{2}], cache_op=Streaming )
T2_g_float[iS4{2}, iS5{2}]
   = T0_g_float[iS0{2}, iS1{2}]
   + T1_l_float[iS3{2}, iS2{2}];

TransformPrinter :
T0_g_float[iS0{2}, iS1{2}]
 logical domain : (iS0{2}, iS1{2})
 contiguity: f f
 loop domain : (iS0{2}, iS1{2})
T1_l_float[iS3{2}, iS2{2}]
 root domain : (iS2{2}, iS3{2})
 logical domain : (iS3{2}, iS2{2})
 contiguity: t t
 loop domain : (iS3{2}, iS2{2})
T2_g_float[iS4{2}, iS5{2}]
 logical domain : (iS4{2}, iS5{2})
 contiguity: t t
 loop domain : (iS4{2}, iS5{2})
} // %kernel
[       OK ] IdModelTest.DetectSelfMapping (0 ms)
[ RUN      ] IdModelTest.ValGraphStmtSort2
[       OK ] IdModelTest.ValGraphStmtSort2 (0 ms)
[ RUN      ] IdModelTest.LoopPromotion1
[       OK ] IdModelTest.LoopPromotion1 (0 ms)
[ RUN      ] IdModelTest.LoopPromotion4
[       OK ] IdModelTest.LoopPromotion4 (1 ms)
[ RUN      ] IdModelTest.LoopPromotion7
[       OK ] IdModelTest.LoopPromotion7 (1 ms)
[ RUN      ] IdModelTest.LoopPromotionTwoStepFailureReproSimple
[       OK ] IdModelTest.LoopPromotionTwoStepFailureReproSimple (3 ms)
[ RUN      ] IdModelTest.SomeButNotAllArePermuted
[       OK ] IdModelTest.SomeButNotAllArePermuted (0 ms)
[ RUN      ] IdModelTest.LoopPromotionWithViewRFactor1
[       OK ] IdModelTest.LoopPromotionWithViewRFactor1 (0 ms)
[ RUN      ] IdModelTest.ParallelTypePropagation
[       OK ] IdModelTest.ParallelTypePropagation (0 ms)
[ RUN      ] IdModelTest.MappingClonedIDs
[       OK ] IdModelTest.MappingClonedIDs (0 ms)
[ RUN      ] IdModelTest.LoopPromotionWithCyclicGraphInlinedBroadcast
[       OK ] IdModelTest.LoopPromotionWithCyclicGraphInlinedBroadcast (3 ms)
[ RUN      ] IdModelTest.CoveredGroups
[       OK ] IdModelTest.CoveredGroups (0 ms)
[ RUN      ] IdModelTest.ScatterLoopMapping
[       OK ] IdModelTest.ScatterLoopMapping (0 ms)
[ RUN      ] IdModelTest.ReproIssue5803
[       OK ] IdModelTest.ReproIssue5803 (108 ms)
[----------] 14 tests from IdModelTest (122 ms total)

[----------] 3 tests from IndexPut
[ RUN      ] IndexPut.AccumulateOpWithBroadcastIDs/2
[       OK ] IndexPut.AccumulateOpWithBroadcastIDs/2 (76 ms)
[ RUN      ] IndexPut.AccumulateOpWithBroadcastIDs/5
[       OK ] IndexPut.AccumulateOpWithBroadcastIDs/5 (74 ms)
[ RUN      ] IndexPut.3D
[       OK ] IndexPut.3D (77 ms)
[----------] 3 tests from IndexPut (228 ms total)

[----------] 5 tests from IndexSelectTest
[ RUN      ] IndexSelectTest.Simple1
[       OK ] IndexSelectTest.Simple1 (480 ms)
[ RUN      ] IndexSelectTest.3DTensor
[       OK ] IndexSelectTest.3DTensor (104 ms)
[ RUN      ] IndexSelectTest.IdxTvFuseable
[       OK ] IndexSelectTest.IdxTvFuseable (108 ms)
[ RUN      ] IndexSelectTest.Dim1InRank3
[       OK ] IndexSelectTest.Dim1InRank3 (112 ms)
[ RUN      ] IndexSelectTest.MultipleIndexSelectIssue
[       OK ] IndexSelectTest.MultipleIndexSelectIssue (103 ms)
[----------] 5 tests from IndexSelectTest (908 ms total)

[----------] 13 tests from IndexingTest
[ RUN      ] IndexingTest.SimpleReduction
[       OK ] IndexingTest.SimpleReduction (2 ms)
[ RUN      ] IndexingTest.Reshape
[       OK ] IndexingTest.Reshape (7 ms)
[ RUN      ] IndexingTest.SimpleBroadcast3
[       OK ] IndexingTest.SimpleBroadcast3 (2 ms)
[ RUN      ] IndexingTest.MultiDevice1DSplit
[       OK ] IndexingTest.MultiDevice1DSplit (1 ms)
[ RUN      ] IndexingTest.MultiDevice2DTranspose
[       OK ] IndexingTest.MultiDevice2DTranspose (1 ms)
[ RUN      ] IndexingTest.NonInnermostVectorize
[       OK ] IndexingTest.NonInnermostVectorize (5 ms)
[ RUN      ] IndexingTest.SimpleUnroll
[       OK ] IndexingTest.SimpleUnroll (5 ms)
[ RUN      ] IndexingTest.ResizePath
[       OK ] IndexingTest.ResizePath (3 ms)
[ RUN      ] IndexingTest.DoubleBuffering6
[       OK ] IndexingTest.DoubleBuffering6 (12 ms)
[ RUN      ] IndexingTest.PerDimLogicalIndices
[       OK ] IndexingTest.PerDimLogicalIndices (2 ms)
[ RUN      ] IndexingTest.ResizeRotation
[       OK ] IndexingTest.ResizeRotation (108 ms)
[ RUN      ] IndexingTest.AlmostExactIndexingUpdate
[       OK ] IndexingTest.AlmostExactIndexingUpdate (82 ms)
[ RUN      ] IndexingTest.StaticIndexing
[       OK ] IndexingTest.StaticIndexing (3 ms)
[----------] 13 tests from IndexingTest (241 ms total)

[----------] 10 tests from PredicateIndexingTest
[ RUN      ] PredicateIndexingTest.SimpleUnroll
[       OK ] PredicateIndexingTest.SimpleUnroll (6 ms)
[ RUN      ] PredicateIndexingTest.NonInnermostVectorize
[       OK ] PredicateIndexingTest.NonInnermostVectorize (6 ms)
[ RUN      ] PredicateIndexingTest.UnrolledCircularBuffering
[       OK ] PredicateIndexingTest.UnrolledCircularBuffering (446 ms)
[ RUN      ] PredicateIndexingTest.UnswitchedCircularBuffering3/false
[       OK ] PredicateIndexingTest.UnswitchedCircularBuffering3/false (133 ms)
[ RUN      ] PredicateIndexingTest.UnswitchedCircularBuffering4
[W122 09:10:24.623438892 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i86
Loop:
FOR i86 in iS6{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 8) )}:
  FOR i78 in iS8{8}:
    IF Manual true:
 (function isUnrolled)
[W122 09:10:24.623888939 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i86
Loop:
FOR i86 in iS6{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 8) )}:
  FOR i78 in iS8{8}:
    IF Manual true:
      T1_l[( ( 8 * ( ( 1 + i86 ) % 2 ) ) + i78 )] view( T1 )
         = Set( T0_g[( ( 8 + ( 8 * i86 ) ) + ( i78 + nvfuser_zero ) )] view( T0 ), cache_op=Streaming )
  FOR i77 in iS4{8}:
    IF Manual true:
 (function isUnrolled)
[W122 09:10:24.624251068 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i86
Loop:
FOR i86 in iS6{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 8) )}:
  FOR i78 in iS8{8}:
    IF Manual true:
      T1_l[( ( 8 * ( ( 1 + i86 ) % 2 ) ) + i78 )] view( T1 )
         = Set( T0_g[( ( 8 + ( 8 * i86 ) ) + ( i78 + nvfuser_zero ) )] view( T0 ), cache_op=Streaming )
  FOR i77 in iS4{8}:
    IF Manual true:
 (function isUnrolled)
[W122 09:10:24.625277352 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i86
Loop:
FOR i86 in iS6{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 8) )}:
  FOR i78 in iS8{8}:
    IF Inline:
 (function isUnrolled)
[W122 09:10:24.625696930 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i86
Loop:
FOR i86 in iS6{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 8) )}:
  FOR i78 in iS8{8}:
    IF Inline:
      T1_l[( ( 8 * ( ( 1 + i86 ) % 2 ) ) + i78 )] view( T1 )
         = Set( T0_g[( ( 8 + ( 8 * i86 ) ) + ( i78 + nvfuser_zero ) )] view( T0 ), cache_op=Streaming )
  FOR i77 in iS4{8}:
    IF Inline:
 (function isUnrolled)
[W122 09:10:24.626091828 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i86
Loop:
FOR i86 in iS6{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 8) )}:
  FOR i78 in iS8{8}:
    IF Inline:
      T1_l[( ( 8 * ( ( 1 + i86 ) % 2 ) ) + i78 )] view( T1 )
         = Set( T0_g[( ( 8 + ( 8 * i86 ) ) + ( i78 + nvfuser_zero ) )] view( T0 ), cache_op=Streaming )
  FOR i77 in iS4{8}:
    IF Inline:
 (function isUnrolled)
[W122 09:10:24.627073573 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i86
Loop:
FOR i86 in iS6{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 8) )}:
  FOR i78 in iS8{8}:
    IF Manual true:
      T1_l[( ( 8 * ( ( 1 + i86 ) % 2 ) ) + i78 )] view( T1 )
         = Set( T0_g[( ( 8 + ( 8 * i86 ) ) + ( i78 + nvfuser_zero ) )] view( T0 ), cache_op=Streaming )
  FOR i77 in iS4{8}:
    IF Manual true:
      T2_g[( ( 8 * i86 ) + ( i77 + nvfuser_zero ) )] view( T2 )
         = T1_l[( ( 8 * ( i86 % 2 ) ) + i77 )] view( T1 )
         + double(1);
 (function isUnrolled)
[W122 09:10:24.627125893 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i86
Loop:
FOR i86 in iS6{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 8) )}:
  FOR i78 in iS8{8}:
    IF Manual true:
      T1_l[( ( 8 * ( ( 1 + i86 ) % 2 ) ) + i78 )] view( T1 )
         = Set( T0_g[( ( 8 + ( 8 * i86 ) ) + ( i78 + nvfuser_zero ) )] view( T0 ), cache_op=Streaming )
  FOR i77 in iS4{8}:
    IF Manual true:
      T2_g[( ( 8 * i86 ) + ( i77 + nvfuser_zero ) )] view( T2 )
         = T1_l[( ( 8 * ( i86 % 2 ) ) + i77 )] view( T1 )
         + double(1);
 (function isUnrolled)
[W122 09:10:24.627646060 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i86
Loop:
FOR i86 in iS6{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 8) )}:
  FOR i78 in iS8{8}:
    IF Inline:
      T1_l[( ( 8 * ( ( 1 + i86 ) % 2 ) ) + i78 )] view( T1 )
         = Set( T0_g[( ( 8 + ( 8 * i86 ) ) + ( i78 + nvfuser_zero ) )] view( T0 ), cache_op=Streaming )
  FOR i77 in iS4{8}:
    IF Inline:
      T2_g[( ( 8 * i86 ) + ( i77 + nvfuser_zero ) )] view( T2 )
         = T1_l[( ( 8 * ( i86 % 2 ) ) + i77 )] view( T1 )
         + double(1);
 (function isUnrolled)
[W122 09:10:24.628267307 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i86
Loop:
FOR i86 in iS6{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 8) )}:
  FOR i78 in iS8{8}:
    IF Inline ( ( ( 8 + ( 8 * i86 ) ) + ( i78 + nvfuser_zero ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
      T1_l[( ( 8 * ( ( 1 + i86 ) % 2 ) ) + i78 )] view( T1 )
         = Set( T0_g[( ( 8 + ( 8 * i86 ) ) + ( i78 + nvfuser_zero ) )] view( T0 ), cache_op=Streaming )
  FOR i77 in iS4{8}:
    IF Inline:
      T2_g[( ( 8 * i86 ) + ( i77 + nvfuser_zero ) )] view( T2 )
         = T1_l[( ( 8 * ( i86 % 2 ) ) + i77 )] view( T1 )
         + double(1);
 (function isUnrolled)
[W122 09:10:24.628838064 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i86
Loop:
FOR i86 in iS6{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 8) )}:
  i329 = ALLOCATE(buffer=i329, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i329 = 8 * i86;
  i331 = ALLOCATE(buffer=i331, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i331 = 8 + i329;
  i413 = ALLOCATE(buffer=i413, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i413 = 8 * i412;
  i464 = ALLOCATE(buffer=i464, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i464 = 8 * i427;
  FOR i78 in iS8{8}:
    IF Manual true:
      T1_l[( ( 8 * ( ( 1 + i86 ) % 2 ) ) + i78 )] view( T1 )
         = Set( T0_g[( ( 8 + ( 8 * i86 ) ) + ( i78 + nvfuser_zero ) )] view( T0 ), cache_op=Streaming )
  FOR i77 in iS4{8}:
    IF Manual true:
      T2_g[( ( 8 * i86 ) + ( i77 + nvfuser_zero ) )] view( T2 )
         = T1_l[( ( 8 * ( i86 % 2 ) ) + i77 )] view( T1 )
         + double(1);
 (function isUnrolled)
[W122 09:10:24.628895834 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i86
Loop:
FOR i86 in iS6{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 8) )}:
  i594 = ALLOCATE(buffer=i594, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i594 = 8 * i86;
  i596 = ALLOCATE(buffer=i596, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i596 = 8 + i594;
  i678 = ALLOCATE(buffer=i678, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i678 = 8 * i677;
  i729 = ALLOCATE(buffer=i729, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i729 = 8 * i692;
  FOR i78 in iS8{8}:
    i598 = ALLOCATE(buffer=i598, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i598 = i596 + i561;
    IF Inline ( ( ( 8 + ( 8 * i86 ) ) + ( i78 + nvfuser_zero ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
      T1_l[( ( 8 * ( ( 1 + i86 ) % 2 ) ) + i78 )] view( T1 )
         = Set( T0_g[( ( 8 + ( 8 * i86 ) ) + ( i78 + nvfuser_zero ) )] view( T0 ), cache_op=Streaming )
  FOR i77 in iS4{8}:
    i759 = ALLOCATE(buffer=i759, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i759 = i594 + i738;
    IF Inline ( ( ( 8 * i86 ) + ( i77 + nvfuser_zero ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
      T2_g[( ( 8 * i86 ) + ( i77 + nvfuser_zero ) )] view( T2 )
         = T1_l[( ( 8 * ( i86 % 2 ) ) + i77 )] view( T1 )
         + double(1);
 (function isUnrolled)
[       OK ] PredicateIndexingTest.UnswitchedCircularBuffering4 (114 ms)
[ RUN      ] PredicateIndexingTest.NonDivisibleSplitWithCircularBuffering
[       OK ] PredicateIndexingTest.NonDivisibleSplitWithCircularBuffering (116 ms)
[ RUN      ] PredicateIndexingTest.UnswitchPredicateIssueRepro681/true
[       OK ] PredicateIndexingTest.UnswitchPredicateIssueRepro681/true (138 ms)
[ RUN      ] PredicateIndexingTest.NonDivisibleSplitWithNonLogicalToLoopDomains
[       OK ] PredicateIndexingTest.NonDivisibleSplitWithNonLogicalToLoopDomains (79 ms)
[ RUN      ] PredicateIndexingTest.ParallelDimensionPredicateWithUnswitch2
[       OK ] PredicateIndexingTest.ParallelDimensionPredicateWithUnswitch2 (3 ms)
[ RUN      ] PredicateIndexingTest.NonTrivialSizeOneDomain
[       OK ] PredicateIndexingTest.NonTrivialSizeOneDomain (96 ms)
[----------] 10 tests from PredicateIndexingTest (1140 ms total)

[----------] 2 tests from ContigIndexingTest
[ RUN      ] ContigIndexingTest.BroadcastInlining
[       OK ] ContigIndexingTest.BroadcastInlining (3 ms)
[ RUN      ] ContigIndexingTest.ConcretizedBroadcastMerge
[       OK ] ContigIndexingTest.ConcretizedBroadcastMerge (85 ms)
[----------] 2 tests from ContigIndexingTest (88 ms total)

[----------] 1 test from ContigPredicateIndexingTest
[ RUN      ] ContigPredicateIndexingTest.SimpleUnswitch
[       OK ] ContigPredicateIndexingTest.SimpleUnswitch (12 ms)
[----------] 1 test from ContigPredicateIndexingTest (12 ms total)

[----------] 2 tests from AdvancedIndexingIdModelTest
[ RUN      ] AdvancedIndexingIdModelTest.21
/opt/pytorch/nvfuser/tests/cpp/test_indexing_advanced.cpp:898: Skipped
Not supported yet

[  SKIPPED ] AdvancedIndexingIdModelTest.21 (0 ms)
[ RUN      ] AdvancedIndexingIdModelTest.IndexSplitMerge
[W122 09:10:24.272104941 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i129
Loop:
FOR i129 in iS12{( ceilDiv(( ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) * 3 ), 5) )}:
  FOR i130 in iS13{5}:
    FOR i117 in iS10{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:10:24.273035046 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i129
Loop:
FOR i129 in iS12{( ceilDiv(( ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) * 3 ), 5) )}:
  FOR i130 in iS13{5}:
    FOR i117 in iS10{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:10:24.276448999 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i129
Loop:
FOR i129 in iS12{( ceilDiv(( ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) * 3 ), 5) )}:
  FOR i130 in iS13{5}:
    FOR i117 in iS10{4}:
      IF Inline:
        T3_g[( ( ( ( ( 3 * ( (( (( getMetaData(T1) )).logical_size ))[1] ) ) * i159 ) + ( ( (( (( getMetaData(T1) )).logical_size ))[1] ) * ( ( ( 5 * i129 ) + i130 ) / ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) ) ) ) + ( 4 * ( ( ( 5 * i129 ) + i130 ) % ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) ) ) ) + ( i117 + nvfuser_zero ) )] view( T3 )
           = T1_g[( ( ( ( ( 3 * ( (( (( getMetaData(T1) )).alloc_stride ))[0] ) ) * i159 ) + ( ( (( (( getMetaData(T1) )).alloc_stride ))[0] ) * ( ( ( 5 * i129 ) + i130 ) / ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) ) ) ) + ( ( 4 * ( (( (( getMetaData(T1) )).alloc_stride ))[1] ) ) * ( ( ( 5 * i129 ) + i130 ) % ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) ) ) ) + ( ( (( (( getMetaData(T1) )).alloc_stride ))[1] ) * ( i117 + nvfuser_zero ) ) )] view( T1 )
           + T2_l[( ( ( ( 5 * i129 ) + i130 ) / ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) ) % 5 )] view( T2 );
 (function isUnrolled)
[W122 09:10:24.279786822 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i129
Loop:
FOR i129 in iS12{( ceilDiv(( ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) * 3 ), 5) )}:
  i788 = ALLOCATE(buffer=i788, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i788 = 5 * i129;
  FOR i130 in iS13{5}:
    i790 = ALLOCATE(buffer=i790, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i790 = i788 + i130;
    i792 = ALLOCATE(buffer=i792, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i792 = i790 / i16;
    i814 = ALLOCATE(buffer=i814, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i814 = i790 % i16;
    i816 = ALLOCATE(buffer=i816, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i816 = i795 + i815;
    i960 = ALLOCATE(buffer=i960, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i960 = i792 % 5;
    i1396 = ALLOCATE(buffer=i1396, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i1396 = 4 * i814;
    i1397 = ALLOCATE(buffer=i1397, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i1397 = i1395 + i1396;
    i1936 = ALLOCATE(buffer=i1936, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i1936 = i788 + i1524;
    b1941 = ALLOCATE(buffer=b1941, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    b1941 = b1937 && b1940;
    FOR i117 in iS10{4}:
      i367 = ALLOCATE(buffer=i367, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      i367 = i117 + nvfuser_zero;
      IF Inline ( ( ( ( ( 5 * i129 ) + ( i130 + nvfuser_zero ) ) < ( 3 * ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) ) ) && ( ( ( 3 * i159 ) + ( ( ( 5 * i129 ) + ( i130 + nvfuser_zero ) ) / ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) ) && ( ( ( 4 * ( ( ( 5 * i129 ) + i130 ) % ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) ) ) + ( i117 + nvfuser_zero ) ) < ( (( (( getMetaData(T1) )).logical_size ))[1] ) ) ):
        T3_g[( ( ( ( ( 3 * ( (( (( getMetaData(T1) )).logical_size ))[1] ) ) * i159 ) + ( ( (( (( getMetaData(T1) )).logical_size ))[1] ) * ( ( ( 5 * i129 ) + i130 ) / ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) ) ) ) + ( 4 * ( ( ( 5 * i129 ) + i130 ) % ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) ) ) ) + ( i117 + nvfuser_zero ) )] view( T3 )
           = T1_g[( ( ( ( ( 3 * ( (( (( getMetaData(T1) )).alloc_stride ))[0] ) ) * i159 ) + ( ( (( (( getMetaData(T1) )).alloc_stride ))[0] ) * ( ( ( 5 * i129 ) + i130 ) / ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) ) ) ) + ( ( 4 * ( (( (( getMetaData(T1) )).alloc_stride ))[1] ) ) * ( ( ( 5 * i129 ) + i130 ) % ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) ) ) ) + ( ( (( (( getMetaData(T1) )).alloc_stride ))[1] ) * ( i117 + nvfuser_zero ) ) )] view( T1 )
           + T2_l[( ( ( ( 5 * i129 ) + i130 ) / ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) ) % 5 )] view( T2 );
 (function isUnrolled)
[W122 09:10:24.280155940 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i129
Loop:
FOR i129 in iS12{( ceilDiv(( ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) * 3 ), 5) )}:
  i788 = ALLOCATE(buffer=i788, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i788 = 5 * i129;
  FOR i130 in iS13{5}:
    i790 = ALLOCATE(buffer=i790, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i790 = i788 + i130;
    i792 = ALLOCATE(buffer=i792, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i792 = i790 / i16;
    i814 = ALLOCATE(buffer=i814, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i814 = i790 % i16;
    i816 = ALLOCATE(buffer=i816, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i816 = i795 + i815;
    i960 = ALLOCATE(buffer=i960, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i960 = i792 % 5;
    i1396 = ALLOCATE(buffer=i1396, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i1396 = 4 * i814;
    i1397 = ALLOCATE(buffer=i1397, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i1397 = i1395 + i1396;
    i1936 = ALLOCATE(buffer=i1936, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i1936 = i788 + i1524;
    b1941 = ALLOCATE(buffer=b1941, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    b1941 = b1937 && b1940;
    FOR i117 in iS10{4}:
      i367 = ALLOCATE(buffer=i367, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      i367 = i117 + nvfuser_zero;
      IF Inline ( ( ( ( ( 5 * i129 ) + ( i130 + nvfuser_zero ) ) < ( 3 * ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) ) ) && ( ( ( 3 * i159 ) + ( ( ( 5 * i129 ) + ( i130 + nvfuser_zero ) ) / ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) ) && ( ( ( 4 * ( ( ( 5 * i129 ) + i130 ) % ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) ) ) + ( i117 + nvfuser_zero ) ) < ( (( (( getMetaData(T1) )).logical_size ))[1] ) ) ):
        T3_g[( ( ( ( ( 3 * ( (( (( getMetaData(T1) )).logical_size ))[1] ) ) * i159 ) + ( ( (( (( getMetaData(T1) )).logical_size ))[1] ) * ( ( ( 5 * i129 ) + i130 ) / ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) ) ) ) + ( 4 * ( ( ( 5 * i129 ) + i130 ) % ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) ) ) ) + ( i117 + nvfuser_zero ) )] view( T3 )
           = T1_g[( ( ( ( ( 3 * ( (( (( getMetaData(T1) )).alloc_stride ))[0] ) ) * i159 ) + ( ( (( (( getMetaData(T1) )).alloc_stride ))[0] ) * ( ( ( 5 * i129 ) + i130 ) / ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) ) ) ) + ( ( 4 * ( (( (( getMetaData(T1) )).alloc_stride ))[1] ) ) * ( ( ( 5 * i129 ) + i130 ) % ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) ) ) ) + ( ( (( (( getMetaData(T1) )).alloc_stride ))[1] ) * ( i117 + nvfuser_zero ) ) )] view( T1 )
           + T2_l[( ( ( ( 5 * i129 ) + i130 ) / ( ceilDiv(( (( (( getMetaData(T1) )).logical_size ))[1] ), 4) ) ) % 5 )] view( T2 );
  NVFUSER_UPDATE_MAGIC_ZERO;
 (function isUnrolled)
[       OK ] AdvancedIndexingIdModelTest.IndexSplitMerge (144 ms)
[----------] 2 tests from AdvancedIndexingIdModelTest (144 ms total)

[----------] 3 tests from InliningTest
[ RUN      ] InliningTest.InliningMismatchedDims4
[       OK ] InliningTest.InliningMismatchedDims4 (626 ms)
[ RUN      ] InliningTest.IsAllowedID
[       OK ] InliningTest.IsAllowedID (0 ms)
[ RUN      ] InliningTest.GetMaxPosAllNormalization
[       OK ] InliningTest.GetMaxPosAllNormalization (1 ms)
[----------] 3 tests from InliningTest (628 ms total)

[----------] 2 tests from IntervalAnalysisTest
[ RUN      ] IntervalAnalysisTest.UnaryOps
[       OK ] IntervalAnalysisTest.UnaryOps (0 ms)
[ RUN      ] IntervalAnalysisTest.ParallelLoops
[       OK ] IntervalAnalysisTest.ParallelLoops (0 ms)
[----------] 2 tests from IntervalAnalysisTest (0 ms total)

[----------] 2 tests from IterVisitorTest
[ RUN      ] IterVisitorTest.IterVisitorTraverseAttributes
[       OK ] IterVisitorTest.IterVisitorTraverseAttributes (0 ms)
[ RUN      ] IterVisitorTest.NonTerminatingOutput
[       OK ] IterVisitorTest.NonTerminatingOutput (0 ms)
[----------] 2 tests from IterVisitorTest (0 ms total)

[----------] 1 test from LinkedHashMapTest
[ RUN      ] LinkedHashMapTest.Erase
[       OK ] LinkedHashMapTest.Erase (0 ms)
[----------] 1 test from LinkedHashMapTest (0 ms total)

[----------] 5 tests from LoopDomainSchedulingTest
[ RUN      ] LoopDomainSchedulingTest.ReshapeSplitThenMerge
[       OK ] LoopDomainSchedulingTest.ReshapeSplitThenMerge (86 ms)
[ RUN      ] LoopDomainSchedulingTest.ReshapeTraversalDirection
[       OK ] LoopDomainSchedulingTest.ReshapeTraversalDirection (0 ms)
[ RUN      ] LoopDomainSchedulingTest.ScheduleLoopDomainsBy2
[       OK ] LoopDomainSchedulingTest.ScheduleLoopDomainsBy2 (0 ms)
[ RUN      ] LoopDomainSchedulingTest.CancelReshape1
[       OK ] LoopDomainSchedulingTest.CancelReshape1 (91 ms)
[ RUN      ] LoopDomainSchedulingTest.CancelReshape4
[       OK ] LoopDomainSchedulingTest.CancelReshape4 (0 ms)
[----------] 5 tests from LoopDomainSchedulingTest (178 ms total)

[----------] 4 tests from BlockQuantizationValidationTest
[ RUN      ] BlockQuantizationValidationTest.InputMustBeInLocalMemory
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] BlockQuantizationValidationTest.InputMustBeInLocalMemory (0 ms)
[ RUN      ] BlockQuantizationValidationTest.InvalidSwizzlePermutationOnBlockScales
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] BlockQuantizationValidationTest.InvalidSwizzlePermutationOnBlockScales (0 ms)
[ RUN      ] BlockQuantizationValidationTest.GroupIDMustBeInnermost
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] BlockQuantizationValidationTest.GroupIDMustBeInnermost (0 ms)
[ RUN      ] BlockQuantizationValidationTest.MergesMustBeContiguous
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] BlockQuantizationValidationTest.MergesMustBeContiguous (0 ms)
[----------] 4 tests from BlockQuantizationValidationTest (0 ms total)

[----------] 4 tests from FMinFMaxPromotionTest
[ RUN      ] FMinFMaxPromotionTest.BasicMaxSum
[       OK ] FMinFMaxPromotionTest.BasicMaxSum (148 ms)
[ RUN      ] FMinFMaxPromotionTest.MaxSumSameAxesUnary
[       OK ] FMinFMaxPromotionTest.MaxSumSameAxesUnary (148 ms)
[ RUN      ] FMinFMaxPromotionTest.WrongBroadcast
[       OK ] FMinFMaxPromotionTest.WrongBroadcast (157 ms)
[ RUN      ] FMinFMaxPromotionTest.NormalizationDifferentAxes
[       OK ] FMinFMaxPromotionTest.NormalizationDifferentAxes (246 ms)
[----------] 4 tests from FMinFMaxPromotionTest (701 ms total)

[----------] 1 test from MemoryTest
[ RUN      ] MemoryTest.RefineCachePolicy
PRINTING: __tmp_c2_nvfuser_none_f0_c0_r0_g0.ptx
Removing __tmp_c2_nvfuser_none_f0_c0_r0_g0.ptx
[       OK ] MemoryTest.RefineCachePolicy (94 ms)
[----------] 1 test from MemoryTest (94 ms total)

[----------] 4 tests from TMAIndexingTest
[ RUN      ] TMAIndexingTest.NonOneElementStride
[       OK ] TMAIndexingTest.NonOneElementStride (509 ms)
[ RUN      ] TMAIndexingTest.DefineBoxByCompositing2
[       OK ] TMAIndexingTest.DefineBoxByCompositing2 (143 ms)
[ RUN      ] TMAIndexingTest.DefineBoxByRotation2
[       OK ] TMAIndexingTest.DefineBoxByRotation2 (81 ms)
[ RUN      ] TMAIndexingTest.NonTrivialGmemAllocationDomain2
[       OK ] TMAIndexingTest.NonTrivialGmemAllocationDomain2 (100 ms)
[----------] 4 tests from TMAIndexingTest (836 ms total)

[----------] 1 test from TMAMiscTest
[ RUN      ] TMAMiscTest.DisableIndexHoisting
[       OK ] TMAMiscTest.DisableIndexHoisting (78 ms)
[----------] 1 test from TMAMiscTest (78 ms total)

[----------] 3 tests from TMACompileTimeInvalidTest
[ RUN      ] TMACompileTimeInvalidTest.BulkNotInTMA
[       OK ] TMACompileTimeInvalidTest.BulkNotInTMA (0 ms)
[ RUN      ] TMACompileTimeInvalidTest.SizeOfTransfer
[       OK ] TMACompileTimeInvalidTest.SizeOfTransfer (2 ms)
[ RUN      ] TMACompileTimeInvalidTest.InnermostElementStrideNotOne
[       OK ] TMACompileTimeInvalidTest.InnermostElementStrideNotOne (1 ms)
[----------] 3 tests from TMACompileTimeInvalidTest (3 ms total)

[----------] 1 test from TMARuntimeInvalidTest
[ RUN      ] TMARuntimeInvalidTest.MisalignedGlobalStride
[       OK ] TMARuntimeInvalidTest.MisalignedGlobalStride (179 ms)
[----------] 1 test from TMARuntimeInvalidTest (179 ms total)

[----------] 5 tests from TMADocTest
[ RUN      ] TMADocTest.Figure13a
/opt/pytorch/nvfuser/tests/cpp/test_memory.cpp:2237: Skipped
TODO: add check for this invalid case.

[  SKIPPED ] TMADocTest.Figure13a (0 ms)
[ RUN      ] TMADocTest.Figure14b
[       OK ] TMADocTest.Figure14b (93 ms)
[ RUN      ] TMADocTest.Figure13d
/opt/pytorch/nvfuser/tests/cpp/test_memory.cpp:2472: Skipped
TODO: add check for this invalid case.

[  SKIPPED ] TMADocTest.Figure13d (0 ms)
[ RUN      ] TMADocTest.Figure14e
[       OK ] TMADocTest.Figure14e (98 ms)
[ RUN      ] TMADocTest.Figure15c
/opt/pytorch/nvfuser/tests/cpp/test_memory.cpp:2717: Skipped
TODO: add check for this invalid case.

[  SKIPPED ] TMADocTest.Figure15c (0 ms)
[----------] 5 tests from TMADocTest (191 ms total)

[----------] 2 tests from TMATest
[ RUN      ] TMATest.CpAsyncBulk1D
[       OK ] TMATest.CpAsyncBulk1D (99 ms)
[ RUN      ] TMATest.CpAsyncBulk1dPipplined
[       OK ] TMATest.CpAsyncBulk1dPipplined (6 ms)
[----------] 2 tests from TMATest (105 ms total)

[----------] 4 tests from MovePadTest
[ RUN      ] MovePadTest.UnaryCat
[       OK ] MovePadTest.UnaryCat (93 ms)
[ RUN      ] MovePadTest.BinaryBroadcastOnCatDim
[       OK ] MovePadTest.BinaryBroadcastOnCatDim (165 ms)
[ RUN      ] MovePadTest.CascadePadCase0
[       OK ] MovePadTest.CascadePadCase0 (84 ms)
[ RUN      ] MovePadTest.NotMergeNegativePad
[       OK ] MovePadTest.NotMergeNegativePad (157 ms)
[----------] 4 tests from MovePadTest (501 ms total)

[----------] 2 tests from MoveRepeatForwardTest
[ RUN      ] MoveRepeatForwardTest.Simple
[       OK ] MoveRepeatForwardTest.Simple (110 ms)
[ DISABLED ] MoveRepeatForwardTest.DISABLED_ConflictingSlice
[ RUN      ] MoveRepeatForwardTest.MoveRepeatWithNonRepeatedInputs
[       OK ] MoveRepeatForwardTest.MoveRepeatWithNonRepeatedInputs (122 ms)
[----------] 2 tests from MoveRepeatForwardTest (233 ms total)

[----------] 6 tests from MoveSplitCatTest
[ RUN      ] MoveSplitCatTest.Cancellable_SetWithoutPermute
[       OK ] MoveSplitCatTest.Cancellable_SetWithoutPermute (2 ms)
[ RUN      ] MoveSplitCatTest.Cancellable_PermuteInBetween
[       OK ] MoveSplitCatTest.Cancellable_PermuteInBetween (3 ms)
[ RUN      ] MoveSplitCatTest.Noncancellable_WrongAxis
[       OK ] MoveSplitCatTest.Noncancellable_WrongAxis (165 ms)
[ RUN      ] MoveSplitCatTest.Noncancellable_UnsupportedOps
[       OK ] MoveSplitCatTest.Noncancellable_UnsupportedOps (123 ms)
[ RUN      ] MoveSplitCatTest.Cancellable_Issue1768
[       OK ] MoveSplitCatTest.Cancellable_Issue1768 (311 ms)
[ RUN      ] MoveSplitCatTest.MultipleCatsOnSameSplit
[       OK ] MoveSplitCatTest.MultipleCatsOnSameSplit (201 ms)
[----------] 6 tests from MoveSplitCatTest (809 ms total)

[----------] 1 test from MetaTest
[ RUN      ] MetaTest.Matmul1D
[       OK ] MetaTest.Matmul1D (3 ms)
[----------] 1 test from MetaTest (3 ms total)

[----------] 2 tests from NoOpTest
[ RUN      ] NoOpTest.FusionNullScheduler2
[       OK ] NoOpTest.FusionNullScheduler2 (68 ms)
[ RUN      ] NoOpTest.View
[       OK ] NoOpTest.View (1 ms)
[----------] 2 tests from NoOpTest (70 ms total)

[----------] 20 tests from OuterReductionTest
[ RUN      ] OuterReductionTest.GridPersistentReductionOuterNormLikeHalf256x7x512
[       OK ] OuterReductionTest.GridPersistentReductionOuterNormLikeHalf256x7x512 (964 ms)
[ RUN      ] OuterReductionTest.GridPersistentReductionOuterNormLikeFloat256x7x512
[       OK ] OuterReductionTest.GridPersistentReductionOuterNormLikeFloat256x7x512 (472 ms)
[ RUN      ] OuterReductionTest.GridPersistentWelfordOuterNormLikeHalf256x7x512
[       OK ] OuterReductionTest.GridPersistentWelfordOuterNormLikeHalf256x7x512 (1219 ms)
[ RUN      ] OuterReductionTest.GridPersistentWelfordOuterNormLikeHalf256x28x128
[       OK ] OuterReductionTest.GridPersistentWelfordOuterNormLikeHalf256x28x128 (1241 ms)
[ RUN      ] OuterReductionTest.GridPersistentWelfordOuterNormLikeFloat256x14x512
[       OK ] OuterReductionTest.GridPersistentWelfordOuterNormLikeFloat256x14x512 (700 ms)
[ RUN      ] OuterReductionTest.GridPersistentWelfordOuterNormLikeFloat32x32x128
[       OK ] OuterReductionTest.GridPersistentWelfordOuterNormLikeFloat32x32x128 (529 ms)
[ RUN      ] OuterReductionTest.GridPersistentBatchNormChannelsLastHalf256x28x512
[       OK ] OuterReductionTest.GridPersistentBatchNormChannelsLastHalf256x28x512 (1533 ms)
[ RUN      ] OuterReductionTest.GridPersistentBatchNormChannelsLastFloat256x14x512
[       OK ] OuterReductionTest.GridPersistentBatchNormChannelsLastFloat256x14x512 (902 ms)
[ RUN      ] OuterReductionTest.GridPersistentReductionOuterNormBwdLikeHalf256x7x512
[       OK ] OuterReductionTest.GridPersistentReductionOuterNormBwdLikeHalf256x7x512 (3491 ms)
[ RUN      ] OuterReductionTest.GridPersistentReductionOuterNormBwdLikeFloat256x7x512
[       OK ] OuterReductionTest.GridPersistentReductionOuterNormBwdLikeFloat256x7x512 (672 ms)
[ RUN      ] OuterReductionTest.GridPersistentBatchNormChannelsLastBwdHalf256x7x512
[       OK ] OuterReductionTest.GridPersistentBatchNormChannelsLastBwdHalf256x7x512 (3574 ms)
[ RUN      ] OuterReductionTest.GridPersistentBatchNormChannelsLastBwdFloat256x7x512
[       OK ] OuterReductionTest.GridPersistentBatchNormChannelsLastBwdFloat256x7x512 (891 ms)
[ RUN      ] OuterReductionTest.GridPersistentWelfordOuterNormLikeHalf256x7x512Scheduler
[       OK ] OuterReductionTest.GridPersistentWelfordOuterNormLikeHalf256x7x512Scheduler (823 ms)
[ RUN      ] OuterReductionTest.GridPersistentWelfordOuterNormLikeFloat256x7x512Scheduler
[       OK ] OuterReductionTest.GridPersistentWelfordOuterNormLikeFloat256x7x512Scheduler (554 ms)
[ RUN      ] OuterReductionTest.GridPersistentWelfordOuterNormWithWeithtsLikeHalf256x7x512Scheduler
[       OK ] OuterReductionTest.GridPersistentWelfordOuterNormWithWeithtsLikeHalf256x7x512Scheduler (1007 ms)
[ RUN      ] OuterReductionTest.GridPersistentBatchNormChannelsLastHalf256x7x512Scheduler
[       OK ] OuterReductionTest.GridPersistentBatchNormChannelsLastHalf256x7x512Scheduler (1297 ms)
[ RUN      ] OuterReductionTest.GridPersistentBatchNormChannelsLastFloat256x7x512Scheduler
[       OK ] OuterReductionTest.GridPersistentBatchNormChannelsLastFloat256x7x512Scheduler (795 ms)
[ RUN      ] OuterReductionTest.IterGroupedBlockReduction
[       OK ] OuterReductionTest.IterGroupedBlockReduction (280 ms)
[ RUN      ] OuterReductionTest.IterGroupedBlockReductionShmooTests
[       OK ] OuterReductionTest.IterGroupedBlockReductionShmooTests (24870 ms)
[ RUN      ] OuterReductionTest.IterGroupedMultipleReductions
[       OK ] OuterReductionTest.IterGroupedMultipleReductions (353 ms)
[----------] 20 tests from OuterReductionTest (46176 ms total)

[----------] 2 tests from RingBasedOverlapTest
[ RUN      ] RingBasedOverlapTest.ColumnAndSequenceParallelLinear_Forward
[       OK ] RingBasedOverlapTest.ColumnAndSequenceParallelLinear_Forward (0 ms)
[ RUN      ] RingBasedOverlapTest.RowAndSequenceParallelLinear_Forward
[       OK ] RingBasedOverlapTest.RowAndSequenceParallelLinear_Forward (0 ms)
[----------] 2 tests from RingBasedOverlapTest (0 ms total)

[----------] 1 test from CollectiveBasedOverlapTest
[ RUN      ] CollectiveBasedOverlapTest.RowParallelLinear_Forward
[       OK ] CollectiveBasedOverlapTest.RowParallelLinear_Forward (0 ms)
[----------] 1 test from CollectiveBasedOverlapTest (0 ms total)

[----------] 9 tests from PersistentBufferTest
[ RUN      ] PersistentBufferTest.FusionPersistentBufferCalculation2_CUDA
[       OK ] PersistentBufferTest.FusionPersistentBufferCalculation2_CUDA (0 ms)
[ RUN      ] PersistentBufferTest.FusionPersistentBufferProjection_CUDA
[       OK ] PersistentBufferTest.FusionPersistentBufferProjection_CUDA (118 ms)
[ RUN      ] PersistentBufferTest.FusionLayerNormFusedOpsRedundantCast_CUDA
[       OK ] PersistentBufferTest.FusionLayerNormFusedOpsRedundantCast_CUDA (541 ms)
[ RUN      ] PersistentBufferTest.ChainProjectionToPersistentProducer
[       OK ] PersistentBufferTest.ChainProjectionToPersistentProducer (323 ms)
[ RUN      ] PersistentBufferTest.ProjectToInputsAndBroadcastTvs2
[       OK ] PersistentBufferTest.ProjectToInputsAndBroadcastTvs2 (25 ms)
[ RUN      ] PersistentBufferTest.PostReductionBroadcastCheckMultiBcastDims
[       OK ] PersistentBufferTest.PostReductionBroadcastCheckMultiBcastDims (133 ms)
[ RUN      ] PersistentBufferTest.GetResolutionIssue1123
[       OK ] PersistentBufferTest.GetResolutionIssue1123 (0 ms)
[ RUN      ] PersistentBufferTest.BroadcastSync1
[       OK ] PersistentBufferTest.BroadcastSync1 (220 ms)
[ RUN      ] PersistentBufferTest.BroadcastSyncProjectToInputs
[       OK ] PersistentBufferTest.BroadcastSyncProjectToInputs (152 ms)
[----------] 9 tests from PersistentBufferTest (1516 ms total)

[----------] 1 test from TmaPersistentTestF
[ RUN      ] TmaPersistentTestF.TmaInnerPersistent
[       OK ] TmaPersistentTestF.TmaInnerPersistent (126 ms)
[----------] 1 test from TmaPersistentTestF (126 ms total)

[----------] 10 tests from PointwiseTest
[ RUN      ] PointwiseTest.VectorizeStrideContiguity3D
[       OK ] PointwiseTest.VectorizeStrideContiguity3D (341 ms)
[ RUN      ] PointwiseTest.VectorizeStrideContiguitySelfOverlapping
[       OK ] PointwiseTest.VectorizeStrideContiguitySelfOverlapping (368 ms)
[ RUN      ] PointwiseTest.Issue1567VectorizationFactorAnalysisCase0
[       OK ] PointwiseTest.Issue1567VectorizationFactorAnalysisCase0 (98 ms)
[ RUN      ] PointwiseTest.VectorizeIssue1567VectorizationFactorAnalysisCase3
[       OK ] PointwiseTest.VectorizeIssue1567VectorizationFactorAnalysisCase3 (134 ms)
[ RUN      ] PointwiseTest.VectorizeWithBroadcastAndReshape2
[       OK ] PointwiseTest.VectorizeWithBroadcastAndReshape2 (143 ms)
[ RUN      ] PointwiseTest.Heuristicst1Compute2Unroll4
[       OK ] PointwiseTest.Heuristicst1Compute2Unroll4 (114 ms)
[ RUN      ] PointwiseTest.DomainMapTestEg0
[       OK ] PointwiseTest.DomainMapTestEg0 (104 ms)
[ RUN      ] PointwiseTest.DomainMapFactory
[       OK ] PointwiseTest.DomainMapFactory (168 ms)
[ RUN      ] PointwiseTest.DomainMapSlice0
[       OK ] PointwiseTest.DomainMapSlice0 (90 ms)
[ RUN      ] PointwiseTest.InnerDimAllocationTransformationOnConsumer
[       OK ] PointwiseTest.InnerDimAllocationTransformationOnConsumer (0 ms)
[----------] 10 tests from PointwiseTest (1563 ms total)

[----------] 3 tests from TmaPointwiseTestF
[ RUN      ] TmaPointwiseTestF.TmaDomainBroadcastIllegal
[       OK ] TmaPointwiseTestF.TmaDomainBroadcastIllegal (117 ms)
[ RUN      ] TmaPointwiseTestF.SplitGridDim2D
[       OK ] TmaPointwiseTestF.SplitGridDim2D (242 ms)
[ RUN      ] TmaPointwiseTestF.InnerDimOne
[       OK ] TmaPointwiseTestF.InnerDimOne (80 ms)
[----------] 3 tests from TmaPointwiseTestF (440 ms total)

[----------] 1 test from PolymorphicValueTest
[ RUN      ] PolymorphicValueTest.OpaqueEquality
[       OK ] PolymorphicValueTest.OpaqueEquality (0 ms)
[----------] 1 test from PolymorphicValueTest (0 ms total)

[----------] 4 tests from PredicateEliminationTest
[ RUN      ] PredicateEliminationTest.1
[       OK ] PredicateEliminationTest.1 (9 ms)
[ RUN      ] PredicateEliminationTest.4
[       OK ] PredicateEliminationTest.4 (2491 ms)
[ RUN      ] PredicateEliminationTest.7
[       OK ] PredicateEliminationTest.7 (121 ms)
[ RUN      ] PredicateEliminationTest.ExtentEqualToMaxParallelTypeExtent
[       OK ] PredicateEliminationTest.ExtentEqualToMaxParallelTypeExtent (99 ms)
[----------] 4 tests from PredicateEliminationTest (2721 ms total)

[----------] 10 tests from PresegTest
[ RUN      ] PresegTest.FusionTestOptimizationPassFlag
[       OK ] PresegTest.FusionTestOptimizationPassFlag (0 ms)
[ RUN      ] PresegTest.FusionRemoveEmptyOutput
[       OK ] PresegTest.FusionRemoveEmptyOutput (68 ms)
[ RUN      ] PresegTest.FusionRemoveEmptyWelford
[W122 09:11:22.763926161 ReduceOps.cpp:1857] Warning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (function warn_invalid_degrees_of_freedom)
[       OK ] PresegTest.FusionRemoveEmptyWelford (125 ms)
[ RUN      ] PresegTest.FusionRemoveEmptyMatmul
[       OK ] PresegTest.FusionRemoveEmptyMatmul (73 ms)
[ RUN      ] PresegTest.DisjointSetsOfExtents
[       OK ] PresegTest.DisjointSetsOfExtents (92 ms)
[ RUN      ] PresegTest.TranslateRepeatToExpand2
[       OK ] PresegTest.TranslateRepeatToExpand2 (93 ms)
[ RUN      ] PresegTest.TranslateRepeatToExpand5
[       OK ] PresegTest.TranslateRepeatToExpand5 (83 ms)
[ RUN      ] PresegTest.FusionTestCastOptimizationMetaOp1
[       OK ] PresegTest.FusionTestCastOptimizationMetaOp1 (85 ms)
[ RUN      ] PresegTest.FusionTestCastOptimizationMetaOp4
[       OK ] PresegTest.FusionTestCastOptimizationMetaOp4 (84 ms)
[ RUN      ] PresegTest.MoveGatherOverCast
[       OK ] PresegTest.MoveGatherOverCast (443 ms)
[----------] 10 tests from PresegTest (1150 ms total)

[----------] 14 tests from RaggedIterDomainTest
[ RUN      ] RaggedIterDomainTest.Parallelization
[       OK ] RaggedIterDomainTest.Parallelization (0 ms)
[ RUN      ] RaggedIterDomainTest.MultiDimensionalExtents
[       OK ] RaggedIterDomainTest.MultiDimensionalExtents (0 ms)
[ RUN      ] RaggedIterDomainTest.IterVisitor
[       OK ] RaggedIterDomainTest.IterVisitor (0 ms)
[ RUN      ] RaggedIterDomainTest.TensorViewPartition
[       OK ] RaggedIterDomainTest.TensorViewPartition (0 ms)
[ RUN      ] RaggedIterDomainTest.CombineValidationNullRagged
[       OK ] RaggedIterDomainTest.CombineValidationNullRagged (0 ms)
[ RUN      ] RaggedIterDomainTest.AsNestedThenCombine
[       OK ] RaggedIterDomainTest.AsNestedThenCombine (0 ms)
[ RUN      ] RaggedIterDomainTest.AsNested1DTensor
[       OK ] RaggedIterDomainTest.AsNested1DTensor (0 ms)
[ RUN      ] RaggedIterDomainTest.AsNestedValidationMultiDimExtents
[       OK ] RaggedIterDomainTest.AsNestedValidationMultiDimExtents (0 ms)
[ RUN      ] RaggedIterDomainTest.BinaryOpMixedInputsError
[       OK ] RaggedIterDomainTest.BinaryOpMixedInputsError (0 ms)
[ RUN      ] RaggedIterDomainTest.BroadcastWithNestedTensors
[       OK ] RaggedIterDomainTest.BroadcastWithNestedTensors (0 ms)
[ RUN      ] RaggedIterDomainTest.PermuteWithNestedTensors
[       OK ] RaggedIterDomainTest.PermuteWithNestedTensors (0 ms)
[ RUN      ] RaggedIterDomainTest.ReductionOnComponentDimError
/opt/pytorch/nvfuser/tests/cpp/test_ragged_iter_domain.cpp:973: Skipped
TODO: Implement validation to prevent reduction of component dimension. Currently there is no explicit marking of which IterDomains are component dimensions, so this validation cannot be implemented yet.

[  SKIPPED ] RaggedIterDomainTest.ReductionOnComponentDimError (0 ms)
[ RUN      ] RaggedIterDomainTest.SliceRaggedDimensionError
[       OK ] RaggedIterDomainTest.SliceRaggedDimensionError (0 ms)
[ RUN      ] RaggedIterDomainTest.PadRaggedDimensionError
[       OK ] RaggedIterDomainTest.PadRaggedDimensionError (0 ms)
[----------] 14 tests from RaggedIterDomainTest (0 ms total)

[----------] 12 tests from ReductionTest
[ RUN      ] ReductionTest.GridAllreduce3
[       OK ] ReductionTest.GridAllreduce3 (126 ms)
[ RUN      ] ReductionTest.GridAllreduce6
[       OK ] ReductionTest.GridAllreduce6 (164 ms)
[ RUN      ] ReductionTest.FusedReductionBatchnorm
[       OK ] ReductionTest.FusedReductionBatchnorm (3752 ms)
[ RUN      ] ReductionTest.GroupedReduction3
[       OK ] ReductionTest.GroupedReduction3 (127 ms)
[ RUN      ] ReductionTest.GroupedReduction6
[       OK ] ReductionTest.GroupedReduction6 (96 ms)
[ RUN      ] ReductionTest.GroupedReductionRfactor2
[       OK ] ReductionTest.GroupedReductionRfactor2 (124 ms)
[ RUN      ] ReductionTest.GroupAllreduce2
[       OK ] ReductionTest.GroupAllreduce2 (164 ms)
[ RUN      ] ReductionTest.GroupAllreduce5
[       OK ] ReductionTest.GroupAllreduce5 (360 ms)
[ RUN      ] ReductionTest.GroupedReductionChannelsLastBatchNormLike
[       OK ] ReductionTest.GroupedReductionChannelsLastBatchNormLike (235 ms)
[ RUN      ] ReductionTest.CrossIterationGroupedGridAllreduce2
[       OK ] ReductionTest.CrossIterationGroupedGridAllreduce2 (298 ms)
[ RUN      ] ReductionTest.CrossIterationGroupedGridAllreduceWelford1
[       OK ] ReductionTest.CrossIterationGroupedGridAllreduceWelford1 (283 ms)
[ RUN      ] ReductionTest.GeluBwdReduction
[       OK ] ReductionTest.GeluBwdReduction (192 ms)
[----------] 12 tests from ReductionTest (5925 ms total)

[----------] 2 tests from PointwiseFusedReductionTest
[ RUN      ] PointwiseFusedReductionTest.InnerReductionNonBroadcast
[       OK ] PointwiseFusedReductionTest.InnerReductionNonBroadcast (163 ms)
[ RUN      ] PointwiseFusedReductionTest.OuterReductionBroadcast
[       OK ] PointwiseFusedReductionTest.OuterReductionBroadcast (282 ms)
[----------] 2 tests from PointwiseFusedReductionTest (445 ms total)

[----------] 5 tests from RemoveBcastSqueezeTest
[ RUN      ] RemoveBcastSqueezeTest.BcastSqueezeUnmatchedDim
[       OK ] RemoveBcastSqueezeTest.BcastSqueezeUnmatchedDim (0 ms)
[ RUN      ] RemoveBcastSqueezeTest.BcastSqueezeInputBcast
[       OK ] RemoveBcastSqueezeTest.BcastSqueezeInputBcast (0 ms)
[ RUN      ] RemoveBcastSqueezeTest.SqueezeBcastOutputSqueeze
[       OK ] RemoveBcastSqueezeTest.SqueezeBcastOutputSqueeze (0 ms)
[ RUN      ] RemoveBcastSqueezeTest.BcastSqueezeBcast
[       OK ] RemoveBcastSqueezeTest.BcastSqueezeBcast (0 ms)
[ RUN      ] RemoveBcastSqueezeTest.SqueezeBcastSetBcast
[       OK ] RemoveBcastSqueezeTest.SqueezeBcastSetBcast (0 ms)
[----------] 5 tests from RemoveBcastSqueezeTest (2 ms total)

[----------] 2 tests from RemoveTrivialOpsTest
[ RUN      ] RemoveTrivialOpsTest.Broadcast
[       OK ] RemoveTrivialOpsTest.Broadcast (302 ms)
[ RUN      ] RemoveTrivialOpsTest.BroadcastSqueeze
[       OK ] RemoveTrivialOpsTest.BroadcastSqueeze (304 ms)
[----------] 2 tests from RemoveTrivialOpsTest (606 ms total)

[----------] 1 test from ReplayTest
[ RUN      ] ReplayTest.ReplaySplitOnReduction
[       OK ] ReplayTest.ReplaySplitOnReduction (0 ms)
[----------] 1 test from ReplayTest (0 ms total)

[----------] 10 tests from ReshardingTest
[ RUN      ] ReshardingTest.SplitingView
[       OK ] ReshardingTest.SplitingView (0 ms)
[ RUN      ] ReshardingTest.Set_DifferentMeshes
[       OK ] ReshardingTest.Set_DifferentMeshes (0 ms)
[ RUN      ] ReshardingTest.Sum_SameMesh_NoParallelTypes
[       OK ] ReshardingTest.Sum_SameMesh_NoParallelTypes (0 ms)
[ RUN      ] ReshardingTest.Sum_UnshardedAxis
[       OK ] ReshardingTest.Sum_UnshardedAxis (0 ms)
[ RUN      ] ReshardingTest.Add_DifferentMeshes
[       OK ] ReshardingTest.Add_DifferentMeshes (0 ms)
[ RUN      ] ReshardingTest.Add_SameMesh_SameParallelTypes
[       OK ] ReshardingTest.Add_SameMesh_SameParallelTypes (0 ms)
[ RUN      ] ReshardingTest.Matmul_NoResharding
[       OK ] ReshardingTest.Matmul_NoResharding (0 ms)
[ RUN      ] ReshardingTest.ReduceScatter
[       OK ] ReshardingTest.ReduceScatter (0 ms)
[ RUN      ] ReshardingTest.ReshardingSqueeze
[       OK ] ReshardingTest.ReshardingSqueeze (0 ms)
[ DISABLED ] ReshardingTest.DISABLED_NonreshardingSqueeze
[ RUN      ] ReshardingTest.InsertShardedAxisReordering
[       OK ] ReshardingTest.InsertShardedAxisReordering (0 ms)
[----------] 10 tests from ReshardingTest (2 ms total)

[----------] 1 test from ReshardingSelectOpTest
[ RUN      ] ReshardingSelectOpTest.ReshardingSelectIntoNonDeviceDim
[       OK ] ReshardingSelectOpTest.ReshardingSelectIntoNonDeviceDim (0 ms)
[----------] 1 test from ReshardingSelectOpTest (0 ms total)

[----------] 35 tests from ResizeTest
[ RUN      ] ResizeTest.Pad3
[       OK ] ResizeTest.Pad3 (264 ms)
[ RUN      ] ResizeTest.Pad6
[       OK ] ResizeTest.Pad6 (20054 ms)
[ RUN      ] ResizeTest.PadScheduler2
[       OK ] ResizeTest.PadScheduler2 (103 ms)
[ RUN      ] ResizeTest.Cat1
[       OK ] ResizeTest.Cat1 (80 ms)
[ RUN      ] ResizeTest.Cat4
[       OK ] ResizeTest.Cat4 (82 ms)
[ RUN      ] ResizeTest.Cat7
[       OK ] ResizeTest.Cat7 (329 ms)
[ RUN      ] ResizeTest.CatScheduler3
[       OK ] ResizeTest.CatScheduler3 (162 ms)
[ RUN      ] ResizeTest.Slice3
[       OK ] ResizeTest.Slice3 (0 ms)
[ RUN      ] ResizeTest.SliceConstantShmoo
[       OK ] ResizeTest.SliceConstantShmoo (792 ms)
[ RUN      ] ResizeTest.SliceScheduler1
[       OK ] ResizeTest.SliceScheduler1 (2 ms)
[ RUN      ] ResizeTest.SliceReduceScheduler1
[       OK ] ResizeTest.SliceReduceScheduler1 (185 ms)
[ RUN      ] ResizeTest.CatReduceScheduler1
[       OK ] ResizeTest.CatReduceScheduler1 (120 ms)
[ RUN      ] ResizeTest.SoftmaxSliceScheduler1
[       OK ] ResizeTest.SoftmaxSliceScheduler1 (190 ms)
[ RUN      ] ResizeTest.PadToEmptyTensor
[       OK ] ResizeTest.PadToEmptyTensor (106 ms)
[ RUN      ] ResizeTest.FusionSliceForNanoGPT2
[       OK ] ResizeTest.FusionSliceForNanoGPT2 (151 ms)
[ DISABLED ] ResizeTest.DISABLED_ResizePermuteAndSlice
[ RUN      ] ResizeTest.FusionSizeZeroSliceSplitSchedule
[       OK ] ResizeTest.FusionSizeZeroSliceSplitSchedule (117 ms)
[ RUN      ] ResizeTest.MultiSliceEmpty
[       OK ] ResizeTest.MultiSliceEmpty (68 ms)
[ RUN      ] ResizeTest.ResizePadToBroadcastDynamic
[       OK ] ResizeTest.ResizePadToBroadcastDynamic (245 ms)
[ RUN      ] ResizeTest.SliceAndReshape2
[       OK ] ResizeTest.SliceAndReshape2 (153 ms)
[ RUN      ] ResizeTest.Slice1DVectorize2Manual
[       OK ] ResizeTest.Slice1DVectorize2Manual (81 ms)
[ RUN      ] ResizeTest.Slice1DVectorizeManual4
[       OK ] ResizeTest.Slice1DVectorizeManual4 (76 ms)
[ RUN      ] ResizeTest.Slice3DVectorize2
[       OK ] ResizeTest.Slice3DVectorize2 (96 ms)
[ RUN      ] ResizeTest.ReshapeToSlice
[       OK ] ResizeTest.ReshapeToSlice (97 ms)
[ RUN      ] ResizeTest.PadExpandedEmpty
[       OK ] ResizeTest.PadExpandedEmpty (82 ms)
[ RUN      ] ResizeTest.DynamicReshapeIssue1393
[       OK ] ResizeTest.DynamicReshapeIssue1393 (91 ms)
[ RUN      ] ResizeTest.CatMemoryPromotionReducedFloating
[       OK ] ResizeTest.CatMemoryPromotionReducedFloating (597 ms)
[ RUN      ] ResizeTest.Chunk_NegativeSize
[       OK ] ResizeTest.Chunk_NegativeSize (1 ms)
[ RUN      ] ResizeTest.SliceScheduledLikeProducer
[       OK ] ResizeTest.SliceScheduledLikeProducer (73 ms)
[ RUN      ] ResizeTest.SliceThenPadRightHalf
[       OK ] ResizeTest.SliceThenPadRightHalf (74 ms)
[ RUN      ] ResizeTest.VectorizePadLowering
[       OK ] ResizeTest.VectorizePadLowering (77 ms)
[ RUN      ] ResizeTest.VectorizeFactorTwo
[       OK ] ResizeTest.VectorizeFactorTwo (82 ms)
[ RUN      ] ResizeTest.PadAndCacheUses
[       OK ] ResizeTest.PadAndCacheUses (95 ms)
[ RUN      ] ResizeTest.DoNotFuseResizeAndIndexOps
[       OK ] ResizeTest.DoNotFuseResizeAndIndexOps (87 ms)
[ RUN      ] ResizeTest.AvoidCachingSliceInput
[       OK ] ResizeTest.AvoidCachingSliceInput (313 ms)
[ DISABLED ] ResizeTest.DISABLED_VectorizeOuterSliceMultiplePaths
[ RUN      ] ResizeTest.VectorizeOuterPad
[       OK ] ResizeTest.VectorizeOuterPad (172 ms)
[----------] 35 tests from ResizeTest (25314 ms total)

[----------] 8 tests from ResizeSchedulerTest
[ RUN      ] ResizeSchedulerTest.PropagateSliceToInputs/Scheduler
[       OK ] ResizeSchedulerTest.PropagateSliceToInputs/Scheduler (130 ms)
[ RUN      ] ResizeSchedulerTest.PropagateSliceToInputsWithReshape2/Manual
[       OK ] ResizeSchedulerTest.PropagateSliceToInputsWithReshape2/Manual (119 ms)
[ RUN      ] ResizeSchedulerTest.PropagateMultipleSlicesToInputs1/Scheduler
[       OK ] ResizeSchedulerTest.PropagateMultipleSlicesToInputs1/Scheduler (135 ms)
[ RUN      ] ResizeSchedulerTest.PropagateMultipleSlicesToInputs2
[       OK ] ResizeSchedulerTest.PropagateMultipleSlicesToInputs2 (210 ms)
[ RUN      ] ResizeSchedulerTest.PropagateMultipleSlicesToInputs6
[       OK ] ResizeSchedulerTest.PropagateMultipleSlicesToInputs6 (220 ms)
[ RUN      ] ResizeSchedulerTest.SliceRotateCat/Manual
[       OK ] ResizeSchedulerTest.SliceRotateCat/Manual (129 ms)
[ RUN      ] ResizeSchedulerTest.SliceRotateCatResidual/Scheduler
[       OK ] ResizeSchedulerTest.SliceRotateCatResidual/Scheduler (165 ms)
[ RUN      ] ResizeSchedulerTest.PropagateCatToInputs/Manual
[       OK ] ResizeSchedulerTest.PropagateCatToInputs/Manual (129 ms)
[----------] 8 tests from ResizeSchedulerTest (1241 ms total)

[----------] 1 test from RopeTest
[ RUN      ] RopeTest.EndingRepeatWithNoBroadcastOp
/opt/pytorch/nvfuser/tests/cpp/test_rope.cpp:1741: Skipped
Disabled due to as cancelReshape is disabled

[  SKIPPED ] RopeTest.EndingRepeatWithNoBroadcastOp (0 ms)
[----------] 1 test from RopeTest (0 ms total)

[----------] 1 test from ScalarHoistTest
[ RUN      ] ScalarHoistTest.IndexHoist2
[       OK ] ScalarHoistTest.IndexHoist2 (89 ms)
[----------] 1 test from ScalarHoistTest (89 ms total)

[----------] 4 tests from ScatterTest
[ RUN      ] ScatterTest.BlockCountingWithGmem/manual
[       OK ] ScatterTest.BlockCountingWithGmem/manual (74 ms)
[ RUN      ] ScatterTest.GridCounting
[       OK ] ScatterTest.GridCounting (83 ms)
[ RUN      ] ScatterTest.BlockCountingWithShmem2DExact/auto
[       OK ] ScatterTest.BlockCountingWithShmem2DExact/auto (83 ms)
[ RUN      ] ScatterTest.CacheAfter
[       OK ] ScatterTest.CacheAfter (73 ms)
[----------] 4 tests from ScatterTest (314 ms total)

[----------] 4 tests from SdpaTest
[ RUN      ] SdpaTest.NonCausalAttnSymbolic
[       OK ] SdpaTest.NonCausalAttnSymbolic (5 ms)
[ RUN      ] SdpaTest.NonCausalAttnConcreteBwd
[       OK ] SdpaTest.NonCausalAttnConcreteBwd (6 ms)
[ RUN      ] SdpaTest.AttnFwdBwd
[       OK ] SdpaTest.AttnFwdBwd (616 ms)
[ RUN      ] SdpaTest.ComputeAt
[       OK ] SdpaTest.ComputeAt (5 ms)
[----------] 4 tests from SdpaTest (634 ms total)

[----------] 8 tests from SegmentationTest
[ RUN      ] SegmentationTest.Issue1284_Repro2
[       OK ] SegmentationTest.Issue1284_Repro2 (182 ms)
[ RUN      ] SegmentationTest.EnforceSegmentationByCachingBeforeAndAfter
[       OK ] SegmentationTest.EnforceSegmentationByCachingBeforeAndAfter (145 ms)
[ RUN      ] SegmentationTest.InputForwardingUntilOutput
[       OK ] SegmentationTest.InputForwardingUntilOutput (148 ms)
[ RUN      ] SegmentationTest.ForceFp16Simple
[       OK ] SegmentationTest.ForceFp16Simple (160 ms)
[ RUN      ] SegmentationTest.ForceBf16NotAllCast
[       OK ] SegmentationTest.ForceBf16NotAllCast (275 ms)
[ RUN      ] SegmentationTest.AliasedOutputOnSegmentation
[       OK ] SegmentationTest.AliasedOutputOnSegmentation (131 ms)
[ RUN      ] SegmentationTest.PrivatizeUpcast
[       OK ] SegmentationTest.PrivatizeUpcast (170 ms)
[ RUN      ] SegmentationTest.RevertPrivatizedUpcastAndSqueeze
[       OK ] SegmentationTest.RevertPrivatizedUpcastAndSqueeze (137 ms)
[----------] 8 tests from SegmentationTest (1351 ms total)

[----------] 1 test from SelectTest
[ RUN      ] SelectTest.Pointwise
[       OK ] SelectTest.Pointwise (179 ms)
[----------] 1 test from SelectTest (179 ms total)

[----------] 1 test from SerialGridReductionTest
[ RUN      ] SerialGridReductionTest.Scheduling
[       OK ] SerialGridReductionTest.Scheduling (1812 ms)
[----------] 1 test from SerialGridReductionTest (1812 ms total)

[----------] 6 tests from ShardingTest
[ RUN      ] ShardingTest.MultipleDIDx
[       OK ] ShardingTest.MultipleDIDx (0 ms)
[ RUN      ] ShardingTest.PropagateSharding
[       OK ] ShardingTest.PropagateSharding (0 ms)
[ RUN      ] ShardingTest.ComputeIndex/false
[       OK ] ShardingTest.ComputeIndex/false (89 ms)
[ RUN      ] ShardingTest.ResidualAdd
[       OK ] ShardingTest.ResidualAdd (0 ms)
[ RUN      ] ShardingTest.ShardedNonDivisibleReshape
[       OK ] ShardingTest.ShardedNonDivisibleReshape (0 ms)
[ RUN      ] ShardingTest.PropagationDoesNotOverwrite
[       OK ] ShardingTest.PropagationDoesNotOverwrite (0 ms)
[----------] 6 tests from ShardingTest (90 ms total)

[----------] 3 tests from SmemReuseTest
[ RUN      ] SmemReuseTest.NeedsReorderedPush
[       OK ] SmemReuseTest.NeedsReorderedPush (10 ms)
[ RUN      ] SmemReuseTest.MultiplePromoteReuse
[       OK ] SmemReuseTest.MultiplePromoteReuse (11 ms)
[ RUN      ] SmemReuseTest.RegisterReuseWithDifferentVectorizationFactor
[       OK ] SmemReuseTest.RegisterReuseWithDifferentVectorizationFactor (773 ms)
[----------] 3 tests from SmemReuseTest (795 ms total)

[----------] 2 tests from StreamTest
[ RUN      ] StreamTest.AddPerStream
[       OK ] StreamTest.AddPerStream (74 ms)
[ RUN      ] StreamTest.BackwardPropagation
[       OK ] StreamTest.BackwardPropagation (0 ms)
[----------] 2 tests from StreamTest (75 ms total)

[----------] 1 test from SwizzleTest
[ RUN      ] SwizzleTest.Transpose1
[       OK ] SwizzleTest.Transpose1 (104 ms)
[----------] 1 test from SwizzleTest (104 ms total)

[----------] 3 tests from TensorFactoryTest
[ RUN      ] TensorFactoryTest.StandaloneOnes
[       OK ] TensorFactoryTest.StandaloneOnes (5325 ms)
[ RUN      ] TensorFactoryTest.StandaloneARange
[       OK ] TensorFactoryTest.StandaloneARange (4626 ms)
[ RUN      ] TensorFactoryTest.MetadataAsTensor
[       OK ] TensorFactoryTest.MetadataAsTensor (84 ms)
[----------] 3 tests from TensorFactoryTest (10037 ms total)

[----------] 2 tests from TMemTest
[ RUN      ] TMemTest.GmemRegTMemRegGmemCopy
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] TMemTest.GmemRegTMemRegGmemCopy (0 ms)
[ RUN      ] TMemTest.dtypes
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] TMemTest.dtypes (0 ms)
[----------] 2 tests from TMemTest (0 ms total)

[----------] 1 test from TMemTestCompileOnly
[ RUN      ] TMemTestCompileOnly.WrongStride
[       OK ] TMemTestCompileOnly.WrongStride (2 ms)
[----------] 1 test from TMemTestCompileOnly (2 ms total)

[----------] 12 tests from TransposeTest
[ RUN      ] TransposeTest.FusionScheduleTransposeMultipleInput
[       OK ] TransposeTest.FusionScheduleTransposeMultipleInput (403 ms)
[ RUN      ] TransposeTest.FusionScheduleTransposeMatchingSkipConnection
[       OK ] TransposeTest.FusionScheduleTransposeMatchingSkipConnection (376 ms)
[ RUN      ] TransposeTest.FusionScheduleBroadcastOnly
[       OK ] TransposeTest.FusionScheduleBroadcastOnly (909 ms)
[ RUN      ] TransposeTest.FusionViewNoTranspose
[       OK ] TransposeTest.FusionViewNoTranspose (3 ms)
[ RUN      ] TransposeTest.FusionScheduleTransposeSmall
[       OK ] TransposeTest.FusionScheduleTransposeSmall (525 ms)
[ RUN      ] TransposeTest.FusionScheduleTransposeSmallInnerSize3
[       OK ] TransposeTest.FusionScheduleTransposeSmallInnerSize3 (1013 ms)
[ RUN      ] TransposeTest.FusionTransposeBankConflict2
[       OK ] TransposeTest.FusionTransposeBankConflict2 (5 ms)
[ RUN      ] TransposeTest.FusionTransposeBankConflict5
[       OK ] TransposeTest.FusionTransposeBankConflict5 (4 ms)
[ RUN      ] TransposeTest.FusionTransposeBankConflict8
[       OK ] TransposeTest.FusionTransposeBankConflict8 (4 ms)
[ RUN      ] TransposeTest.TransposeAggregatedVectorizationWidth
[       OK ] TransposeTest.TransposeAggregatedVectorizationWidth (234 ms)
[ RUN      ] TransposeTest.ReshapePermuteTransposeSchedulerRejectByTransposeViewPropagator
[       OK ] TransposeTest.ReshapePermuteTransposeSchedulerRejectByTransposeViewPropagator (197 ms)
[ RUN      ] TransposeTest.TransposeSplitAggregatedVectorizationWidth
[       OK ] TransposeTest.TransposeSplitAggregatedVectorizationWidth (260 ms)
[----------] 12 tests from TransposeTest (3938 ms total)

[----------] 6 tests from UtilsTest
[ RUN      ] UtilsTest.FunctionTrace1
/opt/pytorch/nvfuser/tests/cpp/test_utils.cpp:56: Skipped
Test only runs in debug mode

[  SKIPPED ] UtilsTest.FunctionTrace1 (0 ms)
[ RUN      ] UtilsTest.FusionMergeDims
[       OK ] UtilsTest.FusionMergeDims (0 ms)
[ RUN      ] UtilsTest.FusionDisjointViewSet
[       OK ] UtilsTest.FusionDisjointViewSet (0 ms)
[ RUN      ] UtilsTest.ProveLinearAndGetStride
[       OK ] UtilsTest.ProveLinearAndGetStride (76 ms)
[ RUN      ] UtilsTest.Generator1
[       OK ] UtilsTest.Generator1 (0 ms)
[ RUN      ] UtilsTest.Generator4
[       OK ] UtilsTest.Generator4 (0 ms)
[----------] 6 tests from UtilsTest (77 ms total)

[----------] 7 tests from VectorizeHelperTest
[ RUN      ] VectorizeHelperTest.BackwardMapper1
[       OK ] VectorizeHelperTest.BackwardMapper1 (0 ms)
[ RUN      ] VectorizeHelperTest.BackwardMapper4
[       OK ] VectorizeHelperTest.BackwardMapper4 (0 ms)
[ RUN      ] VectorizeHelperTest.BackwardMapper7
[       OK ] VectorizeHelperTest.BackwardMapper7 (0 ms)
[ RUN      ] VectorizeHelperTest.ForwardMapper1
[       OK ] VectorizeHelperTest.ForwardMapper1 (0 ms)
[ RUN      ] VectorizeHelperTest.ForwardMapper4
[       OK ] VectorizeHelperTest.ForwardMapper4 (0 ms)
[ RUN      ] VectorizeHelperTest.ForwardMapper7
[       OK ] VectorizeHelperTest.ForwardMapper7 (0 ms)
[ RUN      ] VectorizeHelperTest.MapperAdvanced
[       OK ] VectorizeHelperTest.MapperAdvanced (1 ms)
[----------] 7 tests from VectorizeHelperTest (2 ms total)

[----------] 1 test from TestCpp23BackPort
[ RUN      ] TestCpp23BackPort.ZipWithReverse
[       OK ] TestCpp23BackPort.ZipWithReverse (0 ms)
[----------] 1 test from TestCpp23BackPort (0 ms total)

[----------] 2 tests from VectorizationAnalysisTest
[ RUN      ] VectorizationAnalysisTest.ContigInnerDimsMapperResizeFastestDimensionC2P
[       OK ] VectorizationAnalysisTest.ContigInnerDimsMapperResizeFastestDimensionC2P (0 ms)
[ RUN      ] VectorizationAnalysisTest.ContigInnerDimsMapperResizeStacked
[       OK ] VectorizationAnalysisTest.ContigInnerDimsMapperResizeStacked (0 ms)
[----------] 2 tests from VectorizationAnalysisTest (1 ms total)

[----------] 3 tests from WelfordTest
[ RUN      ] WelfordTest.BlockWelfordNoInit
[       OK ] WelfordTest.BlockWelfordNoInit (96 ms)
[ RUN      ] WelfordTest.BlockWelfordOp
[       OK ] WelfordTest.BlockWelfordOp (104 ms)
[ RUN      ] WelfordTest.WelfordSchedule
[       OK ] WelfordTest.WelfordSchedule (128 ms)
[----------] 3 tests from WelfordTest (329 ms total)

[----------] 150 tests from NonTma/CircularBufferingTest
[ RUN      ] NonTma/CircularBufferingTest.SingleDim1/stage_2_prefetch_neg2
[       OK ] NonTma/CircularBufferingTest.SingleDim1/stage_2_prefetch_neg2 (84 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim1/stage_2_prefetch_1
[       OK ] NonTma/CircularBufferingTest.SingleDim1/stage_2_prefetch_1 (82 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim1/stage_5_prefetch_neg3
[       OK ] NonTma/CircularBufferingTest.SingleDim1/stage_5_prefetch_neg3 (91 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim1/stage_5_prefetch_0
[       OK ] NonTma/CircularBufferingTest.SingleDim1/stage_5_prefetch_0 (89 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim1/stage_5_prefetch_3
[       OK ] NonTma/CircularBufferingTest.SingleDim1/stage_5_prefetch_3 (98 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim1/stage_9_prefetch_neg8
[       OK ] NonTma/CircularBufferingTest.SingleDim1/stage_9_prefetch_neg8 (82 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim1/stage_9_prefetch_neg5
[       OK ] NonTma/CircularBufferingTest.SingleDim1/stage_9_prefetch_neg5 (98 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim1/stage_9_prefetch_neg2
[       OK ] NonTma/CircularBufferingTest.SingleDim1/stage_9_prefetch_neg2 (109 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim1/stage_9_prefetch_1
[       OK ] NonTma/CircularBufferingTest.SingleDim1/stage_9_prefetch_1 (59 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim1/stage_9_prefetch_4
[       OK ] NonTma/CircularBufferingTest.SingleDim1/stage_9_prefetch_4 (60 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim1/stage_9_prefetch_7
[       OK ] NonTma/CircularBufferingTest.SingleDim1/stage_9_prefetch_7 (60 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim2/stage_2_prefetch_neg1
[W122 09:12:17.019983198 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.020403306 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.020429095 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.020664974 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.021062802 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.021330891 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.021454410 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 2 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.021585369 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 2 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.021628379 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 2 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.022299366 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 2 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.022506575 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 2 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.022560404 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 2 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.022612814 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 2 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.023135731 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 2 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.023338250 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 2 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.023392400 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 2 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.023448900 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 1 + i94 ) % 2 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 2 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDim2/stage_2_prefetch_neg1 (83 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim2/stage_5_prefetch_neg5
[W122 09:12:17.103495277 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.103795226 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.103818876 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.103984865 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.104271283 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( T0_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.104527572 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( T0_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.104649382 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( T0_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.104777401 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( T0_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.104818161 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( T0_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.104986830 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( T0_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.105193889 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( T0_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.105243028 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( T0_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.105297418 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( T0_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.105709326 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( i94 % 5 )] view( T1 )
           = Set( T0_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.105907555 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( i94 % 5 )] view( T1 )
           = Set( T0_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.105962995 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( i94 % 5 )] view( T1 )
           = Set( T0_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.106025274 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( i94 % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( i94 % 5 )] view( T1 )
           = Set( T0_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDim2/stage_5_prefetch_neg5 (84 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim2/stage_5_prefetch_neg2
[W122 09:12:17.188629089 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.189038777 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.189063847 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.189301045 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.189694653 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 384 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.189959562 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 384 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.190094871 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 384 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.190233771 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 384 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.190278980 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 384 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.191247805 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 384 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.191464724 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 384 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.191516344 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 384 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.191569694 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 384 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.192094901 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 384 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 384 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.192306700 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 384 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 384 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.192361730 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 384 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 384 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.192420489 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 384 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 384 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.193021356 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  i364 = ALLOCATE(buffer=i364, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i364 = i362 % 5;
  i421 = ALLOCATE(buffer=i421, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i421 = 128 * i94;
  i426 = ALLOCATE(buffer=i426, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i426 = i425 + i421;
  i573 = ALLOCATE(buffer=i573, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i573 = i259 + i421;
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 384 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 3 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 384 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline ( ( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDim2/stage_5_prefetch_neg2 (103 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim2/stage_5_prefetch_1
[W122 09:12:17.291941427 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.292354055 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.292379094 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.292606853 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.292984901 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.293253120 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.293375589 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.293506089 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.293548368 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.294214985 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.294425274 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.294480294 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.294532723 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.295055121 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.295257590 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.295312619 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.295369929 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 1 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDim2/stage_5_prefetch_1 (84 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim2/stage_5_prefetch_4
[W122 09:12:17.376737880 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.377135498 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.377160098 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.377385806 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.377766125 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 512 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.378028843 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 512 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.378154943 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 512 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.378286862 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 512 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.378328172 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 512 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.379274407 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 512 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.379483986 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 512 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.379536845 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 512 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.379588855 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 512 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.380112442 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 512 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 512 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.380314061 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 512 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 512 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.380369431 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 512 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 512 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.380427871 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 512 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 512 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.381046668 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  i364 = ALLOCATE(buffer=i364, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i364 = i362 % 5;
  i421 = ALLOCATE(buffer=i421, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i421 = 128 * i94;
  i426 = ALLOCATE(buffer=i426, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i426 = i425 + i421;
  i573 = ALLOCATE(buffer=i573, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i573 = i259 + i421;
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 512 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 4 + i94 ) % 5 )] view( T1 )
           = Set( T0_g[( ( ( 512 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 5 )] view( T1 )
           + double(1);
      IF Inline ( ( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDim2/stage_5_prefetch_4 (103 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim2/stage_9_prefetch_neg7
[W122 09:12:17.480046468 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.480444876 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.480469615 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.480698444 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.481086102 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.481346791 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.481469960 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.481601580 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.481643219 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:17.482592835 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.482803543 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.482857503 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.482909973 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.483435760 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.483640679 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.483695819 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.483754189 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:17.484371355 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  i364 = ALLOCATE(buffer=i364, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i364 = i362 % 9;
  i421 = ALLOCATE(buffer=i421, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i421 = 128 * i94;
  i426 = ALLOCATE(buffer=i426, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i426 = i425 + i421;
  i573 = ALLOCATE(buffer=i573, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i573 = i259 + i421;
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline ( ( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDim2/stage_9_prefetch_neg7 (92 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim2/stage_9_prefetch_neg4
[W122 09:12:18.572682270 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.573082438 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.573106388 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.573334767 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.573714685 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.573972114 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.574107823 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.574242512 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.574285102 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.575233137 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.575441956 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.575495106 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.575547776 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.576071173 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.576274602 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.576327662 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.576385361 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.576967388 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  i364 = ALLOCATE(buffer=i364, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i364 = i362 % 9;
  i421 = ALLOCATE(buffer=i421, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i421 = 128 * i94;
  i426 = ALLOCATE(buffer=i426, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i426 = i425 + i421;
  i573 = ALLOCATE(buffer=i573, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i573 = i259 + i421;
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline ( ( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDim2/stage_9_prefetch_neg4 (106 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim2/stage_9_prefetch_neg1
[W122 09:12:18.679449020 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.679839278 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.679863618 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.680098277 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.680484345 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.680747064 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.680872453 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.681011432 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.681054232 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.682010497 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.682220046 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.682274016 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.682327776 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.682851153 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.683062102 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.683115442 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.683173791 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.683792028 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  i364 = ALLOCATE(buffer=i364, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i364 = i362 % 9;
  i421 = ALLOCATE(buffer=i421, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i421 = 128 * i94;
  i426 = ALLOCATE(buffer=i426, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i426 = i425 + i421;
  i573 = ALLOCATE(buffer=i573, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i573 = i259 + i421;
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline ( ( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDim2/stage_9_prefetch_neg1 (112 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim2/stage_9_prefetch_2
[W122 09:12:18.792092710 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.792488388 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.792512858 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.792741057 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.793134805 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.793396383 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.793519443 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.793652132 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.793695292 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.794652867 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.794864436 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.794918056 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.794972455 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.795494933 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.795700812 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.795754941 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.795813251 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.796428208 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  i364 = ALLOCATE(buffer=i364, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i364 = i362 % 9;
  i421 = ALLOCATE(buffer=i421, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i421 = 128 * i94;
  i426 = ALLOCATE(buffer=i426, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i426 = i425 + i421;
  i573 = ALLOCATE(buffer=i573, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i573 = i259 + i421;
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 2 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline ( ( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDim2/stage_9_prefetch_2 (59 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim2/stage_9_prefetch_5
[W122 09:12:18.851484224 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.851878212 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.851902212 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.852136541 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.852520309 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.852779238 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.852900777 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.853038826 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.853080886 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.854031371 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.854243280 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.854297550 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.854352059 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.854870757 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.855076776 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.855131455 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.855190075 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.855804682 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  i364 = ALLOCATE(buffer=i364, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i364 = i362 % 9;
  i421 = ALLOCATE(buffer=i421, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i421 = 128 * i94;
  i426 = ALLOCATE(buffer=i426, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i426 = i425 + i421;
  i573 = ALLOCATE(buffer=i573, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i573 = i259 + i421;
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 5 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline ( ( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDim2/stage_9_prefetch_5 (60 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim2/stage_9_prefetch_8
[W122 09:12:18.911678764 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.912084102 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.912109222 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.912339821 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.912725139 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.912987947 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.913119017 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.913252536 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.913295596 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
 (function isUnrolled)
[W122 09:12:18.914261401 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.914474910 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.914528319 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.914581699 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.915105876 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.915310805 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.915366335 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.915424625 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline:
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[W122 09:12:18.916045332 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i94
Loop:
FOR i94 in iS4{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  i364 = ALLOCATE(buffer=i364, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i364 = i362 % 9;
  i421 = ALLOCATE(buffer=i421, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i421 = 128 * i94;
  i426 = ALLOCATE(buffer=i426, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i426 = i425 + i421;
  i573 = ALLOCATE(buffer=i573, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i573 = i259 + i421;
  FOR blockIdx.x in iblockIdx.x6{4}:
    FOR threadIdx.x in ithreadIdx.x7{32}:
      IF Inline true:
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( 0, cache_op=Streaming )
      IF Inline ( ( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( ( 8 + i94 ) % 9 )] view( T1 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T0 ), cache_op=Streaming )
      T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ) = ALLOCATE(buffer=T2_l_float[iS8{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}, iblockIdx.x10{4}, ithreadIdx.x11{32}] ca_pos( 3 ) produce_pos( 3 ), mem_type=register, size=1, zero_init=false, resets_to_zero=false)
      IF Inline true:
        T2_l[0] view( T2 )
           = T1_l[( i94 % 9 )] view( T1 )
           + double(1);
      IF Inline ( ( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T3_g[( ( threadIdx.x + ( 32 * blockIdx.x ) ) + ( 128 * i94 ) )] view( T3 )
           = Set( T2_l[0] view( T2 ), cache_op=Streaming )
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDim2/stage_9_prefetch_8 (60 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim3/stage_2_prefetch_0
[       OK ] NonTma/CircularBufferingTest.SingleDim3/stage_2_prefetch_0 (87 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim3/stage_5_prefetch_neg4
[       OK ] NonTma/CircularBufferingTest.SingleDim3/stage_5_prefetch_neg4 (90 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim3/stage_5_prefetch_neg1
[W122 09:12:18.151698548 scalar_hoist.cpp:344] Warning: Encountered loop with no iterations. Stop value 0 is same as start value 0. This could indicate a suboptimal schedule such as circular-buffering a loop that has only a single iteration. (function operator())
[       OK ] NonTma/CircularBufferingTest.SingleDim3/stage_5_prefetch_neg1 (88 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim3/stage_5_prefetch_2
[       OK ] NonTma/CircularBufferingTest.SingleDim3/stage_5_prefetch_2 (89 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim3/stage_9_prefetch_neg9
[       OK ] NonTma/CircularBufferingTest.SingleDim3/stage_9_prefetch_neg9 (86 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim3/stage_9_prefetch_neg6
[       OK ] NonTma/CircularBufferingTest.SingleDim3/stage_9_prefetch_neg6 (88 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim3/stage_9_prefetch_neg3
[       OK ] NonTma/CircularBufferingTest.SingleDim3/stage_9_prefetch_neg3 (91 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim3/stage_9_prefetch_0
[       OK ] NonTma/CircularBufferingTest.SingleDim3/stage_9_prefetch_0 (59 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim3/stage_9_prefetch_3
[       OK ] NonTma/CircularBufferingTest.SingleDim3/stage_9_prefetch_3 (60 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDim3/stage_9_prefetch_6
[       OK ] NonTma/CircularBufferingTest.SingleDim3/stage_9_prefetch_6 (61 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_2_prefetch_neg2
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_2_prefetch_neg2 (101 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_2_prefetch_1
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_2_prefetch_1 (108 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_5_prefetch_neg3
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_5_prefetch_neg3 (109 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_5_prefetch_0
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_5_prefetch_0 (100 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_5_prefetch_3
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_5_prefetch_3 (107 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_9_prefetch_neg8
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_9_prefetch_neg8 (108 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_9_prefetch_neg5
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_9_prefetch_neg5 (113 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_9_prefetch_neg2
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_9_prefetch_neg2 (114 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_9_prefetch_1
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_9_prefetch_1 (68 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_9_prefetch_4
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_9_prefetch_4 (69 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_9_prefetch_7
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch1/stage_9_prefetch_7 (71 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_2_prefetch_neg1
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_2_prefetch_neg1 (96 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_5_prefetch_neg5
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_5_prefetch_neg5 (92 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_5_prefetch_neg2
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_5_prefetch_neg2 (112 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_5_prefetch_1
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_5_prefetch_1 (97 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_5_prefetch_4
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_5_prefetch_4 (105 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_9_prefetch_neg7
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_9_prefetch_neg7 (105 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_9_prefetch_neg4
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_9_prefetch_neg4 (108 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_9_prefetch_neg1
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_9_prefetch_neg1 (110 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_9_prefetch_2
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_9_prefetch_2 (67 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_9_prefetch_5
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_9_prefetch_5 (67 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_9_prefetch_8
[       OK ] NonTma/CircularBufferingTest.SingleDimUnswitch2/stage_9_prefetch_8 (65 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnroll/stage_2_prefetch_0
[       OK ] NonTma/CircularBufferingTest.SingleDimUnroll/stage_2_prefetch_0 (229 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnroll/stage_5_prefetch_neg4
[       OK ] NonTma/CircularBufferingTest.SingleDimUnroll/stage_5_prefetch_neg4 (234 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnroll/stage_5_prefetch_neg1
[       OK ] NonTma/CircularBufferingTest.SingleDimUnroll/stage_5_prefetch_neg1 (253 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnroll/stage_5_prefetch_2
[       OK ] NonTma/CircularBufferingTest.SingleDimUnroll/stage_5_prefetch_2 (240 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnroll/stage_9_prefetch_neg9
[       OK ] NonTma/CircularBufferingTest.SingleDimUnroll/stage_9_prefetch_neg9 (225 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnroll/stage_9_prefetch_neg6
[       OK ] NonTma/CircularBufferingTest.SingleDimUnroll/stage_9_prefetch_neg6 (249 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnroll/stage_9_prefetch_neg3
[       OK ] NonTma/CircularBufferingTest.SingleDimUnroll/stage_9_prefetch_neg3 (262 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnroll/stage_9_prefetch_0
[       OK ] NonTma/CircularBufferingTest.SingleDimUnroll/stage_9_prefetch_0 (68 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnroll/stage_9_prefetch_3
[       OK ] NonTma/CircularBufferingTest.SingleDimUnroll/stage_9_prefetch_3 (82 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimUnroll/stage_9_prefetch_6
[       OK ] NonTma/CircularBufferingTest.SingleDimUnroll/stage_9_prefetch_6 (81 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_2_prefetch_neg2
[W122 09:12:23.809160564 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
 (function isUnrolled)
[W122 09:12:23.809498413 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( i89 % 2 ) )] view( T1 )
           = Set( T0_g[( ( 4 * threadIdx.x ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:23.809870921 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( i89 % 2 ) )] view( T1 )
           = Set( T0_g[( ( 4 * threadIdx.x ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:23.810348628 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( i89 % 2 ) )] view( T1 )
           = Set( T0_g[( ( 4 * threadIdx.x ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 2 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.810386248 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( i89 % 2 ) )] view( T1 )
           = Set( T0_g[( ( 4 * threadIdx.x ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 2 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.810500988 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( i89 % 2 ) )] view( T1 )
           = Set( T0_g[( ( 4 * threadIdx.x ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 2 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.810992345 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 3 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( i89 % 2 ) )] view( T1 )
           = Set( T0_g[( ( 4 * threadIdx.x ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 2 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.811039005 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 3 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( i89 % 2 ) )] view( T1 )
           = Set( T0_g[( ( 4 * threadIdx.x ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 2 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.811097244 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 3 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( i89 % 2 ) )] view( T1 )
           = Set( T0_g[( ( 4 * threadIdx.x ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 2 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.811630582 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  i207 = ALLOCATE(buffer=i207, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i207 = 128 * i89;
  i209 = ALLOCATE(buffer=i209, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i209 = i205 + i207;
  i245 = ALLOCATE(buffer=i245, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i245 = 4 * i214;
  b437 = ALLOCATE(buffer=b437, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  b437 = i436 < i30;
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 3 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( i89 % 2 ) )] view( T1 )
           = Set( T0_g[( ( 4 * threadIdx.x ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline ( ( ( 3 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 2 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_2_prefetch_neg2 (90 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_2_prefetch_1
[W122 09:12:23.899540609 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
 (function isUnrolled)
[W122 09:12:23.899952837 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 1 + i89 ) % 2 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:23.900345025 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 1 + i89 ) % 2 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:23.901300060 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 1 + i89 ) % 2 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 2 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.901341060 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 1 + i89 ) % 2 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 2 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.901446409 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 1 + i89 ) % 2 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 2 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.902065466 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 131 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 1 + i89 ) % 2 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 2 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.902107716 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 131 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 1 + i89 ) % 2 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 2 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.902153055 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 131 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 1 + i89 ) % 2 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 2 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.902778382 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  i304 = ALLOCATE(buffer=i304, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i304 = 128 * i89;
  i436 = ALLOCATE(buffer=i436, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i436 = 4 * i395;
  i482 = ALLOCATE(buffer=i482, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i482 = i226 + i304;
  b782 = ALLOCATE(buffer=b782, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  b782 = i781 < i30;
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 131 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 1 + i89 ) % 2 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline ( ( ( 3 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 2 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_2_prefetch_1 (86 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_5_prefetch_neg3
[W122 09:12:23.986494631 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
 (function isUnrolled)
[W122 09:12:23.986910219 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 2 + i89 ) % 5 ) )] view( T1 )
           = Set( T0_g[( ( 256 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:23.987309957 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 2 + i89 ) % 5 ) )] view( T1 )
           = Set( T0_g[( ( 256 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:23.988532290 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 2 + i89 ) % 5 ) )] view( T1 )
           = Set( T0_g[( ( 256 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 5 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.988574640 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 2 + i89 ) % 5 ) )] view( T1 )
           = Set( T0_g[( ( 256 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 5 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.988685110 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 2 + i89 ) % 5 ) )] view( T1 )
           = Set( T0_g[( ( 256 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 5 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.989301246 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 259 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 2 + i89 ) % 5 ) )] view( T1 )
           = Set( T0_g[( ( 256 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 5 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.989344526 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 259 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 2 + i89 ) % 5 ) )] view( T1 )
           = Set( T0_g[( ( 256 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 5 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.989389716 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 259 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 2 + i89 ) % 5 ) )] view( T1 )
           = Set( T0_g[( ( 256 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 5 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.990049523 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  i327 = ALLOCATE(buffer=i327, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i327 = 128 * i89;
  i459 = ALLOCATE(buffer=i459, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i459 = 4 * i418;
  i505 = ALLOCATE(buffer=i505, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i505 = i235 + i327;
  b838 = ALLOCATE(buffer=b838, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  b838 = i837 < i30;
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 259 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 2 + i89 ) % 5 ) )] view( T1 )
           = Set( T0_g[( ( 256 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline ( ( ( 3 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 5 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_5_prefetch_neg3 (98 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_5_prefetch_0
[W122 09:12:23.084855974 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
 (function isUnrolled)
[W122 09:12:23.085200412 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( i89 % 5 ) )] view( T1 )
           = Set( T0_g[( ( 4 * threadIdx.x ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:23.085567310 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( i89 % 5 ) )] view( T1 )
           = Set( T0_g[( ( 4 * threadIdx.x ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:23.086038608 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( i89 % 5 ) )] view( T1 )
           = Set( T0_g[( ( 4 * threadIdx.x ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 5 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.086077588 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( i89 % 5 ) )] view( T1 )
           = Set( T0_g[( ( 4 * threadIdx.x ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 5 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.086193427 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( i89 % 5 ) )] view( T1 )
           = Set( T0_g[( ( 4 * threadIdx.x ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 5 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.086678635 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 3 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( i89 % 5 ) )] view( T1 )
           = Set( T0_g[( ( 4 * threadIdx.x ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 5 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.086718715 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 3 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( i89 % 5 ) )] view( T1 )
           = Set( T0_g[( ( 4 * threadIdx.x ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 5 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.086777514 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 3 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( i89 % 5 ) )] view( T1 )
           = Set( T0_g[( ( 4 * threadIdx.x ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 5 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.087313602 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  i207 = ALLOCATE(buffer=i207, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i207 = 128 * i89;
  i209 = ALLOCATE(buffer=i209, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i209 = i205 + i207;
  i245 = ALLOCATE(buffer=i245, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i245 = 4 * i214;
  b437 = ALLOCATE(buffer=b437, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  b437 = i436 < i30;
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 3 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( i89 % 5 ) )] view( T1 )
           = Set( T0_g[( ( 4 * threadIdx.x ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline ( ( ( 3 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 5 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_5_prefetch_0 (92 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_5_prefetch_3
[W122 09:12:23.177763426 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
 (function isUnrolled)
[W122 09:12:23.178184063 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 3 + i89 ) % 5 ) )] view( T1 )
           = Set( T0_g[( ( 384 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:23.178569231 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 3 + i89 ) % 5 ) )] view( T1 )
           = Set( T0_g[( ( 384 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:23.179808435 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 3 + i89 ) % 5 ) )] view( T1 )
           = Set( T0_g[( ( 384 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 5 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.179849855 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 3 + i89 ) % 5 ) )] view( T1 )
           = Set( T0_g[( ( 384 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 5 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.179954204 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 3 + i89 ) % 5 ) )] view( T1 )
           = Set( T0_g[( ( 384 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 5 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.180562261 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 387 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 3 + i89 ) % 5 ) )] view( T1 )
           = Set( T0_g[( ( 384 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 5 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.180604391 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 387 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 3 + i89 ) % 5 ) )] view( T1 )
           = Set( T0_g[( ( 384 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 5 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.180648861 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 387 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 3 + i89 ) % 5 ) )] view( T1 )
           = Set( T0_g[( ( 384 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 5 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.181283057 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  i327 = ALLOCATE(buffer=i327, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i327 = 128 * i89;
  i459 = ALLOCATE(buffer=i459, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i459 = 4 * i418;
  i505 = ALLOCATE(buffer=i505, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i505 = i235 + i327;
  b838 = ALLOCATE(buffer=b838, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  b838 = i837 < i30;
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 387 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 3 + i89 ) % 5 ) )] view( T1 )
           = Set( T0_g[( ( 384 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline ( ( ( 3 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 5 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_5_prefetch_3 (110 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_9_prefetch_neg8
[W122 09:12:23.288641424 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
 (function isUnrolled)
[W122 09:12:23.289066822 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 1 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:23.289445680 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 1 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:23.290406935 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 1 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.290447835 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 1 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.290556425 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 1 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.291184931 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 131 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 1 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.291227181 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 131 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 1 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.291272881 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 131 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 1 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.291878268 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  i304 = ALLOCATE(buffer=i304, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i304 = 128 * i89;
  i436 = ALLOCATE(buffer=i436, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i436 = 4 * i395;
  i482 = ALLOCATE(buffer=i482, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i482 = i226 + i304;
  b782 = ALLOCATE(buffer=b782, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  b782 = i781 < i30;
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 131 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 1 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline ( ( ( 3 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_9_prefetch_neg8 (90 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_9_prefetch_neg5
[W122 09:12:23.379233178 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
 (function isUnrolled)
[W122 09:12:23.379643906 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 4 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 512 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:23.380039333 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 4 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 512 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:23.381263307 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 4 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 512 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.381304687 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 4 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 512 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.381422536 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 4 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 512 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.382033593 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 515 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 4 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 512 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.382075863 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 515 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 4 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 512 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.382121663 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 515 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 4 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 512 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:23.382764579 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  i327 = ALLOCATE(buffer=i327, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i327 = 128 * i89;
  i459 = ALLOCATE(buffer=i459, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i459 = 4 * i418;
  i505 = ALLOCATE(buffer=i505, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i505 = i235 + i327;
  b838 = ALLOCATE(buffer=b838, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  b838 = i837 < i30;
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 515 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 4 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 512 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline ( ( ( 3 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_9_prefetch_neg5 (112 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_9_prefetch_neg2
[W122 09:12:23.491686048 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
 (function isUnrolled)
[W122 09:12:23.492116336 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 7 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 896 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:23.492503154 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 7 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 896 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.493712608 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 7 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 896 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.493754268 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 7 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 896 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.493859797 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 7 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 896 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.494474614 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 899 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 7 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 896 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.494517494 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 899 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 7 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 896 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.494563064 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 899 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 7 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 896 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.495199910 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  i327 = ALLOCATE(buffer=i327, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i327 = 128 * i89;
  i459 = ALLOCATE(buffer=i459, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i459 = 4 * i418;
  i505 = ALLOCATE(buffer=i505, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i505 = i235 + i327;
  b838 = ALLOCATE(buffer=b838, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  b838 = i837 < i30;
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 899 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 7 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 896 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline ( ( ( 3 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_9_prefetch_neg2 (132 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_9_prefetch_1
[W122 09:12:24.623613779 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
 (function isUnrolled)
[W122 09:12:24.624036107 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 1 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.624419015 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 1 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.625376290 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 1 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.625417249 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 1 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.625533699 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 1 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.626162556 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 131 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 1 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.626205295 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 131 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 1 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.626250945 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 131 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 1 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.626880352 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  i304 = ALLOCATE(buffer=i304, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i304 = 128 * i89;
  i436 = ALLOCATE(buffer=i436, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i436 = 4 * i395;
  i482 = ALLOCATE(buffer=i482, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i482 = i226 + i304;
  b782 = ALLOCATE(buffer=b782, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  b782 = i781 < i30;
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 131 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 1 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 128 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline ( ( ( 3 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_9_prefetch_1 (59 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_9_prefetch_4
[W122 09:12:24.682866633 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
 (function isUnrolled)
[W122 09:12:24.683290211 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 4 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 512 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.683681289 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 4 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 512 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.684923943 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 4 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 512 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.684966903 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 4 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 512 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.685084592 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 4 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 512 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.685697269 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 515 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 4 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 512 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.685739249 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 515 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 4 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 512 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.685785268 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 515 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 4 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 512 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.686462385 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  i327 = ALLOCATE(buffer=i327, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i327 = 128 * i89;
  i459 = ALLOCATE(buffer=i459, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i459 = 4 * i418;
  i505 = ALLOCATE(buffer=i505, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i505 = i235 + i327;
  b838 = ALLOCATE(buffer=b838, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  b838 = i837 < i30;
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 515 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 4 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 512 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline ( ( ( 3 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_9_prefetch_4 (59 ms)
[ RUN      ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_9_prefetch_7
[W122 09:12:24.742869124 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
 (function isUnrolled)
[W122 09:12:24.743290272 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 7 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 896 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.743676780 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 7 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 896 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.744883434 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 7 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 896 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.744926214 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 7 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 896 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.745041693 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize:
        T1_l[( 4 * ( ( 7 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 896 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.745647580 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 899 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 7 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 896 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.745688830 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 899 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 7 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 896 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.745734060 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 899 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 7 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 896 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline:
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[W122 09:12:24.746365316 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i89
Loop:
FOR i89 in iS3{( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 128) )}:
  i327 = ALLOCATE(buffer=i327, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i327 = 128 * i89;
  i459 = ALLOCATE(buffer=i459, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i459 = 4 * i418;
  i505 = ALLOCATE(buffer=i505, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i505 = i235 + i327;
  b838 = ALLOCATE(buffer=b838, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  b838 = i837 < i30;
  FOR threadIdx.x in ithreadIdx.x5{32}:
    FOR i81 in iV10{4}:
      IF Vectorize ( ( ( 899 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T1_l[( 4 * ( ( 7 + i89 ) % 9 ) )] view( T1 )
           = Set( T0_g[( ( 896 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) )] view( T0 ), cache_op=Streaming )
    FOR i80 in iS6{4}:
      IF Inline ( ( ( 3 + ( 4 * threadIdx.x ) ) + ( 128 * i89 ) ) < ( (( (( getMetaData(T0) )).logical_size ))[0] ) ):
        T2_g[( ( ( 4 * threadIdx.x ) + ( 128 * i89 ) ) + ( i80 + nvfuser_zero ) )] view( T2 )
           = T1_l[( ( 4 * ( i89 % 9 ) ) + i80 )] view( T1 )
           + double(1);
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.SingleDimVectorize/stage_9_prefetch_7 (59 ms)
[ RUN      ] NonTma/CircularBufferingTest.MultipleTensors/stage_2_prefetch_neg1
[W122 09:12:24.805091214 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.805621621 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 1 + i124 ) % 2 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.806207108 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 1 + i124 ) % 2 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.806707196 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 1 + i124 ) % 2 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 1 + i124 ) % 2 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.807107563 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 1 + i124 ) % 2 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 1 + i124 ) % 2 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.807504641 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 1 + i124 ) % 2 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 1 + i124 ) % 2 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.809354172 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 1 + i124 ) % 2 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 1 + i124 ) % 2 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 2 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 2 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:24.810461966 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 128 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 1 + i124 ) % 2 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 1 + i124 ) % 2 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 2 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 2 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:24.811540231 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 128 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 1 + i124 ) % 2 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( 128 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( ( 1 + i124 ) % 2 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 2 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 2 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:24.812642365 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  i515 = ALLOCATE(buffer=i515, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i515 = 128 * i124;
  i517 = ALLOCATE(buffer=i517, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i517 = i513 + i515;
  i603 = ALLOCATE(buffer=i603, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i603 = 4 * i602;
  i855 = ALLOCATE(buffer=i855, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i855 = 4 * i814;
  i967 = ALLOCATE(buffer=i967, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i967 = threadIdx.x + i515;
  i1347 = ALLOCATE(buffer=i1347, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1347 = i1337 + i515;
  i1650 = ALLOCATE(buffer=i1650, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1650 = i1066 + i515;
  FOR i109 in iS12{4}:
    i519 = ALLOCATE(buffer=i519, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i519 = 32 * i449;
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 128 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 1 + i124 ) % 2 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    i709 = ALLOCATE(buffer=i709, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i709 = 32 * i639;
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( 128 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( ( 1 + i124 ) % 2 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    i857 = ALLOCATE(buffer=i857, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i857 = i855 + i108;
    i963 = ALLOCATE(buffer=i963, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i963 = 32 * i913;
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline ( ( ( ( -( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i108 + nvfuser_zero ) ) ) ):
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 2 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 2 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.MultipleTensors/stage_2_prefetch_neg1 (106 ms)
[ RUN      ] NonTma/CircularBufferingTest.MultipleTensors/stage_5_prefetch_neg5
[W122 09:12:24.910422051 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.910862309 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( i124 % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.911354596 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( i124 % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.911768584 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( i124 % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( i124 % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.912147102 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( i124 % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( i124 % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.912532160 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( i124 % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( i124 % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.913161177 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( i124 % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( i124 % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:24.914083722 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( -( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( i124 % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( i124 % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:24.914993928 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( -( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( i124 % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( -( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( i124 % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:24.915924623 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  i313 = ALLOCATE(buffer=i313, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i313 = 128 * i124;
  i315 = ALLOCATE(buffer=i315, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i315 = threadIdx.x + i313;
  i362 = ALLOCATE(buffer=i362, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i362 = 4 * i323;
  i796 = ALLOCATE(buffer=i796, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i796 = i786 + i313;
  FOR i109 in iS12{4}:
    i317 = ALLOCATE(buffer=i317, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i317 = 32 * i265;
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( -( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( i124 % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    i443 = ALLOCATE(buffer=i443, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i443 = 32 * i391;
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( -( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( i124 % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    i554 = ALLOCATE(buffer=i554, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i554 = i362 + i108;
    i659 = ALLOCATE(buffer=i659, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i659 = 32 * i609;
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline ( ( ( ( -( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i108 + nvfuser_zero ) ) ) ):
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.MultipleTensors/stage_5_prefetch_neg5 (95 ms)
[ RUN      ] NonTma/CircularBufferingTest.MultipleTensors/stage_5_prefetch_neg2
[W122 09:12:24.007766720 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.008298727 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 3 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 384 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.008864624 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 3 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 384 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.009358842 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 3 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 384 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 3 + i124 ) % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 384 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.009738500 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 3 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 384 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 3 + i124 ) % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 384 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.010150767 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 3 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 384 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 3 + i124 ) % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 384 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.012577835 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 3 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 384 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 3 + i124 ) % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 384 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:24.013677599 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 384 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 3 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 384 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 3 + i124 ) % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 384 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:24.014767134 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 384 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 3 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 384 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( 384 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( ( 3 + i124 ) % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 384 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:24.015889568 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  i595 = ALLOCATE(buffer=i595, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i595 = 128 * i124;
  i597 = ALLOCATE(buffer=i597, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i597 = i593 + i595;
  i683 = ALLOCATE(buffer=i683, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i683 = 4 * i682;
  i935 = ALLOCATE(buffer=i935, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i935 = 4 * i894;
  i1047 = ALLOCATE(buffer=i1047, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1047 = threadIdx.x + i595;
  i1496 = ALLOCATE(buffer=i1496, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1496 = i1486 + i595;
  i1799 = ALLOCATE(buffer=i1799, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1799 = i1175 + i595;
  FOR i109 in iS12{4}:
    i599 = ALLOCATE(buffer=i599, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i599 = 32 * i529;
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 384 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 3 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 384 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    i789 = ALLOCATE(buffer=i789, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i789 = 32 * i719;
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( 384 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( ( 3 + i124 ) % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 384 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    i937 = ALLOCATE(buffer=i937, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i937 = i935 + i108;
    i1043 = ALLOCATE(buffer=i1043, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i1043 = 32 * i993;
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline ( ( ( ( -( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i108 + nvfuser_zero ) ) ) ):
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.MultipleTensors/stage_5_prefetch_neg2 (155 ms)
[ RUN      ] NonTma/CircularBufferingTest.MultipleTensors/stage_5_prefetch_1
[W122 09:12:24.162331763 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.162849191 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 1 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.163429488 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 1 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.163919245 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 1 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 1 + i124 ) % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.164307823 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 1 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 1 + i124 ) % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.164694571 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 1 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 1 + i124 ) % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.166552782 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 1 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 1 + i124 ) % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:24.167666656 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 128 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 1 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 1 + i124 ) % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:24.168755950 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 128 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 1 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( 128 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( ( 1 + i124 ) % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:24.169835425 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  i515 = ALLOCATE(buffer=i515, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i515 = 128 * i124;
  i517 = ALLOCATE(buffer=i517, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i517 = i513 + i515;
  i603 = ALLOCATE(buffer=i603, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i603 = 4 * i602;
  i855 = ALLOCATE(buffer=i855, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i855 = 4 * i814;
  i967 = ALLOCATE(buffer=i967, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i967 = threadIdx.x + i515;
  i1347 = ALLOCATE(buffer=i1347, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1347 = i1337 + i515;
  i1650 = ALLOCATE(buffer=i1650, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1650 = i1066 + i515;
  FOR i109 in iS12{4}:
    i519 = ALLOCATE(buffer=i519, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i519 = 32 * i449;
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 128 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 1 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    i709 = ALLOCATE(buffer=i709, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i709 = 32 * i639;
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( 128 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( ( 1 + i124 ) % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 128 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    i857 = ALLOCATE(buffer=i857, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i857 = i855 + i108;
    i963 = ALLOCATE(buffer=i963, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i963 = 32 * i913;
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline ( ( ( ( -( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i108 + nvfuser_zero ) ) ) ):
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.MultipleTensors/stage_5_prefetch_1 (104 ms)
[ RUN      ] NonTma/CircularBufferingTest.MultipleTensors/stage_5_prefetch_4
[W122 09:12:24.267370912 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.267884320 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 4 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 512 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.268459397 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 4 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 512 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.268948714 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 4 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 512 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 4 + i124 ) % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 512 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.269342662 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 4 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 512 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 4 + i124 ) % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 512 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.269751120 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 4 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 512 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 4 + i124 ) % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 512 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.272183978 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 4 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 512 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 4 + i124 ) % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 512 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:24.273292992 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 512 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 4 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 512 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 4 + i124 ) % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 512 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:24.274374656 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 512 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 4 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 512 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( 512 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( ( 4 + i124 ) % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 512 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:24.275515510 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  i595 = ALLOCATE(buffer=i595, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i595 = 128 * i124;
  i597 = ALLOCATE(buffer=i597, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i597 = i593 + i595;
  i683 = ALLOCATE(buffer=i683, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i683 = 4 * i682;
  i935 = ALLOCATE(buffer=i935, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i935 = 4 * i894;
  i1047 = ALLOCATE(buffer=i1047, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1047 = threadIdx.x + i595;
  i1496 = ALLOCATE(buffer=i1496, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1496 = i1486 + i595;
  i1799 = ALLOCATE(buffer=i1799, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1799 = i1175 + i595;
  FOR i109 in iS12{4}:
    i599 = ALLOCATE(buffer=i599, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i599 = 32 * i529;
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 512 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 4 + i124 ) % 5 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 512 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    i789 = ALLOCATE(buffer=i789, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i789 = 32 * i719;
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( 512 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( ( 4 + i124 ) % 5 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 512 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    i937 = ALLOCATE(buffer=i937, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i937 = i935 + i108;
    i1043 = ALLOCATE(buffer=i1043, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i1043 = 32 * i993;
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline ( ( ( ( -( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i108 + nvfuser_zero ) ) ) ):
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 5 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.MultipleTensors/stage_5_prefetch_4 (165 ms)
[ RUN      ] NonTma/CircularBufferingTest.MultipleTensors/stage_9_prefetch_neg7
[W122 09:12:24.432995549 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.433522506 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.434094993 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.434586231 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.434966299 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.435379267 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:24.437813334 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:24.438921069 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 256 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:24.440026943 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 256 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( 256 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:24.441183267 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  i595 = ALLOCATE(buffer=i595, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i595 = 128 * i124;
  i597 = ALLOCATE(buffer=i597, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i597 = i593 + i595;
  i683 = ALLOCATE(buffer=i683, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i683 = 4 * i682;
  i935 = ALLOCATE(buffer=i935, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i935 = 4 * i894;
  i1047 = ALLOCATE(buffer=i1047, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1047 = threadIdx.x + i595;
  i1496 = ALLOCATE(buffer=i1496, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1496 = i1486 + i595;
  i1799 = ALLOCATE(buffer=i1799, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1799 = i1175 + i595;
  FOR i109 in iS12{4}:
    i599 = ALLOCATE(buffer=i599, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i599 = 32 * i529;
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 256 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    i789 = ALLOCATE(buffer=i789, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i789 = 32 * i719;
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( 256 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    i937 = ALLOCATE(buffer=i937, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i937 = i935 + i108;
    i1043 = ALLOCATE(buffer=i1043, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i1043 = 32 * i993;
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline ( ( ( ( -( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i108 + nvfuser_zero ) ) ) ):
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.MultipleTensors/stage_9_prefetch_neg7 (131 ms)
[ RUN      ] NonTma/CircularBufferingTest.MultipleTensors/stage_9_prefetch_neg4
[W122 09:12:25.564179513 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.564698991 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.565272618 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.565759795 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.566145113 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.566555551 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.568997518 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:25.570118813 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 640 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:25.571200357 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 640 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( 640 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:25.572303382 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  i595 = ALLOCATE(buffer=i595, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i595 = 128 * i124;
  i597 = ALLOCATE(buffer=i597, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i597 = i593 + i595;
  i683 = ALLOCATE(buffer=i683, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i683 = 4 * i682;
  i935 = ALLOCATE(buffer=i935, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i935 = 4 * i894;
  i1047 = ALLOCATE(buffer=i1047, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1047 = threadIdx.x + i595;
  i1496 = ALLOCATE(buffer=i1496, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1496 = i1486 + i595;
  i1799 = ALLOCATE(buffer=i1799, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1799 = i1175 + i595;
  FOR i109 in iS12{4}:
    i599 = ALLOCATE(buffer=i599, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i599 = 32 * i529;
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 640 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    i789 = ALLOCATE(buffer=i789, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i789 = 32 * i719;
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( 640 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    i937 = ALLOCATE(buffer=i937, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i937 = i935 + i108;
    i1043 = ALLOCATE(buffer=i1043, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i1043 = 32 * i993;
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline ( ( ( ( -( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i108 + nvfuser_zero ) ) ) ):
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.MultipleTensors/stage_9_prefetch_neg4 (189 ms)
[ RUN      ] NonTma/CircularBufferingTest.MultipleTensors/stage_9_prefetch_neg1
[W122 09:12:25.754046005 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.754565532 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.755134100 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.755619637 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.755997545 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.756410223 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.758815041 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:25.759916175 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 1024 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:25.761009479 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 1024 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( 1024 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:25.762132634 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  i595 = ALLOCATE(buffer=i595, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i595 = 128 * i124;
  i597 = ALLOCATE(buffer=i597, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i597 = i593 + i595;
  i683 = ALLOCATE(buffer=i683, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i683 = 4 * i682;
  i935 = ALLOCATE(buffer=i935, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i935 = 4 * i894;
  i1047 = ALLOCATE(buffer=i1047, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1047 = threadIdx.x + i595;
  i1496 = ALLOCATE(buffer=i1496, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1496 = i1486 + i595;
  i1799 = ALLOCATE(buffer=i1799, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1799 = i1175 + i595;
  FOR i109 in iS12{4}:
    i599 = ALLOCATE(buffer=i599, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i599 = 32 * i529;
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 1024 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    i789 = ALLOCATE(buffer=i789, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i789 = 32 * i719;
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( 1024 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    i937 = ALLOCATE(buffer=i937, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i937 = i935 + i108;
    i1043 = ALLOCATE(buffer=i1043, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i1043 = 32 * i993;
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline ( ( ( ( -( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i108 + nvfuser_zero ) ) ) ):
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.MultipleTensors/stage_9_prefetch_neg1 (223 ms)
[ RUN      ] NonTma/CircularBufferingTest.MultipleTensors/stage_9_prefetch_2
[W122 09:12:25.977164056 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.977684923 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.978263450 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.978756538 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.979149226 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.979561383 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.981991301 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:25.983607153 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 256 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:25.984783947 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 256 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( 256 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:25.986104950 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  i595 = ALLOCATE(buffer=i595, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i595 = 128 * i124;
  i597 = ALLOCATE(buffer=i597, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i597 = i593 + i595;
  i683 = ALLOCATE(buffer=i683, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i683 = 4 * i682;
  i935 = ALLOCATE(buffer=i935, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i935 = 4 * i894;
  i1047 = ALLOCATE(buffer=i1047, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1047 = threadIdx.x + i595;
  i1496 = ALLOCATE(buffer=i1496, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1496 = i1486 + i595;
  i1799 = ALLOCATE(buffer=i1799, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1799 = i1175 + i595;
  FOR i109 in iS12{4}:
    i599 = ALLOCATE(buffer=i599, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i599 = 32 * i529;
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 256 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    i789 = ALLOCATE(buffer=i789, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i789 = 32 * i719;
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( 256 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( ( 2 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 256 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    i937 = ALLOCATE(buffer=i937, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i937 = i935 + i108;
    i1043 = ALLOCATE(buffer=i1043, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i1043 = 32 * i993;
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline ( ( ( ( -( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i108 + nvfuser_zero ) ) ) ):
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.MultipleTensors/stage_9_prefetch_2 (68 ms)
[ RUN      ] NonTma/CircularBufferingTest.MultipleTensors/stage_9_prefetch_5
[W122 09:12:25.045680013 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.046213620 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.046786877 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.047283545 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.047666113 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.048079130 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.050514008 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:25.051615522 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 640 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:25.052707367 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 640 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( 640 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:25.053810851 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  i595 = ALLOCATE(buffer=i595, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i595 = 128 * i124;
  i597 = ALLOCATE(buffer=i597, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i597 = i593 + i595;
  i683 = ALLOCATE(buffer=i683, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i683 = 4 * i682;
  i935 = ALLOCATE(buffer=i935, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i935 = 4 * i894;
  i1047 = ALLOCATE(buffer=i1047, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1047 = threadIdx.x + i595;
  i1496 = ALLOCATE(buffer=i1496, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1496 = i1486 + i595;
  i1799 = ALLOCATE(buffer=i1799, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1799 = i1175 + i595;
  FOR i109 in iS12{4}:
    i599 = ALLOCATE(buffer=i599, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i599 = 32 * i529;
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 640 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    i789 = ALLOCATE(buffer=i789, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i789 = 32 * i719;
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( 640 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( ( 5 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 640 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    i937 = ALLOCATE(buffer=i937, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i937 = i935 + i108;
    i1043 = ALLOCATE(buffer=i1043, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i1043 = 32 * i993;
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline ( ( ( ( -( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i108 + nvfuser_zero ) ) ) ):
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.MultipleTensors/stage_9_prefetch_5 (66 ms)
[ RUN      ] NonTma/CircularBufferingTest.MultipleTensors/stage_9_prefetch_8
[W122 09:12:25.112183280 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.112710448 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.113285315 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.113772292 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.114166380 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.114575198 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
 (function isUnrolled)
[W122 09:12:25.116997285 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline:
        T3_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:25.118112290 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 1024 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline:
        T2_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:25.119200484 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  FOR i109 in iS12{4}:
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 1024 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( 1024 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline:
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[W122 09:12:25.120307208 kernel_ir.cpp:168] Warning: Unroll required but not possible. Register allocation disabled.
Loop index: i124
Loop:
FOR i124 in iS7{( ceilDiv(( ceilDiv(( (( (( getMetaData(T0) )).logical_size ))[0] ), 32) ), 4) )}:
  i595 = ALLOCATE(buffer=i595, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i595 = 128 * i124;
  i597 = ALLOCATE(buffer=i597, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i597 = i593 + i595;
  i683 = ALLOCATE(buffer=i683, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i683 = 4 * i682;
  i935 = ALLOCATE(buffer=i935, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i935 = 4 * i894;
  i1047 = ALLOCATE(buffer=i1047, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1047 = threadIdx.x + i595;
  i1496 = ALLOCATE(buffer=i1496, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1496 = i1486 + i595;
  i1799 = ALLOCATE(buffer=i1799, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
  i1799 = i1175 + i595;
  FOR i109 in iS12{4}:
    i599 = ALLOCATE(buffer=i599, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i599 = 32 * i529;
    FOR threadIdx.x in ithreadIdx.x10{32}:
      IF Inline ( ( ( ( 1024 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i109 + nvfuser_zero ) ) ) ):
        T3_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i109 )] view( T3 )
           = Set( T1_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i109 + nvfuser_zero ) ) )] view( T1 ), cache_op=Streaming )
  FOR i113 in iS20{4}:
    i789 = ALLOCATE(buffer=i789, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i789 = 32 * i719;
    FOR threadIdx.x in ithreadIdx.x18{32}:
      IF Inline ( ( ( ( 1024 - ( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i113 + nvfuser_zero ) ) ) ):
        T2_l[( ( 4 * ( ( 8 + i124 ) % 9 ) ) + i113 )] view( T2 )
           = Set( T0_g[( ( ( 1024 + threadIdx.x ) + ( 128 * i124 ) ) + ( 32 * ( i113 + nvfuser_zero ) ) )] view( T0 ), cache_op=Streaming )
  FOR i108 in iS8{4}:
    i937 = ALLOCATE(buffer=i937, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i937 = i935 + i108;
    i1043 = ALLOCATE(buffer=i1043, mem_type=register, size=1, zero_init=false, resets_to_zero=false)
    i1043 = 32 * i993;
    FOR threadIdx.x in ithreadIdx.x6{32}:
      IF Inline ( ( ( ( -( (( (( getMetaData(T0) )).logical_size ))[0] ) ) + threadIdx.x ) + ( 128 * i124 ) ) < ( -( 32 * ( i108 + nvfuser_zero ) ) ) ):
        T4_g[( ( threadIdx.x + ( 128 * i124 ) ) + ( 32 * ( i108 + nvfuser_zero ) ) )] view( T4 )
           = T2_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T2 )
           + T3_l[( ( 4 * ( i124 % 9 ) ) + i108 )] view( T3 );
 (function isUnrolled)
[       OK ] NonTma/CircularBufferingTest.MultipleTensors/stage_9_prefetch_8 (67 ms)
[ RUN      ] NonTma/CircularBufferingTest.NestedTensors/stage_2_prefetch_0
[       OK ] NonTma/CircularBufferingTest.NestedTensors/stage_2_prefetch_0 (91 ms)
[ RUN      ] NonTma/CircularBufferingTest.NestedTensors/stage_5_prefetch_neg4
[       OK ] NonTma/CircularBufferingTest.NestedTensors/stage_5_prefetch_neg4 (102 ms)
[ RUN      ] NonTma/CircularBufferingTest.NestedTensors/stage_5_prefetch_neg1
[       OK ] NonTma/CircularBufferingTest.NestedTensors/stage_5_prefetch_neg1 (152 ms)
[ RUN      ] NonTma/CircularBufferingTest.NestedTensors/stage_5_prefetch_2
[       OK ] NonTma/CircularBufferingTest.NestedTensors/stage_5_prefetch_2 (124 ms)
[ RUN      ] NonTma/CircularBufferingTest.NestedTensors/stage_9_prefetch_neg9
[       OK ] NonTma/CircularBufferingTest.NestedTensors/stage_9_prefetch_neg9 (89 ms)
[ RUN      ] NonTma/CircularBufferingTest.NestedTensors/stage_9_prefetch_neg6
[       OK ] NonTma/CircularBufferingTest.NestedTensors/stage_9_prefetch_neg6 (143 ms)
[ RUN      ] NonTma/CircularBufferingTest.NestedTensors/stage_9_prefetch_neg3
[       OK ] NonTma/CircularBufferingTest.NestedTensors/stage_9_prefetch_neg3 (183 ms)
[ RUN      ] NonTma/CircularBufferingTest.NestedTensors/stage_9_prefetch_0
[       OK ] NonTma/CircularBufferingTest.NestedTensors/stage_9_prefetch_0 (59 ms)
[ RUN      ] NonTma/CircularBufferingTest.NestedTensors/stage_9_prefetch_3
[       OK ] NonTma/CircularBufferingTest.NestedTensors/stage_9_prefetch_3 (64 ms)
[ RUN      ] NonTma/CircularBufferingTest.NestedTensors/stage_9_prefetch_6
[       OK ] NonTma/CircularBufferingTest.NestedTensors/stage_9_prefetch_6 (66 ms)
[ RUN      ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_2_prefetch_neg2
[       OK ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_2_prefetch_neg2 (792 ms)
[ RUN      ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_2_prefetch_1
[       OK ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_2_prefetch_1 (946 ms)
[ RUN      ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_5_prefetch_neg3
[       OK ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_5_prefetch_neg3 (4677 ms)
[ RUN      ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_5_prefetch_0
[       OK ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_5_prefetch_0 (859 ms)
[ RUN      ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_5_prefetch_3
[       OK ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_5_prefetch_3 (7791 ms)
[ RUN      ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_9_prefetch_neg8
[       OK ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_9_prefetch_neg8 (1039 ms)
[ RUN      ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_9_prefetch_neg5
[       OK ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_9_prefetch_neg5 (3161 ms)
[ RUN      ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_9_prefetch_neg2
[       OK ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_9_prefetch_neg2 (3572 ms)
[ RUN      ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_9_prefetch_1
[       OK ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_9_prefetch_1 (123 ms)
[ RUN      ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_9_prefetch_4
[       OK ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_9_prefetch_4 (127 ms)
[ RUN      ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_9_prefetch_7
[       OK ] NonTma/CircularBufferingTest.SmemBlockGemmCache/stage_9_prefetch_7 (127 ms)
[ RUN      ] NonTma/CircularBufferingTest.Vector/stage_2_prefetch_neg1
[       OK ] NonTma/CircularBufferingTest.Vector/stage_2_prefetch_neg1 (108 ms)
[ RUN      ] NonTma/CircularBufferingTest.Vector/stage_5_prefetch_neg5
[       OK ] NonTma/CircularBufferingTest.Vector/stage_5_prefetch_neg5 (109 ms)
[ RUN      ] NonTma/CircularBufferingTest.Vector/stage_5_prefetch_neg2
[       OK ] NonTma/CircularBufferingTest.Vector/stage_5_prefetch_neg2 (122 ms)
[ RUN      ] NonTma/CircularBufferingTest.Vector/stage_5_prefetch_1
[       OK ] NonTma/CircularBufferingTest.Vector/stage_5_prefetch_1 (110 ms)
[ RUN      ] NonTma/CircularBufferingTest.Vector/stage_5_prefetch_4
[       OK ] NonTma/CircularBufferingTest.Vector/stage_5_prefetch_4 (113 ms)
[ RUN      ] NonTma/CircularBufferingTest.Vector/stage_9_prefetch_neg7
[       OK ] NonTma/CircularBufferingTest.Vector/stage_9_prefetch_neg7 (118 ms)
[ RUN      ] NonTma/CircularBufferingTest.Vector/stage_9_prefetch_neg4
[       OK ] NonTma/CircularBufferingTest.Vector/stage_9_prefetch_neg4 (112 ms)
[ RUN      ] NonTma/CircularBufferingTest.Vector/stage_9_prefetch_neg1
[       OK ] NonTma/CircularBufferingTest.Vector/stage_9_prefetch_neg1 (111 ms)
[ RUN      ] NonTma/CircularBufferingTest.Vector/stage_9_prefetch_2
[       OK ] NonTma/CircularBufferingTest.Vector/stage_9_prefetch_2 (59 ms)
[ RUN      ] NonTma/CircularBufferingTest.Vector/stage_9_prefetch_5
[       OK ] NonTma/CircularBufferingTest.Vector/stage_9_prefetch_5 (59 ms)
[ RUN      ] NonTma/CircularBufferingTest.Vector/stage_9_prefetch_8
[       OK ] NonTma/CircularBufferingTest.Vector/stage_9_prefetch_8 (60 ms)
[ RUN      ] NonTma/CircularBufferingTest.CpAsync1/stage_2_prefetch_0
[       OK ] NonTma/CircularBufferingTest.CpAsync1/stage_2_prefetch_0 (88 ms)
[ RUN      ] NonTma/CircularBufferingTest.CpAsync1/stage_5_prefetch_neg4
[       OK ] NonTma/CircularBufferingTest.CpAsync1/stage_5_prefetch_neg4 (88 ms)
[ RUN      ] NonTma/CircularBufferingTest.CpAsync1/stage_5_prefetch_neg1
[       OK ] NonTma/CircularBufferingTest.CpAsync1/stage_5_prefetch_neg1 (106 ms)
[ RUN      ] NonTma/CircularBufferingTest.CpAsync1/stage_5_prefetch_2
[       OK ] NonTma/CircularBufferingTest.CpAsync1/stage_5_prefetch_2 (95 ms)
[ RUN      ] NonTma/CircularBufferingTest.CpAsync1/stage_9_prefetch_neg9
[       OK ] NonTma/CircularBufferingTest.CpAsync1/stage_9_prefetch_neg9 (90 ms)
[ RUN      ] NonTma/CircularBufferingTest.CpAsync1/stage_9_prefetch_neg6
[       OK ] NonTma/CircularBufferingTest.CpAsync1/stage_9_prefetch_neg6 (101 ms)
[ RUN      ] NonTma/CircularBufferingTest.CpAsync1/stage_9_prefetch_neg3
[       OK ] NonTma/CircularBufferingTest.CpAsync1/stage_9_prefetch_neg3 (114 ms)
[ RUN      ] NonTma/CircularBufferingTest.CpAsync1/stage_9_prefetch_0
[       OK ] NonTma/CircularBufferingTest.CpAsync1/stage_9_prefetch_0 (57 ms)
[ RUN      ] NonTma/CircularBufferingTest.CpAsync1/stage_9_prefetch_3
[       OK ] NonTma/CircularBufferingTest.CpAsync1/stage_9_prefetch_3 (60 ms)
[ RUN      ] NonTma/CircularBufferingTest.CpAsync1/stage_9_prefetch_6
[       OK ] NonTma/CircularBufferingTest.CpAsync1/stage_9_prefetch_6 (60 ms)
[ RUN      ] NonTma/CircularBufferingTest.CpAsync2/stage_2_prefetch_neg2
[       OK ] NonTma/CircularBufferingTest.CpAsync2/stage_2_prefetch_neg2 (89 ms)
[ RUN      ] NonTma/CircularBufferingTest.CpAsync2/stage_2_prefetch_1
[       OK ] NonTma/CircularBufferingTest.CpAsync2/stage_2_prefetch_1 (94 ms)
[ RUN      ] NonTma/CircularBufferingTest.CpAsync2/stage_5_prefetch_neg3
[       OK ] NonTma/CircularBufferingTest.CpAsync2/stage_5_prefetch_neg3 (106 ms)
[ RUN      ] NonTma/CircularBufferingTest.CpAsync2/stage_5_prefetch_0
[       OK ] NonTma/CircularBufferingTest.CpAsync2/stage_5_prefetch_0 (89 ms)
[ RUN      ] NonTma/CircularBufferingTest.CpAsync2/stage_5_prefetch_3
[       OK ] NonTma/CircularBufferingTest.CpAsync2/stage_5_prefetch_3 (117 ms)
[ RUN      ] NonTma/CircularBufferingTest.CpAsync2/stage_9_prefetch_neg8
[       OK ] NonTma/CircularBufferingTest.CpAsync2/stage_9_prefetch_neg8 (95 ms)
[ RUN      ] NonTma/CircularBufferingTest.CpAsync2/stage_9_prefetch_neg5
[       OK ] NonTma/CircularBufferingTest.CpAsync2/stage_9_prefetch_neg5 (127 ms)
[ RUN      ] NonTma/CircularBufferingTest.CpAsync2/stage_9_prefetch_neg2
[       OK ] NonTma/CircularBufferingTest.CpAsync2/stage_9_prefetch_neg2 (156 ms)
[ RUN      ] NonTma/CircularBufferingTest.CpAsync2/stage_9_prefetch_1
[       OK ] NonTma/CircularBufferingTest.CpAsync2/stage_9_prefetch_1 (61 ms)
[ RUN      ] NonTma/CircularBufferingTest.CpAsync2/stage_9_prefetch_4
[       OK ] NonTma/CircularBufferingTest.CpAsync2/stage_9_prefetch_4 (61 ms)
[ RUN      ] NonTma/CircularBufferingTest.CpAsync2/stage_9_prefetch_7
[       OK ] NonTma/CircularBufferingTest.CpAsync2/stage_9_prefetch_7 (62 ms)
[ RUN      ] NonTma/CircularBufferingTest.NoSync/stage_2_prefetch_neg1
[       OK ] NonTma/CircularBufferingTest.NoSync/stage_2_prefetch_neg1 (102 ms)
[ RUN      ] NonTma/CircularBufferingTest.NoSync/stage_5_prefetch_neg5
[       OK ] NonTma/CircularBufferingTest.NoSync/stage_5_prefetch_neg5 (94 ms)
[ RUN      ] NonTma/CircularBufferingTest.NoSync/stage_5_prefetch_neg2
[       OK ] NonTma/CircularBufferingTest.NoSync/stage_5_prefetch_neg2 (132 ms)
[ RUN      ] NonTma/CircularBufferingTest.NoSync/stage_5_prefetch_1
[       OK ] NonTma/CircularBufferingTest.NoSync/stage_5_prefetch_1 (103 ms)
[ RUN      ] NonTma/CircularBufferingTest.NoSync/stage_5_prefetch_4
[       OK ] NonTma/CircularBufferingTest.NoSync/stage_5_prefetch_4 (143 ms)
[ RUN      ] NonTma/CircularBufferingTest.NoSync/stage_9_prefetch_neg7
[       OK ] NonTma/CircularBufferingTest.NoSync/stage_9_prefetch_neg7 (118 ms)
[ RUN      ] NonTma/CircularBufferingTest.NoSync/stage_9_prefetch_neg4
[       OK ] NonTma/CircularBufferingTest.NoSync/stage_9_prefetch_neg4 (155 ms)
[ RUN      ] NonTma/CircularBufferingTest.NoSync/stage_9_prefetch_neg1
[       OK ] NonTma/CircularBufferingTest.NoSync/stage_9_prefetch_neg1 (193 ms)
[ RUN      ] NonTma/CircularBufferingTest.NoSync/stage_9_prefetch_2
[       OK ] NonTma/CircularBufferingTest.NoSync/stage_9_prefetch_2 (66 ms)
[ RUN      ] NonTma/CircularBufferingTest.NoSync/stage_9_prefetch_5
[       OK ] NonTma/CircularBufferingTest.NoSync/stage_9_prefetch_5 (66 ms)
[ RUN      ] NonTma/CircularBufferingTest.NoSync/stage_9_prefetch_8
[       OK ] NonTma/CircularBufferingTest.NoSync/stage_9_prefetch_8 (67 ms)
[----------] 150 tests from NonTma/CircularBufferingTest (37628 ms total)

[----------] 528 tests from Hopper/TmaCircularBufferingTest
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1322: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (100 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (57 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (95 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (109 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (105 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (106 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (96 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (100 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (111 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (101 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (101 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1322: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1322: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (111 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (113 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (113 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (59 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile (100 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (101 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (58 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1322: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (108 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (113 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (106 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (115 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (118 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1322: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (131 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1384: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (105 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (62 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (109 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (128 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (125 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (123 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (110 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (129 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (131 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (117 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (116 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1384: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1384: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (128 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (132 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (130 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (63 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile (101 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (119 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (64 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1384: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (153 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (130 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (124 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (135 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (179 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1384: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (195 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1457: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (102 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (61 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (106 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (126 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (122 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (121 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (108 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (124 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (130 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (113 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (112 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1457: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1457: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (126 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (130 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (129 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (64 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile (97 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (113 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (63 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1457: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (141 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (125 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (119 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (132 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (157 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1457: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (173 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (108 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (67 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (111 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (123 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (110 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (126 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (115 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (117 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (125 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (131 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile (102 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (68 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (137 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (123 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (156 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1512: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (174 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1611: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (107 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (66 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (104 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (119 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (116 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (118 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (104 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (115 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (122 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (108 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (110 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1611: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1611: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (120 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (125 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (125 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (65 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile (112 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (110 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (68 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1611: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (118 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (120 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (118 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (125 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (129 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1611: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (144 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1661: Skipped
Needs shared memory predicate, but current needsSharedMemoryPredicate() returns false

[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1759: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (110 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (65 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (115 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (127 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (124 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (124 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (115 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (128 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (129 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (119 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (119 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1759: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1759: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (126 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (130 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (127 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (67 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile (122 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (122 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (66 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1759: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (135 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (127 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (123 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (131 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (134 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1759: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (143 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1830: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (105 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (59 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (94 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (112 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (109 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (107 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (97 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (102 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (116 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (103 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (101 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1830: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1830: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (111 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (116 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (113 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (62 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile (99 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (1 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (103 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (62 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1830: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (113 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (113 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (107 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (118 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (120 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1830: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (134 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1933: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (183 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (200 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (263 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1933: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1933: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile (192 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1933: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (261 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1878: Skipped
Bdimx is dynamic, Warp Specialization is disabled.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (248 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:1933: Skipped
1D TMA load can only be used with WarpSpecialized circular buffer.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (302 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2037: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (263 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2037: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (98 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2037: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (3 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (189 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2037: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2022: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2022: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (293 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2022: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2022: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2037: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2022: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2037: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (307 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2022: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (303 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2037: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (304 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2037: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2022: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2037: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2022: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2037: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (226 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2037: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (234 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2037: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2022: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2022: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2022: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2022: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (100 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2037: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile (260 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2037: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (194 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2022: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2022: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2037: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (492 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2022: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2022: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2022: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (784 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2037: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (816 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2183: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (264 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2183: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (98 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2183: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (185 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2183: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2168: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2168: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (290 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2168: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2168: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2183: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2168: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2183: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (302 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2168: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (301 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2183: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (304 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2183: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2168: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2183: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2168: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2183: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (225 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2183: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (232 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2183: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2168: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2168: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2168: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2168: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (100 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2183: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile (259 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2183: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (2 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile (191 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2168: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2168: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2183: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (487 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2168: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2168: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2168: Skipped
Warp Specialization with TIDx used for both computation and load, requires TIDx to be a multiple of 128.

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile (785 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
/opt/pytorch/nvfuser/tests/cpp/test_circular_buffering.cpp:2183: Skipped
LoadStoreOpType::CpAsyncBulk only supports 1D TMA

[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk (0 ms)
[ RUN      ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[       OK ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile (813 ms)
[----------] 528 tests from Hopper/TmaCircularBufferingTest (29713 ms total)

[----------] 3 tests from Hopper/TmaRegisterSharing
[ RUN      ] Hopper/TmaRegisterSharing.CtaShapeShmoo/cta_32_4_2_pt_threadIdx_z
[       OK ] Hopper/TmaRegisterSharing.CtaShapeShmoo/cta_32_4_2_pt_threadIdx_z (113 ms)
[ RUN      ] Hopper/TmaRegisterSharing.CtaShapeShmoo/cta_128_2_1_pt_threadIdx_z
[       OK ] Hopper/TmaRegisterSharing.CtaShapeShmoo/cta_128_2_1_pt_threadIdx_z (2 ms)
[ RUN      ] Hopper/TmaRegisterSharing.CtaShapeShmoo/cta_256_1_1_pt_threadIdx_z
[       OK ] Hopper/TmaRegisterSharing.CtaShapeShmoo/cta_256_1_1_pt_threadIdx_z (1 ms)
[----------] 3 tests from Hopper/TmaRegisterSharing (117 ms total)

[----------] 1 test from SiblingPingPongCircularBuffering
[ RUN      ] SiblingPingPongCircularBuffering.TwoTmaLoads/IdModel_1_stage_slice_position_3
[       OK ] SiblingPingPongCircularBuffering.TwoTmaLoads/IdModel_1_stage_slice_position_3 (345 ms)
[----------] 1 test from SiblingPingPongCircularBuffering (345 ms total)

[----------] 1 test from InnerOuterReshapeTest
[ RUN      ] InnerOuterReshapeTest.ReshapeOuterDimTrueOrFalse/true
[       OK ] InnerOuterReshapeTest.ReshapeOuterDimTrueOrFalse/true (494 ms)
[----------] 1 test from InnerOuterReshapeTest (494 ms total)

[----------] 80 tests from TmaWarpSpecializedTest
[ RUN      ] TmaWarpSpecializedTest.SimpleFusion/contig_0_dtype_float_batch_2048_hidden_1024
[       OK ] TmaWarpSpecializedTest.SimpleFusion/contig_0_dtype_float_batch_2048_hidden_1024 (418 ms)
[ RUN      ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype___bfloat_batch_2048_hidden_1280
[       OK ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype___bfloat_batch_2048_hidden_1280 (522 ms)
[ RUN      ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype_float_batch_2048_hidden_1792
[       OK ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype_float_batch_2048_hidden_1792 (453 ms)
[ RUN      ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype___bfloat_batch_2048_hidden_2048
[       OK ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype___bfloat_batch_2048_hidden_2048 (497 ms)
[ RUN      ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype_float_batch_2048_hidden_2560
[       OK ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype_float_batch_2048_hidden_2560 (440 ms)
[ RUN      ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype___bfloat_batch_2048_hidden_2816
[       OK ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype___bfloat_batch_2048_hidden_2816 (611 ms)
[ RUN      ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype_float_batch_2048_hidden_3328
[       OK ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype_float_batch_2048_hidden_3328 (458 ms)
[ RUN      ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype___bfloat_batch_2048_hidden_3584
[       OK ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype___bfloat_batch_2048_hidden_3584 (512 ms)
[ RUN      ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype_float_batch_2048_hidden_4096
[       OK ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype_float_batch_2048_hidden_4096 (483 ms)
[ RUN      ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype___bfloat_batch_2048_hidden_4352
[       OK ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype___bfloat_batch_2048_hidden_4352 (669 ms)
[ RUN      ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype_float_batch_2048_hidden_4864
[       OK ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype_float_batch_2048_hidden_4864 (507 ms)
[ RUN      ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype___bfloat_batch_2048_hidden_5120
[       OK ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype___bfloat_batch_2048_hidden_5120 (634 ms)
[ RUN      ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype_float_batch_2048_hidden_5632
[       OK ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype_float_batch_2048_hidden_5632 (514 ms)
[ RUN      ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype___bfloat_batch_2048_hidden_5888
[       OK ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype___bfloat_batch_2048_hidden_5888 (638 ms)
[ RUN      ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype_float_batch_2048_hidden_6400
[       OK ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype_float_batch_2048_hidden_6400 (570 ms)
[ RUN      ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype___bfloat_batch_2048_hidden_6656
[       OK ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype___bfloat_batch_2048_hidden_6656 (616 ms)
[ RUN      ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype_float_batch_2048_hidden_7168
[       OK ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype_float_batch_2048_hidden_7168 (532 ms)
[ RUN      ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype___bfloat_batch_2048_hidden_7424
[       OK ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype___bfloat_batch_2048_hidden_7424 (607 ms)
[ RUN      ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype_float_batch_2048_hidden_7936
[       OK ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype_float_batch_2048_hidden_7936 (562 ms)
[ RUN      ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype___bfloat_batch_2048_hidden_8192
[       OK ] TmaWarpSpecializedTest.SimpleFusion/contig_1_dtype___bfloat_batch_2048_hidden_8192 (589 ms)
[ RUN      ] TmaWarpSpecializedTest.RMSNormBwd/contig_0_dtype_float_batch_2048_hidden_1024
[       OK ] TmaWarpSpecializedTest.RMSNormBwd/contig_0_dtype_float_batch_2048_hidden_1024 (590 ms)
[ RUN      ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_1280
[       OK ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_1280 (724 ms)
[ RUN      ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype_float_batch_2048_hidden_1792
[       OK ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype_float_batch_2048_hidden_1792 (613 ms)
[ RUN      ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_2048
[       OK ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_2048 (683 ms)
[ RUN      ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype_float_batch_2048_hidden_2560
[       OK ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype_float_batch_2048_hidden_2560 (613 ms)
[ RUN      ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_2816
[       OK ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_2816 (702 ms)
[ RUN      ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype_float_batch_2048_hidden_3328
[       OK ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype_float_batch_2048_hidden_3328 (590 ms)
[ RUN      ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_3584
[       OK ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_3584 (748 ms)
[ RUN      ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype_float_batch_2048_hidden_4096
[       OK ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype_float_batch_2048_hidden_4096 (623 ms)
[ RUN      ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_4352
[       OK ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_4352 (752 ms)
[ RUN      ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype_float_batch_2048_hidden_4864
[       OK ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype_float_batch_2048_hidden_4864 (693 ms)
[ RUN      ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_5120
[       OK ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_5120 (733 ms)
[ RUN      ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype_float_batch_2048_hidden_5632
[       OK ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype_float_batch_2048_hidden_5632 (653 ms)
[ RUN      ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_5888
[       OK ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_5888 (785 ms)
[ RUN      ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype_float_batch_2048_hidden_6400
[       OK ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype_float_batch_2048_hidden_6400 (748 ms)
[ RUN      ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_6656
[       OK ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_6656 (830 ms)
[ RUN      ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype_float_batch_2048_hidden_7168
[       OK ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype_float_batch_2048_hidden_7168 (689 ms)
[ RUN      ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_7424
[       OK ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_7424 (788 ms)
[ RUN      ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype_float_batch_2048_hidden_7936
[       OK ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype_float_batch_2048_hidden_7936 (776 ms)
[ RUN      ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_8192
[       OK ] TmaWarpSpecializedTest.RMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_8192 (760 ms)
[ RUN      ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_0_dtype_float_batch_2048_hidden_1024
[       OK ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_0_dtype_float_batch_2048_hidden_1024 (600 ms)
[ RUN      ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_1280
[       OK ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_1280 (746 ms)
[ RUN      ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype_float_batch_2048_hidden_1792
[       OK ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype_float_batch_2048_hidden_1792 (629 ms)
[ RUN      ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_2048
[       OK ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_2048 (698 ms)
[ RUN      ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype_float_batch_2048_hidden_2560
[       OK ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype_float_batch_2048_hidden_2560 (620 ms)
[ RUN      ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_2816
[       OK ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_2816 (712 ms)
[ RUN      ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype_float_batch_2048_hidden_3328
[       OK ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype_float_batch_2048_hidden_3328 (619 ms)
[ RUN      ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_3584
[       OK ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_3584 (756 ms)
[ RUN      ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype_float_batch_2048_hidden_4096
[       OK ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype_float_batch_2048_hidden_4096 (654 ms)
[ RUN      ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_4352
[       OK ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_4352 (783 ms)
[ RUN      ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype_float_batch_2048_hidden_4864
[       OK ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype_float_batch_2048_hidden_4864 (738 ms)
[ RUN      ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_5120
[       OK ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_5120 (771 ms)
[ RUN      ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype_float_batch_2048_hidden_5632
[       OK ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype_float_batch_2048_hidden_5632 (687 ms)
[ RUN      ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_5888
[       OK ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_5888 (818 ms)
[ RUN      ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype_float_batch_2048_hidden_6400
[       OK ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype_float_batch_2048_hidden_6400 (804 ms)
[ RUN      ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_6656
[       OK ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_6656 (851 ms)
[ RUN      ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype_float_batch_2048_hidden_7168
[       OK ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype_float_batch_2048_hidden_7168 (708 ms)
[ RUN      ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_7424
[       OK ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_7424 (818 ms)
[ RUN      ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype_float_batch_2048_hidden_7936
[       OK ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype_float_batch_2048_hidden_7936 (828 ms)
[ RUN      ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_8192
[       OK ] TmaWarpSpecializedTest.ThunderRMSNormBwd/contig_1_dtype___bfloat_batch_2048_hidden_8192 (782 ms)
[ RUN      ] TmaWarpSpecializedTest.LayerNormBackward/contig_0_dtype_float_batch_2048_hidden_1024
[       OK ] TmaWarpSpecializedTest.LayerNormBackward/contig_0_dtype_float_batch_2048_hidden_1024 (859 ms)
[ RUN      ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype___bfloat_batch_2048_hidden_1280
[       OK ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype___bfloat_batch_2048_hidden_1280 (1033 ms)
[ RUN      ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype_float_batch_2048_hidden_1792
[       OK ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype_float_batch_2048_hidden_1792 (886 ms)
[ RUN      ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype___bfloat_batch_2048_hidden_2048
[       OK ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype___bfloat_batch_2048_hidden_2048 (985 ms)
[ RUN      ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype_float_batch_2048_hidden_2560
[       OK ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype_float_batch_2048_hidden_2560 (805 ms)
[ RUN      ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype___bfloat_batch_2048_hidden_2816
[       OK ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype___bfloat_batch_2048_hidden_2816 (1002 ms)
[ RUN      ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype_float_batch_2048_hidden_3328
[       OK ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype_float_batch_2048_hidden_3328 (889 ms)
[ RUN      ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype___bfloat_batch_2048_hidden_3584
[       OK ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype___bfloat_batch_2048_hidden_3584 (913 ms)
[ RUN      ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype_float_batch_2048_hidden_4096
[       OK ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype_float_batch_2048_hidden_4096 (999 ms)
[ RUN      ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype___bfloat_batch_2048_hidden_4352
[       OK ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype___bfloat_batch_2048_hidden_4352 (1140 ms)
[ RUN      ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype_float_batch_2048_hidden_4864
[       OK ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype_float_batch_2048_hidden_4864 (1096 ms)
[ RUN      ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype___bfloat_batch_2048_hidden_5120
[       OK ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype___bfloat_batch_2048_hidden_5120 (1113 ms)
[ RUN      ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype_float_batch_2048_hidden_5632
[       OK ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype_float_batch_2048_hidden_5632 (1046 ms)
[ RUN      ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype___bfloat_batch_2048_hidden_5888
[       OK ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype___bfloat_batch_2048_hidden_5888 (1142 ms)
[ RUN      ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype_float_batch_2048_hidden_6400
[       OK ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype_float_batch_2048_hidden_6400 (1181 ms)
[ RUN      ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype___bfloat_batch_2048_hidden_6656
[       OK ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype___bfloat_batch_2048_hidden_6656 (1240 ms)
[ RUN      ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype_float_batch_2048_hidden_7168
[       OK ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype_float_batch_2048_hidden_7168 (1055 ms)
[ RUN      ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype___bfloat_batch_2048_hidden_7424
[       OK ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype___bfloat_batch_2048_hidden_7424 (647 ms)
[ RUN      ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype_float_batch_2048_hidden_7936
[       OK ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype_float_batch_2048_hidden_7936 (644 ms)
[ RUN      ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype___bfloat_batch_2048_hidden_8192
[       OK ] TmaWarpSpecializedTest.LayerNormBackward/contig_1_dtype___bfloat_batch_2048_hidden_8192 (647 ms)
[----------] 80 tests from TmaWarpSpecializedTest (58907 ms total)

[----------] 2 tests from NonConcretizedDomainTest
[ RUN      ] NonConcretizedDomainTest.OnNonReductionTv/non_concretized_pos_2
[       OK ] NonConcretizedDomainTest.OnNonReductionTv/non_concretized_pos_2 (3378 ms)
[ RUN      ] NonConcretizedDomainTest.OnReductionTv/non_concretized_pos_2
[       OK ] NonConcretizedDomainTest.OnReductionTv/non_concretized_pos_2 (255 ms)
[----------] 2 tests from NonConcretizedDomainTest (3634 ms total)

[----------] 10 tests from Fp4CastTest
[ RUN      ] Fp4CastTest.Fp4ToHighPrecision/float_Vectorize_8
/opt/pytorch/nvfuser/tests/cpp/test_gpu1.cpp:2808: Skipped
Requires GPU capability above 10.0 to run.


[  SKIPPED ] Fp4CastTest.Fp4ToHighPrecision/float_Vectorize_8 (0 ms)
[ RUN      ] Fp4CastTest.Fp4ToHighPrecision/double_Vectorize_4
/opt/pytorch/nvfuser/tests/cpp/test_gpu1.cpp:2808: Skipped
Requires GPU capability above 10.0 to run.


[  SKIPPED ] Fp4CastTest.Fp4ToHighPrecision/double_Vectorize_4 (0 ms)
[ RUN      ] Fp4CastTest.Fp4ToHighPrecision/__bfloat_Vectorize_2
/opt/pytorch/nvfuser/tests/cpp/test_gpu1.cpp:2808: Skipped
Requires GPU capability above 10.0 to run.


[  SKIPPED ] Fp4CastTest.Fp4ToHighPrecision/__bfloat_Vectorize_2 (0 ms)
[ RUN      ] Fp4CastTest.Fp4ToHighPrecision/__bfloat_Vectorize_16
/opt/pytorch/nvfuser/tests/cpp/test_gpu1.cpp:2808: Skipped
Requires GPU capability above 10.0 to run.


[  SKIPPED ] Fp4CastTest.Fp4ToHighPrecision/__bfloat_Vectorize_16 (0 ms)
[ RUN      ] Fp4CastTest.Fp4ToHighPrecision/__half_Vectorize_8
/opt/pytorch/nvfuser/tests/cpp/test_gpu1.cpp:2808: Skipped
Requires GPU capability above 10.0 to run.


[  SKIPPED ] Fp4CastTest.Fp4ToHighPrecision/__half_Vectorize_8 (0 ms)
[ RUN      ] Fp4CastTest.HighPrecisionToFp4/float_Vectorize_4
/opt/pytorch/nvfuser/tests/cpp/test_gpu1.cpp:2808: Skipped
Requires GPU capability above 10.0 to run.


[  SKIPPED ] Fp4CastTest.HighPrecisionToFp4/float_Vectorize_4 (0 ms)
[ RUN      ] Fp4CastTest.HighPrecisionToFp4/double_Vectorize_2
/opt/pytorch/nvfuser/tests/cpp/test_gpu1.cpp:2808: Skipped
Requires GPU capability above 10.0 to run.


[  SKIPPED ] Fp4CastTest.HighPrecisionToFp4/double_Vectorize_2 (0 ms)
[ RUN      ] Fp4CastTest.HighPrecisionToFp4/double_Vectorize_16
/opt/pytorch/nvfuser/tests/cpp/test_gpu1.cpp:2808: Skipped
Requires GPU capability above 10.0 to run.


[  SKIPPED ] Fp4CastTest.HighPrecisionToFp4/double_Vectorize_16 (0 ms)
[ RUN      ] Fp4CastTest.HighPrecisionToFp4/__bfloat_Vectorize_8
/opt/pytorch/nvfuser/tests/cpp/test_gpu1.cpp:2808: Skipped
Requires GPU capability above 10.0 to run.


[  SKIPPED ] Fp4CastTest.HighPrecisionToFp4/__bfloat_Vectorize_8 (0 ms)
[ RUN      ] Fp4CastTest.HighPrecisionToFp4/__half_Vectorize_4
/opt/pytorch/nvfuser/tests/cpp/test_gpu1.cpp:2808: Skipped
Requires GPU capability above 10.0 to run.


[  SKIPPED ] Fp4CastTest.HighPrecisionToFp4/__half_Vectorize_4 (0 ms)
[----------] 10 tests from Fp4CastTest (0 ms total)

[----------] 2 tests from AdvancedDtypeTest
[ RUN      ] AdvancedDtypeTest.CopyKernelPointer/DynamicShape0
[       OK ] AdvancedDtypeTest.CopyKernelPointer/DynamicShape0 (68 ms)
[ RUN      ] AdvancedDtypeTest.CopyKernel3ByteArray/DynamicShape1
[       OK ] AdvancedDtypeTest.CopyKernel3ByteArray/DynamicShape1 (69 ms)
[----------] 2 tests from AdvancedDtypeTest (138 ms total)

[----------] 8 tests from Float4E2m1ManualScheduleTestAllArch
[ RUN      ] Float4E2m1ManualScheduleTestAllArch.CopyKernelContiguous/Vectorize2_DynamicShape0
[       OK ] Float4E2m1ManualScheduleTestAllArch.CopyKernelContiguous/Vectorize2_DynamicShape0 (72 ms)
[ RUN      ] Float4E2m1ManualScheduleTestAllArch.CopyKernelContiguous/Vectorize4_DynamicShape1
[       OK ] Float4E2m1ManualScheduleTestAllArch.CopyKernelContiguous/Vectorize4_DynamicShape1 (73 ms)
[ RUN      ] Float4E2m1ManualScheduleTestAllArch.CopyKernelContiguous/Vectorize16_DynamicShape0
[       OK ] Float4E2m1ManualScheduleTestAllArch.CopyKernelContiguous/Vectorize16_DynamicShape0 (73 ms)
[ RUN      ] Float4E2m1ManualScheduleTestAllArch.CopyKernelContiguous/Vectorize32_DynamicShape1
[       OK ] Float4E2m1ManualScheduleTestAllArch.CopyKernelContiguous/Vectorize32_DynamicShape1 (73 ms)
[ RUN      ] Float4E2m1ManualScheduleTestAllArch.CopyKernelDiscontiguous/Vectorize2_DynamicShape0
[       OK ] Float4E2m1ManualScheduleTestAllArch.CopyKernelDiscontiguous/Vectorize2_DynamicShape0 (76 ms)
[ RUN      ] Float4E2m1ManualScheduleTestAllArch.CopyKernelDiscontiguous/Vectorize4_DynamicShape1
[       OK ] Float4E2m1ManualScheduleTestAllArch.CopyKernelDiscontiguous/Vectorize4_DynamicShape1 (78 ms)
[ RUN      ] Float4E2m1ManualScheduleTestAllArch.CopyKernelDiscontiguous/Vectorize16_DynamicShape0
[       OK ] Float4E2m1ManualScheduleTestAllArch.CopyKernelDiscontiguous/Vectorize16_DynamicShape0 (77 ms)
[ RUN      ] Float4E2m1ManualScheduleTestAllArch.CopyKernelDiscontiguous/Vectorize32_DynamicShape1
[       OK ] Float4E2m1ManualScheduleTestAllArch.CopyKernelDiscontiguous/Vectorize32_DynamicShape1 (80 ms)
[----------] 8 tests from Float4E2m1ManualScheduleTestAllArch (605 ms total)

[----------] 42 tests from ReductionNoODim
[ RUN      ] ReductionNoODim.Test/dtype_double_rsize_4
[       OK ] ReductionNoODim.Test/dtype_double_rsize_4 (87 ms)
[ RUN      ] ReductionNoODim.Test/dtype_double_rsize_32
[       OK ] ReductionNoODim.Test/dtype_double_rsize_32 (58 ms)
[ RUN      ] ReductionNoODim.Test/dtype_double_rsize_256
[       OK ] ReductionNoODim.Test/dtype_double_rsize_256 (97 ms)
[ RUN      ] ReductionNoODim.Test/dtype_double_rsize_2048
[       OK ] ReductionNoODim.Test/dtype_double_rsize_2048 (124 ms)
[ RUN      ] ReductionNoODim.Test/dtype_double_rsize_16384
[       OK ] ReductionNoODim.Test/dtype_double_rsize_16384 (72 ms)
[ RUN      ] ReductionNoODim.Test/dtype_double_rsize_131072
[       OK ] ReductionNoODim.Test/dtype_double_rsize_131072 (72 ms)
[ RUN      ] ReductionNoODim.Test/dtype_double_rsize_1048576
[       OK ] ReductionNoODim.Test/dtype_double_rsize_1048576 (73 ms)
[ RUN      ] ReductionNoODim.Test/dtype_float_rsize_4
[       OK ] ReductionNoODim.Test/dtype_float_rsize_4 (89 ms)
[ RUN      ] ReductionNoODim.Test/dtype_float_rsize_32
[       OK ] ReductionNoODim.Test/dtype_float_rsize_32 (58 ms)
[ RUN      ] ReductionNoODim.Test/dtype_float_rsize_256
[       OK ] ReductionNoODim.Test/dtype_float_rsize_256 (96 ms)
[ RUN      ] ReductionNoODim.Test/dtype_float_rsize_2048
[       OK ] ReductionNoODim.Test/dtype_float_rsize_2048 (99 ms)
[ RUN      ] ReductionNoODim.Test/dtype_float_rsize_16384
[       OK ] ReductionNoODim.Test/dtype_float_rsize_16384 (126 ms)
[ RUN      ] ReductionNoODim.Test/dtype_float_rsize_131072
[       OK ] ReductionNoODim.Test/dtype_float_rsize_131072 (71 ms)
[ RUN      ] ReductionNoODim.Test/dtype_float_rsize_1048576
[       OK ] ReductionNoODim.Test/dtype_float_rsize_1048576 (72 ms)
[ RUN      ] ReductionNoODim.Test/dtype___half_rsize_4
[       OK ] ReductionNoODim.Test/dtype___half_rsize_4 (89 ms)
[ RUN      ] ReductionNoODim.Test/dtype___half_rsize_32
[       OK ] ReductionNoODim.Test/dtype___half_rsize_32 (89 ms)
[ RUN      ] ReductionNoODim.Test/dtype___half_rsize_256
[       OK ] ReductionNoODim.Test/dtype___half_rsize_256 (102 ms)
[ RUN      ] ReductionNoODim.Test/dtype___half_rsize_2048
[       OK ] ReductionNoODim.Test/dtype___half_rsize_2048 (109 ms)
[ RUN      ] ReductionNoODim.Test/dtype___half_rsize_16384
[       OK ] ReductionNoODim.Test/dtype___half_rsize_16384 (139 ms)
[ RUN      ] ReductionNoODim.Test/dtype___half_rsize_131072
[       OK ] ReductionNoODim.Test/dtype___half_rsize_131072 (75 ms)
[ RUN      ] ReductionNoODim.Test/dtype___half_rsize_1048576
[       OK ] ReductionNoODim.Test/dtype___half_rsize_1048576 (75 ms)
[ RUN      ] ReductionNoODim.Test/dtype_std__complex_float__rsize_4
[W122 09:14:30.662443071 Copy.cpp:308] Warning: Casting complex values to real discards the imaginary part (function operator())
[       OK ] ReductionNoODim.Test/dtype_std__complex_float__rsize_4 (92 ms)
[ RUN      ] ReductionNoODim.Test/dtype_std__complex_float__rsize_32
[       OK ] ReductionNoODim.Test/dtype_std__complex_float__rsize_32 (59 ms)
[ RUN      ] ReductionNoODim.Test/dtype_std__complex_float__rsize_256
[       OK ] ReductionNoODim.Test/dtype_std__complex_float__rsize_256 (102 ms)
[ RUN      ] ReductionNoODim.Test/dtype_std__complex_float__rsize_2048
[       OK ] ReductionNoODim.Test/dtype_std__complex_float__rsize_2048 (132 ms)
[ RUN      ] ReductionNoODim.Test/dtype_std__complex_float__rsize_16384
[       OK ] ReductionNoODim.Test/dtype_std__complex_float__rsize_16384 (73 ms)
[ RUN      ] ReductionNoODim.Test/dtype_std__complex_float__rsize_131072
[       OK ] ReductionNoODim.Test/dtype_std__complex_float__rsize_131072 (72 ms)
[ RUN      ] ReductionNoODim.Test/dtype_std__complex_float__rsize_1048576
[       OK ] ReductionNoODim.Test/dtype_std__complex_float__rsize_1048576 (73 ms)
[ RUN      ] ReductionNoODim.Test/dtype_std__complex_double__rsize_4
[       OK ] ReductionNoODim.Test/dtype_std__complex_double__rsize_4 (94 ms)
[ RUN      ] ReductionNoODim.Test/dtype_std__complex_double__rsize_32
[       OK ] ReductionNoODim.Test/dtype_std__complex_double__rsize_32 (59 ms)
[ RUN      ] ReductionNoODim.Test/dtype_std__complex_double__rsize_256
[       OK ] ReductionNoODim.Test/dtype_std__complex_double__rsize_256 (101 ms)
[ RUN      ] ReductionNoODim.Test/dtype_std__complex_double__rsize_2048
[       OK ] ReductionNoODim.Test/dtype_std__complex_double__rsize_2048 (132 ms)
[ RUN      ] ReductionNoODim.Test/dtype_std__complex_double__rsize_16384
[       OK ] ReductionNoODim.Test/dtype_std__complex_double__rsize_16384 (72 ms)
[ RUN      ] ReductionNoODim.Test/dtype_std__complex_double__rsize_131072
[       OK ] ReductionNoODim.Test/dtype_std__complex_double__rsize_131072 (72 ms)
[ RUN      ] ReductionNoODim.Test/dtype_std__complex_double__rsize_1048576
[       OK ] ReductionNoODim.Test/dtype_std__complex_double__rsize_1048576 (72 ms)
[ RUN      ] ReductionNoODim.Test/dtype___bfloat_rsize_4
[       OK ] ReductionNoODim.Test/dtype___bfloat_rsize_4 (89 ms)
[ RUN      ] ReductionNoODim.Test/dtype___bfloat_rsize_32
[       OK ] ReductionNoODim.Test/dtype___bfloat_rsize_32 (88 ms)
[ RUN      ] ReductionNoODim.Test/dtype___bfloat_rsize_256
[       OK ] ReductionNoODim.Test/dtype___bfloat_rsize_256 (101 ms)
[ RUN      ] ReductionNoODim.Test/dtype___bfloat_rsize_2048
[       OK ] ReductionNoODim.Test/dtype___bfloat_rsize_2048 (109 ms)
[ RUN      ] ReductionNoODim.Test/dtype___bfloat_rsize_16384
[       OK ] ReductionNoODim.Test/dtype___bfloat_rsize_16384 (139 ms)
[ RUN      ] ReductionNoODim.Test/dtype___bfloat_rsize_131072
[       OK ] ReductionNoODim.Test/dtype___bfloat_rsize_131072 (75 ms)
[ RUN      ] ReductionNoODim.Test/dtype___bfloat_rsize_1048576
[       OK ] ReductionNoODim.Test/dtype___bfloat_rsize_1048576 (76 ms)
[----------] 42 tests from ReductionNoODim (3779 ms total)

[----------] 168 tests from ReductionWithIterDim
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_1_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_1_isize_320_raxis_0 (98 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_2_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_2_isize_160_raxis_1 (94 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_4_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_4_isize_160_raxis_0 (96 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_4_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_4_isize_320_raxis_1 (93 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_8_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_8_isize_320_raxis_0 (120 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_16_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_16_isize_160_raxis_1 (61 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_32_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_32_isize_160_raxis_0 (128 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_32_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_32_isize_320_raxis_1 (61 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_64_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_64_isize_320_raxis_0 (152 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_128_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_128_isize_160_raxis_1 (62 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_256_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_256_isize_160_raxis_0 (71 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_256_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_256_isize_320_raxis_1 (105 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_512_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_512_isize_320_raxis_0 (152 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_1024_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_1024_isize_160_raxis_1 (100 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_2048_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_2048_isize_160_raxis_0 (225 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_2048_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_2048_isize_320_raxis_1 (64 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_4096_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_4096_isize_320_raxis_0 (225 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_8192_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_8192_isize_160_raxis_1 (63 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_16384_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_16384_isize_160_raxis_0 (102 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_16384_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_16384_isize_320_raxis_1 (64 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_32768_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_32768_isize_320_raxis_0 (104 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_65536_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_65536_isize_160_raxis_1 (68 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_131072_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_131072_isize_160_raxis_0 (160 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_131072_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_131072_isize_320_raxis_1 (64 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_262144_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_262144_isize_320_raxis_0 (101 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_524288_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_524288_isize_160_raxis_1 (65 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_1048576_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_1048576_isize_160_raxis_0 (84 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_double_rsize_1048576_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_double_rsize_1048576_isize_320_raxis_1 (70 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_1_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_1_isize_320_raxis_0 (98 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_2_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_2_isize_160_raxis_1 (92 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_4_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_4_isize_160_raxis_0 (95 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_4_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_4_isize_320_raxis_1 (93 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_8_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_8_isize_320_raxis_0 (119 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_16_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_16_isize_160_raxis_1 (62 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_32_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_32_isize_160_raxis_0 (143 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_32_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_32_isize_320_raxis_1 (62 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_64_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_64_isize_320_raxis_0 (175 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_128_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_128_isize_160_raxis_1 (62 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_256_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_256_isize_160_raxis_0 (163 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_256_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_256_isize_320_raxis_1 (104 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_512_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_512_isize_320_raxis_0 (81 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_1024_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_1024_isize_160_raxis_1 (106 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_2048_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_2048_isize_160_raxis_0 (161 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_2048_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_2048_isize_320_raxis_1 (64 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_4096_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_4096_isize_320_raxis_0 (253 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_8192_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_8192_isize_160_raxis_1 (64 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_16384_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_16384_isize_160_raxis_0 (246 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_16384_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_16384_isize_320_raxis_1 (65 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_32768_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_32768_isize_320_raxis_0 (101 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_65536_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_65536_isize_160_raxis_1 (64 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_131072_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_131072_isize_160_raxis_0 (197 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_131072_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_131072_isize_320_raxis_1 (64 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_262144_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_262144_isize_320_raxis_0 (100 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_524288_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_524288_isize_160_raxis_1 (64 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_1048576_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_1048576_isize_160_raxis_0 (83 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_float_rsize_1048576_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_float_rsize_1048576_isize_320_raxis_1 (66 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_1_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_1_isize_320_raxis_0 (104 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_2_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_2_isize_160_raxis_1 (95 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_4_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_4_isize_160_raxis_0 (102 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_4_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_4_isize_320_raxis_1 (94 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_8_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_8_isize_320_raxis_0 (133 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_16_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_16_isize_160_raxis_1 (63 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_32_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_32_isize_160_raxis_0 (159 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_32_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_32_isize_320_raxis_1 (93 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_64_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_64_isize_320_raxis_0 (271 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_128_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_128_isize_160_raxis_1 (63 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_256_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_256_isize_160_raxis_0 (313 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_256_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_256_isize_320_raxis_1 (108 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_512_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_512_isize_320_raxis_0 (91 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_1024_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_1024_isize_160_raxis_1 (117 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_2048_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_2048_isize_160_raxis_0 (77 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_2048_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_2048_isize_320_raxis_1 (70 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_4096_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_4096_isize_320_raxis_0 (271 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_8192_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_8192_isize_160_raxis_1 (111 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_16384_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_16384_isize_160_raxis_0 (355 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_16384_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_16384_isize_320_raxis_1 (66 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_32768_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_32768_isize_320_raxis_0 (364 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_65536_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_65536_isize_160_raxis_1 (67 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_131072_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_131072_isize_160_raxis_0 (113 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_131072_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_131072_isize_320_raxis_1 (66 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_262144_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_262144_isize_320_raxis_0 (112 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_524288_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_524288_isize_160_raxis_1 (66 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_1048576_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_1048576_isize_160_raxis_0 (365 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___half_rsize_1048576_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___half_rsize_1048576_isize_320_raxis_1 (68 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_1_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_1_isize_320_raxis_0 (103 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_2_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_2_isize_160_raxis_1 (97 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_4_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_4_isize_160_raxis_0 (99 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_4_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_4_isize_320_raxis_1 (98 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_8_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_8_isize_320_raxis_0 (129 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_16_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_16_isize_160_raxis_1 (62 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_32_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_32_isize_160_raxis_0 (143 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_32_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_32_isize_320_raxis_1 (64 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_64_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_64_isize_320_raxis_0 (167 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_128_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_128_isize_160_raxis_1 (63 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_256_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_256_isize_160_raxis_0 (72 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_256_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_256_isize_320_raxis_1 (110 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_512_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_512_isize_320_raxis_0 (167 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_1024_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_1024_isize_160_raxis_1 (105 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_2048_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_2048_isize_160_raxis_0 (252 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_2048_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_2048_isize_320_raxis_1 (64 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_4096_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_4096_isize_320_raxis_0 (250 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_8192_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_8192_isize_160_raxis_1 (64 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_16384_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_16384_isize_160_raxis_0 (102 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_16384_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_16384_isize_320_raxis_1 (64 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_32768_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_32768_isize_320_raxis_0 (102 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_65536_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_65536_isize_160_raxis_1 (64 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_131072_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_131072_isize_160_raxis_0 (179 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_131072_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_131072_isize_320_raxis_1 (66 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_262144_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_262144_isize_320_raxis_0 (104 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_524288_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_524288_isize_160_raxis_1 (66 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_1048576_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_1048576_isize_160_raxis_0 (86 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_1048576_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_float__rsize_1048576_isize_320_raxis_1 (72 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_1_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_1_isize_320_raxis_0 (82 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_2_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_2_isize_160_raxis_1 (98 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_4_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_4_isize_160_raxis_0 (99 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_4_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_4_isize_320_raxis_1 (97 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_8_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_8_isize_320_raxis_0 (105 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_16_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_16_isize_160_raxis_1 (62 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_32_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_32_isize_160_raxis_0 (125 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_32_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_32_isize_320_raxis_1 (63 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_64_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_64_isize_320_raxis_0 (72 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_128_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_128_isize_160_raxis_1 (64 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_256_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_256_isize_160_raxis_0 (74 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_256_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_256_isize_320_raxis_1 (109 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_512_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_512_isize_320_raxis_0 (124 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_1024_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_1024_isize_160_raxis_1 (106 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_2048_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_2048_isize_160_raxis_0 (163 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_2048_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_2048_isize_320_raxis_1 (103 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_4096_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_4096_isize_320_raxis_0 (162 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_8192_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_8192_isize_160_raxis_1 (64 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_16384_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_16384_isize_160_raxis_0 (84 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_16384_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_16384_isize_320_raxis_1 (65 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_32768_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_32768_isize_320_raxis_0 (85 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_65536_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_65536_isize_160_raxis_1 (65 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_131072_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_131072_isize_160_raxis_0 (85 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_131072_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_131072_isize_320_raxis_1 (66 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_262144_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_262144_isize_320_raxis_0 (86 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_524288_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_524288_isize_160_raxis_1 (66 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_1048576_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_1048576_isize_160_raxis_0 (87 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_1048576_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype_std__complex_double__rsize_1048576_isize_320_raxis_1 (74 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_1_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_1_isize_320_raxis_0 (107 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_2_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_2_isize_160_raxis_1 (97 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_4_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_4_isize_160_raxis_0 (105 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_4_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_4_isize_320_raxis_1 (96 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_8_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_8_isize_320_raxis_0 (132 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_16_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_16_isize_160_raxis_1 (64 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_32_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_32_isize_160_raxis_0 (160 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_32_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_32_isize_320_raxis_1 (95 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_64_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_64_isize_320_raxis_0 (273 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_128_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_128_isize_160_raxis_1 (64 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_256_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_256_isize_160_raxis_0 (313 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_256_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_256_isize_320_raxis_1 (109 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_512_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_512_isize_320_raxis_0 (91 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_1024_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_1024_isize_160_raxis_1 (118 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_2048_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_2048_isize_160_raxis_0 (78 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_2048_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_2048_isize_320_raxis_1 (71 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_4096_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_4096_isize_320_raxis_0 (272 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_8192_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_8192_isize_160_raxis_1 (113 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_16384_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_16384_isize_160_raxis_0 (359 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_16384_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_16384_isize_320_raxis_1 (67 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_32768_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_32768_isize_320_raxis_0 (363 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_65536_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_65536_isize_160_raxis_1 (67 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_131072_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_131072_isize_160_raxis_0 (114 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_131072_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_131072_isize_320_raxis_1 (67 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_262144_isize_320_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_262144_isize_320_raxis_0 (114 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_524288_isize_160_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_524288_isize_160_raxis_1 (67 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_1048576_isize_160_raxis_0
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_1048576_isize_160_raxis_0 (363 ms)
[ RUN      ] ReductionWithIterDim.Test/dtype___bfloat_rsize_1048576_isize_320_raxis_1
[       OK ] ReductionWithIterDim.Test/dtype___bfloat_rsize_1048576_isize_320_raxis_1 (68 ms)
[----------] 168 tests from ReductionWithIterDim (19521 ms total)

[----------] 13 tests from AdvancedIndexingTest
[ RUN      ] AdvancedIndexingTest.1/Legacy
[       OK ] AdvancedIndexingTest.1/Legacy (159 ms)
[ RUN      ] AdvancedIndexingTest.2/IdModel
[       OK ] AdvancedIndexingTest.2/IdModel (161 ms)
[ RUN      ] AdvancedIndexingTest.4/Legacy
[       OK ] AdvancedIndexingTest.4/Legacy (192 ms)
[ RUN      ] AdvancedIndexingTest.5/IdModel
[       OK ] AdvancedIndexingTest.5/IdModel (118 ms)
[ RUN      ] AdvancedIndexingTest.7/Legacy
[       OK ] AdvancedIndexingTest.7/Legacy (108 ms)
[ RUN      ] AdvancedIndexingTest.8/IdModel
[       OK ] AdvancedIndexingTest.8/IdModel (744 ms)
[ RUN      ] AdvancedIndexingTest.10/Legacy
[       OK ] AdvancedIndexingTest.10/Legacy (95 ms)
[ RUN      ] AdvancedIndexingTest.11/IdModel
[       OK ] AdvancedIndexingTest.11/IdModel (84 ms)
[ RUN      ] AdvancedIndexingTest.13/Legacy
[       OK ] AdvancedIndexingTest.13/Legacy (96 ms)
[ RUN      ] AdvancedIndexingTest.14/IdModel
[       OK ] AdvancedIndexingTest.14/IdModel (81 ms)
[ RUN      ] AdvancedIndexingTest.16/Legacy
[       OK ] AdvancedIndexingTest.16/Legacy (116 ms)
[ RUN      ] AdvancedIndexingTest.17/IdModel
[       OK ] AdvancedIndexingTest.17/IdModel (122 ms)
[ RUN      ] AdvancedIndexingTest.19/Legacy
[       OK ] AdvancedIndexingTest.19/Legacy (130 ms)
[----------] 13 tests from AdvancedIndexingTest (2214 ms total)

[----------] 2 tests from MXFP8QuantizationTest
[ RUN      ] MXFP8QuantizationTest.AutoScheduleOp/float_2048x128
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MXFP8QuantizationTest.AutoScheduleOp/float_2048x128 (0 ms)
[ RUN      ] MXFP8QuantizationTest.AutoScheduleOp/__bfloat_2048x128
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MXFP8QuantizationTest.AutoScheduleOp/__bfloat_2048x128 (0 ms)
[----------] 2 tests from MXFP8QuantizationTest (0 ms total)

[----------] 2 tests from NVFP4QuantizeTest
[ RUN      ] NVFP4QuantizeTest.WithoutPerTensorAmax/float
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] NVFP4QuantizeTest.WithoutPerTensorAmax/float (0 ms)
[ RUN      ] NVFP4QuantizeTest.WithPerTensorAmax/__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] NVFP4QuantizeTest.WithPerTensorAmax/__bfloat (0 ms)
[----------] 2 tests from NVFP4QuantizeTest (0 ms total)

[----------] 5 tests from BlockQuantizationTest
[ RUN      ] BlockQuantizationTest.ScheduleAsPointwise/float_GroupWidth4
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] BlockQuantizationTest.ScheduleAsPointwise/float_GroupWidth4 (0 ms)
[ RUN      ] BlockQuantizationTest.ScheduleAsPointwise/__bfloat_GroupWidth8
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] BlockQuantizationTest.ScheduleAsPointwise/__bfloat_GroupWidth8 (0 ms)
[ RUN      ] BlockQuantizationTest.ScheduleAsPointwise/__half_GroupWidth8
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] BlockQuantizationTest.ScheduleAsPointwise/__half_GroupWidth8 (0 ms)
[ RUN      ] BlockQuantizationTest.ScheduleAsPointwise2D/__bfloat_GroupWidth2
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] BlockQuantizationTest.ScheduleAsPointwise2D/__bfloat_GroupWidth2 (0 ms)
[ RUN      ] BlockQuantizationTest.ScheduleAsPointwise2D/__half_GroupWidth2
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] BlockQuantizationTest.ScheduleAsPointwise2D/__half_GroupWidth2 (0 ms)
[----------] 5 tests from BlockQuantizationTest (0 ms total)

[----------] 11 tests from BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest
[ RUN      ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/float_1024x1024_NoGlobalScale_NoSwizzle
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/float_1024x1024_NoGlobalScale_NoSwizzle (0 ms)
[ RUN      ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/float_1024x1024_WithGlobalScale_WithSwizzle
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/float_1024x1024_WithGlobalScale_WithSwizzle (0 ms)
[ RUN      ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/float_128x64_WithGlobalScale_NoSwizzle
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/float_128x64_WithGlobalScale_NoSwizzle (0 ms)
[ RUN      ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/float_2048x128_NoGlobalScale_WithSwizzle
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/float_2048x128_NoGlobalScale_WithSwizzle (0 ms)
[ RUN      ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/float_2048x2048_NoGlobalScale_NoSwizzle
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/float_2048x2048_NoGlobalScale_NoSwizzle (0 ms)
[ RUN      ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/float_2048x2048_WithGlobalScale_WithSwizzle
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/float_2048x2048_WithGlobalScale_WithSwizzle (0 ms)
[ RUN      ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/__bfloat_1024x1024_WithGlobalScale_NoSwizzle
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/__bfloat_1024x1024_WithGlobalScale_NoSwizzle (0 ms)
[ RUN      ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/__bfloat_128x64_NoGlobalScale_WithSwizzle
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/__bfloat_128x64_NoGlobalScale_WithSwizzle (0 ms)
[ RUN      ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/__bfloat_2048x128_NoGlobalScale_NoSwizzle
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/__bfloat_2048x128_NoGlobalScale_NoSwizzle (0 ms)
[ RUN      ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/__bfloat_2048x128_WithGlobalScale_WithSwizzle
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/__bfloat_2048x128_WithGlobalScale_WithSwizzle (0 ms)
[ RUN      ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/__bfloat_2048x2048_WithGlobalScale_NoSwizzle
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/__bfloat_2048x2048_WithGlobalScale_NoSwizzle (0 ms)
[----------] 11 tests from BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest (0 ms total)

[----------] 1 test from MathOptTest/NanReductionTest
[ RUN      ] MathOptTest/NanReductionTest.Test/fmax
[       OK ] MathOptTest/NanReductionTest.Test/fmax (71 ms)
[----------] 1 test from MathOptTest/NanReductionTest (71 ms total)

[----------] 1 test from CacheGlobalLoads/MemoryTest
[ RUN      ] CacheGlobalLoads/MemoryTest.LoadCache/AllLevels
PRINTING: __tmp_c2_nvfuser_none_f0_c0_r0_g0.ptx
Removing __tmp_c2_nvfuser_none_f0_c0_r0_g0.ptx
[       OK ] CacheGlobalLoads/MemoryTest.LoadCache/AllLevels (79 ms)
[----------] 1 test from CacheGlobalLoads/MemoryTest (79 ms total)

[----------] 40 tests from TMASimpleLdstTest
[ RUN      ] TMASimpleLdstTest.Load/1D_NoSwizzle___half
[       OK ] TMASimpleLdstTest.Load/1D_NoSwizzle___half (77 ms)
[ RUN      ] TMASimpleLdstTest.Load/4D_NoSwizzle___half
[       OK ] TMASimpleLdstTest.Load/4D_NoSwizzle___half (106 ms)
[ RUN      ] TMASimpleLdstTest.Load/2D_NoSwizzle_float
[       OK ] TMASimpleLdstTest.Load/2D_NoSwizzle_float (84 ms)
[ RUN      ] TMASimpleLdstTest.Load/5D_NoSwizzle_float
[       OK ] TMASimpleLdstTest.Load/5D_NoSwizzle_float (128 ms)
[ RUN      ] TMASimpleLdstTest.Load/3D_NoSwizzle_double
[       OK ] TMASimpleLdstTest.Load/3D_NoSwizzle_double (91 ms)
[ RUN      ] TMASimpleLdstTest.Load/1D_128B___half
[       OK ] TMASimpleLdstTest.Load/1D_128B___half (82 ms)
[ RUN      ] TMASimpleLdstTest.Load/4D_128B___half
[       OK ] TMASimpleLdstTest.Load/4D_128B___half (177 ms)
[ RUN      ] TMASimpleLdstTest.Load/2D_128B_float
[       OK ] TMASimpleLdstTest.Load/2D_128B_float (103 ms)
[ RUN      ] TMASimpleLdstTest.Load/5D_128B_float
[       OK ] TMASimpleLdstTest.Load/5D_128B_float (220 ms)
[ RUN      ] TMASimpleLdstTest.Load/3D_128B_double
[       OK ] TMASimpleLdstTest.Load/3D_128B_double (115 ms)
[ RUN      ] TMASimpleLdstTest.Load/1D_64B___half
[       OK ] TMASimpleLdstTest.Load/1D_64B___half (84 ms)
[ RUN      ] TMASimpleLdstTest.Load/4D_64B___half
[       OK ] TMASimpleLdstTest.Load/4D_64B___half (205 ms)
[ RUN      ] TMASimpleLdstTest.Load/2D_64B_float
[       OK ] TMASimpleLdstTest.Load/2D_64B_float (122 ms)
[ RUN      ] TMASimpleLdstTest.Load/5D_64B_float
[       OK ] TMASimpleLdstTest.Load/5D_64B_float (251 ms)
[ RUN      ] TMASimpleLdstTest.Load/3D_64B_double
[       OK ] TMASimpleLdstTest.Load/3D_64B_double (134 ms)
[ RUN      ] TMASimpleLdstTest.Load/1D_32B___half
[       OK ] TMASimpleLdstTest.Load/1D_32B___half (85 ms)
[ RUN      ] TMASimpleLdstTest.Load/4D_32B___half
[       OK ] TMASimpleLdstTest.Load/4D_32B___half (206 ms)
[ RUN      ] TMASimpleLdstTest.Load/2D_32B_float
[       OK ] TMASimpleLdstTest.Load/2D_32B_float (119 ms)
[ RUN      ] TMASimpleLdstTest.Load/5D_32B_float
[       OK ] TMASimpleLdstTest.Load/5D_32B_float (251 ms)
[ RUN      ] TMASimpleLdstTest.Load/3D_32B_double
[       OK ] TMASimpleLdstTest.Load/3D_32B_double (134 ms)
[ RUN      ] TMASimpleLdstTest.Store/1D_NoSwizzle___half
[       OK ] TMASimpleLdstTest.Store/1D_NoSwizzle___half (73 ms)
[ RUN      ] TMASimpleLdstTest.Store/4D_NoSwizzle___half
[       OK ] TMASimpleLdstTest.Store/4D_NoSwizzle___half (101 ms)
[ RUN      ] TMASimpleLdstTest.Store/2D_NoSwizzle_float
[       OK ] TMASimpleLdstTest.Store/2D_NoSwizzle_float (82 ms)
[ RUN      ] TMASimpleLdstTest.Store/5D_NoSwizzle_float
[       OK ] TMASimpleLdstTest.Store/5D_NoSwizzle_float (121 ms)
[ RUN      ] TMASimpleLdstTest.Store/3D_NoSwizzle_double
[       OK ] TMASimpleLdstTest.Store/3D_NoSwizzle_double (87 ms)
[ RUN      ] TMASimpleLdstTest.Store/1D_128B___half
[       OK ] TMASimpleLdstTest.Store/1D_128B___half (81 ms)
[ RUN      ] TMASimpleLdstTest.Store/4D_128B___half
[       OK ] TMASimpleLdstTest.Store/4D_128B___half (172 ms)
[ RUN      ] TMASimpleLdstTest.Store/2D_128B_float
[       OK ] TMASimpleLdstTest.Store/2D_128B_float (98 ms)
[ RUN      ] TMASimpleLdstTest.Store/5D_128B_float
[       OK ] TMASimpleLdstTest.Store/5D_128B_float (212 ms)
[ RUN      ] TMASimpleLdstTest.Store/3D_128B_double
[       OK ] TMASimpleLdstTest.Store/3D_128B_double (112 ms)
[ RUN      ] TMASimpleLdstTest.Store/1D_64B___half
[       OK ] TMASimpleLdstTest.Store/1D_64B___half (82 ms)
[ RUN      ] TMASimpleLdstTest.Store/4D_64B___half
[       OK ] TMASimpleLdstTest.Store/4D_64B___half (200 ms)
[ RUN      ] TMASimpleLdstTest.Store/2D_64B_float
[       OK ] TMASimpleLdstTest.Store/2D_64B_float (118 ms)
[ RUN      ] TMASimpleLdstTest.Store/5D_64B_float
[       OK ] TMASimpleLdstTest.Store/5D_64B_float (244 ms)
[ RUN      ] TMASimpleLdstTest.Store/3D_64B_double
[       OK ] TMASimpleLdstTest.Store/3D_64B_double (131 ms)
[ RUN      ] TMASimpleLdstTest.Store/1D_32B___half
[       OK ] TMASimpleLdstTest.Store/1D_32B___half (82 ms)
[ RUN      ] TMASimpleLdstTest.Store/4D_32B___half
[       OK ] TMASimpleLdstTest.Store/4D_32B___half (201 ms)
[ RUN      ] TMASimpleLdstTest.Store/2D_32B_float
[       OK ] TMASimpleLdstTest.Store/2D_32B_float (117 ms)
[ RUN      ] TMASimpleLdstTest.Store/5D_32B_float
[       OK ] TMASimpleLdstTest.Store/5D_32B_float (242 ms)
[ RUN      ] TMASimpleLdstTest.Store/3D_32B_double
[       OK ] TMASimpleLdstTest.Store/3D_32B_double (130 ms)
[----------] 40 tests from TMASimpleLdstTest (5483 ms total)

[----------] 20 tests from TMALoadTestWithABroadcastDim
[ RUN      ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/0
[       OK ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/0 (433 ms)
[ RUN      ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/3
[       OK ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/3 (1030 ms)
[ RUN      ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/6
[       OK ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/6 (966 ms)
[ RUN      ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/9
[       OK ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/9 (1138 ms)
[ RUN      ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/12
[       OK ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/12 (148 ms)
[ RUN      ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/15
[       OK ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/15 (202 ms)
[ RUN      ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/18
[       OK ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/18 (200 ms)
[ RUN      ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/21
[       OK ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/21 (206 ms)
[ RUN      ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/24
[       OK ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/24 (1367 ms)
[ RUN      ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/27
[       OK ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/27 (2959 ms)
[ RUN      ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/30
[       OK ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/30 (2872 ms)
[ RUN      ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/33
[       OK ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/33 (3205 ms)
[ RUN      ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/36
[       OK ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/36 (1594 ms)
[ RUN      ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/39
[       OK ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/39 (3054 ms)
[ RUN      ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/42
[       OK ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/42 (3002 ms)
[ RUN      ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/45
[       OK ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/45 (3316 ms)
[ RUN      ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/48
[       OK ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/48 (56 ms)
[ RUN      ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/51
[       OK ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/51 (59 ms)
[ RUN      ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/54
[       OK ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/54 (59 ms)
[ RUN      ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/57
[       OK ] TMALoadTestWithABroadcastDim.LoadWithBroadcast/57 (59 ms)
[----------] 20 tests from TMALoadTestWithABroadcastDim (25934 ms total)

[----------] 4 tests from LdMatrixTest
[ RUN      ] LdMatrixTest.Regular/A_16x8
[       OK ] LdMatrixTest.Regular/A_16x8 (138 ms)
[ RUN      ] LdMatrixTest.Regular/B_8x16
[       OK ] LdMatrixTest.Regular/B_8x16 (140 ms)
[ RUN      ] LdMatrixTest.Transpose/A_16x8
[       OK ] LdMatrixTest.Transpose/A_16x8 (140 ms)
[ RUN      ] LdMatrixTest.Transpose/B_8x16
[       OK ] LdMatrixTest.Transpose/B_8x16 (133 ms)
[----------] 4 tests from LdMatrixTest (552 ms total)

[----------] 43 tests from StMatrixTest
[ RUN      ] StMatrixTest.Regular/m_64_n_8_tile_m_16_tile_n_8_H
[       OK ] StMatrixTest.Regular/m_64_n_8_tile_m_16_tile_n_8_H (92 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_8_tile_m_16_tile_n_16_T
/opt/pytorch/nvfuser/tests/cpp/test_memory.cpp:2932: Skipped
Fractional tiling is not supported/tested

[  SKIPPED ] StMatrixTest.Regular/m_64_n_8_tile_m_16_tile_n_16_T (0 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_16_tile_m_16_tile_n_16_H
[       OK ] StMatrixTest.Regular/m_64_n_16_tile_m_16_tile_n_16_H (101 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_24_tile_m_16_tile_n_8_T
[       OK ] StMatrixTest.Regular/m_64_n_24_tile_m_16_tile_n_8_T (109 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_32_tile_m_16_tile_n_8_H
[       OK ] StMatrixTest.Regular/m_64_n_32_tile_m_16_tile_n_8_H (118 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_32_tile_m_16_tile_n_16_T
[       OK ] StMatrixTest.Regular/m_64_n_32_tile_m_16_tile_n_16_T (121 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_40_tile_m_16_tile_n_16_H
/opt/pytorch/nvfuser/tests/cpp/test_memory.cpp:2932: Skipped
Fractional tiling is not supported/tested

[  SKIPPED ] StMatrixTest.Regular/m_64_n_40_tile_m_16_tile_n_16_H (0 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_48_tile_m_16_tile_n_8_T
[       OK ] StMatrixTest.Regular/m_64_n_48_tile_m_16_tile_n_8_T (138 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_56_tile_m_16_tile_n_8_H
[       OK ] StMatrixTest.Regular/m_64_n_56_tile_m_16_tile_n_8_H (148 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_56_tile_m_16_tile_n_16_T
/opt/pytorch/nvfuser/tests/cpp/test_memory.cpp:2932: Skipped
Fractional tiling is not supported/tested

[  SKIPPED ] StMatrixTest.Regular/m_64_n_56_tile_m_16_tile_n_16_T (0 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_64_tile_m_16_tile_n_16_H
[       OK ] StMatrixTest.Regular/m_64_n_64_tile_m_16_tile_n_16_H (160 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_72_tile_m_16_tile_n_8_T
[       OK ] StMatrixTest.Regular/m_64_n_72_tile_m_16_tile_n_8_T (173 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_80_tile_m_16_tile_n_8_H
[       OK ] StMatrixTest.Regular/m_64_n_80_tile_m_16_tile_n_8_H (189 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_80_tile_m_16_tile_n_16_T
[       OK ] StMatrixTest.Regular/m_64_n_80_tile_m_16_tile_n_16_T (187 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_88_tile_m_16_tile_n_16_H
/opt/pytorch/nvfuser/tests/cpp/test_memory.cpp:2932: Skipped
Fractional tiling is not supported/tested

[  SKIPPED ] StMatrixTest.Regular/m_64_n_88_tile_m_16_tile_n_16_H (0 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_96_tile_m_16_tile_n_8_T
[       OK ] StMatrixTest.Regular/m_64_n_96_tile_m_16_tile_n_8_T (217 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_104_tile_m_16_tile_n_8_H
[       OK ] StMatrixTest.Regular/m_64_n_104_tile_m_16_tile_n_8_H (231 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_104_tile_m_16_tile_n_16_T
/opt/pytorch/nvfuser/tests/cpp/test_memory.cpp:2932: Skipped
Fractional tiling is not supported/tested

[  SKIPPED ] StMatrixTest.Regular/m_64_n_104_tile_m_16_tile_n_16_T (0 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_112_tile_m_16_tile_n_16_H
[       OK ] StMatrixTest.Regular/m_64_n_112_tile_m_16_tile_n_16_H (251 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_120_tile_m_16_tile_n_8_T
[       OK ] StMatrixTest.Regular/m_64_n_120_tile_m_16_tile_n_8_T (268 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_128_tile_m_16_tile_n_8_H
[       OK ] StMatrixTest.Regular/m_64_n_128_tile_m_16_tile_n_8_H (282 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_128_tile_m_16_tile_n_16_T
[       OK ] StMatrixTest.Regular/m_64_n_128_tile_m_16_tile_n_16_T (286 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_136_tile_m_16_tile_n_16_H
/opt/pytorch/nvfuser/tests/cpp/test_memory.cpp:2932: Skipped
Fractional tiling is not supported/tested

[  SKIPPED ] StMatrixTest.Regular/m_64_n_136_tile_m_16_tile_n_16_H (0 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_144_tile_m_16_tile_n_8_T
[       OK ] StMatrixTest.Regular/m_64_n_144_tile_m_16_tile_n_8_T (320 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_152_tile_m_16_tile_n_8_H
[       OK ] StMatrixTest.Regular/m_64_n_152_tile_m_16_tile_n_8_H (347 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_152_tile_m_16_tile_n_16_T
/opt/pytorch/nvfuser/tests/cpp/test_memory.cpp:2932: Skipped
Fractional tiling is not supported/tested

[  SKIPPED ] StMatrixTest.Regular/m_64_n_152_tile_m_16_tile_n_16_T (0 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_160_tile_m_16_tile_n_16_H
[       OK ] StMatrixTest.Regular/m_64_n_160_tile_m_16_tile_n_16_H (367 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_168_tile_m_16_tile_n_8_T
[       OK ] StMatrixTest.Regular/m_64_n_168_tile_m_16_tile_n_8_T (394 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_176_tile_m_16_tile_n_8_H
[       OK ] StMatrixTest.Regular/m_64_n_176_tile_m_16_tile_n_8_H (408 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_176_tile_m_16_tile_n_16_T
[       OK ] StMatrixTest.Regular/m_64_n_176_tile_m_16_tile_n_16_T (409 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_184_tile_m_16_tile_n_16_H
/opt/pytorch/nvfuser/tests/cpp/test_memory.cpp:2932: Skipped
Fractional tiling is not supported/tested

[  SKIPPED ] StMatrixTest.Regular/m_64_n_184_tile_m_16_tile_n_16_H (0 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_192_tile_m_16_tile_n_8_T
[       OK ] StMatrixTest.Regular/m_64_n_192_tile_m_16_tile_n_8_T (457 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_200_tile_m_16_tile_n_8_H
[       OK ] StMatrixTest.Regular/m_64_n_200_tile_m_16_tile_n_8_H (481 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_200_tile_m_16_tile_n_16_T
/opt/pytorch/nvfuser/tests/cpp/test_memory.cpp:2932: Skipped
Fractional tiling is not supported/tested

[  SKIPPED ] StMatrixTest.Regular/m_64_n_200_tile_m_16_tile_n_16_T (0 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_208_tile_m_16_tile_n_16_H
[       OK ] StMatrixTest.Regular/m_64_n_208_tile_m_16_tile_n_16_H (509 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_216_tile_m_16_tile_n_8_T
[       OK ] StMatrixTest.Regular/m_64_n_216_tile_m_16_tile_n_8_T (538 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_224_tile_m_16_tile_n_8_H
[       OK ] StMatrixTest.Regular/m_64_n_224_tile_m_16_tile_n_8_H (566 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_224_tile_m_16_tile_n_16_T
[       OK ] StMatrixTest.Regular/m_64_n_224_tile_m_16_tile_n_16_T (568 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_232_tile_m_16_tile_n_16_H
/opt/pytorch/nvfuser/tests/cpp/test_memory.cpp:2932: Skipped
Fractional tiling is not supported/tested

[  SKIPPED ] StMatrixTest.Regular/m_64_n_232_tile_m_16_tile_n_16_H (0 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_240_tile_m_16_tile_n_8_T
[       OK ] StMatrixTest.Regular/m_64_n_240_tile_m_16_tile_n_8_T (628 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_248_tile_m_16_tile_n_8_H
[       OK ] StMatrixTest.Regular/m_64_n_248_tile_m_16_tile_n_8_H (666 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_248_tile_m_16_tile_n_16_T
/opt/pytorch/nvfuser/tests/cpp/test_memory.cpp:2932: Skipped
Fractional tiling is not supported/tested

[  SKIPPED ] StMatrixTest.Regular/m_64_n_248_tile_m_16_tile_n_16_T (0 ms)
[ RUN      ] StMatrixTest.Regular/m_64_n_256_tile_m_16_tile_n_16_H
[       OK ] StMatrixTest.Regular/m_64_n_256_tile_m_16_tile_n_16_H (694 ms)
[----------] 43 tests from StMatrixTest (10439 ms total)

[----------] 1 test from TMATest/TMA1dPredicateTest
[ RUN      ] TMATest/TMA1dPredicateTest.testUnrollCircularBuffer/has_unroll_1_has_circular_buffer_0
[       OK ] TMATest/TMA1dPredicateTest.testUnrollCircularBuffer/has_unroll_1_has_circular_buffer_0 (89 ms)
[----------] 1 test from TMATest/TMA1dPredicateTest (89 ms total)

[----------] 2 tests from EmbeddingFwdMemoryFormats/EmbeddingFwdMetaTest
[ RUN      ] EmbeddingFwdMemoryFormats/EmbeddingFwdMetaTest.MemoryLayouts/input_RowMajor_weight_RowMajor
[       OK ] EmbeddingFwdMemoryFormats/EmbeddingFwdMetaTest.MemoryLayouts/input_RowMajor_weight_RowMajor (0 ms)
[ RUN      ] EmbeddingFwdMemoryFormats/EmbeddingFwdMetaTest.MemoryLayouts/input_ColMajor_weight_ColMajor
[       OK ] EmbeddingFwdMemoryFormats/EmbeddingFwdMetaTest.MemoryLayouts/input_ColMajor_weight_ColMajor (0 ms)
[----------] 2 tests from EmbeddingFwdMemoryFormats/EmbeddingFwdMetaTest (0 ms total)

[----------] 1 test from MemoryFormatCombinations/MetaTestGroupedMma2D2D
[ RUN      ] MemoryFormatCombinations/MetaTestGroupedMma2D2D.MemoryFormats/Mat1Transposed_Mat2Contiguous
[       OK ] MemoryFormatCombinations/MetaTestGroupedMma2D2D.MemoryFormats/Mat1Transposed_Mat2Contiguous (31 ms)
[----------] 1 test from MemoryFormatCombinations/MetaTestGroupedMma2D2D (31 ms total)

[----------] 4 tests from MemoryFormatCombinations/MetaTestGroupedMma3D2D
[ RUN      ] MemoryFormatCombinations/MetaTestGroupedMma3D2D.MemoryFormats/Mat1Perm012_Mat2Transposed
[       OK ] MemoryFormatCombinations/MetaTestGroupedMma3D2D.MemoryFormats/Mat1Perm012_Mat2Transposed (1 ms)
[ RUN      ] MemoryFormatCombinations/MetaTestGroupedMma3D2D.MemoryFormats/Mat1Perm102_Mat2Contiguous
[       OK ] MemoryFormatCombinations/MetaTestGroupedMma3D2D.MemoryFormats/Mat1Perm102_Mat2Contiguous (2 ms)
[ RUN      ] MemoryFormatCombinations/MetaTestGroupedMma3D2D.MemoryFormats/Mat1Perm120_Mat2Transposed
[       OK ] MemoryFormatCombinations/MetaTestGroupedMma3D2D.MemoryFormats/Mat1Perm120_Mat2Transposed (0 ms)
[ RUN      ] MemoryFormatCombinations/MetaTestGroupedMma3D2D.MemoryFormats/Mat1Perm210_Mat2Contiguous
[       OK ] MemoryFormatCombinations/MetaTestGroupedMma3D2D.MemoryFormats/Mat1Perm210_Mat2Contiguous (0 ms)
[----------] 4 tests from MemoryFormatCombinations/MetaTestGroupedMma3D2D (4 ms total)

[----------] 4 tests from MemoryFormatCombinations/MetaTestGroupedMma2D3D
[ RUN      ] MemoryFormatCombinations/MetaTestGroupedMma2D3D.MemoryFormats/Mat1Contiguous_Mat2Perm021
[       OK ] MemoryFormatCombinations/MetaTestGroupedMma2D3D.MemoryFormats/Mat1Contiguous_Mat2Perm021 (1 ms)
[ RUN      ] MemoryFormatCombinations/MetaTestGroupedMma2D3D.MemoryFormats/Mat1Contiguous_Mat2Perm201
[       OK ] MemoryFormatCombinations/MetaTestGroupedMma2D3D.MemoryFormats/Mat1Contiguous_Mat2Perm201 (0 ms)
[ RUN      ] MemoryFormatCombinations/MetaTestGroupedMma2D3D.MemoryFormats/Mat1Transposed_Mat2Perm021
[       OK ] MemoryFormatCombinations/MetaTestGroupedMma2D3D.MemoryFormats/Mat1Transposed_Mat2Perm021 (1 ms)
[ RUN      ] MemoryFormatCombinations/MetaTestGroupedMma2D3D.MemoryFormats/Mat1Transposed_Mat2Perm201
[       OK ] MemoryFormatCombinations/MetaTestGroupedMma2D3D.MemoryFormats/Mat1Transposed_Mat2Perm201 (0 ms)
[----------] 4 tests from MemoryFormatCombinations/MetaTestGroupedMma2D3D (3 ms total)

[----------] 9 tests from PersistentBufferTest/LayerNormSharedMemoryTest
[ RUN      ] PersistentBufferTest/LayerNormSharedMemoryTest.FusionLayerNormSharedMemoryBuffer_CUDA/dtype___half_hidden_36864
[       OK ] PersistentBufferTest/LayerNormSharedMemoryTest.FusionLayerNormSharedMemoryBuffer_CUDA/dtype___half_hidden_36864 (372 ms)
[ RUN      ] PersistentBufferTest/LayerNormSharedMemoryTest.FusionLayerNormSharedMemoryBuffer_CUDA/dtype___half_hidden_49152
[       OK ] PersistentBufferTest/LayerNormSharedMemoryTest.FusionLayerNormSharedMemoryBuffer_CUDA/dtype___half_hidden_49152 (335 ms)
[ RUN      ] PersistentBufferTest/LayerNormSharedMemoryTest.FusionLayerNormSharedMemoryBuffer_CUDA/dtype___half_hidden_61440
[       OK ] PersistentBufferTest/LayerNormSharedMemoryTest.FusionLayerNormSharedMemoryBuffer_CUDA/dtype___half_hidden_61440 (361 ms)
[ RUN      ] PersistentBufferTest/LayerNormSharedMemoryTest.FusionLayerNormSharedMemoryBuffer_CUDA/dtype___half_hidden_73728
[       OK ] PersistentBufferTest/LayerNormSharedMemoryTest.FusionLayerNormSharedMemoryBuffer_CUDA/dtype___half_hidden_73728 (454 ms)
[ RUN      ] PersistentBufferTest/LayerNormSharedMemoryTest.FusionLayerNormSharedMemoryBuffer_CUDA/dtype_float_hidden_32768
[       OK ] PersistentBufferTest/LayerNormSharedMemoryTest.FusionLayerNormSharedMemoryBuffer_CUDA/dtype_float_hidden_32768 (275 ms)
[ RUN      ] PersistentBufferTest/LayerNormSharedMemoryTest.FusionLayerNormSharedMemoryBuffer_CUDA/dtype_float_hidden_45056
[       OK ] PersistentBufferTest/LayerNormSharedMemoryTest.FusionLayerNormSharedMemoryBuffer_CUDA/dtype_float_hidden_45056 (348 ms)
[ RUN      ] PersistentBufferTest/LayerNormSharedMemoryTest.FusionLayerNormSharedMemoryBuffer_CUDA/dtype_float_hidden_57344
[       OK ] PersistentBufferTest/LayerNormSharedMemoryTest.FusionLayerNormSharedMemoryBuffer_CUDA/dtype_float_hidden_57344 (378 ms)
[ RUN      ] PersistentBufferTest/LayerNormSharedMemoryTest.FusionLayerNormSharedMemoryBuffer_CUDA/dtype_float_hidden_69632
[       OK ] PersistentBufferTest/LayerNormSharedMemoryTest.FusionLayerNormSharedMemoryBuffer_CUDA/dtype_float_hidden_69632 (337 ms)
[ RUN      ] PersistentBufferTest/LayerNormSharedMemoryTest.FusionLayerNormSharedMemoryBuffer_CUDA/dtype_float_hidden_81920
[       OK ] PersistentBufferTest/LayerNormSharedMemoryTest.FusionLayerNormSharedMemoryBuffer_CUDA/dtype_float_hidden_81920 (347 ms)
[----------] 9 tests from PersistentBufferTest/LayerNormSharedMemoryTest (3210 ms total)

[----------] 42 tests from TmaPersistentTestP
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_66_4
[       OK ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_66_4 (163 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_66_32
[       OK ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_66_32 (159 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_66_256
[       OK ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_66_256 (162 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_66_2048
[       OK ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_66_2048 (200 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_66_16384
[       OK ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_66_16384 (278 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_66_131072
[       OK ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_66_131072 (382 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_66_1048576
[       OK ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_66_1048576 (667 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_2048_4
[       OK ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_2048_4 (170 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_2048_32
[       OK ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_2048_32 (171 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_2048_256
[       OK ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_2048_256 (176 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_2048_2048
[       OK ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_2048_2048 (338 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_2048_16384
[       OK ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_2048_16384 (133 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_2048_131072
[       OK ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_2048_131072 (160 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_2048_1048576
[       OK ] TmaPersistentTestP.TmaInnerPersistentRmsNorm/__bfloat_2048_1048576 (781 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_66_4
[       OK ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_66_4 (227 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_66_32
[       OK ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_66_32 (219 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_66_256
[       OK ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_66_256 (209 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_66_2048
[       OK ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_66_2048 (249 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_66_16384
[       OK ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_66_16384 (308 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_66_131072
[       OK ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_66_131072 (427 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_66_1048576
[       OK ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_66_1048576 (683 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_2048_4
[       OK ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_2048_4 (235 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_2048_32
[       OK ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_2048_32 (243 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_2048_256
[       OK ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_2048_256 (234 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_2048_2048
[       OK ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_2048_2048 (425 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_2048_16384
[       OK ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_2048_16384 (175 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_2048_131072
[       OK ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_2048_131072 (216 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_2048_1048576
[       OK ] TmaPersistentTestP.TmaInnerPersistentLayerNorm/__bfloat_2048_1048576 (811 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_66_4
[       OK ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_66_4 (167 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_66_32
[       OK ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_66_32 (155 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_66_256
[       OK ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_66_256 (145 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_66_2048
[       OK ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_66_2048 (201 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_66_16384
[       OK ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_66_16384 (265 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_66_131072
[       OK ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_66_131072 (435 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_66_1048576
[       OK ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_66_1048576 (784 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_2048_4
[       OK ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_2048_4 (160 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_2048_32
[       OK ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_2048_32 (166 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_2048_256
[       OK ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_2048_256 (157 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_2048_2048
[       OK ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_2048_2048 (346 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_2048_16384
[       OK ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_2048_16384 (123 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_2048_131072
[       OK ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_2048_131072 (145 ms)
[ RUN      ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_2048_1048576
[       OK ] TmaPersistentTestP.TmaInnerPersistentSoftmax/__bfloat_2048_1048576 (910 ms)
[----------] 42 tests from TmaPersistentTestP (12779 ms total)

[----------] 2 tests from PointwiseParamsTest
[ RUN      ] PointwiseParamsTest.UnrollOnTopOfVectorize/vect_1_inner_unroll_2_outer_unroll_1
[       OK ] PointwiseParamsTest.UnrollOnTopOfVectorize/vect_1_inner_unroll_2_outer_unroll_1 (123 ms)
[ RUN      ] PointwiseParamsTest.UnrollOnTopOfVectorize/vect_4_inner_unroll_1_outer_unroll_2
[       OK ] PointwiseParamsTest.UnrollOnTopOfVectorize/vect_4_inner_unroll_1_outer_unroll_2 (113 ms)
[----------] 2 tests from PointwiseParamsTest (236 ms total)

[----------] 72 tests from TmaPointwiseTest
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_1_ndim_1_use_tma_store_0_auto_schedule_1
/opt/pytorch/nvfuser/tests/cpp/test_pointwise.cpp:1535: Skipped
Total bytes is not divisible by 16, can't use TMA, total_elem_count: 1, dtype_bytes: 4

[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_1_ndim_1_use_tma_store_0_auto_schedule_1 (0 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_1_ndim_2_use_tma_store_0_auto_schedule_1
/opt/pytorch/nvfuser/tests/cpp/test_pointwise.cpp:1535: Skipped
Total bytes is not divisible by 16, can't use TMA, total_elem_count: 2, dtype_bytes: 4

[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_1_ndim_2_use_tma_store_0_auto_schedule_1 (0 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_1_ndim_3_use_tma_store_0_auto_schedule_1
/opt/pytorch/nvfuser/tests/cpp/test_pointwise.cpp:1544: Skipped
Total elements is not divisible by min_inner_dim or equal to min_inner_dim, can't use TMA, total_elem_count: 8, min_inner_dim: 8

[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_1_ndim_3_use_tma_store_0_auto_schedule_1 (0 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_2_ndim_1_use_tma_store_0_auto_schedule_1
/opt/pytorch/nvfuser/tests/cpp/test_pointwise.cpp:1535: Skipped
Total bytes is not divisible by 16, can't use TMA, total_elem_count: 2, dtype_bytes: 4

[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_2_ndim_1_use_tma_store_0_auto_schedule_1 (0 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_2_ndim_2_use_tma_store_0_auto_schedule_1
/opt/pytorch/nvfuser/tests/cpp/test_pointwise.cpp:1544: Skipped
Total elements is not divisible by min_inner_dim or equal to min_inner_dim, can't use TMA, total_elem_count: 4, min_inner_dim: 8

[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_2_ndim_2_use_tma_store_0_auto_schedule_1 (0 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_2_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_2_ndim_3_use_tma_store_0_auto_schedule_1 (104 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_4_ndim_1_use_tma_store_0_auto_schedule_1
/opt/pytorch/nvfuser/tests/cpp/test_pointwise.cpp:1544: Skipped
Total elements is not divisible by min_inner_dim or equal to min_inner_dim, can't use TMA, total_elem_count: 4, min_inner_dim: 8

[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_4_ndim_1_use_tma_store_0_auto_schedule_1 (0 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_4_ndim_2_use_tma_store_0_auto_schedule_1
/opt/pytorch/nvfuser/tests/cpp/test_pointwise.cpp:1544: Skipped
Total elements is not divisible by min_inner_dim or equal to min_inner_dim, can't use TMA, total_elem_count: 8, min_inner_dim: 8

[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_4_ndim_2_use_tma_store_0_auto_schedule_1 (0 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_4_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_4_ndim_3_use_tma_store_0_auto_schedule_1 (105 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_8_ndim_1_use_tma_store_0_auto_schedule_1
/opt/pytorch/nvfuser/tests/cpp/test_pointwise.cpp:1544: Skipped
Total elements is not divisible by min_inner_dim or equal to min_inner_dim, can't use TMA, total_elem_count: 8, min_inner_dim: 8

[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_8_ndim_1_use_tma_store_0_auto_schedule_1 (0 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_8_ndim_2_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_8_ndim_2_use_tma_store_0_auto_schedule_1 (100 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_8_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_8_ndim_3_use_tma_store_0_auto_schedule_1 (105 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_16_ndim_1_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_16_ndim_1_use_tma_store_0_auto_schedule_1 (96 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_16_ndim_2_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_16_ndim_2_use_tma_store_0_auto_schedule_1 (102 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_16_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_16_ndim_3_use_tma_store_0_auto_schedule_1 (105 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_32_ndim_1_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_32_ndim_1_use_tma_store_0_auto_schedule_1 (97 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_32_ndim_2_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_32_ndim_2_use_tma_store_0_auto_schedule_1 (101 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_32_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_32_ndim_3_use_tma_store_0_auto_schedule_1 (104 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_64_ndim_1_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_64_ndim_1_use_tma_store_0_auto_schedule_1 (97 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_64_ndim_2_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_64_ndim_2_use_tma_store_0_auto_schedule_1 (100 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_64_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_64_ndim_3_use_tma_store_0_auto_schedule_1 (101 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_128_ndim_1_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_128_ndim_1_use_tma_store_0_auto_schedule_1 (97 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_128_ndim_2_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_128_ndim_2_use_tma_store_0_auto_schedule_1 (100 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_128_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_128_ndim_3_use_tma_store_0_auto_schedule_1 (104 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_256_ndim_1_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_256_ndim_1_use_tma_store_0_auto_schedule_1 (97 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_256_ndim_2_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_256_ndim_2_use_tma_store_0_auto_schedule_1 (97 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_256_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_256_ndim_3_use_tma_store_0_auto_schedule_1 (103 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_512_ndim_1_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_512_ndim_1_use_tma_store_0_auto_schedule_1 (93 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_512_ndim_2_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_512_ndim_2_use_tma_store_0_auto_schedule_1 (99 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_512_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_512_ndim_3_use_tma_store_0_auto_schedule_1 (107 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_1024_ndim_1_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_1024_ndim_1_use_tma_store_0_auto_schedule_1 (96 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_1024_ndim_2_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_1024_ndim_2_use_tma_store_0_auto_schedule_1 (99 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_1024_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_1024_ndim_3_use_tma_store_0_auto_schedule_1 (78 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_2048_ndim_1_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_2048_ndim_1_use_tma_store_0_auto_schedule_1 (95 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_2048_ndim_2_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_2048_ndim_2_use_tma_store_0_auto_schedule_1 (102 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_2048_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_2048_ndim_3_use_tma_store_0_auto_schedule_1 (77 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_4096_ndim_1_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_4096_ndim_1_use_tma_store_0_auto_schedule_1 (99 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_4096_ndim_2_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_4096_ndim_2_use_tma_store_0_auto_schedule_1 (74 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_4096_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_4096_ndim_3_use_tma_store_0_auto_schedule_1 (77 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_8192_ndim_1_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_8192_ndim_1_use_tma_store_0_auto_schedule_1 (69 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_8192_ndim_2_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_8192_ndim_2_use_tma_store_0_auto_schedule_1 (74 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_8192_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_8192_ndim_3_use_tma_store_0_auto_schedule_1 (77 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_16384_ndim_1_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_16384_ndim_1_use_tma_store_0_auto_schedule_1 (70 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_16384_ndim_2_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_16384_ndim_2_use_tma_store_0_auto_schedule_1 (73 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_16384_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_16384_ndim_3_use_tma_store_0_auto_schedule_1 (79 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_32768_ndim_1_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_32768_ndim_1_use_tma_store_0_auto_schedule_1 (70 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_32768_ndim_2_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_32768_ndim_2_use_tma_store_0_auto_schedule_1 (74 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_32768_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_32768_ndim_3_use_tma_store_0_auto_schedule_1 (77 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_65536_ndim_1_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_65536_ndim_1_use_tma_store_0_auto_schedule_1 (70 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_65536_ndim_2_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_65536_ndim_2_use_tma_store_0_auto_schedule_1 (74 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_65536_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_65536_ndim_3_use_tma_store_0_auto_schedule_1 (77 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_131072_ndim_1_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_131072_ndim_1_use_tma_store_0_auto_schedule_1 (70 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_131072_ndim_2_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_131072_ndim_2_use_tma_store_0_auto_schedule_1 (74 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_131072_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_131072_ndim_3_use_tma_store_0_auto_schedule_1 (78 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_262144_ndim_1_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_262144_ndim_1_use_tma_store_0_auto_schedule_1 (70 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_262144_ndim_2_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_262144_ndim_2_use_tma_store_0_auto_schedule_1 (74 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_262144_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_262144_ndim_3_use_tma_store_0_auto_schedule_1 (78 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_524288_ndim_1_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_524288_ndim_1_use_tma_store_0_auto_schedule_1 (70 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_524288_ndim_2_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_524288_ndim_2_use_tma_store_0_auto_schedule_1 (74 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_524288_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_524288_ndim_3_use_tma_store_0_auto_schedule_1 (81 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_1048576_ndim_1_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_1048576_ndim_1_use_tma_store_0_auto_schedule_1 (70 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_1048576_ndim_2_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_1048576_ndim_2_use_tma_store_0_auto_schedule_1 (74 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_1048576_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_1048576_ndim_3_use_tma_store_0_auto_schedule_1 (78 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_1048584_ndim_1_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_1048584_ndim_1_use_tma_store_0_auto_schedule_1 (116 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_1048584_ndim_2_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_1048584_ndim_2_use_tma_store_0_auto_schedule_1 (120 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_1048584_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_1048584_ndim_3_use_tma_store_0_auto_schedule_1 (115 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_1048583_ndim_1_use_tma_store_0_auto_schedule_1
/opt/pytorch/nvfuser/tests/cpp/test_pointwise.cpp:1535: Skipped
Total bytes is not divisible by 16, can't use TMA, total_elem_count: 1048583, dtype_bytes: 4

[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_1048583_ndim_1_use_tma_store_0_auto_schedule_1 (0 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_1048583_ndim_2_use_tma_store_0_auto_schedule_1
/opt/pytorch/nvfuser/tests/cpp/test_pointwise.cpp:1535: Skipped
Total bytes is not divisible by 16, can't use TMA, total_elem_count: 2097166, dtype_bytes: 4

[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_1048583_ndim_2_use_tma_store_0_auto_schedule_1 (0 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_1048583_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_1048583_ndim_3_use_tma_store_0_auto_schedule_1 (118 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_1023_ndim_1_use_tma_store_0_auto_schedule_1
/opt/pytorch/nvfuser/tests/cpp/test_pointwise.cpp:1535: Skipped
Total bytes is not divisible by 16, can't use TMA, total_elem_count: 1023, dtype_bytes: 4

[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_1023_ndim_1_use_tma_store_0_auto_schedule_1 (0 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_1023_ndim_2_use_tma_store_0_auto_schedule_1
/opt/pytorch/nvfuser/tests/cpp/test_pointwise.cpp:1535: Skipped
Total bytes is not divisible by 16, can't use TMA, total_elem_count: 2046, dtype_bytes: 4

[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_1023_ndim_2_use_tma_store_0_auto_schedule_1 (0 ms)
[ RUN      ] TmaPointwiseTest.NoBroadcast/dim0_1023_ndim_3_use_tma_store_0_auto_schedule_1
[       OK ] TmaPointwiseTest.NoBroadcast/dim0_1023_ndim_3_use_tma_store_0_auto_schedule_1 (114 ms)
[----------] 72 tests from TmaPointwiseTest (5400 ms total)

[----------] 2 tests from TmaPointwiseBcastTest
[ RUN      ] TmaPointwiseBcastTest.InnerOuterBcast/use_auto_scheduler_1_tma_inner_bcast_0_tma_outer_bcast_0
[       OK ] TmaPointwiseBcastTest.InnerOuterBcast/use_auto_scheduler_1_tma_inner_bcast_0_tma_outer_bcast_0 (127 ms)
[ RUN      ] TmaPointwiseBcastTest.InnerOuterBcast/use_auto_scheduler_0_tma_inner_bcast_1_tma_outer_bcast_0
[       OK ] TmaPointwiseBcastTest.InnerOuterBcast/use_auto_scheduler_0_tma_inner_bcast_1_tma_outer_bcast_0 (113 ms)
[----------] 2 tests from TmaPointwiseBcastTest (240 ms total)

[----------] 3 tests from TranslateNoReductionMatmulTest
[ RUN      ] TranslateNoReductionMatmulTest.Test/8x1_1
[       OK ] TranslateNoReductionMatmulTest.Test/8x1_1 (107 ms)
[ RUN      ] TranslateNoReductionMatmulTest.Test/2x3x4x1_2x3x1x5
[       OK ] TranslateNoReductionMatmulTest.Test/2x3x4x1_2x3x1x5 (145 ms)
[ RUN      ] TranslateNoReductionMatmulTest.Test/1_2x3x1x5
[       OK ] TranslateNoReductionMatmulTest.Test/1_2x3x1x5 (98 ms)
[----------] 3 tests from TranslateNoReductionMatmulTest (351 ms total)

[----------] 16 tests from TmaInnerReductionManualTest
[ RUN      ] TmaInnerReductionManualTest.Basic/ndim_2_inner_size_1
/opt/pytorch/nvfuser/tests/cpp/test_reduction.cpp:2672: Skipped
Reduced bytes is not divisible by 16, can't use TMA, reduced_elem_count: 1, dtype_bytes: 4

[  SKIPPED ] TmaInnerReductionManualTest.Basic/ndim_2_inner_size_1 (0 ms)
[ RUN      ] TmaInnerReductionManualTest.Basic/ndim_2_inner_size_8
[       OK ] TmaInnerReductionManualTest.Basic/ndim_2_inner_size_8 (114 ms)
[ RUN      ] TmaInnerReductionManualTest.Basic/ndim_2_inner_size_64
[       OK ] TmaInnerReductionManualTest.Basic/ndim_2_inner_size_64 (63 ms)
[ RUN      ] TmaInnerReductionManualTest.Basic/ndim_2_inner_size_512
[       OK ] TmaInnerReductionManualTest.Basic/ndim_2_inner_size_512 (63 ms)
[ RUN      ] TmaInnerReductionManualTest.Basic/ndim_2_inner_size_4096
[       OK ] TmaInnerReductionManualTest.Basic/ndim_2_inner_size_4096 (63 ms)
[ RUN      ] TmaInnerReductionManualTest.Basic/ndim_2_inner_size_32768
[       OK ] TmaInnerReductionManualTest.Basic/ndim_2_inner_size_32768 (64 ms)
[ RUN      ] TmaInnerReductionManualTest.Basic/ndim_2_inner_size_262144
[       OK ] TmaInnerReductionManualTest.Basic/ndim_2_inner_size_262144 (126 ms)
[ RUN      ] TmaInnerReductionManualTest.Basic/ndim_2_inner_size_1048584
[       OK ] TmaInnerReductionManualTest.Basic/ndim_2_inner_size_1048584 (150 ms)
[ RUN      ] TmaInnerReductionManualTest.Basic/ndim_3_inner_size_1
[       OK ] TmaInnerReductionManualTest.Basic/ndim_3_inner_size_1 (112 ms)
[ RUN      ] TmaInnerReductionManualTest.Basic/ndim_3_inner_size_8
[       OK ] TmaInnerReductionManualTest.Basic/ndim_3_inner_size_8 (65 ms)
[ RUN      ] TmaInnerReductionManualTest.Basic/ndim_3_inner_size_64
[       OK ] TmaInnerReductionManualTest.Basic/ndim_3_inner_size_64 (70 ms)
[ RUN      ] TmaInnerReductionManualTest.Basic/ndim_3_inner_size_512
[       OK ] TmaInnerReductionManualTest.Basic/ndim_3_inner_size_512 (66 ms)
[ RUN      ] TmaInnerReductionManualTest.Basic/ndim_3_inner_size_4096
[       OK ] TmaInnerReductionManualTest.Basic/ndim_3_inner_size_4096 (65 ms)
[ RUN      ] TmaInnerReductionManualTest.Basic/ndim_3_inner_size_32768
[       OK ] TmaInnerReductionManualTest.Basic/ndim_3_inner_size_32768 (127 ms)
[ RUN      ] TmaInnerReductionManualTest.Basic/ndim_3_inner_size_262144
[       OK ] TmaInnerReductionManualTest.Basic/ndim_3_inner_size_262144 (72 ms)
[ RUN      ] TmaInnerReductionManualTest.Basic/ndim_3_inner_size_1048584
[       OK ] TmaInnerReductionManualTest.Basic/ndim_3_inner_size_1048584 (158 ms)
[----------] 16 tests from TmaInnerReductionManualTest (1382 ms total)

[----------] 16 tests from TmaInnerReductionTest
[ RUN      ] TmaInnerReductionTest.Sum/float_1
[       OK ] TmaInnerReductionTest.Sum/float_1 (94 ms)
[ RUN      ] TmaInnerReductionTest.Sum/float_8
[       OK ] TmaInnerReductionTest.Sum/float_8 (103 ms)
[ RUN      ] TmaInnerReductionTest.Sum/float_64
[       OK ] TmaInnerReductionTest.Sum/float_64 (71 ms)
[ RUN      ] TmaInnerReductionTest.Sum/float_512
[       OK ] TmaInnerReductionTest.Sum/float_512 (111 ms)
[ RUN      ] TmaInnerReductionTest.Sum/float_4096
[       OK ] TmaInnerReductionTest.Sum/float_4096 (67 ms)
[ RUN      ] TmaInnerReductionTest.Sum/float_32768
[       OK ] TmaInnerReductionTest.Sum/float_32768 (67 ms)
[ RUN      ] TmaInnerReductionTest.Sum/float_262144
[       OK ] TmaInnerReductionTest.Sum/float_262144 (115 ms)
[ RUN      ] TmaInnerReductionTest.Sum/float_1048584
[       OK ] TmaInnerReductionTest.Sum/float_1048584 (123 ms)
[ RUN      ] TmaInnerReductionTest.Sum/__bfloat_1
[       OK ] TmaInnerReductionTest.Sum/__bfloat_1 (100 ms)
[ RUN      ] TmaInnerReductionTest.Sum/__bfloat_8
[       OK ] TmaInnerReductionTest.Sum/__bfloat_8 (110 ms)
[ RUN      ] TmaInnerReductionTest.Sum/__bfloat_64
[       OK ] TmaInnerReductionTest.Sum/__bfloat_64 (75 ms)
[ RUN      ] TmaInnerReductionTest.Sum/__bfloat_512
[       OK ] TmaInnerReductionTest.Sum/__bfloat_512 (130 ms)
[ RUN      ] TmaInnerReductionTest.Sum/__bfloat_4096
[       OK ] TmaInnerReductionTest.Sum/__bfloat_4096 (71 ms)
[ RUN      ] TmaInnerReductionTest.Sum/__bfloat_32768
[       OK ] TmaInnerReductionTest.Sum/__bfloat_32768 (71 ms)
[ RUN      ] TmaInnerReductionTest.Sum/__bfloat_262144
[       OK ] TmaInnerReductionTest.Sum/__bfloat_262144 (123 ms)
[ RUN      ] TmaInnerReductionTest.Sum/__bfloat_1048584
[       OK ] TmaInnerReductionTest.Sum/__bfloat_1048584 (140 ms)
[----------] 16 tests from TmaInnerReductionTest (1578 ms total)

[----------] 1 test from MistralRopeTest
[ RUN      ] MistralRopeTest.Fwd1/32_128_8_128_1_4096
[       OK ] MistralRopeTest.Fwd1/32_128_8_128_1_4096 (234 ms)
[----------] 1 test from MistralRopeTest (234 ms total)

[----------] 1 test from Phi3RopeTest
[ RUN      ] Phi3RopeTest.Fwd/32_96_32_128_1_8192
[       OK ] Phi3RopeTest.Fwd/32_96_32_128_1_8192 (925 ms)
[----------] 1 test from Phi3RopeTest (925 ms total)

[----------] 3 tests from LitgptRopeTest
[ RUN      ] LitgptRopeTest.Fwd/32_128_8_128_2_8192
[       OK ] LitgptRopeTest.Fwd/32_128_8_128_2_8192 (849 ms)
[ RUN      ] LitgptRopeTest.Bwd/32_128_32_128_2_4096
[       OK ] LitgptRopeTest.Bwd/32_128_32_128_2_4096 (2002 ms)
[ RUN      ] LitgptRopeTest.Bwd/8_16_4_16_2_8
[       OK ] LitgptRopeTest.Bwd/8_16_4_16_2_8 (1873 ms)
[----------] 3 tests from LitgptRopeTest (4726 ms total)

[----------] 12 tests from ScatterAccumulateTest
[ RUN      ] ScatterAccumulateTest.BlockParallelScatterAccumulate/8_8_int64_t_min
[       OK ] ScatterAccumulateTest.BlockParallelScatterAccumulate/8_8_int64_t_min (71 ms)
[ RUN      ] ScatterAccumulateTest.BlockParallelScatterAccumulate/8_8_float_min
[       OK ] ScatterAccumulateTest.BlockParallelScatterAccumulate/8_8_float_min (2 ms)
[ RUN      ] ScatterAccumulateTest.BlockParallelScatterAccumulate/8_32_int64_t_min
[       OK ] ScatterAccumulateTest.BlockParallelScatterAccumulate/8_32_int64_t_min (71 ms)
[ RUN      ] ScatterAccumulateTest.BlockParallelScatterAccumulate/8_32_float_min
[       OK ] ScatterAccumulateTest.BlockParallelScatterAccumulate/8_32_float_min (2 ms)
[ RUN      ] ScatterAccumulateTest.BlockParallelScatterAccumulate/8_128_int64_t_min
[       OK ] ScatterAccumulateTest.BlockParallelScatterAccumulate/8_128_int64_t_min (52 ms)
[ RUN      ] ScatterAccumulateTest.BlockParallelScatterAccumulate/8_128_float_min
[       OK ] ScatterAccumulateTest.BlockParallelScatterAccumulate/8_128_float_min (2 ms)
[ RUN      ] ScatterAccumulateTest.BlockParallelScatterAccumulate/32_8_int64_t_min
[       OK ] ScatterAccumulateTest.BlockParallelScatterAccumulate/32_8_int64_t_min (71 ms)
[ RUN      ] ScatterAccumulateTest.BlockParallelScatterAccumulate/32_8_float_min
[       OK ] ScatterAccumulateTest.BlockParallelScatterAccumulate/32_8_float_min (2 ms)
[ RUN      ] ScatterAccumulateTest.BlockParallelScatterAccumulate/32_32_int64_t_min
[       OK ] ScatterAccumulateTest.BlockParallelScatterAccumulate/32_32_int64_t_min (52 ms)
[ RUN      ] ScatterAccumulateTest.BlockParallelScatterAccumulate/32_32_float_min
[       OK ] ScatterAccumulateTest.BlockParallelScatterAccumulate/32_32_float_min (2 ms)
[ RUN      ] ScatterAccumulateTest.BlockParallelScatterAccumulate/32_128_int64_t_min
[       OK ] ScatterAccumulateTest.BlockParallelScatterAccumulate/32_128_int64_t_min (73 ms)
[ RUN      ] ScatterAccumulateTest.BlockParallelScatterAccumulate/32_128_float_min
[       OK ] ScatterAccumulateTest.BlockParallelScatterAccumulate/32_128_float_min (2 ms)
[----------] 12 tests from ScatterAccumulateTest (408 ms total)

[----------] 3 tests from TMemAllocationSize
[ RUN      ] TMemAllocationSize.CopyKernel/63cols
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] TMemAllocationSize.CopyKernel/63cols (0 ms)
[ RUN      ] TMemAllocationSize.CopyKernel/127cols
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] TMemAllocationSize.CopyKernel/127cols (0 ms)
[ RUN      ] TMemAllocationSize.CopyKernel/255cols
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] TMemAllocationSize.CopyKernel/255cols (0 ms)
[----------] 3 tests from TMemAllocationSize (0 ms total)

[----------] 3 tests from UnaryTests/UnaryTest
[ RUN      ] UnaryTests/UnaryTest.Neg/__half
[       OK ] UnaryTests/UnaryTest.Neg/__half (91 ms)
[ RUN      ] UnaryTests/UnaryTest.Neg/double
[       OK ] UnaryTests/UnaryTest.Neg/double (87 ms)
[ RUN      ] UnaryTests/UnaryTest.Neg/std__complex_float_
[       OK ] UnaryTests/UnaryTest.Neg/std__complex_float_ (90 ms)
[----------] 3 tests from UnaryTests/UnaryTest (269 ms total)

[----------] 9 tests from VectorizationCastTest
[ RUN      ] VectorizationCastTest.CastKernel/from_float_to___half_Vectorize_4
[       OK ] VectorizationCastTest.CastKernel/from_float_to___half_Vectorize_4 (78 ms)
[ RUN      ] VectorizationCastTest.CastKernel/from_float_to___e4m3_Vectorize_2
[       OK ] VectorizationCastTest.CastKernel/from_float_to___e4m3_Vectorize_2 (75 ms)
[ RUN      ] VectorizationCastTest.CastKernel/from_float_to___e5m2_Vectorize_4
[       OK ] VectorizationCastTest.CastKernel/from_float_to___e5m2_Vectorize_4 (78 ms)
[ RUN      ] VectorizationCastTest.CastKernel/from___half_to___e4m3_Vectorize_4
[       OK ] VectorizationCastTest.CastKernel/from___half_to___e4m3_Vectorize_4 (78 ms)
[ RUN      ] VectorizationCastTest.CastKernel/from___e5m2_to___half_Vectorize_2
[       OK ] VectorizationCastTest.CastKernel/from___e5m2_to___half_Vectorize_2 (74 ms)
[ RUN      ] VectorizationCastTest.CastKernel/from_float_to___e2m1_Vectorize_2
/opt/pytorch/nvfuser/tests/cpp/test_vectorization.cpp:714: Skipped
Requires GPU capability above 10.0 to run.


[  SKIPPED ] VectorizationCastTest.CastKernel/from_float_to___e2m1_Vectorize_2 (0 ms)
[ RUN      ] VectorizationCastTest.CastKernel/from___e2m1_to___half_Vectorize_4
/opt/pytorch/nvfuser/tests/cpp/test_vectorization.cpp:714: Skipped
Requires GPU capability above 10.0 to run.


[  SKIPPED ] VectorizationCastTest.CastKernel/from___e2m1_to___half_Vectorize_4 (0 ms)
[ RUN      ] VectorizationCastTest.CastKernel/from___bfloat_to___e8m0_Vectorize_2
/opt/pytorch/nvfuser/tests/cpp/test_vectorization.cpp:714: Skipped
Requires GPU capability above 10.0 to run.


[  SKIPPED ] VectorizationCastTest.CastKernel/from___bfloat_to___e8m0_Vectorize_2 (0 ms)
[ RUN      ] VectorizationCastTest.CastKernel/from___e8m0_to___bfloat_Vectorize_4
/opt/pytorch/nvfuser/tests/cpp/test_vectorization.cpp:714: Skipped
Requires GPU capability above 10.0 to run.


[  SKIPPED ] VectorizationCastTest.CastKernel/from___e8m0_to___bfloat_Vectorize_4 (0 ms)
[----------] 9 tests from VectorizationCastTest (385 ms total)

[----------] 4 tests from Vect256Test
[ RUN      ] Vect256Test.DateTypeCacheOps/dtype_double_cache_op_Global
/opt/pytorch/nvfuser/tests/cpp/test_vectorization.cpp:855: Skipped
Requires GPU capability above 10.0 to run.


[  SKIPPED ] Vect256Test.DateTypeCacheOps/dtype_double_cache_op_Global (0 ms)
[ RUN      ] Vect256Test.DateTypeCacheOps/dtype_float_cache_op_Global
/opt/pytorch/nvfuser/tests/cpp/test_vectorization.cpp:855: Skipped
Requires GPU capability above 10.0 to run.


[  SKIPPED ] Vect256Test.DateTypeCacheOps/dtype_float_cache_op_Global (0 ms)
[ RUN      ] Vect256Test.DateTypeCacheOps/dtype___half_cache_op_Global
/opt/pytorch/nvfuser/tests/cpp/test_vectorization.cpp:855: Skipped
Requires GPU capability above 10.0 to run.


[  SKIPPED ] Vect256Test.DateTypeCacheOps/dtype___half_cache_op_Global (0 ms)
[ RUN      ] Vect256Test.DateTypeCacheOps/dtype___bfloat_cache_op_Global
/opt/pytorch/nvfuser/tests/cpp/test_vectorization.cpp:855: Skipped
Requires GPU capability above 10.0 to run.


[  SKIPPED ] Vect256Test.DateTypeCacheOps/dtype___bfloat_cache_op_Global (0 ms)
[----------] 4 tests from Vect256Test (0 ms total)

[----------] 168 tests from WelfordReductionTest
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_1_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_double_redu_1_iter_320_axis_0 (150 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_2_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_double_redu_2_iter_160_axis_1 (129 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_4_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_double_redu_4_iter_160_axis_0 (137 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_4_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_double_redu_4_iter_320_axis_1 (130 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_8_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_double_redu_8_iter_320_axis_0 (169 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_16_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_double_redu_16_iter_160_axis_1 (78 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_32_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_double_redu_32_iter_160_axis_0 (218 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_32_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_double_redu_32_iter_320_axis_1 (78 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_64_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_double_redu_64_iter_320_axis_0 (239 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_128_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_double_redu_128_iter_160_axis_1 (79 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_256_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_double_redu_256_iter_160_axis_0 (95 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_256_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_double_redu_256_iter_320_axis_1 (154 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_512_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_double_redu_512_iter_320_axis_0 (127 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_1024_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_double_redu_1024_iter_160_axis_1 (139 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_2048_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_double_redu_2048_iter_160_axis_0 (397 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_2048_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_double_redu_2048_iter_320_axis_1 (78 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_4096_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_double_redu_4096_iter_320_axis_0 (150 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_8192_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_double_redu_8192_iter_160_axis_1 (78 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_16384_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_double_redu_16384_iter_160_axis_0 (149 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_16384_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_double_redu_16384_iter_320_axis_1 (78 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_32768_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_double_redu_32768_iter_320_axis_0 (149 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_65536_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_double_redu_65536_iter_160_axis_1 (78 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_131072_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_double_redu_131072_iter_160_axis_0 (281 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_131072_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_double_redu_131072_iter_320_axis_1 (78 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_262144_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_double_redu_262144_iter_320_axis_0 (149 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_524288_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_double_redu_524288_iter_160_axis_1 (78 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_1048576_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_double_redu_1048576_iter_160_axis_0 (116 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_double_redu_1048576_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_double_redu_1048576_iter_320_axis_1 (86 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_1_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_float_redu_1_iter_320_axis_0 (149 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_2_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_float_redu_2_iter_160_axis_1 (129 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_4_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_float_redu_4_iter_160_axis_0 (135 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_4_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_float_redu_4_iter_320_axis_1 (76 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_8_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_float_redu_8_iter_320_axis_0 (166 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_16_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_float_redu_16_iter_160_axis_1 (76 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_32_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_float_redu_32_iter_160_axis_0 (259 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_32_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_float_redu_32_iter_320_axis_1 (78 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_64_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_float_redu_64_iter_320_axis_0 (233 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_128_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_float_redu_128_iter_160_axis_1 (76 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_256_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_float_redu_256_iter_160_axis_0 (300 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_256_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_float_redu_256_iter_320_axis_1 (150 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_512_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_float_redu_512_iter_320_axis_0 (125 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_1024_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_float_redu_1024_iter_160_axis_1 (158 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_2048_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_float_redu_2048_iter_160_axis_0 (96 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_2048_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_float_redu_2048_iter_320_axis_1 (146 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_4096_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_float_redu_4096_iter_320_axis_0 (568 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_8192_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_float_redu_8192_iter_160_axis_1 (78 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_16384_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_float_redu_16384_iter_160_axis_0 (384 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_16384_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_float_redu_16384_iter_320_axis_1 (78 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_32768_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_float_redu_32768_iter_320_axis_0 (149 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_65536_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_float_redu_65536_iter_160_axis_1 (77 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_131072_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_float_redu_131072_iter_160_axis_0 (375 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_131072_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_float_redu_131072_iter_320_axis_1 (79 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_262144_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_float_redu_262144_iter_320_axis_0 (150 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_524288_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_float_redu_524288_iter_160_axis_1 (78 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_1048576_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_float_redu_1048576_iter_160_axis_0 (115 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_float_redu_1048576_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_float_redu_1048576_iter_320_axis_1 (80 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_1_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype___half_redu_1_iter_320_axis_0 (159 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_2_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype___half_redu_2_iter_160_axis_1 (134 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_4_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype___half_redu_4_iter_160_axis_0 (141 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_4_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype___half_redu_4_iter_320_axis_1 (133 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_8_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype___half_redu_8_iter_320_axis_0 (176 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_16_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype___half_redu_16_iter_160_axis_1 (80 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_32_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype___half_redu_32_iter_160_axis_0 (272 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_32_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype___half_redu_32_iter_320_axis_1 (81 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_64_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype___half_redu_64_iter_320_axis_0 (247 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_128_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype___half_redu_128_iter_160_axis_1 (80 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_256_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype___half_redu_256_iter_160_axis_0 (314 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_256_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype___half_redu_256_iter_320_axis_1 (157 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_512_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype___half_redu_512_iter_320_axis_0 (134 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_1024_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype___half_redu_1024_iter_160_axis_1 (180 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_2048_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype___half_redu_2048_iter_160_axis_0 (101 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_2048_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype___half_redu_2048_iter_320_axis_1 (94 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_4096_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype___half_redu_4096_iter_320_axis_0 (583 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_8192_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype___half_redu_8192_iter_160_axis_1 (165 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_16384_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype___half_redu_16384_iter_160_axis_0 (404 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_16384_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype___half_redu_16384_iter_320_axis_1 (82 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_32768_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype___half_redu_32768_iter_320_axis_0 (159 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_65536_iter_160_axis_1
/opt/pytorch/nvfuser/tests/cpp/test_welford.cpp:888: Skipped
Skipping large reduction dims (65536) for half and bfloat16

[  SKIPPED ] WelfordReductionTest.Test/dtype___half_redu_65536_iter_160_axis_1 (0 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_131072_iter_160_axis_0
/opt/pytorch/nvfuser/tests/cpp/test_welford.cpp:888: Skipped
Skipping large reduction dims (131072) for half and bfloat16

[  SKIPPED ] WelfordReductionTest.Test/dtype___half_redu_131072_iter_160_axis_0 (0 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_131072_iter_320_axis_1
/opt/pytorch/nvfuser/tests/cpp/test_welford.cpp:888: Skipped
Skipping large reduction dims (131072) for half and bfloat16

[  SKIPPED ] WelfordReductionTest.Test/dtype___half_redu_131072_iter_320_axis_1 (0 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_262144_iter_320_axis_0
/opt/pytorch/nvfuser/tests/cpp/test_welford.cpp:888: Skipped
Skipping large reduction dims (262144) for half and bfloat16

[  SKIPPED ] WelfordReductionTest.Test/dtype___half_redu_262144_iter_320_axis_0 (0 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_524288_iter_160_axis_1
/opt/pytorch/nvfuser/tests/cpp/test_welford.cpp:888: Skipped
Skipping large reduction dims (524288) for half and bfloat16

[  SKIPPED ] WelfordReductionTest.Test/dtype___half_redu_524288_iter_160_axis_1 (0 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_1048576_iter_160_axis_0
/opt/pytorch/nvfuser/tests/cpp/test_welford.cpp:888: Skipped
Skipping large reduction dims (1048576) for half and bfloat16

[  SKIPPED ] WelfordReductionTest.Test/dtype___half_redu_1048576_iter_160_axis_0 (0 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___half_redu_1048576_iter_320_axis_1
/opt/pytorch/nvfuser/tests/cpp/test_welford.cpp:888: Skipped
Skipping large reduction dims (1048576) for half and bfloat16

[  SKIPPED ] WelfordReductionTest.Test/dtype___half_redu_1048576_iter_320_axis_1 (0 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_1_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_1_iter_320_axis_0 (215 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_2_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_2_iter_160_axis_1 (177 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_4_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_4_iter_160_axis_0 (205 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_4_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_4_iter_320_axis_1 (177 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_8_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_8_iter_320_axis_0 (251 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_16_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_16_iter_160_axis_1 (100 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_32_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_32_iter_160_axis_0 (409 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_32_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_32_iter_320_axis_1 (101 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_64_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_64_iter_320_axis_0 (380 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_128_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_128_iter_160_axis_1 (100 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_256_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_256_iter_160_axis_0 (126 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_256_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_256_iter_320_axis_1 (218 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_512_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_512_iter_320_axis_0 (183 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_1024_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_1024_iter_160_axis_1 (197 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_2048_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_2048_iter_160_axis_0 (680 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_2048_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_2048_iter_320_axis_1 (100 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_4096_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_4096_iter_320_axis_0 (220 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_8192_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_8192_iter_160_axis_1 (102 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_16384_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_16384_iter_160_axis_0 (219 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_16384_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_16384_iter_320_axis_1 (101 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_32768_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_32768_iter_320_axis_0 (218 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_65536_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_65536_iter_160_axis_1 (100 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_131072_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_131072_iter_160_axis_0 (526 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_131072_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_131072_iter_320_axis_1 (101 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_262144_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_262144_iter_320_axis_0 (222 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_524288_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_524288_iter_160_axis_1 (101 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_1048576_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_1048576_iter_160_axis_0 (156 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_float__redu_1048576_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_float__redu_1048576_iter_320_axis_1 (109 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_1_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_1_iter_320_axis_0 (137 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_2_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_2_iter_160_axis_1 (177 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_4_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_4_iter_160_axis_0 (213 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_4_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_4_iter_320_axis_1 (176 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_8_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_8_iter_320_axis_0 (273 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_16_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_16_iter_160_axis_1 (100 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_32_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_32_iter_160_axis_0 (330 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_32_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_32_iter_320_axis_1 (101 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_64_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_64_iter_320_axis_0 (127 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_128_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_128_iter_160_axis_1 (100 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_256_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_256_iter_160_axis_0 (127 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_256_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_256_iter_320_axis_1 (223 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_512_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_512_iter_320_axis_0 (127 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_1024_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_1024_iter_160_axis_1 (216 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_2048_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_2048_iter_160_axis_0 (438 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_2048_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_2048_iter_320_axis_1 (203 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_4096_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_4096_iter_320_axis_0 (156 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_8192_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_8192_iter_160_axis_1 (101 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_16384_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_16384_iter_160_axis_0 (156 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_16384_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_16384_iter_320_axis_1 (101 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_32768_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_32768_iter_320_axis_0 (155 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_65536_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_65536_iter_160_axis_1 (101 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_131072_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_131072_iter_160_axis_0 (155 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_131072_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_131072_iter_320_axis_1 (101 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_262144_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_262144_iter_320_axis_0 (158 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_524288_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_524288_iter_160_axis_1 (105 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_1048576_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_1048576_iter_160_axis_0 (162 ms)
[ RUN      ] WelfordReductionTest.Test/dtype_std__complex_double__redu_1048576_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype_std__complex_double__redu_1048576_iter_320_axis_1 (115 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_1_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype___bfloat_redu_1_iter_320_axis_0 (160 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_2_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype___bfloat_redu_2_iter_160_axis_1 (133 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_4_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype___bfloat_redu_4_iter_160_axis_0 (141 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_4_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype___bfloat_redu_4_iter_320_axis_1 (133 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_8_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype___bfloat_redu_8_iter_320_axis_0 (176 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_16_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype___bfloat_redu_16_iter_160_axis_1 (80 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_32_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype___bfloat_redu_32_iter_160_axis_0 (271 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_32_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype___bfloat_redu_32_iter_320_axis_1 (81 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_64_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype___bfloat_redu_64_iter_320_axis_0 (245 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_128_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype___bfloat_redu_128_iter_160_axis_1 (80 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_256_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype___bfloat_redu_256_iter_160_axis_0 (315 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_256_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype___bfloat_redu_256_iter_320_axis_1 (156 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_512_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype___bfloat_redu_512_iter_320_axis_0 (134 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_1024_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype___bfloat_redu_1024_iter_160_axis_1 (179 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_2048_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype___bfloat_redu_2048_iter_160_axis_0 (100 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_2048_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype___bfloat_redu_2048_iter_320_axis_1 (92 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_4096_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype___bfloat_redu_4096_iter_320_axis_0 (580 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_8192_iter_160_axis_1
[       OK ] WelfordReductionTest.Test/dtype___bfloat_redu_8192_iter_160_axis_1 (165 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_16384_iter_160_axis_0
[       OK ] WelfordReductionTest.Test/dtype___bfloat_redu_16384_iter_160_axis_0 (399 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_16384_iter_320_axis_1
[       OK ] WelfordReductionTest.Test/dtype___bfloat_redu_16384_iter_320_axis_1 (82 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_32768_iter_320_axis_0
[       OK ] WelfordReductionTest.Test/dtype___bfloat_redu_32768_iter_320_axis_0 (161 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_65536_iter_160_axis_1
/opt/pytorch/nvfuser/tests/cpp/test_welford.cpp:888: Skipped
Skipping large reduction dims (65536) for half and bfloat16

[  SKIPPED ] WelfordReductionTest.Test/dtype___bfloat_redu_65536_iter_160_axis_1 (0 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_131072_iter_160_axis_0
/opt/pytorch/nvfuser/tests/cpp/test_welford.cpp:888: Skipped
Skipping large reduction dims (131072) for half and bfloat16

[  SKIPPED ] WelfordReductionTest.Test/dtype___bfloat_redu_131072_iter_160_axis_0 (0 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_131072_iter_320_axis_1
/opt/pytorch/nvfuser/tests/cpp/test_welford.cpp:888: Skipped
Skipping large reduction dims (131072) for half and bfloat16

[  SKIPPED ] WelfordReductionTest.Test/dtype___bfloat_redu_131072_iter_320_axis_1 (0 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_262144_iter_320_axis_0
/opt/pytorch/nvfuser/tests/cpp/test_welford.cpp:888: Skipped
Skipping large reduction dims (262144) for half and bfloat16

[  SKIPPED ] WelfordReductionTest.Test/dtype___bfloat_redu_262144_iter_320_axis_0 (0 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_524288_iter_160_axis_1
/opt/pytorch/nvfuser/tests/cpp/test_welford.cpp:888: Skipped
Skipping large reduction dims (524288) for half and bfloat16

[  SKIPPED ] WelfordReductionTest.Test/dtype___bfloat_redu_524288_iter_160_axis_1 (0 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_1048576_iter_160_axis_0
/opt/pytorch/nvfuser/tests/cpp/test_welford.cpp:888: Skipped
Skipping large reduction dims (1048576) for half and bfloat16

[  SKIPPED ] WelfordReductionTest.Test/dtype___bfloat_redu_1048576_iter_160_axis_0 (0 ms)
[ RUN      ] WelfordReductionTest.Test/dtype___bfloat_redu_1048576_iter_320_axis_1
/opt/pytorch/nvfuser/tests/cpp/test_welford.cpp:888: Skipped
Skipping large reduction dims (1048576) for half and bfloat16

[  SKIPPED ] WelfordReductionTest.Test/dtype___bfloat_redu_1048576_iter_320_axis_1 (0 ms)
[----------] 168 tests from WelfordReductionTest (26677 ms total)

[----------] Global test environment tear-down
[==========] 2134 tests from 137 test suites ran. (436760 ms total)
[  PASSED  ] 1831 tests.
[  SKIPPED ] 303 tests, listed below:
[  SKIPPED ] Gpu3Test.FusionVectorizeContigIndexFail_CUDA
[  SKIPPED ] Gpu3Test.FusionVectorizeContigIndexValidationFail_CUDA
[  SKIPPED ] AdvancedIndexingIdModelTest.21
[  SKIPPED ] BlockQuantizationValidationTest.InputMustBeInLocalMemory
[  SKIPPED ] BlockQuantizationValidationTest.InvalidSwizzlePermutationOnBlockScales
[  SKIPPED ] BlockQuantizationValidationTest.GroupIDMustBeInnermost
[  SKIPPED ] BlockQuantizationValidationTest.MergesMustBeContiguous
[  SKIPPED ] TMADocTest.Figure13a
[  SKIPPED ] TMADocTest.Figure13d
[  SKIPPED ] TMADocTest.Figure15c
[  SKIPPED ] RaggedIterDomainTest.ReductionOnComponentDimError
[  SKIPPED ] RopeTest.EndingRepeatWithNoBroadcastOp
[  SKIPPED ] TMemTest.GmemRegTMemRegGmemCopy
[  SKIPPED ] TMemTest.dtypes
[  SKIPPED ] UtilsTest.FunctionTrace1
[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDim/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDim/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDimUnroll/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.SingleDimUnswitch/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MultiDim/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Pointwise/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Pointwise/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg2_M_500_N_1024_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg2_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_2_prefetch_1_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_0_M_1024_N_2048_Pipelined_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_2_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_3_M_500_N_1024_Pipelined_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.PointwiseCpAsync/stage_4_prefetch_3_M_1024_N_2048_PipelinedMBarrierForWAR_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.InnerReduction/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.OuterReduction/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg4_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg4_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg2_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg2_M_1024_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_0_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Persistent/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.Matmul/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_neg2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_neg2_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_neg1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_0_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_2_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg4_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg4_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg3_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg3_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg3_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg3_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg2_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg2_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg1_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg1_M_500_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_neg1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_0_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_0_M_1024_N_1024_WarpSpecializedOnTIDyRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_1_M_128_N_2048_WarpSpecializedOnTIDyRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_1_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_1_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_2_M_128_N_2048_Pipelined_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_2_M_1024_N_1024_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_2_M_1024_N_2048_WarpSpecializedOnTIDxRegisterSharing_NoneStageSlicePosition_None_CpAsyncBulkTensorTile
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_3_M_128_N_2048_WarpSpecializedOnTIDxRegisterSharing_64_168StageSlicePosition_None_CpAsyncBulk
[  SKIPPED ] Hopper/TmaCircularBufferingTest.MatmulWithBroadcastedInput/stage_4_prefetch_3_M_1024_N_1024_Pipelined_CpAsyncBulk
[  SKIPPED ] Fp4CastTest.Fp4ToHighPrecision/float_Vectorize_8
[  SKIPPED ] Fp4CastTest.Fp4ToHighPrecision/double_Vectorize_4
[  SKIPPED ] Fp4CastTest.Fp4ToHighPrecision/__bfloat_Vectorize_2
[  SKIPPED ] Fp4CastTest.Fp4ToHighPrecision/__bfloat_Vectorize_16
[  SKIPPED ] Fp4CastTest.Fp4ToHighPrecision/__half_Vectorize_8
[  SKIPPED ] Fp4CastTest.HighPrecisionToFp4/float_Vectorize_4
[  SKIPPED ] Fp4CastTest.HighPrecisionToFp4/double_Vectorize_2
[  SKIPPED ] Fp4CastTest.HighPrecisionToFp4/double_Vectorize_16
[  SKIPPED ] Fp4CastTest.HighPrecisionToFp4/__bfloat_Vectorize_8
[  SKIPPED ] Fp4CastTest.HighPrecisionToFp4/__half_Vectorize_4
[  SKIPPED ] MXFP8QuantizationTest.AutoScheduleOp/float_2048x128
[  SKIPPED ] MXFP8QuantizationTest.AutoScheduleOp/__bfloat_2048x128
[  SKIPPED ] NVFP4QuantizeTest.WithoutPerTensorAmax/float
[  SKIPPED ] NVFP4QuantizeTest.WithPerTensorAmax/__bfloat
[  SKIPPED ] BlockQuantizationTest.ScheduleAsPointwise/float_GroupWidth4
[  SKIPPED ] BlockQuantizationTest.ScheduleAsPointwise/__bfloat_GroupWidth8
[  SKIPPED ] BlockQuantizationTest.ScheduleAsPointwise/__half_GroupWidth8
[  SKIPPED ] BlockQuantizationTest.ScheduleAsPointwise2D/__bfloat_GroupWidth2
[  SKIPPED ] BlockQuantizationTest.ScheduleAsPointwise2D/__half_GroupWidth2
[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/float_1024x1024_NoGlobalScale_NoSwizzle
[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/float_1024x1024_WithGlobalScale_WithSwizzle
[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/float_128x64_WithGlobalScale_NoSwizzle
[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/float_2048x128_NoGlobalScale_WithSwizzle
[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/float_2048x2048_NoGlobalScale_NoSwizzle
[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/float_2048x2048_WithGlobalScale_WithSwizzle
[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/__bfloat_1024x1024_WithGlobalScale_NoSwizzle
[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/__bfloat_128x64_NoGlobalScale_WithSwizzle
[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/__bfloat_2048x128_NoGlobalScale_NoSwizzle
[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/__bfloat_2048x128_WithGlobalScale_WithSwizzle
[  SKIPPED ] BlockQuantizationSchedulingTestSuite/BlockQuantizationSchedulingTest.AutoScheduleSingleOp/__bfloat_2048x2048_WithGlobalScale_NoSwizzle
[  SKIPPED ] StMatrixTest.Regular/m_64_n_8_tile_m_16_tile_n_16_T
[  SKIPPED ] StMatrixTest.Regular/m_64_n_40_tile_m_16_tile_n_16_H
[  SKIPPED ] StMatrixTest.Regular/m_64_n_56_tile_m_16_tile_n_16_T
[  SKIPPED ] StMatrixTest.Regular/m_64_n_88_tile_m_16_tile_n_16_H
[  SKIPPED ] StMatrixTest.Regular/m_64_n_104_tile_m_16_tile_n_16_T
[  SKIPPED ] StMatrixTest.Regular/m_64_n_136_tile_m_16_tile_n_16_H
[  SKIPPED ] StMatrixTest.Regular/m_64_n_152_tile_m_16_tile_n_16_T
[  SKIPPED ] StMatrixTest.Regular/m_64_n_184_tile_m_16_tile_n_16_H
[  SKIPPED ] StMatrixTest.Regular/m_64_n_200_tile_m_16_tile_n_16_T
[  SKIPPED ] StMatrixTest.Regular/m_64_n_232_tile_m_16_tile_n_16_H
[  SKIPPED ] StMatrixTest.Regular/m_64_n_248_tile_m_16_tile_n_16_T
[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_1_ndim_1_use_tma_store_0_auto_schedule_1
[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_1_ndim_2_use_tma_store_0_auto_schedule_1
[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_1_ndim_3_use_tma_store_0_auto_schedule_1
[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_2_ndim_1_use_tma_store_0_auto_schedule_1
[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_2_ndim_2_use_tma_store_0_auto_schedule_1
[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_4_ndim_1_use_tma_store_0_auto_schedule_1
[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_4_ndim_2_use_tma_store_0_auto_schedule_1
[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_8_ndim_1_use_tma_store_0_auto_schedule_1
[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_1048583_ndim_1_use_tma_store_0_auto_schedule_1
[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_1048583_ndim_2_use_tma_store_0_auto_schedule_1
[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_1023_ndim_1_use_tma_store_0_auto_schedule_1
[  SKIPPED ] TmaPointwiseTest.NoBroadcast/dim0_1023_ndim_2_use_tma_store_0_auto_schedule_1
[  SKIPPED ] TmaInnerReductionManualTest.Basic/ndim_2_inner_size_1
[  SKIPPED ] TMemAllocationSize.CopyKernel/63cols
[  SKIPPED ] TMemAllocationSize.CopyKernel/127cols
[  SKIPPED ] TMemAllocationSize.CopyKernel/255cols
[  SKIPPED ] VectorizationCastTest.CastKernel/from_float_to___e2m1_Vectorize_2
[  SKIPPED ] VectorizationCastTest.CastKernel/from___e2m1_to___half_Vectorize_4
[  SKIPPED ] VectorizationCastTest.CastKernel/from___bfloat_to___e8m0_Vectorize_2
[  SKIPPED ] VectorizationCastTest.CastKernel/from___e8m0_to___bfloat_Vectorize_4
[  SKIPPED ] Vect256Test.DateTypeCacheOps/dtype_double_cache_op_Global
[  SKIPPED ] Vect256Test.DateTypeCacheOps/dtype_float_cache_op_Global
[  SKIPPED ] Vect256Test.DateTypeCacheOps/dtype___half_cache_op_Global
[  SKIPPED ] Vect256Test.DateTypeCacheOps/dtype___bfloat_cache_op_Global
[  SKIPPED ] WelfordReductionTest.Test/dtype___half_redu_65536_iter_160_axis_1
[  SKIPPED ] WelfordReductionTest.Test/dtype___half_redu_131072_iter_160_axis_0
[  SKIPPED ] WelfordReductionTest.Test/dtype___half_redu_131072_iter_320_axis_1
[  SKIPPED ] WelfordReductionTest.Test/dtype___half_redu_262144_iter_320_axis_0
[  SKIPPED ] WelfordReductionTest.Test/dtype___half_redu_524288_iter_160_axis_1
[  SKIPPED ] WelfordReductionTest.Test/dtype___half_redu_1048576_iter_160_axis_0
[  SKIPPED ] WelfordReductionTest.Test/dtype___half_redu_1048576_iter_320_axis_1
[  SKIPPED ] WelfordReductionTest.Test/dtype___bfloat_redu_65536_iter_160_axis_1
[  SKIPPED ] WelfordReductionTest.Test/dtype___bfloat_redu_131072_iter_160_axis_0
[  SKIPPED ] WelfordReductionTest.Test/dtype___bfloat_redu_131072_iter_320_axis_1
[  SKIPPED ] WelfordReductionTest.Test/dtype___bfloat_redu_262144_iter_320_axis_0
[  SKIPPED ] WelfordReductionTest.Test/dtype___bfloat_redu_524288_iter_160_axis_1
[  SKIPPED ] WelfordReductionTest.Test/dtype___bfloat_redu_1048576_iter_160_axis_0
[  SKIPPED ] WelfordReductionTest.Test/dtype___bfloat_redu_1048576_iter_320_axis_1

  YOU HAVE 1 DISABLED TEST

+ rv=0
+ for f in __tmp_*
+ [[ __tmp_* == __tmp_c* ]]
+ fs='__tmp_*'
++ sed s/__tmp_/__tmp_c2_/g
++ echo '__tmp_*'
+ ft='__tmp_c2_*'
+ mv '__tmp_*' '__tmp_c2_*'
mv: cannot stat '__tmp_*': No such file or directory
+ counter=3
+ for f in $test_files
+ [[ /opt/pytorch/nvfuser/bin/test_multidevice_tutorial == *matmul* ]]
+ [[ /opt/pytorch/nvfuser/bin/test_multidevice_tutorial == *multidevice* ]]
+ executable='mpirun -np 1 /opt/pytorch/nvfuser/bin/test_multidevice_tutorial'
+ '[' '' = 1 ']'
++ basename /opt/pytorch/nvfuser/bin/test_multidevice_tutorial
+ report_name=TEST-report-test_multidevice_tutorial.xml
+ sed_flag=s/__tmp_/__tmp_c3_/g
+ mpirun -np 1 /opt/pytorch/nvfuser/bin/test_multidevice_tutorial --gtest_output=xml:TEST-report-test_multidevice_tutorial.xml
+ sed -u s/__tmp_/__tmp_c3_/g
[0;33mNote: This is test shard 2 of 3.
[m[0;32m[==========] [mRunning 3 tests from 1 test suite.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m3 tests from MultiDeviceTutorial
[0;32m[ RUN      ] [mMultiDeviceTutorial.DeviceMeshesNoResharding
[0;32m[       OK ] [mMultiDeviceTutorial.DeviceMeshesNoResharding (434 ms)
[0;32m[ RUN      ] [mMultiDeviceTutorial.HostIrLaunchingFusion
[0;32m[       OK ] [mMultiDeviceTutorial.HostIrLaunchingFusion (124 ms)
[0;32m[ RUN      ] [mMultiDeviceTutorial.HostIrKernelPipelining
[W122 09:16:36.917040144 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit7, Inputs:{T3_l_float[iS5{i2}], T4_l_float[iS6{i4}], }, Outputs:{T4_l_float[iS6{i4}], })
 (function handle)
[W122 09:16:36.992019907 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit5, Inputs:{T2_l_float[iS4{i2}], }, Outputs:{T3_l_float[iS5{i2}], })
 (function handle)
[W122 09:16:36.992358576 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit7, Inputs:{T3_l_float[iS5{i2}], T4_l_float[iS6{i4}], }, Outputs:{T4_l_float[iS6{i4}], })
 (function handle)
[W122 09:16:36.992392525 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit5, Inputs:{T2_l_float[iS4{i2}], }, Outputs:{T3_l_float[iS5{i2}], })
 (function handle)
[W122 09:16:36.992569505 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit7, Inputs:{T3_l_float[iS5{i2}], T4_l_float[iS6{i4}], }, Outputs:{T4_l_float[iS6{i4}], })
 (function handle)
[W122 09:16:36.992602184 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit5, Inputs:{T2_l_float[iS4{i2}], }, Outputs:{T3_l_float[iS5{i2}], })
 (function handle)
[W122 09:16:36.992716494 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit7, Inputs:{T3_l_float[iS5{i2}], T4_l_float[iS6{i4}], }, Outputs:{T4_l_float[iS6{i4}], })
 (function handle)
[W122 09:16:36.992747024 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit5, Inputs:{T2_l_float[iS4{i2}], }, Outputs:{T3_l_float[iS5{i2}], })
 (function handle)
[W122 09:16:36.992774164 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit7, Inputs:{T3_l_float[iS5{i2}], T4_l_float[iS6{i4}], }, Outputs:{T4_l_float[iS6{i4}], })
 (function handle)
[W122 09:16:36.992792693 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit5, Inputs:{T2_l_float[iS4{i2}], }, Outputs:{T3_l_float[iS5{i2}], })
 (function handle)
[W122 09:16:36.992815013 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit7, Inputs:{T3_l_float[iS5{i2}], T4_l_float[iS6{i4}], }, Outputs:{T4_l_float[iS6{i4}], })
 (function handle)
[W122 09:16:36.992832373 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit5, Inputs:{T2_l_float[iS4{i2}], }, Outputs:{T3_l_float[iS5{i2}], })
 (function handle)
[W122 09:16:36.992853853 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit7, Inputs:{T3_l_float[iS5{i2}], T4_l_float[iS6{i4}], }, Outputs:{T4_l_float[iS6{i4}], })
 (function handle)
[W122 09:16:36.992870823 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit5, Inputs:{T2_l_float[iS4{i2}], }, Outputs:{T3_l_float[iS5{i2}], })
 (function handle)
[W122 09:16:36.992892263 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit7, Inputs:{T3_l_float[iS5{i2}], T4_l_float[iS6{i4}], }, Outputs:{T4_l_float[iS6{i4}], })
 (function handle)
[W122 09:16:36.992908723 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit5, Inputs:{T2_l_float[iS4{i2}], }, Outputs:{T3_l_float[iS5{i2}], })
 (function handle)
[W122 09:16:36.992930303 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit7, Inputs:{T3_l_float[iS5{i2}], T4_l_float[iS6{i4}], }, Outputs:{T4_l_float[iS6{i4}], })
 (function handle)
[W122 09:16:36.992946973 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit5, Inputs:{T2_l_float[iS4{i2}], }, Outputs:{T3_l_float[iS5{i2}], })
 (function handle)
[W122 09:16:36.992967493 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit7, Inputs:{T3_l_float[iS5{i2}], T4_l_float[iS6{i4}], }, Outputs:{T4_l_float[iS6{i4}], })
 (function handle)
[W122 09:16:36.992983652 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit5, Inputs:{T2_l_float[iS4{i2}], }, Outputs:{T3_l_float[iS5{i2}], })
 (function handle)
[W122 09:16:36.993011322 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit7, Inputs:{T3_l_float[iS5{i2}], T4_l_float[iS6{i4}], }, Outputs:{T4_l_float[iS6{i4}], })
 (function handle)
[W122 09:16:36.993027902 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit5, Inputs:{T2_l_float[iS4{i2}], }, Outputs:{T3_l_float[iS5{i2}], })
 (function handle)
[W122 09:16:36.993047922 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit7, Inputs:{T3_l_float[iS5{i2}], T4_l_float[iS6{i4}], }, Outputs:{T4_l_float[iS6{i4}], })
 (function handle)
[W122 09:16:36.993064162 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit5, Inputs:{T2_l_float[iS4{i2}], }, Outputs:{T3_l_float[iS5{i2}], })
 (function handle)
[W122 09:16:36.993084672 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit7, Inputs:{T3_l_float[iS5{i2}], T4_l_float[iS6{i4}], }, Outputs:{T4_l_float[iS6{i4}], })
 (function handle)
[W122 09:16:36.993101162 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit5, Inputs:{T2_l_float[iS4{i2}], }, Outputs:{T3_l_float[iS5{i2}], })
 (function handle)
[W122 09:16:36.993121002 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit7, Inputs:{T3_l_float[iS5{i2}], T4_l_float[iS6{i4}], }, Outputs:{T4_l_float[iS6{i4}], })
 (function handle)
[W122 09:16:36.993138642 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit5, Inputs:{T2_l_float[iS4{i2}], }, Outputs:{T3_l_float[iS5{i2}], })
 (function handle)
[W122 09:16:36.993158662 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit7, Inputs:{T3_l_float[iS5{i2}], T4_l_float[iS6{i4}], }, Outputs:{T4_l_float[iS6{i4}], })
 (function handle)
[W122 09:16:36.993175011 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit5, Inputs:{T2_l_float[iS4{i2}], }, Outputs:{T3_l_float[iS5{i2}], })
 (function handle)
[W122 09:16:36.993194651 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit7, Inputs:{T3_l_float[iS5{i2}], T4_l_float[iS6{i4}], }, Outputs:{T4_l_float[iS6{i4}], })
 (function handle)
[0;32m[       OK ] [mMultiDeviceTutorial.HostIrKernelPipelining (160 ms)
[0;32m[----------] [m3 tests from MultiDeviceTutorial (719 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m3 tests from 1 test suite ran. (931 ms total)
[0;32m[  PASSED  ] [m3 tests.
+ rv=0
+ for f in __tmp_*
+ [[ __tmp_* == __tmp_c* ]]
+ fs='__tmp_*'
++ echo '__tmp_*'
++ sed s/__tmp_/__tmp_c3_/g
+ ft='__tmp_c3_*'
+ mv '__tmp_*' '__tmp_c3_*'
mv: cannot stat '__tmp_*': No such file or directory
+ counter=4
+ for f in $test_files
+ [[ /opt/pytorch/nvfuser/bin/test_topk == *matmul* ]]
+ [[ /opt/pytorch/nvfuser/bin/test_topk == *multidevice* ]]
+ executable=/opt/pytorch/nvfuser/bin/test_topk
++ basename /opt/pytorch/nvfuser/bin/test_topk
+ report_name=TEST-report-test_topk.xml
+ sed_flag=s/__tmp_/__tmp_c4_/g
+ /opt/pytorch/nvfuser/bin/test_topk --gtest_output=xml:TEST-report-test_topk.xml
+ sed -u s/__tmp_/__tmp_c4_/g
Running main() from /opt/pytorch/nvfuser/third_party/googletest/googletest/src/gtest_main.cc
Note: This is test shard 2 of 3.
[==========] Running 13 tests from 5 test suites.
[----------] Global test environment set-up.
[----------] 3 tests from TopKDynamicTest
[ RUN      ] TopKDynamicTest.K1BroadcastConcretization
[       OK ] TopKDynamicTest.K1BroadcastConcretization (207 ms)
[ RUN      ] TopKDynamicTest.TopKThenReshape
[       OK ] TopKDynamicTest.TopKThenReshape (0 ms)
[ RUN      ] TopKDynamicTest.KZeroConcretization
[       OK ] TopKDynamicTest.KZeroConcretization (0 ms)
[----------] 3 tests from TopKDynamicTest (208 ms total)

[----------] 1 test from TopKTest
[ RUN      ] TopKTest.ZeroDimensionalInput
[       OK ] TopKTest.ZeroDimensionalInput (89 ms)
[----------] 1 test from TopKTest (90 ms total)

[----------] 2 tests from TopkDeviceFuncTest
[ RUN      ] TopkDeviceFuncTest.VariableKValues
[       OK ] TopkDeviceFuncTest.VariableKValues (43 ms)
[ RUN      ] TopkDeviceFuncTest.MultiDim2dTopkFloat
[       OK ] TopkDeviceFuncTest.MultiDim2dTopkFloat (0 ms)
[----------] 2 tests from TopkDeviceFuncTest (43 ms total)

[----------] 1 test from TopKTest/TopKTestBasicExecution
[ RUN      ] TopKTest/TopKTestBasicExecution.ParameterizedBasicExecution/Half
[       OK ] TopKTest/TopKTestBasicExecution.ParameterizedBasicExecution/Half (1365 ms)
[----------] 1 test from TopKTest/TopKTestBasicExecution (1365 ms total)

[----------] 6 tests from TopKParameterizedWithBlockandBatch
[ RUN      ] TopKParameterizedWithBlockandBatch.SharedMemoryRequirement/128_1_0_0
[       OK ] TopKParameterizedWithBlockandBatch.SharedMemoryRequirement/128_1_0_0 (1261 ms)
[ RUN      ] TopKParameterizedWithBlockandBatch.SharedMemoryRequirement/128_1_1_1
[       OK ] TopKParameterizedWithBlockandBatch.SharedMemoryRequirement/128_1_1_1 (1347 ms)
[ RUN      ] TopKParameterizedWithBlockandBatch.SharedMemoryRequirement/256_1_1_0
[       OK ] TopKParameterizedWithBlockandBatch.SharedMemoryRequirement/256_1_1_0 (1314 ms)
[ RUN      ] TopKParameterizedWithBlockandBatch.SharedMemoryRequirement/512_1_0_1
[       OK ] TopKParameterizedWithBlockandBatch.SharedMemoryRequirement/512_1_0_1 (1334 ms)
[ RUN      ] TopKParameterizedWithBlockandBatch.SharedMemoryRequirement/1024_1_0_0
[       OK ] TopKParameterizedWithBlockandBatch.SharedMemoryRequirement/1024_1_0_0 (1316 ms)
[ RUN      ] TopKParameterizedWithBlockandBatch.SharedMemoryRequirement/1024_1_1_1
[       OK ] TopKParameterizedWithBlockandBatch.SharedMemoryRequirement/1024_1_1_1 (0 ms)
[----------] 6 tests from TopKParameterizedWithBlockandBatch (6575 ms total)

[----------] Global test environment tear-down
[==========] 13 tests from 5 test suites ran. (8282 ms total)
[  PASSED  ] 13 tests.
+ rv=0
+ for f in __tmp_*
+ [[ __tmp_* == __tmp_c* ]]
+ fs='__tmp_*'
++ sed s/__tmp_/__tmp_c4_/g
++ echo '__tmp_*'
+ ft='__tmp_c4_*'
+ mv '__tmp_*' '__tmp_c4_*'
mv: cannot stat '__tmp_*': No such file or directory
+ counter=5
+ for f in $test_files
+ [[ /opt/pytorch/nvfuser/bin/test_layout_op == *matmul* ]]
+ [[ /opt/pytorch/nvfuser/bin/test_layout_op == *multidevice* ]]
+ executable=/opt/pytorch/nvfuser/bin/test_layout_op
++ basename /opt/pytorch/nvfuser/bin/test_layout_op
+ report_name=TEST-report-test_layout_op.xml
+ sed_flag=s/__tmp_/__tmp_c5_/g
+ /opt/pytorch/nvfuser/bin/test_layout_op --gtest_output=xml:TEST-report-test_layout_op.xml
+ sed -u s/__tmp_/__tmp_c5_/g
Running main() from /opt/pytorch/nvfuser/third_party/googletest/googletest/src/gtest_main.cc
Note: This is test shard 2 of 3.
[==========] Running 3 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 3 tests from LayoutOpTest
[ RUN      ] LayoutOpTest.ManualKernel
[       OK ] LayoutOpTest.ManualKernel (353 ms)
[ RUN      ] LayoutOpTest.SchedulerKernelWithOffsetsProducer
[       OK ] LayoutOpTest.SchedulerKernelWithOffsetsProducer (227 ms)
[ RUN      ] LayoutOpTest.GroupedBlockQuantizeOp
/opt/pytorch/nvfuser/tests/cpp/test_layout_op.cpp:382: Skipped
skipping test because fp8 requires compute capability 10.0 or above

[  SKIPPED ] LayoutOpTest.GroupedBlockQuantizeOp (0 ms)
[----------] 3 tests from LayoutOpTest (581 ms total)

[----------] Global test environment tear-down
[==========] 3 tests from 1 test suite ran. (581 ms total)
[  PASSED  ] 2 tests.
[  SKIPPED ] 1 test, listed below:
[  SKIPPED ] LayoutOpTest.GroupedBlockQuantizeOp
+ rv=0
+ for f in __tmp_*
+ [[ __tmp_* == __tmp_c* ]]
+ fs='__tmp_*'
++ echo '__tmp_*'
++ sed s/__tmp_/__tmp_c5_/g
+ ft='__tmp_c5_*'
+ mv '__tmp_*' '__tmp_c5_*'
mv: cannot stat '__tmp_*': No such file or directory
+ counter=6
+ for f in $test_files
+ [[ /opt/pytorch/nvfuser/bin/test_moe == *matmul* ]]
+ [[ /opt/pytorch/nvfuser/bin/test_moe == *multidevice* ]]
+ executable=/opt/pytorch/nvfuser/bin/test_moe
++ basename /opt/pytorch/nvfuser/bin/test_moe
+ report_name=TEST-report-test_moe.xml
+ sed_flag=s/__tmp_/__tmp_c6_/g
+ /opt/pytorch/nvfuser/bin/test_moe --gtest_output=xml:TEST-report-test_moe.xml
+ sed -u s/__tmp_/__tmp_c6_/g
Running main() from /opt/pytorch/nvfuser/third_party/googletest/googletest/src/gtest_main.cc
Note: This is test shard 2 of 3.
[==========] Running 11 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 11 tests from SgLangMoETest
[ RUN      ] SgLangMoETest.ComputeProblemSizes/16_32_4_128_manual
[       OK ] SgLangMoETest.ComputeProblemSizes/16_32_4_128_manual (326 ms)
[ RUN      ] SgLangMoETest.ComputeProblemSizes/16_1_4_128_auto
[       OK ] SgLangMoETest.ComputeProblemSizes/16_1_4_128_auto (84 ms)
[ RUN      ] SgLangMoETest.ComputeProblemSizes/16_1_1_128_manual
[       OK ] SgLangMoETest.ComputeProblemSizes/16_1_1_128_manual (72 ms)
[ RUN      ] SgLangMoETest.ComputeExpertOffsets/16_32_1_128_auto
[       OK ] SgLangMoETest.ComputeExpertOffsets/16_32_1_128_auto (1192 ms)
[ RUN      ] SgLangMoETest.ComputeExpertOffsets/16_1_4_128_manual
[       OK ] SgLangMoETest.ComputeExpertOffsets/16_1_4_128_manual (1129 ms)
[ RUN      ] SgLangMoETest.ComputeExpertBlockScaleOffsets/16_32_4_128_auto
[       OK ] SgLangMoETest.ComputeExpertBlockScaleOffsets/16_32_4_128_auto (1199 ms)
[ RUN      ] SgLangMoETest.ComputeExpertBlockScaleOffsets/16_32_1_128_manual
[       OK ] SgLangMoETest.ComputeExpertBlockScaleOffsets/16_32_1_128_manual (1163 ms)
[ RUN      ] SgLangMoETest.ComputeExpertBlockScaleOffsets/16_1_1_128_auto
[       OK ] SgLangMoETest.ComputeExpertBlockScaleOffsets/16_1_1_128_auto (1076 ms)
[ RUN      ] SgLangMoETest.ComputeArgSort/16_32_4_128_manual
[       OK ] SgLangMoETest.ComputeArgSort/16_32_4_128_manual (1272 ms)
[ RUN      ] SgLangMoETest.ComputeArgSort/16_1_4_128_auto
[       OK ] SgLangMoETest.ComputeArgSort/16_1_4_128_auto (1282 ms)
[ RUN      ] SgLangMoETest.ComputeArgSort/16_1_1_128_manual
[       OK ] SgLangMoETest.ComputeArgSort/16_1_1_128_manual (1240 ms)
[----------] 11 tests from SgLangMoETest (10040 ms total)

[----------] Global test environment tear-down
[==========] 11 tests from 1 test suite ran. (10040 ms total)
[  PASSED  ] 11 tests.
+ rv=0
+ for f in __tmp_*
+ [[ __tmp_* == __tmp_c* ]]
+ fs='__tmp_*'
++ echo '__tmp_*'
++ sed s/__tmp_/__tmp_c6_/g
+ ft='__tmp_c6_*'
+ mv '__tmp_*' '__tmp_c6_*'
mv: cannot stat '__tmp_*': No such file or directory
+ counter=7
+ for f in $test_files
+ [[ /opt/pytorch/nvfuser/bin/test_tutorial == *matmul* ]]
+ [[ /opt/pytorch/nvfuser/bin/test_tutorial == *multidevice* ]]
+ executable=/opt/pytorch/nvfuser/bin/test_tutorial
++ basename /opt/pytorch/nvfuser/bin/test_tutorial
+ report_name=TEST-report-test_tutorial.xml
+ sed_flag=s/__tmp_/__tmp_c7_/g
+ /opt/pytorch/nvfuser/bin/test_tutorial --gtest_output=xml:TEST-report-test_tutorial.xml
+ sed -u s/__tmp_/__tmp_c7_/g
Running main() from /opt/pytorch/nvfuser/third_party/googletest/googletest/src/gtest_main.cc
Note: This is test shard 2 of 3.
[==========] Running 14 tests from 5 test suites.
[----------] Global test environment set-up.
[----------] 3 tests from Tutorial
[ RUN      ] Tutorial.Reduction
[       OK ] Tutorial.Reduction (854 ms)
[ RUN      ] Tutorial.IdModelReshapeAnalysis
[       OK ] Tutorial.IdModelReshapeAnalysis (0 ms)
[ RUN      ] Tutorial.PointwiseBroadcastTMA
[       OK ] Tutorial.PointwiseBroadcastTMA (133 ms)
[----------] 3 tests from Tutorial (988 ms total)

[----------] 1 test from CuTeTutorial
[ RUN      ] CuTeTutorial.VectorizeThreadLayout
// Codegen generated code
__global__ void CUDAGeneratedKernel(Tensor<__bfloat, 2, 2> T0, Tensor<__bfloat, 2, 2> T1) {
  #pragma unroll
  for(nvfuser_index_t i0 = 0LL; i0 < 64LL; ++i0) {
    nvfuser_index_t i1;
    i1 = 16LL * i0;
    #pragma unroll
    for(nvfuser_index_t i2 = 0LL; i2 < 16LL; ++i2) {
      nvfuser_index_t i3;
      i3 = i1 + i2;
      T1[i3]
         = T0[i3];
    }
  }
}
// Codegen generated code
__global__ void CUDAGeneratedKernel(Tensor<__bfloat, 2, 2> T0, Tensor<__bfloat, 2, 2> T1) {
  #pragma unroll
  for(nvfuser_index_t i0 = 0LL; i0 < 8LL; ++i0) {
    nvfuser_index_t i1;
    i1 = 16LL * i0;
    #pragma unroll
    for(nvfuser_index_t i2 = 0LL; i2 < 16LL; ++i2) {
      nvfuser_index_t i3;
      i3 = i1 + i2;
      #pragma unroll
      for(nvfuser_index_t i4 = 0LL; i4 < 8LL; ++i4) {
        nvfuser_index_t i5;
        i5 = i3 + (128LL * i4);
        if (((i0 + (8LL * i4)) < 64LL)) {
          T1[i5]
             = T0[i5];
        }
      }
    }
  }
}
[       OK ] CuTeTutorial.VectorizeThreadLayout (683 ms)
[----------] 1 test from CuTeTutorial (683 ms total)

[----------] 3 tests from TMemTutorialC
[ RUN      ] TMemTutorialC.TooManyLanes
[       OK ] TMemTutorialC.TooManyLanes (3 ms)
[ RUN      ] TMemTutorialC.NotContiguous
[       OK ] TMemTutorialC.NotContiguous (1 ms)
[ RUN      ] TMemTutorialC.WrongSubpartition2
[       OK ] TMemTutorialC.WrongSubpartition2 (1 ms)
[----------] 3 tests from TMemTutorialC (7 ms total)

[----------] 3 tests from TMemTutorialR
[ RUN      ] TMemTutorialR.WarpGroupXYColZ
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] TMemTutorialR.WarpGroupXYColZ (0 ms)
[ RUN      ] TMemTutorialR.Complicated1
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] TMemTutorialR.Complicated1 (0 ms)
[ RUN      ] TMemTutorialR.Vectorization
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] TMemTutorialR.Vectorization (0 ms)
[----------] 3 tests from TMemTutorialR (0 ms total)

[----------] 4 tests from HopperLdStMatrixTutorial
[ RUN      ] HopperLdStMatrixTutorial.SetShmoo/swizzle_32B_cta_m_64_cta_n_256
[       OK ] HopperLdStMatrixTutorial.SetShmoo/swizzle_32B_cta_m_64_cta_n_256 (146 ms)
[ RUN      ] HopperLdStMatrixTutorial.SetShmoo/swizzle_32B_cta_m_128_cta_n_512
[       OK ] HopperLdStMatrixTutorial.SetShmoo/swizzle_32B_cta_m_128_cta_n_512 (185 ms)
[ RUN      ] HopperLdStMatrixTutorial.SetShmoo/swizzle_64B_cta_m_128_cta_n_256
[       OK ] HopperLdStMatrixTutorial.SetShmoo/swizzle_64B_cta_m_128_cta_n_256 (133 ms)
[ RUN      ] HopperLdStMatrixTutorial.SetShmoo/swizzle_128B_cta_m_64_cta_n_512
[       OK ] HopperLdStMatrixTutorial.SetShmoo/swizzle_128B_cta_m_64_cta_n_512 (138 ms)
[----------] 4 tests from HopperLdStMatrixTutorial (605 ms total)

[----------] Global test environment tear-down
[==========] 14 tests from 5 test suites ran. (2285 ms total)
[  PASSED  ] 11 tests.
[  SKIPPED ] 3 tests, listed below:
[  SKIPPED ] TMemTutorialR.WarpGroupXYColZ
[  SKIPPED ] TMemTutorialR.Complicated1
[  SKIPPED ] TMemTutorialR.Vectorization
+ rv=0
+ for f in __tmp_*
+ [[ __tmp_* == __tmp_c* ]]
+ fs='__tmp_*'
++ sed s/__tmp_/__tmp_c7_/g
++ echo '__tmp_*'
+ ft='__tmp_c7_*'
+ mv '__tmp_*' '__tmp_c7_*'
mv: cannot stat '__tmp_*': No such file or directory
+ counter=8
+ for f in $test_files
+ [[ /opt/pytorch/nvfuser/bin/test_argsort == *matmul* ]]
+ [[ /opt/pytorch/nvfuser/bin/test_argsort == *multidevice* ]]
+ executable=/opt/pytorch/nvfuser/bin/test_argsort
++ basename /opt/pytorch/nvfuser/bin/test_argsort
+ report_name=TEST-report-test_argsort.xml
+ sed_flag=s/__tmp_/__tmp_c8_/g
+ /opt/pytorch/nvfuser/bin/test_argsort --gtest_output=xml:TEST-report-test_argsort.xml
+ sed -u s/__tmp_/__tmp_c8_/g
Running main() from /opt/pytorch/nvfuser/third_party/googletest/googletest/src/gtest_main.cc
Note: This is test shard 2 of 3.
[==========] Running 38 tests from 5 test suites.
[----------] Global test environment set-up.
[----------] 2 tests from ArgsortTest
[ RUN      ] ArgsortTest.Predication
[       OK ] ArgsortTest.Predication (1586 ms)
[ RUN      ] ArgsortTest.OuterArgsortWithGrouping
[       OK ] ArgsortTest.OuterArgsortWithGrouping (1331 ms)
[----------] 2 tests from ArgsortTest (2917 ms total)

[----------] 1 test from ArgSortDeviceFuncTest
[ RUN      ] ArgSortDeviceFuncTest.DataTypeSupport
[       OK ] ArgSortDeviceFuncTest.DataTypeSupport (2 ms)
[----------] 1 test from ArgSortDeviceFuncTest (3 ms total)

[----------] 1 test from ArgsortTest/ArgsortTestBasicExecution
[ RUN      ] ArgsortTest/ArgsortTestBasicExecution.ParameterizedBasicExecution/Half
[       OK ] ArgsortTest/ArgsortTestBasicExecution.ParameterizedBasicExecution/Half (1279 ms)
[----------] 1 test from ArgsortTest/ArgsortTestBasicExecution (1279 ms total)

[----------] 27 tests from ArgsortParameterizedWithBlockAndBatch
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/128_1_0_0
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/128_1_0_0 (1260 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/128_1_1_1
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/128_1_1_1 (1347 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/128_2_1_0
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/128_2_1_0 (1286 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/128_3_0_1
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/128_3_0_1 (1347 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/128_8_0_0
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/128_8_0_0 (1300 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/128_8_1_1
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/128_8_1_1 (1421 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/512_1_1_0
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/512_1_1_0 (1314 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/512_2_0_1
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/512_2_0_1 (1356 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/512_3_0_0
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/512_3_0_0 (1321 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/512_3_1_1
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/512_3_1_1 (1415 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/512_8_1_0
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/512_8_1_0 (1330 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/1024_1_0_1
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/1024_1_0_1 (0 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/1024_2_0_0
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/1024_2_0_0 (1283 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/1024_2_1_1
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/1024_2_1_1 (1380 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/1024_3_1_0
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/1024_3_1_0 (1336 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/1024_8_0_1
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/1024_8_0_1 (1367 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/2048_1_0_0
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/2048_1_0_0 (0 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/2048_1_1_1
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/2048_1_1_1 (0 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/2048_2_1_0
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/2048_2_1_0 (1358 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/2048_3_0_1
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/2048_3_0_1 (0 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/2048_8_0_0
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/2048_8_0_0 (1292 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/2048_8_1_1
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/2048_8_1_1 (1442 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/4096_1_1_0
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/4096_1_1_0 (0 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/4096_2_0_1
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/4096_2_0_1 (0 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/4096_3_0_0
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/4096_3_0_0 (0 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/4096_3_1_1
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/4096_3_1_1 (0 ms)
[ RUN      ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/4096_8_1_0
[       OK ] ArgsortParameterizedWithBlockAndBatch.SharedMemoryRequirement/4096_8_1_0 (1369 ms)
[----------] 27 tests from ArgsortParameterizedWithBlockAndBatch (25532 ms total)

[----------] 7 tests from BlockSizeAndItemsPerThread/ArgSortComprehensiveTest
[ RUN      ] BlockSizeAndItemsPerThread/ArgSortComprehensiveTest.ComprehensiveValidation/BlockSize32_ItemsPerThread2
[       OK ] BlockSizeAndItemsPerThread/ArgSortComprehensiveTest.ComprehensiveValidation/BlockSize32_ItemsPerThread2 (0 ms)
[ RUN      ] BlockSizeAndItemsPerThread/ArgSortComprehensiveTest.ComprehensiveValidation/BlockSize32_ItemsPerThread5
[       OK ] BlockSizeAndItemsPerThread/ArgSortComprehensiveTest.ComprehensiveValidation/BlockSize32_ItemsPerThread5 (0 ms)
[ RUN      ] BlockSizeAndItemsPerThread/ArgSortComprehensiveTest.ComprehensiveValidation/BlockSize64_ItemsPerThread3
[       OK ] BlockSizeAndItemsPerThread/ArgSortComprehensiveTest.ComprehensiveValidation/BlockSize64_ItemsPerThread3 (0 ms)
[ RUN      ] BlockSizeAndItemsPerThread/ArgSortComprehensiveTest.ComprehensiveValidation/BlockSize128_ItemsPerThread1
[       OK ] BlockSizeAndItemsPerThread/ArgSortComprehensiveTest.ComprehensiveValidation/BlockSize128_ItemsPerThread1 (0 ms)
[ RUN      ] BlockSizeAndItemsPerThread/ArgSortComprehensiveTest.ComprehensiveValidation/BlockSize128_ItemsPerThread4
[       OK ] BlockSizeAndItemsPerThread/ArgSortComprehensiveTest.ComprehensiveValidation/BlockSize128_ItemsPerThread4 (0 ms)
[ RUN      ] BlockSizeAndItemsPerThread/ArgSortComprehensiveTest.ComprehensiveValidation/BlockSize256_ItemsPerThread2
[       OK ] BlockSizeAndItemsPerThread/ArgSortComprehensiveTest.ComprehensiveValidation/BlockSize256_ItemsPerThread2 (0 ms)
[ RUN      ] BlockSizeAndItemsPerThread/ArgSortComprehensiveTest.ComprehensiveValidation/BlockSize256_ItemsPerThread5
[       OK ] BlockSizeAndItemsPerThread/ArgSortComprehensiveTest.ComprehensiveValidation/BlockSize256_ItemsPerThread5 (0 ms)
[----------] 7 tests from BlockSizeAndItemsPerThread/ArgSortComprehensiveTest (0 ms total)

[----------] Global test environment tear-down
[==========] 38 tests from 5 test suites ran. (29733 ms total)
[  PASSED  ] 38 tests.
+ rv=0
+ for f in __tmp_*
+ [[ __tmp_* == __tmp_c* ]]
+ fs='__tmp_*'
++ echo '__tmp_*'
++ sed s/__tmp_/__tmp_c8_/g
+ ft='__tmp_c8_*'
+ mv '__tmp_*' '__tmp_c8_*'
mv: cannot stat '__tmp_*': No such file or directory
+ counter=9
+ for f in $test_files
+ [[ /opt/pytorch/nvfuser/bin/test_host_ir == *matmul* ]]
+ [[ /opt/pytorch/nvfuser/bin/test_host_ir == *multidevice* ]]
+ executable=/opt/pytorch/nvfuser/bin/test_host_ir
++ basename /opt/pytorch/nvfuser/bin/test_host_ir
+ report_name=TEST-report-test_host_ir.xml
+ sed_flag=s/__tmp_/__tmp_c9_/g
+ /opt/pytorch/nvfuser/bin/test_host_ir --gtest_output=xml:TEST-report-test_host_ir.xml
+ sed -u s/__tmp_/__tmp_c9_/g
Running main() from /opt/pytorch/nvfuser/third_party/googletest/googletest/src/gtest_main.cc
Note: This is test shard 2 of 3.
[==========] Running 28 tests from 13 test suites.
[----------] Global test environment set-up.
[----------] 1 test from HostIrEvaluatorTest
[ RUN      ] HostIrEvaluatorTest.InplaceUpdateInLoop
[       OK ] HostIrEvaluatorTest.InplaceUpdateInLoop (339 ms)
[----------] 1 test from HostIrEvaluatorTest (339 ms total)

[----------] 2 tests from HostIrIntegrationTest
[ RUN      ] HostIrIntegrationTest.Sum_Kernel
[       OK ] HostIrIntegrationTest.Sum_Kernel (113 ms)
[ DISABLED ] HostIrIntegrationTest.DISABLED_ExprEvalAndKernel
[ RUN      ] HostIrIntegrationTest.InsertDeallocations
[       OK ] HostIrIntegrationTest.InsertDeallocations (223 ms)
[----------] 2 tests from HostIrIntegrationTest (336 ms total)

[----------] 5 tests from HirLowerStreamTest
[ RUN      ] HirLowerStreamTest.InputsAreNotStreamParallelized
/opt/pytorch/nvfuser/tests/cpp/test_host_ir_stream_lowering.cpp:39: Skipped
Inputs can now be stream parallelized

[  SKIPPED ] HirLowerStreamTest.InputsAreNotStreamParallelized (0 ms)
[ RUN      ] HirLowerStreamTest.SingleSetOp
[       OK ] HirLowerStreamTest.SingleSetOp (3 ms)
[ RUN      ] HirLowerStreamTest.TwoSetOps
[       OK ] HirLowerStreamTest.TwoSetOps (0 ms)
[ RUN      ] HirLowerStreamTest.Reduction
[       OK ] HirLowerStreamTest.Reduction (0 ms)
[ RUN      ] HirLowerStreamTest.Matmul_N
[       OK ] HirLowerStreamTest.Matmul_N (39 ms)
[----------] 5 tests from HirLowerStreamTest (43 ms total)

[----------] 5 tests from MultiDeviceExecutorLowerStreamTest
[ RUN      ] MultiDeviceExecutorLowerStreamTest.InputsAreNotStreamParallelized
/opt/pytorch/nvfuser/tests/cpp/test_host_ir_stream_lowering.cpp:511: Skipped
Inputs can now be stream parallelized

[  SKIPPED ] MultiDeviceExecutorLowerStreamTest.InputsAreNotStreamParallelized (0 ms)
[ RUN      ] MultiDeviceExecutorLowerStreamTest.SingleSetOp
[       OK ] MultiDeviceExecutorLowerStreamTest.SingleSetOp (0 ms)
[ RUN      ] MultiDeviceExecutorLowerStreamTest.TwoSetOps
[       OK ] MultiDeviceExecutorLowerStreamTest.TwoSetOps (0 ms)
[ RUN      ] MultiDeviceExecutorLowerStreamTest.Reduction
[       OK ] MultiDeviceExecutorLowerStreamTest.Reduction (0 ms)
[ RUN      ] MultiDeviceExecutorLowerStreamTest.Matmul_N
[       OK ] MultiDeviceExecutorLowerStreamTest.Matmul_N (0 ms)
[----------] 5 tests from MultiDeviceExecutorLowerStreamTest (2 ms total)

[----------] 2 tests from StreamTest
[ RUN      ] StreamTest.HostIrSetStream
[       OK ] StreamTest.HostIrSetStream (0 ms)
[ RUN      ] StreamTest.ByIndex
[       OK ] StreamTest.ByIndex (0 ms)
[----------] 2 tests from StreamTest (0 ms total)

[----------] 1 test from LinearHostIrTest
[ RUN      ] LinearHostIrTest.HostIr
[       OK ] LinearHostIrTest.HostIr (20 ms)
[----------] 1 test from LinearHostIrTest (20 ms total)

[----------] 1 test from ReductionHostIrTest
[ RUN      ] ReductionHostIrTest.Sum
[       OK ] ReductionHostIrTest.Sum (0 ms)
[----------] 1 test from ReductionHostIrTest (0 ms total)

[----------] 1 test from AllocationTest
[ RUN      ] AllocationTest.inHostForLoop
[       OK ] AllocationTest.inHostForLoop (0 ms)
[----------] 1 test from AllocationTest (0 ms total)

[----------] 1 test from HirSetTest
[ RUN      ] HirSetTest.HostIr
[       OK ] HirSetTest.HostIr (0 ms)
[----------] 1 test from HirSetTest (0 ms total)

[----------] 4 tests from HostIrTest
[ RUN      ] HostIrTest.SingleFusion/useFusionExecutor
[       OK ] HostIrTest.SingleFusion/useFusionExecutor (4696 ms)
[ RUN      ] HostIrTest.TwoFusions/useFusionExecutorCache
[       OK ] HostIrTest.TwoFusions/useFusionExecutorCache (188 ms)
[ RUN      ] HostIrTest.ForLoops/useFusionExecutor
[       OK ] HostIrTest.ForLoops/useFusionExecutor (74 ms)
[ RUN      ] HostIrTest.PreAllocatedOutputs/useFusionExecutorCache
[W122 09:17:36.590997608 evaluator.cpp:261] Warning: FusionExecutorCache does not support with preallocated outputs, so we are copying the outputs in expr PostOnStream (HostUnit0, Inputs:{T0_g_float[iS0{4}, iS1{8}, iS2{32}], }, Outputs:{T2_g_float[rS6{4}, iS7{8}, iS8{32}], })
 (function handle)
[       OK ] HostIrTest.PreAllocatedOutputs/useFusionExecutorCache (93 ms)
[----------] 4 tests from HostIrTest (5053 ms total)

[----------] 1 test from StreamHostIrTest
[ RUN      ] StreamHostIrTest.SingleFusionMultipleStreams/useFusionExecutorCache_NStreams4_NIterations1
[       OK ] StreamHostIrTest.SingleFusionMultipleStreams/useFusionExecutorCache_NStreams4_NIterations1 (70 ms)
[----------] 1 test from StreamHostIrTest (70 ms total)

[----------] 1 test from SliceHostIrTest
[ RUN      ] SliceHostIrTest.SlicingTensor/SliceOpInTopLevelExpr
[       OK ] SliceHostIrTest.SlicingTensor/SliceOpInTopLevelExpr (0 ms)
[----------] 1 test from SliceHostIrTest (0 ms total)

[----------] 3 tests from HirBinaryOpTest
[ RUN      ] HirBinaryOpTest.PreAllocatedOutputs/BinaryOpType_add
[       OK ] HirBinaryOpTest.PreAllocatedOutputs/BinaryOpType_add (0 ms)
[ RUN      ] HirBinaryOpTest.PreAllocatedOutputs/BinaryOpType_div
[       OK ] HirBinaryOpTest.PreAllocatedOutputs/BinaryOpType_div (2 ms)
[ RUN      ] HirBinaryOpTest.NonPreAllocatedOutputs/BinaryOpType_mul
[       OK ] HirBinaryOpTest.NonPreAllocatedOutputs/BinaryOpType_mul (0 ms)
[----------] 3 tests from HirBinaryOpTest (3 ms total)

[----------] Global test environment tear-down
[==========] 28 tests from 13 test suites ran. (5869 ms total)
[  PASSED  ] 26 tests.
[  SKIPPED ] 2 tests, listed below:
[  SKIPPED ] HirLowerStreamTest.InputsAreNotStreamParallelized
[  SKIPPED ] MultiDeviceExecutorLowerStreamTest.InputsAreNotStreamParallelized
+ rv=0
+ for f in __tmp_*
+ [[ __tmp_* == __tmp_c* ]]
+ fs='__tmp_*'
++ echo '__tmp_*'
++ sed s/__tmp_/__tmp_c9_/g
+ ft='__tmp_c9_*'
+ mv '__tmp_*' '__tmp_c9_*'
mv: cannot stat '__tmp_*': No such file or directory
+ counter=10
+ for f in $test_files
+ [[ /opt/pytorch/nvfuser/bin/test_matmul == *matmul* ]]
+ [[ '' == 1 ]]
+ [[ /opt/pytorch/nvfuser/bin/test_matmul == *multidevice* ]]
+ executable=/opt/pytorch/nvfuser/bin/test_matmul
++ basename /opt/pytorch/nvfuser/bin/test_matmul
+ report_name=TEST-report-test_matmul.xml
+ sed_flag=s/__tmp_/__tmp_c10_/g
+ /opt/pytorch/nvfuser/bin/test_matmul --gtest_output=xml:TEST-report-test_matmul.xml
+ sed -u s/__tmp_/__tmp_c10_/g
Running main() from /opt/pytorch/nvfuser/third_party/googletest/googletest/src/gtest_main.cc
Note: This is test shard 2 of 3.
[==========] Running 958 tests from 30 test suites.
[----------] Global test environment set-up.
[----------] 1 test from CombineMulSumAsMmaTest
[ RUN      ] CombineMulSumAsMmaTest.MulSumToMatmul_MultipleBroadcasts
/opt/pytorch/nvfuser/tests/cpp/test_translate_mma.cpp:156: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] CombineMulSumAsMmaTest.MulSumToMatmul_MultipleBroadcasts (109 ms)
[----------] 1 test from CombineMulSumAsMmaTest (109 ms total)

[----------] 3 tests from MatmulTest
[ RUN      ] MatmulTest.AmpereMatmulTNCpAsync
[       OK ] MatmulTest.AmpereMatmulTNCpAsync (1061 ms)
[ RUN      ] MatmulTest.AmpereMatmulSmemEpiloguePromotionRequiredA100
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:2463: Skipped
Requires GPU capability >= 8.0 and < 9.0 to run.


[  SKIPPED ] MatmulTest.AmpereMatmulSmemEpiloguePromotionRequiredA100 (0 ms)
[ DISABLED ] MatmulTest.DISABLED_MultipleNonConsecutiveMDims
[ DISABLED ] MatmulTest.DISABLED_MultipleNonConsecutiveNDims
[ RUN      ] MatmulTest.MultipleMDimsBatch
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:3417: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTest.MultipleMDimsBatch (0 ms)
[----------] 3 tests from MatmulTest (1061 ms total)

[----------] 4 tests from HopperMatmulTest
[ RUN      ] HopperMatmulTest.HSH_TN_UseScheduler
[       OK ] HopperMatmulTest.HSH_TN_UseScheduler (911 ms)
[ DISABLED ] HopperMatmulTest.DISABLED_HSH_NT_UseScheduler_MultipleInstructionsPerWarpTile
[ RUN      ] HopperMatmulTest.ScheduleWithTranslation
[       OK ] HopperMatmulTest.ScheduleWithTranslation (228 ms)
[ RUN      ] HopperMatmulTest.EpilogueBiasPersistentBroadcastInputs
[       OK ] HopperMatmulTest.EpilogueBiasPersistentBroadcastInputs (562 ms)
[ RUN      ] HopperMatmulTest.HSS_NT_SplitKTMAStore
[       OK ] HopperMatmulTest.HSS_NT_SplitKTMAStore (998 ms)
[----------] 4 tests from HopperMatmulTest (2700 ms total)

[----------] 1 test from BlackwellMatmulTest
[ RUN      ] BlackwellMatmulTest.EpilogueSiluPersistentBroadcastInputs
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] BlackwellMatmulTest.EpilogueSiluPersistentBroadcastInputs (0 ms)
[----------] 1 test from BlackwellMatmulTest (0 ms total)

[----------] 5 tests from MatmulSchedulerTest
[ RUN      ] MatmulSchedulerTest.BasicMatmulStrictCheckTT
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:1081: Skipped
Requires GPU capability >= 8.0 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest.BasicMatmulStrictCheckTT (0 ms)
[ RUN      ] MatmulSchedulerTest.EpilogueAlpha
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:1279: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest.EpilogueAlpha (0 ms)
[ RUN      ] MatmulSchedulerTest.EpilogueAlphaBeta
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:1464: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest.EpilogueAlphaBeta (0 ms)
[ RUN      ] MatmulSchedulerTest.SegmentMatmulOpPrologue
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:2563: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest.SegmentMatmulOpPrologue (0 ms)
[ RUN      ] MatmulSchedulerTest.PreBroadcastMmaBiasNeg
[       OK ] MatmulSchedulerTest.PreBroadcastMmaBiasNeg (268 ms)
[ DISABLED ] MatmulSchedulerTest.DISABLED_RequireExternalPlugin
[----------] 5 tests from MatmulSchedulerTest (268 ms total)

[----------] 1 test from MatmulSchedulerPluginTest
[ RUN      ] MatmulSchedulerPluginTest.BasicMatmul
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:2505: Skipped
Requires GPU capability >= 8.0 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerPluginTest.BasicMatmul (0 ms)
[----------] 1 test from MatmulSchedulerPluginTest (0 ms total)

[----------] 2 tests from CutlassExecutorTest
[ RUN      ] CutlassExecutorTest.FindBlockScaledOutputs_WithoutGlobalScale
[       OK ] CutlassExecutorTest.FindBlockScaledOutputs_WithoutGlobalScale (0 ms)
[ RUN      ] CutlassExecutorTest.Nvfp4BlockScaledGemmReLU
/opt/pytorch/nvfuser/tests/cpp/test_cutlass_scheduler.cpp:602: Skipped
Skipping test on pre-SM100 GPUs

[  SKIPPED ] CutlassExecutorTest.Nvfp4BlockScaledGemmReLU (0 ms)
[----------] 2 tests from CutlassExecutorTest (0 ms total)

[----------] 5 tests from CombineMulSumAsMmaTestWithLayout
[ RUN      ] CombineMulSumAsMmaTestWithLayout.MulSumToMatmul_Pass/TN
[       OK ] CombineMulSumAsMmaTestWithLayout.MulSumToMatmul_Pass/TN (0 ms)
[ RUN      ] CombineMulSumAsMmaTestWithLayout.AmpereMulSumToMatmul_Schedule/TT
/opt/pytorch/nvfuser/tests/cpp/test_translate_mma.cpp:206: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] CombineMulSumAsMmaTestWithLayout.AmpereMulSumToMatmul_Schedule/TT (0 ms)
[ RUN      ] CombineMulSumAsMmaTestWithLayout.AmpereMulSumToMatmul_Schedule/NN
/opt/pytorch/nvfuser/tests/cpp/test_translate_mma.cpp:206: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] CombineMulSumAsMmaTestWithLayout.AmpereMulSumToMatmul_Schedule/NN (0 ms)
[ RUN      ] CombineMulSumAsMmaTestWithLayout.UseMatmulScheduler/NT
/opt/pytorch/nvfuser/tests/cpp/test_translate_mma.cpp:254: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] CombineMulSumAsMmaTestWithLayout.UseMatmulScheduler/NT (0 ms)
[ RUN      ] CombineMulSumAsMmaTestWithLayout.SwapAandB/TN
[       OK ] CombineMulSumAsMmaTestWithLayout.SwapAandB/TN (0 ms)
[----------] 5 tests from CombineMulSumAsMmaTestWithLayout (0 ms total)

[----------] 5 tests from MatmulNodeTranslationTest
[ RUN      ] MatmulNodeTranslationTest.AutomaticSchedulerMatmulNode/2dA_2dB_nofuse
[       OK ] MatmulNodeTranslationTest.AutomaticSchedulerMatmulNode/2dA_2dB_nofuse (186 ms)
[ RUN      ] MatmulNodeTranslationTest.AutomaticSchedulerMatmulNode/2dA_2dB_transposeA
[W122 09:17:41.566377199 matmul_utils.cpp:1181] Warning: Scheduling a matmul without heuristic plugin. Specify plugin location like this: NVFUSER_MATMUL_HEURISTIC_PLUGIN=/path/to/libmatmulheuristic.so (function operator())
[       OK ] MatmulNodeTranslationTest.AutomaticSchedulerMatmulNode/2dA_2dB_transposeA (557 ms)
[ RUN      ] MatmulNodeTranslationTest.AutomaticSchedulerMatmulNode/1dA_1dB
[       OK ] MatmulNodeTranslationTest.AutomaticSchedulerMatmulNode/1dA_1dB (93 ms)
[ RUN      ] MatmulNodeTranslationTest.AutomaticSchedulerMatmulNode/3dA_2dB
/opt/pytorch/nvfuser/tests/cpp/test_translate_mma.cpp:322: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulNodeTranslationTest.AutomaticSchedulerMatmulNode/3dA_2dB (0 ms)
[ RUN      ] MatmulNodeTranslationTest.AutomaticSchedulerMatmulNode/3dA_4dB
[       OK ] MatmulNodeTranslationTest.AutomaticSchedulerMatmulNode/3dA_4dB (195 ms)
[----------] 5 tests from MatmulNodeTranslationTest (1033 ms total)

[----------] 4 tests from LinearNodeTranslationTest
[ RUN      ] LinearNodeTranslationTest.AutomaticSchedulerLinearNode/1dA_2dB_nofuse
[       OK ] LinearNodeTranslationTest.AutomaticSchedulerLinearNode/1dA_2dB_nofuse (107 ms)
[ RUN      ] LinearNodeTranslationTest.AutomaticSchedulerLinearNode/4dA_2dB_1dBias_nofuse
[       OK ] LinearNodeTranslationTest.AutomaticSchedulerLinearNode/4dA_2dB_1dBias_nofuse (106 ms)
[ RUN      ] LinearNodeTranslationTest.AutomaticSchedulerLinearNode/1dA_2dB
/opt/pytorch/nvfuser/tests/cpp/test_translate_mma.cpp:493: Skipped
Translating linear with batch dims is not yet supported on Hopper

[  SKIPPED ] LinearNodeTranslationTest.AutomaticSchedulerLinearNode/1dA_2dB (0 ms)
[ RUN      ] LinearNodeTranslationTest.AutomaticSchedulerLinearNode/2dA_2dB_1dBias
[       OK ] LinearNodeTranslationTest.AutomaticSchedulerLinearNode/2dA_2dB_1dBias (612 ms)
[----------] 4 tests from LinearNodeTranslationTest (826 ms total)

[----------] 3 tests from TranslationCastTest
[ RUN      ] TranslationCastTest.CountCasts/matmul
[       OK ] TranslationCastTest.CountCasts/matmul (0 ms)
[ RUN      ] TranslationCastTest.CountCasts/matmul_epilogue_twooutputs
[       OK ] TranslationCastTest.CountCasts/matmul_epilogue_twooutputs (0 ms)
[ RUN      ] TranslationCastTest.CountCasts/linear_epilogue
[       OK ] TranslationCastTest.CountCasts/linear_epilogue (0 ms)
[----------] 3 tests from TranslationCastTest (0 ms total)

[----------] 32 tests from MatmulTestWithLayout
[ RUN      ] MatmulTestWithLayout.AmpereMatmul/TN
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:91: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereMatmul/TN (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereMatmulBroadcastBatch/TT
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:150: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulBroadcastBatch/TT (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereMatmulBroadcastBatch/NN
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:150: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulBroadcastBatch/NN (0 ms)
[ RUN      ] MatmulTestWithLayout.AmperePrologueFusionBroadcast/NT
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:213: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmperePrologueFusionBroadcast/NT (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereProloguePointwise/TN
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:268: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereProloguePointwise/TN (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereMatmulBFloat16/TT
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:330: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulBFloat16/TT (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereMatmulBFloat16/NN
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:330: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulBFloat16/NN (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereMatmulPipelineGmem/NT
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:389: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulPipelineGmem/NT (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereSwizzle/TN
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:482: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereSwizzle/TN (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereMatmulRegCircularBuffer/TT
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:602: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulRegCircularBuffer/TT (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereMatmulRegCircularBuffer/NN
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:602: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulRegCircularBuffer/NN (0 ms)
[ RUN      ] MatmulTestWithLayout.TuringMatmul/NT
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:1331: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.TuringMatmul/NT (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereMatmulLargeLoad/TN
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:1869: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulLargeLoad/TN (0 ms)
[ RUN      ] MatmulTestWithLayout.TuringMatmulLargeLoad/TT
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:1927: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.TuringMatmulLargeLoad/TT (0 ms)
[ RUN      ] MatmulTestWithLayout.TuringMatmulLargeLoad/NN
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:1927: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.TuringMatmulLargeLoad/NN (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereMatmulTileCheck4warp/NT
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:1982: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulTileCheck4warp/NT (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereMatmulTileCheck8warp/TN
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:2061: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulTileCheck8warp/TN (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereMatmulTileCheck6warp/TT
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:2136: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulTileCheck6warp/TT (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereMatmulTileCheck6warp/NN
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:2136: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulTileCheck6warp/NN (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereMatmulLargeLoadLargeK/NT
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:2206: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulLargeLoadLargeK/NT (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereSplitKLikeStridedBatchedMatmul/TN
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:2264: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereSplitKLikeStridedBatchedMatmul/TN (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereMatmulSmemEpilogue/TT
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:2313: Skipped
Requires GPU capability >= 8.0 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulSmemEpilogue/TT (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereMatmulSmemEpilogue/NN
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:2313: Skipped
Requires GPU capability >= 8.0 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulSmemEpilogue/NN (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereMatmulSmemEpilogueCast/NT
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:2561: Skipped
Requires GPU capability >= 8.0 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulSmemEpilogueCast/NT (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereMatmulSmemEpilogueRelu/TN
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:2656: Skipped
Requires GPU capability >= 8.0 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulSmemEpilogueRelu/TN (0 ms)
[ RUN      ] MatmulTestWithLayout.FusionAmpereMatmulSplitK_CUDA/TT
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:2753: Skipped
Requires GPU capability >= 8.0 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.FusionAmpereMatmulSplitK_CUDA/TT (0 ms)
[ RUN      ] MatmulTestWithLayout.FusionAmpereMatmulSplitK_CUDA/NN
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:2753: Skipped
Requires GPU capability >= 8.0 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.FusionAmpereMatmulSplitK_CUDA/NN (0 ms)
[ RUN      ] MatmulTestWithLayout.FusionAmpereMatmulSplitKBias_CUDA/NT
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:2828: Skipped
Requires GPU capability >= 8.0 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.FusionAmpereMatmulSplitKBias_CUDA/NT (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereMatmulBatchSplitK/TN
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:2892: Skipped
Requires GPU capability >= 8.0 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulBatchSplitK/TN (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereMatmulBatchSplitKBias/TT
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:2950: Skipped
Requires GPU capability >= 8.0 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulBatchSplitKBias/TT (0 ms)
[ RUN      ] MatmulTestWithLayout.AmpereMatmulBatchSplitKBias/NN
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:2950: Skipped
Requires GPU capability >= 8.0 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulBatchSplitKBias/NN (0 ms)
[ RUN      ] MatmulTestWithLayout.MisalignedVectorization/NT
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:3076: Skipped
Requires GPU capability >= 8.0 and < 9.0 to run.


[  SKIPPED ] MatmulTestWithLayout.MisalignedVectorization/NT (0 ms)
[----------] 32 tests from MatmulTestWithLayout (0 ms total)

[----------] 11 tests from MLPBenchmarkTest
[ RUN      ] MLPBenchmarkTest.FwdGEMM/dataparallel_warpspec
[       OK ] MLPBenchmarkTest.FwdGEMM/dataparallel_warpspec (355 ms)
[ RUN      ] MLPBenchmarkTest.FwdGEMM_BroadcastInputs/dataparallel_non_warpspec
[       OK ] MLPBenchmarkTest.FwdGEMM_BroadcastInputs/dataparallel_non_warpspec (403 ms)
[ RUN      ] MLPBenchmarkTest.FwdGEMM_BroadcastInputs/persistent_warpspec
[       OK ] MLPBenchmarkTest.FwdGEMM_BroadcastInputs/persistent_warpspec (389 ms)
[ RUN      ] MLPBenchmarkTest.FwdEpilogueBiasFusion/persistent_non_warpspec
[       OK ] MLPBenchmarkTest.FwdEpilogueBiasFusion/persistent_non_warpspec (538 ms)
[ RUN      ] MLPBenchmarkTest.FwdEpilogueSiluFusion/dataparallel_warpspec
[       OK ] MLPBenchmarkTest.FwdEpilogueSiluFusion/dataparallel_warpspec (925 ms)
[ RUN      ] MLPBenchmarkTest.FwdEpilogueFusion_BroadcastInputs/dataparallel_non_warpspec
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:4167: Skipped
THIS TEST IS CURRENTLY FAILING


[  SKIPPED ] MLPBenchmarkTest.FwdEpilogueFusion_BroadcastInputs/dataparallel_non_warpspec (0 ms)
[ RUN      ] MLPBenchmarkTest.FwdEpilogueFusion_BroadcastInputs/persistent_warpspec
/opt/pytorch/nvfuser/tests/cpp/test_matmul.cpp:4167: Skipped
THIS TEST IS CURRENTLY FAILING


[  SKIPPED ] MLPBenchmarkTest.FwdEpilogueFusion_BroadcastInputs/persistent_warpspec (0 ms)
[ RUN      ] MLPBenchmarkTest.FwdHorizontalFusion/persistent_non_warpspec
[       OK ] MLPBenchmarkTest.FwdHorizontalFusion/persistent_non_warpspec (814 ms)
[ RUN      ] MLPBenchmarkTest.FwdHorizontalFusion_BroadcastInputs/dataparallel_warpspec
[       OK ] MLPBenchmarkTest.FwdHorizontalFusion_BroadcastInputs/dataparallel_warpspec (741 ms)
[ RUN      ] MLPBenchmarkTest.BatchGEMM/dataparallel_non_warpspec
[       OK ] MLPBenchmarkTest.BatchGEMM/dataparallel_non_warpspec (429 ms)
[ RUN      ] MLPBenchmarkTest.BatchGEMM/persistent_warpspec
[       OK ] MLPBenchmarkTest.BatchGEMM/persistent_warpspec (439 ms)
[----------] 11 tests from MLPBenchmarkTest (5037 ms total)

[----------] 14 tests from MatmulNodeParameterizedTest
[ RUN      ] MatmulNodeParameterizedTest.MatmulNodeConcrete/0
[       OK ] MatmulNodeParameterizedTest.MatmulNodeConcrete/0 (1 ms)
[ RUN      ] MatmulNodeParameterizedTest.MatmulNodeConcrete/3
[       OK ] MatmulNodeParameterizedTest.MatmulNodeConcrete/3 (4 ms)
[ RUN      ] MatmulNodeParameterizedTest.MatmulNodeConcrete/6
[       OK ] MatmulNodeParameterizedTest.MatmulNodeConcrete/6 (1 ms)
[ RUN      ] MatmulNodeParameterizedTest.MatmulNodeConcrete/9
[       OK ] MatmulNodeParameterizedTest.MatmulNodeConcrete/9 (1 ms)
[ RUN      ] MatmulNodeParameterizedTest.MatmulNodeConcrete/12
[       OK ] MatmulNodeParameterizedTest.MatmulNodeConcrete/12 (8 ms)
[ RUN      ] MatmulNodeParameterizedTest.MatmulNodeConcrete/15
[       OK ] MatmulNodeParameterizedTest.MatmulNodeConcrete/15 (9 ms)
[ RUN      ] MatmulNodeParameterizedTest.MatmulNodeConcrete/18
[       OK ] MatmulNodeParameterizedTest.MatmulNodeConcrete/18 (7 ms)
[ RUN      ] MatmulNodeParameterizedTest.MatmulNodeSymbolic/1
[       OK ] MatmulNodeParameterizedTest.MatmulNodeSymbolic/1 (1 ms)
[ RUN      ] MatmulNodeParameterizedTest.MatmulNodeSymbolic/4
[       OK ] MatmulNodeParameterizedTest.MatmulNodeSymbolic/4 (1 ms)
[ RUN      ] MatmulNodeParameterizedTest.MatmulNodeSymbolic/7
[       OK ] MatmulNodeParameterizedTest.MatmulNodeSymbolic/7 (3 ms)
[ RUN      ] MatmulNodeParameterizedTest.MatmulNodeSymbolic/10
[       OK ] MatmulNodeParameterizedTest.MatmulNodeSymbolic/10 (1 ms)
[ RUN      ] MatmulNodeParameterizedTest.MatmulNodeSymbolic/13
[       OK ] MatmulNodeParameterizedTest.MatmulNodeSymbolic/13 (8 ms)
[ RUN      ] MatmulNodeParameterizedTest.MatmulNodeSymbolic/16
[       OK ] MatmulNodeParameterizedTest.MatmulNodeSymbolic/16 (7 ms)
[ RUN      ] MatmulNodeParameterizedTest.MatmulNodeSymbolic/19
[       OK ] MatmulNodeParameterizedTest.MatmulNodeSymbolic/19 (10 ms)
[----------] 14 tests from MatmulNodeParameterizedTest (67 ms total)

[----------] 13 tests from ReductionAxisIsOne/MatmulNodeParameterizedTest
[ RUN      ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeConcrete/2
[       OK ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeConcrete/2 (88 ms)
[ RUN      ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeConcrete/5
[       OK ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeConcrete/5 (143 ms)
[ RUN      ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeConcrete/8
[       OK ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeConcrete/8 (80 ms)
[ RUN      ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeConcrete/11
[       OK ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeConcrete/11 (145 ms)
[ RUN      ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeConcrete/14
[       OK ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeConcrete/14 (145 ms)
[ RUN      ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeConcrete/17
[       OK ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeConcrete/17 (199 ms)
[ RUN      ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeSymbolic/0
[       OK ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeSymbolic/0 (1 ms)
[ RUN      ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeSymbolic/3
[       OK ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeSymbolic/3 (122 ms)
[ RUN      ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeSymbolic/6
[       OK ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeSymbolic/6 (134 ms)
[ RUN      ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeSymbolic/9
[       OK ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeSymbolic/9 (132 ms)
[ RUN      ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeSymbolic/12
[       OK ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeSymbolic/12 (120 ms)
[ RUN      ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeSymbolic/15
[       OK ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeSymbolic/15 (297 ms)
[ RUN      ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeSymbolic/18
[       OK ] ReductionAxisIsOne/MatmulNodeParameterizedTest.MatmulNodeSymbolic/18 (165 ms)
[----------] 13 tests from ReductionAxisIsOne/MatmulNodeParameterizedTest (1776 ms total)

[----------] 7 tests from LinearWithoutBias/LinearNodeParametrizedTest
[ RUN      ] LinearWithoutBias/LinearNodeParametrizedTest.LinearNodeConcrete/1
[       OK ] LinearWithoutBias/LinearNodeParametrizedTest.LinearNodeConcrete/1 (1 ms)
[ RUN      ] LinearWithoutBias/LinearNodeParametrizedTest.LinearNodeConcrete/4
[       OK ] LinearWithoutBias/LinearNodeParametrizedTest.LinearNodeConcrete/4 (8 ms)
[ RUN      ] LinearWithoutBias/LinearNodeParametrizedTest.LinearNodeConcrete/7
[       OK ] LinearWithoutBias/LinearNodeParametrizedTest.LinearNodeConcrete/7 (1 ms)
[ RUN      ] LinearWithoutBias/LinearNodeParametrizedTest.LinearNodeSymbolic/0
[       OK ] LinearWithoutBias/LinearNodeParametrizedTest.LinearNodeSymbolic/0 (2 ms)
[ RUN      ] LinearWithoutBias/LinearNodeParametrizedTest.LinearNodeSymbolic/3
[       OK ] LinearWithoutBias/LinearNodeParametrizedTest.LinearNodeSymbolic/3 (1 ms)
[ RUN      ] LinearWithoutBias/LinearNodeParametrizedTest.LinearNodeSymbolic/6
[       OK ] LinearWithoutBias/LinearNodeParametrizedTest.LinearNodeSymbolic/6 (1 ms)
[ RUN      ] LinearWithoutBias/LinearNodeParametrizedTest.LinearNodeSymbolic/9
[       OK ] LinearWithoutBias/LinearNodeParametrizedTest.LinearNodeSymbolic/9 (1 ms)
[----------] 7 tests from LinearWithoutBias/LinearNodeParametrizedTest (18 ms total)

[----------] 3 tests from LinearWithBias/LinearNodeParametrizedTest
[ RUN      ] LinearWithBias/LinearNodeParametrizedTest.LinearNodeConcrete/2
[       OK ] LinearWithBias/LinearNodeParametrizedTest.LinearNodeConcrete/2 (8 ms)
[ RUN      ] LinearWithBias/LinearNodeParametrizedTest.LinearNodeSymbolic/0
[       OK ] LinearWithBias/LinearNodeParametrizedTest.LinearNodeSymbolic/0 (2 ms)
[ RUN      ] LinearWithBias/LinearNodeParametrizedTest.LinearNodeSymbolic/3
[       OK ] LinearWithBias/LinearNodeParametrizedTest.LinearNodeSymbolic/3 (1 ms)
[----------] 3 tests from LinearWithBias/LinearNodeParametrizedTest (12 ms total)

[----------] 3 tests from LinearReductionAxisIsOne/LinearNodeParametrizedTest
[ RUN      ] LinearReductionAxisIsOne/LinearNodeParametrizedTest.LinearNodeConcrete/1
[       OK ] LinearReductionAxisIsOne/LinearNodeParametrizedTest.LinearNodeConcrete/1 (1 ms)
[ RUN      ] LinearReductionAxisIsOne/LinearNodeParametrizedTest.LinearNodeConcrete/4
[       OK ] LinearReductionAxisIsOne/LinearNodeParametrizedTest.LinearNodeConcrete/4 (1 ms)
[ RUN      ] LinearReductionAxisIsOne/LinearNodeParametrizedTest.LinearNodeSymbolic/2
[       OK ] LinearReductionAxisIsOne/LinearNodeParametrizedTest.LinearNodeSymbolic/2 (1 ms)
[----------] 3 tests from LinearReductionAxisIsOne/LinearNodeParametrizedTest (4 ms total)

[----------] 12 tests from MatmulSchedulerTest/PrecisionParametrizedTest
[ RUN      ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueBias/HSS
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:137: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueBias/HSS (0 ms)
[ RUN      ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueBias/TST
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:137: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueBias/TST (0 ms)
[ RUN      ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueRelu/TSS
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:239: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueRelu/TSS (0 ms)
[ RUN      ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueBiasRelu/HSH
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:326: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueBiasRelu/HSH (0 ms)
[ RUN      ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueReluAux/HSS
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:432: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueReluAux/HSS (0 ms)
[ RUN      ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueReluAux/TST
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:432: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueReluAux/TST (0 ms)
[ RUN      ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueBiasReluAux/TSS
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:526: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueBiasReluAux/TSS (0 ms)
[ RUN      ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueGelu/HSH
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:638: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueGelu/HSH (0 ms)
[ RUN      ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueGeluAux/HSS
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:723: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueGeluAux/HSS (0 ms)
[ RUN      ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueGeluAux/TST
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:723: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueGeluAux/TST (0 ms)
[ RUN      ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueBiasGelu/TSS
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:816: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueBiasGelu/TSS (0 ms)
[ RUN      ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueBiasGeluAux/HSH
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:922: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueBiasGeluAux/HSH (0 ms)
[----------] 12 tests from MatmulSchedulerTest/PrecisionParametrizedTest (0 ms total)

[----------] 11 tests from MatmulSchedulerTestWithLayout
[ RUN      ] MatmulSchedulerTestWithLayout.BasicMatmulRelaxedCheck/TT
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:1130: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTestWithLayout.BasicMatmulRelaxedCheck/TT (0 ms)
[ RUN      ] MatmulSchedulerTestWithLayout.BasicMatmulRelaxedCheck/NN
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:1130: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTestWithLayout.BasicMatmulRelaxedCheck/NN (0 ms)
[ RUN      ] MatmulSchedulerTestWithLayout.StridedBatch/NT
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:1710: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTestWithLayout.StridedBatch/NT (0 ms)
[ RUN      ] MatmulSchedulerTestWithLayout.StridedBatchEpilogueAlphaBeta/TN
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:1764: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTestWithLayout.StridedBatchEpilogueAlphaBeta/TN (0 ms)
[ RUN      ] MatmulSchedulerTestWithLayout.StridedBatchEpilogueAlphaSingleBeta/TT
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:1840: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTestWithLayout.StridedBatchEpilogueAlphaSingleBeta/TT (0 ms)
[ RUN      ] MatmulSchedulerTestWithLayout.StridedBatchEpilogueAlphaSingleBeta/NN
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:1840: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTestWithLayout.StridedBatchEpilogueAlphaSingleBeta/NN (0 ms)
[ RUN      ] MatmulSchedulerTestWithLayout.StridedBatchEpilogueBias/NT
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:1920: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTestWithLayout.StridedBatchEpilogueBias/NT (0 ms)
[ RUN      ] MatmulSchedulerTestWithLayout.StridedBatchEpilogueSingleBias/TN
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:1982: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTestWithLayout.StridedBatchEpilogueSingleBias/TN (0 ms)
[ RUN      ] MatmulSchedulerTestWithLayout.MisalignedVectorization/TT
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:2043: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTestWithLayout.MisalignedVectorization/TT (0 ms)
[ RUN      ] MatmulSchedulerTestWithLayout.MisalignedVectorization/NN
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:2043: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTestWithLayout.MisalignedVectorization/NN (0 ms)
[ RUN      ] MatmulSchedulerTestWithLayout.StridedInputs/NT
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:2197: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTestWithLayout.StridedInputs/NT (0 ms)
[----------] 11 tests from MatmulSchedulerTestWithLayout (0 ms total)

[----------] 1 test from MatmulFusionTest
[ RUN      ] MatmulFusionTest.Llama2FFN/fuse_single
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:2813: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulFusionTest.Llama2FFN/fuse_single (0 ms)
[----------] 1 test from MatmulFusionTest (0 ms total)

[----------] 9 tests from MatmulSchedulerTest/AllocationDomainTest
[ RUN      ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmul/1
[       OK ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmul/1 (271 ms)
[ RUN      ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmulNoTranspose/0
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3014: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmulNoTranspose/0 (0 ms)
[ RUN      ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmulNoTranspose/3
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3014: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmulNoTranspose/3 (0 ms)
[ RUN      ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmulWithPrologueSet/2
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3046: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmulWithPrologueSet/2 (0 ms)
[ RUN      ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmulWithPrologueSetCastSin/1
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3082: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmulWithPrologueSetCastSin/1 (0 ms)
[ RUN      ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmulWithPrologueSetCastSinNoTranspose/0
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3121: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmulWithPrologueSetCastSinNoTranspose/0 (0 ms)
[ RUN      ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmulWithPrologueSetCastSinNoTranspose/3
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3121: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmulWithPrologueSetCastSinNoTranspose/3 (0 ms)
[ RUN      ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmulWithPrologueSetCastSinSetNoTranspose/2
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3157: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmulWithPrologueSetCastSinSetNoTranspose/2 (0 ms)
[ RUN      ] MatmulSchedulerTest/AllocationDomainTest.MatmulWithPrologueSetCastSinTranspose/1
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3194: Skipped
Requires GPU capability >= 7.5 and < 9.0 to run.


[  SKIPPED ] MatmulSchedulerTest/AllocationDomainTest.MatmulWithPrologueSetCastSinTranspose/1 (0 ms)
[----------] 9 tests from MatmulSchedulerTest/AllocationDomainTest (271 ms total)

[----------] 22 tests from General/HopperPlusMatmulSchedulerTest
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MN_512_256_128_MmaMacro_m64_n128_k16
[       OK ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MN_512_256_128_MmaMacro_m64_n128_k16 (231 ms)
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MN_512_256_128_MmaMacro_m128_n128_k16_splitk_2
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MN_512_256_128_MmaMacro_m128_n128_k16_splitk_2 (0 ms)
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MK_512_256_128_MmaMacro_m128_n128_k16
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MK_512_256_128_MmaMacro_m128_n128_k16 (0 ms)
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KN_512_256_128_MmaMacro_m64_n128_k16_splitk_2
[       OK ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KN_512_256_128_MmaMacro_m64_n128_k16_splitk_2 (339 ms)
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KK_512_256_128_MmaMacro_m64_n128_k16
[       OK ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KK_512_256_128_MmaMacro_m64_n128_k16 (225 ms)
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KK_512_256_128_MmaMacro_m128_n128_k16_splitk_2
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KK_512_256_128_MmaMacro_m128_n128_k16_splitk_2 (0 ms)
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MN_512_256_128_MmaMacro_m128_n128_k16_tma_store
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MN_512_256_128_MmaMacro_m128_n128_k16_tma_store (0 ms)
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MK_512_256_128_MmaMacro_m64_n128_k16_tma_store_splitk_2
[       OK ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MK_512_256_128_MmaMacro_m64_n128_k16_tma_store_splitk_2 (360 ms)
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KN_512_256_128_MmaMacro_m64_n128_k16_tma_store
[       OK ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KN_512_256_128_MmaMacro_m64_n128_k16_tma_store (236 ms)
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KN_512_256_128_MmaMacro_m128_n128_k16_tma_store_splitk_2
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KN_512_256_128_MmaMacro_m128_n128_k16_tma_store_splitk_2 (0 ms)
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KK_512_256_128_MmaMacro_m128_n128_k16_tma_store
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KK_512_256_128_MmaMacro_m128_n128_k16_tma_store (0 ms)
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MN_512_256_128_MmaMacro_m64_n128_k16_splitk_2
[       OK ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MN_512_256_128_MmaMacro_m64_n128_k16_splitk_2 (426 ms)
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MK_512_256_128_MmaMacro_m64_n128_k16
[       OK ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MK_512_256_128_MmaMacro_m64_n128_k16 (305 ms)
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MK_512_256_128_MmaMacro_m128_n128_k16_splitk_2
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MK_512_256_128_MmaMacro_m128_n128_k16_splitk_2 (0 ms)
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KN_512_256_128_MmaMacro_m128_n128_k16
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KN_512_256_128_MmaMacro_m128_n128_k16 (0 ms)
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KK_512_256_128_MmaMacro_m64_n128_k16_splitk_2
[       OK ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KK_512_256_128_MmaMacro_m64_n128_k16_splitk_2 (424 ms)
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MN_512_256_128_MmaMacro_m64_n128_k16_tma_store
[       OK ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MN_512_256_128_MmaMacro_m64_n128_k16_tma_store (330 ms)
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MN_512_256_128_MmaMacro_m128_n128_k16_tma_store_splitk_2
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MN_512_256_128_MmaMacro_m128_n128_k16_tma_store_splitk_2 (0 ms)
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MK_512_256_128_MmaMacro_m128_n128_k16_tma_store
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MK_512_256_128_MmaMacro_m128_n128_k16_tma_store (0 ms)
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KN_512_256_128_MmaMacro_m64_n128_k16_tma_store_splitk_2
[       OK ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KN_512_256_128_MmaMacro_m64_n128_k16_tma_store_splitk_2 (462 ms)
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KK_512_256_128_MmaMacro_m64_n128_k16_tma_store
[       OK ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KK_512_256_128_MmaMacro_m64_n128_k16_tma_store (329 ms)
[ RUN      ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KK_512_256_128_MmaMacro_m128_n128_k16_tma_store_splitk_2
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KK_512_256_128_MmaMacro_m128_n128_k16_tma_store_splitk_2 (0 ms)
[----------] 22 tests from General/HopperPlusMatmulSchedulerTest (3672 ms total)

[----------] 26 tests from Swizzle/HopperPlusMatmulSchedulerTest
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MN_512_256_64_MmaMacro_m64_n64_k16_tma_store_128BSwizzle
[       OK ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MN_512_256_64_MmaMacro_m64_n64_k16_tma_store_128BSwizzle (210 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MN_512_256_64_MmaMacro_m128_n256_k16_tma_store_128BSwizzle
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MN_512_256_64_MmaMacro_m128_n256_k16_tma_store_128BSwizzle (0 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MN_512_256_64_MmaMacro_m128_n32_k16_tma_store_64BSwizzle
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MN_512_256_64_MmaMacro_m128_n32_k16_tma_store_64BSwizzle (0 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MK_512_256_64_MmaMacro_m64_n128_k16_tma_store_128BSwizzle
[       OK ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MK_512_256_64_MmaMacro_m64_n128_k16_tma_store_128BSwizzle (233 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MK_512_256_64_MmaMacro_m64_n16_k16_tma_store_32BSwizzle
[       OK ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MK_512_256_64_MmaMacro_m64_n16_k16_tma_store_32BSwizzle (196 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MK_512_256_64_MmaMacro_m128_n64_k16_tma_store_128BSwizzle
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MK_512_256_64_MmaMacro_m128_n64_k16_tma_store_128BSwizzle (0 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KN_512_256_64_MmaMacro_m64_n256_k16_tma_store_128BSwizzle
[       OK ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KN_512_256_64_MmaMacro_m64_n256_k16_tma_store_128BSwizzle (327 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KN_512_256_64_MmaMacro_m64_n32_k16_tma_store_64BSwizzle
[       OK ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KN_512_256_64_MmaMacro_m64_n32_k16_tma_store_64BSwizzle (200 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KN_512_256_64_MmaMacro_m128_n128_k16_tma_store_128BSwizzle
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KN_512_256_64_MmaMacro_m128_n128_k16_tma_store_128BSwizzle (0 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KN_512_256_64_MmaMacro_m128_n16_k16_tma_store_32BSwizzle
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KN_512_256_64_MmaMacro_m128_n16_k16_tma_store_32BSwizzle (0 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KK_512_256_64_MmaMacro_m64_n64_k16_tma_store_128BSwizzle
[       OK ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KK_512_256_64_MmaMacro_m64_n64_k16_tma_store_128BSwizzle (209 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KK_512_256_64_MmaMacro_m128_n256_k16_tma_store_128BSwizzle
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KK_512_256_64_MmaMacro_m128_n256_k16_tma_store_128BSwizzle (0 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KK_512_256_64_MmaMacro_m128_n32_k16_tma_store_64BSwizzle
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KK_512_256_64_MmaMacro_m128_n32_k16_tma_store_64BSwizzle (0 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MN_512_256_64_MmaMacro_m64_n128_k16_tma_store_128BSwizzle
[       OK ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MN_512_256_64_MmaMacro_m64_n128_k16_tma_store_128BSwizzle (201 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MN_512_256_64_MmaMacro_m64_n16_k16_tma_store_32BSwizzle
[       OK ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MN_512_256_64_MmaMacro_m64_n16_k16_tma_store_32BSwizzle (252 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MN_512_256_64_MmaMacro_m128_n64_k16_tma_store_128BSwizzle
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MN_512_256_64_MmaMacro_m128_n64_k16_tma_store_128BSwizzle (0 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MK_512_256_64_MmaMacro_m64_n256_k16_tma_store_128BSwizzle
[       OK ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MK_512_256_64_MmaMacro_m64_n256_k16_tma_store_128BSwizzle (478 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MK_512_256_64_MmaMacro_m64_n32_k16_tma_store_64BSwizzle
[       OK ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MK_512_256_64_MmaMacro_m64_n32_k16_tma_store_64BSwizzle (263 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MK_512_256_64_MmaMacro_m128_n128_k16_tma_store_128BSwizzle
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MK_512_256_64_MmaMacro_m128_n128_k16_tma_store_128BSwizzle (0 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MK_512_256_64_MmaMacro_m128_n16_k16_tma_store_32BSwizzle
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MK_512_256_64_MmaMacro_m128_n16_k16_tma_store_32BSwizzle (0 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KN_512_256_64_MmaMacro_m64_n64_k16_tma_store_128BSwizzle
[       OK ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KN_512_256_64_MmaMacro_m64_n64_k16_tma_store_128BSwizzle (281 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KN_512_256_64_MmaMacro_m128_n256_k16_tma_store_128BSwizzle
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KN_512_256_64_MmaMacro_m128_n256_k16_tma_store_128BSwizzle (0 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KN_512_256_64_MmaMacro_m128_n32_k16_tma_store_64BSwizzle
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KN_512_256_64_MmaMacro_m128_n32_k16_tma_store_64BSwizzle (0 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KK_512_256_64_MmaMacro_m64_n128_k16_tma_store_128BSwizzle
[       OK ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KK_512_256_64_MmaMacro_m64_n128_k16_tma_store_128BSwizzle (200 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KK_512_256_64_MmaMacro_m64_n16_k16_tma_store_32BSwizzle
[       OK ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KK_512_256_64_MmaMacro_m64_n16_k16_tma_store_32BSwizzle (252 ms)
[ RUN      ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KK_512_256_64_MmaMacro_m128_n64_k16_tma_store_128BSwizzle
/opt/pytorch/nvfuser/tests/cpp/test_matmul_scheduler.cpp:3349: Skipped
Requires GPU capability >= 10.0 and < 11.0 to run.


[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KK_512_256_64_MmaMacro_m128_n64_k16_tma_store_128BSwizzle (0 ms)
[----------] 26 tests from Swizzle/HopperPlusMatmulSchedulerTest (3308 ms total)

[----------] 2 tests from Turing/MmaTest
[ RUN      ] Turing/MmaTest.SingleTile/Turing_16_8_8__half
[       OK ] Turing/MmaTest.SingleTile/Turing_16_8_8__half (97 ms)
[ RUN      ] Turing/MmaTest.SingleTileWithStridedInput/Turing_16_8_8__half
[       OK ] Turing/MmaTest.SingleTileWithStridedInput/Turing_16_8_8__half (164 ms)
[----------] 2 tests from Turing/MmaTest (261 ms total)

[----------] 3 tests from Ampere/MmaTest
[ RUN      ] Ampere/MmaTest.SingleTile/Ampere_16_8_16__half
[       OK ] Ampere/MmaTest.SingleTile/Ampere_16_8_16__half (103 ms)
[ RUN      ] Ampere/MmaTest.SingleTile/Ampere_16_16_16__bfloat
[       OK ] Ampere/MmaTest.SingleTile/Ampere_16_16_16__bfloat (111 ms)
[ RUN      ] Ampere/MmaTest.SingleTileWithStridedInput/Ampere_16_16_16__half
[       OK ] Ampere/MmaTest.SingleTileWithStridedInput/Ampere_16_16_16__half (183 ms)
[----------] 3 tests from Ampere/MmaTest (398 ms total)

[----------] 30 tests from MmaTest/HopperRS
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_8_16_TT_128B__half
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_8_16_TT_128B__half (117 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_8_16_TN_NoSwizzle__half
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_8_16_TN_NoSwizzle__half (107 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_24_16_TT_NoSwizzle__half
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_24_16_TT_NoSwizzle__half (122 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_32_16_TN_64B__half
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_32_16_TN_64B__half (143 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_40_16_TT_128B__half
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_40_16_TT_128B__half (140 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_48_16_TT_128B__bfloat
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_48_16_TT_128B__bfloat (141 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_56_16_TT_128B__bfloat
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_56_16_TT_128B__bfloat (143 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_72_16_TT_NoSwizzle__half
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_72_16_TT_NoSwizzle__half (141 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_72_16_TN_NoSwizzle__bfloat
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_72_16_TN_NoSwizzle__bfloat (134 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_96_16_TT_NoSwizzle__half
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_96_16_TT_NoSwizzle__half (148 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_112_16_TT_NoSwizzle__half
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_112_16_TT_NoSwizzle__half (153 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_120_16_TN_64B__half
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_120_16_TN_64B__half (195 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_136_16_TT_NoSwizzle__half
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_136_16_TT_NoSwizzle__half (170 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_144_16_TT_NoSwizzle__half
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_144_16_TT_NoSwizzle__half (168 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_152_16_TN_32B__half
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_152_16_TN_32B__half (181 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_168_16_TT_NoSwizzle__half
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_168_16_TT_NoSwizzle__half (187 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_176_16_TT_64B__half
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_176_16_TT_64B__half (219 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_176_16_TT_32B__bfloat
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_176_16_TT_32B__bfloat (211 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_184_16_TT_NoSwizzle__half
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_184_16_TT_NoSwizzle__half (195 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_184_16_TN_128B__bfloat
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_184_16_TN_128B__bfloat (311 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_192_16_TT_NoSwizzle__bfloat
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_192_16_TT_NoSwizzle__bfloat (189 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_200_16_TT_NoSwizzle__half
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_200_16_TT_NoSwizzle__half (204 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_208_16_TT_NoSwizzle__half
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_208_16_TT_NoSwizzle__half (197 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_208_16_TN_128B__bfloat
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_208_16_TN_128B__bfloat (347 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_216_16_TT_64B__half
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_216_16_TT_64B__half (236 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_224_16_TT_NoSwizzle__half
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_224_16_TT_NoSwizzle__half (209 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_232_16_TT_NoSwizzle__half
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_232_16_TT_NoSwizzle__half (226 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_240_16_TT_32B__bfloat
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_240_16_TT_32B__bfloat (246 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_248_16_TN_NoSwizzle__bfloat
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_248_16_TN_NoSwizzle__bfloat (204 ms)
[ RUN      ] MmaTest/HopperRS.SingleTile/Hopper_64_256_16_TN_64B__bfloat
[       OK ] MmaTest/HopperRS.SingleTile/Hopper_64_256_16_TN_64B__bfloat (300 ms)
[----------] 30 tests from MmaTest/HopperRS (5700 ms total)

[----------] 21 tests from MmaTest/HopperRSStmatrix
[ RUN      ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/2
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:543: Skipped
IdModel does not support stmatrix.x2

[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/2 (0 ms)
[ RUN      ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/5
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:446: Skipped
skipping test as output is not divisible by tile size

[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/5 (0 ms)
[ RUN      ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/8
[       OK ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/8 (135 ms)
[ RUN      ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/11
[       OK ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/11 (160 ms)
[ RUN      ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/14
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:543: Skipped
IdModel does not support stmatrix.x2

[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/14 (0 ms)
[ RUN      ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/17
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:446: Skipped
skipping test as output is not divisible by tile size

[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/17 (0 ms)
[ RUN      ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/20
[       OK ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/20 (163 ms)
[ RUN      ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/23
[       OK ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/23 (197 ms)
[ RUN      ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/26
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:543: Skipped
IdModel does not support stmatrix.x2

[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/26 (0 ms)
[ RUN      ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/29
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:446: Skipped
skipping test as output is not divisible by tile size

[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/29 (0 ms)
[ RUN      ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/32
[       OK ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/32 (187 ms)
[ RUN      ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/35
[       OK ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/35 (224 ms)
[ RUN      ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/38
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:543: Skipped
IdModel does not support stmatrix.x2

[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/38 (0 ms)
[ RUN      ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/41
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:446: Skipped
skipping test as output is not divisible by tile size

[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/41 (0 ms)
[ RUN      ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/44
[       OK ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/44 (217 ms)
[ RUN      ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/47
[       OK ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/47 (280 ms)
[ RUN      ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/50
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:543: Skipped
IdModel does not support stmatrix.x2

[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/50 (0 ms)
[ RUN      ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/53
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:446: Skipped
skipping test as output is not divisible by tile size

[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/53 (0 ms)
[ RUN      ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/56
[       OK ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/56 (247 ms)
[ RUN      ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/59
[       OK ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/59 (300 ms)
[ RUN      ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/62
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:543: Skipped
IdModel does not support stmatrix.x2

[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/62 (0 ms)
[----------] 21 tests from MmaTest/HopperRSStmatrix (2119 ms total)

[----------] 534 tests from MmaTest/HopperSS
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_8_16_TT_NoSwizzle_128B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_8_16_TT_NoSwizzle_128B__half (122 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_8_16_TT_128B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_8_16_TT_128B_NoSwizzle__half (156 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_8_16_TN_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_8_16_TN_NoSwizzle_NoSwizzle__half (114 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_8_16_NN_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_8_16_NN_NoSwizzle_NoSwizzle__half (117 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_8_16_TT_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_8_16_TT_NoSwizzle_NoSwizzle__bfloat (113 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_8_16_TN_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_8_16_TN_64B_128B__bfloat (154 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_8_16_NT_32B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_8_16_NT_32B_64B__bfloat (159 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_16_16_TT_NoSwizzle_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_16_16_TT_NoSwizzle_32B__half (142 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_16_16_TT_64B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_16_16_TT_64B_64B__bfloat (167 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_16_16_NN_128B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_16_16_NN_128B_32B__bfloat (153 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_24_16_TT_64B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_24_16_TT_64B_NoSwizzle__half (157 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_24_16_NN_128B_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_24_16_NN_128B_32B__half (152 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_24_16_TT_32B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_24_16_TT_32B_NoSwizzle__bfloat (144 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_24_16_TN_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_24_16_TN_NoSwizzle_128B__bfloat (146 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_24_16_TN_32B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_24_16_TN_32B_64B__bfloat (164 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_24_16_NT_32B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_24_16_NT_32B_64B__bfloat (179 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_24_16_NN_64B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_24_16_NN_64B_64B__bfloat (174 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_32_16_TT_128B_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_32_16_TT_128B_32B__half (204 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_32_16_TN_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_32_16_TN_NoSwizzle_NoSwizzle__half (124 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_32_16_NT_NoSwizzle_64B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_32_16_NT_NoSwizzle_64B__half (145 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_32_16_TT_128B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_32_16_TT_128B_64B__bfloat (185 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_32_16_NT_128B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_32_16_NT_128B_NoSwizzle__bfloat (140 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_32_16_NN_128B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_32_16_NN_128B_NoSwizzle__bfloat (138 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_40_16_TT_128B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_40_16_TT_128B_NoSwizzle__half (175 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_40_16_TN_128B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_40_16_TN_128B_NoSwizzle__half (171 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_40_16_NN_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_40_16_NN_NoSwizzle_NoSwizzle__half (130 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_40_16_TN_NoSwizzle_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_40_16_TN_NoSwizzle_32B__bfloat (144 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_40_16_NT_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_40_16_NT_64B_128B__bfloat (176 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_48_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_48_16_TT_NoSwizzle_NoSwizzle__half (131 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_48_16_NT_NoSwizzle_128B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_48_16_NT_NoSwizzle_128B__half (145 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_48_16_NN_64B_64B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_48_16_NN_64B_64B__half (186 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_48_16_TT_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_48_16_TT_64B_128B__bfloat (173 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_48_16_TN_128B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_48_16_TN_128B_32B__bfloat (198 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_48_16_NT_32B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_48_16_NT_32B_NoSwizzle__bfloat (167 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_56_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_56_16_TT_NoSwizzle_NoSwizzle__half (137 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_56_16_NN_128B_128B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_56_16_NN_128B_128B__half (182 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_56_16_TT_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_56_16_TT_64B_128B__bfloat (174 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_64_16_TT_128B_64B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_64_16_TT_128B_64B__half (211 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_64_16_NT_128B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_64_16_NT_128B_NoSwizzle__half (149 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_64_16_TT_32B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_64_16_TT_32B_NoSwizzle__bfloat (153 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_64_16_NN_32B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_64_16_NN_32B_32B__bfloat (184 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_72_16_TN_128B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_72_16_TN_128B_NoSwizzle__half (181 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_72_16_NT_32B_128B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_72_16_NT_32B_128B__half (205 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_72_16_NN_64B_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_72_16_NN_64B_32B__half (185 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_72_16_TT_32B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_72_16_TT_32B_128B__bfloat (186 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_72_16_NT_128B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_72_16_NT_128B_32B__bfloat (185 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_72_16_NN_64B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_72_16_NN_64B_32B__bfloat (185 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_80_16_TT_64B_64B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_80_16_TT_64B_64B__half (200 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_80_16_TT_64B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_80_16_TT_64B_64B__bfloat (202 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_80_16_TN_64B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_80_16_TN_64B_64B__bfloat (197 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_80_16_NN_NoSwizzle_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_80_16_NN_NoSwizzle_64B__bfloat (176 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_80_16_NN_32B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_80_16_NN_32B_128B__bfloat (230 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_88_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_88_16_TT_NoSwizzle_NoSwizzle__half (149 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_88_16_TN_64B_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_88_16_TN_64B_32B__half (190 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_88_16_NT_64B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_88_16_NT_64B_NoSwizzle__half (183 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_88_16_TT_128B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_88_16_TT_128B_128B__bfloat (215 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_88_16_NT_128B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_88_16_NT_128B_128B__bfloat (183 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_88_16_NN_128B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_88_16_NN_128B_NoSwizzle__bfloat (153 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_96_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_96_16_TT_NoSwizzle_NoSwizzle__half (147 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_96_16_TN_NoSwizzle_128B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_96_16_TN_NoSwizzle_128B__half (207 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_96_16_NT_128B_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_96_16_NT_128B_32B__half (191 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_96_16_NN_32B_128B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_96_16_NN_32B_128B__half (245 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_96_16_TT_128B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_96_16_TT_128B_64B__bfloat (225 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_96_16_NT_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_96_16_NT_NoSwizzle_128B__bfloat (176 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_104_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_104_16_TT_NoSwizzle_NoSwizzle__half (154 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_104_16_TT_64B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_104_16_TT_64B_NoSwizzle__half (186 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_104_16_TN_NoSwizzle_64B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_104_16_TN_NoSwizzle_64B__half (186 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_104_16_NT_64B_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_104_16_NT_64B_32B__half (220 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_104_16_TT_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_104_16_TT_NoSwizzle_128B__bfloat (175 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_104_16_NT_128B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_104_16_NT_128B_128B__bfloat (187 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_104_16_NN_32B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_104_16_NN_32B_128B__bfloat (257 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_112_16_TT_64B_128B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_112_16_TT_64B_128B__half (210 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_112_16_NT_NoSwizzle_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_112_16_NT_NoSwizzle_32B__half (189 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_112_16_TT_64B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_112_16_TT_64B_NoSwizzle__bfloat (187 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_112_16_TN_128B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_112_16_TN_128B_NoSwizzle__bfloat (192 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_112_16_NN_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_112_16_NN_64B_128B__bfloat (261 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_120_16_TT_64B_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_120_16_TT_64B_32B__half (219 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_120_16_NN_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_120_16_NN_NoSwizzle_NoSwizzle__half (153 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_120_16_TT_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_120_16_TT_64B_128B__bfloat (210 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_120_16_TN_32B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_120_16_TN_32B_NoSwizzle__bfloat (168 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_120_16_NN_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_120_16_NN_NoSwizzle_128B__bfloat (235 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_128_16_TN_NoSwizzle_128B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_128_16_TN_NoSwizzle_128B__half (241 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_128_16_TT_NoSwizzle_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_128_16_TT_NoSwizzle_64B__bfloat (191 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_128_16_TN_NoSwizzle_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_128_16_TN_NoSwizzle_32B__bfloat (172 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_128_16_NT_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_128_16_NT_64B_128B__bfloat (216 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_136_16_TT_NoSwizzle_64B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_136_16_TT_NoSwizzle_64B__half (198 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_136_16_TT_32B_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_136_16_TT_32B_32B__half (212 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_136_16_NT_64B_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_136_16_NT_64B_32B__half (230 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_136_16_NN_64B_128B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_136_16_NN_64B_128B__half (287 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_136_16_NT_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_136_16_NT_NoSwizzle_128B__bfloat (198 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_136_16_NN_32B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_136_16_NN_32B_32B__bfloat (212 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_144_16_TN_NoSwizzle_128B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_144_16_TN_NoSwizzle_128B__half (261 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_144_16_NT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_144_16_NT_NoSwizzle_NoSwizzle__half (170 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_144_16_NT_32B_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_144_16_NT_32B_32B__half (232 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_144_16_TT_128B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_144_16_TT_128B_NoSwizzle__bfloat (215 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_144_16_NT_32B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_144_16_NT_32B_NoSwizzle__bfloat (203 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_144_16_NN_32B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_144_16_NN_32B_128B__bfloat (298 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_152_16_TT_64B_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_152_16_TT_64B_32B__half (235 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_152_16_TN_64B_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_152_16_TN_64B_32B__half (216 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_152_16_NT_64B_128B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_152_16_NT_64B_128B__half (234 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_152_16_TT_64B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_152_16_TT_64B_32B__bfloat (232 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_152_16_TN_NoSwizzle_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_152_16_TN_NoSwizzle_32B__bfloat (181 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_152_16_NT_64B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_152_16_NT_64B_32B__bfloat (235 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_152_16_NN_32B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_152_16_NN_32B_128B__bfloat (307 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_160_16_TT_64B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_160_16_TT_64B_NoSwizzle__half (205 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_160_16_TN_32B_64B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_160_16_TN_32B_64B__half (246 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_160_16_NT_128B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_160_16_NT_128B_NoSwizzle__half (188 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_160_16_TT_128B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_160_16_TT_128B_NoSwizzle__bfloat (224 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_160_16_NT_64B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_160_16_NT_64B_64B__bfloat (236 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_160_16_NN_64B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_160_16_NN_64B_NoSwizzle__bfloat (196 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_168_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_168_16_TT_NoSwizzle_NoSwizzle__half (185 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_168_16_TT_128B_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_168_16_TT_128B_32B__half (257 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_168_16_TN_32B_128B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_168_16_TN_32B_128B__half (321 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_168_16_NN_NoSwizzle_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_168_16_NN_NoSwizzle_32B__half (193 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_168_16_NN_32B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_168_16_NN_32B_NoSwizzle__half (203 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_168_16_TN_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_168_16_TN_NoSwizzle_NoSwizzle__bfloat (164 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_168_16_NT_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_168_16_NT_NoSwizzle_NoSwizzle__bfloat (191 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_176_16_TT_128B_128B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_176_16_TT_128B_128B__half (246 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_176_16_NT_32B_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_176_16_NT_32B_32B__half (246 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_176_16_TT_NoSwizzle_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_176_16_TT_NoSwizzle_64B__bfloat (215 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_176_16_TT_32B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_176_16_TT_32B_32B__bfloat (228 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_176_16_NT_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_176_16_NT_64B_128B__bfloat (240 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_176_16_NN_32B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_176_16_NN_32B_NoSwizzle__bfloat (205 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_184_16_TT_128B_64B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_184_16_TT_128B_64B__half (267 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_184_16_TN_128B_128B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_184_16_TN_128B_128B__half (361 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_184_16_NN_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_184_16_NN_NoSwizzle_NoSwizzle__half (175 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_184_16_TN_128B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_184_16_TN_128B_128B__bfloat (362 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_184_16_TN_32B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_184_16_TN_32B_64B__bfloat (263 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_192_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_192_16_TT_NoSwizzle_NoSwizzle__half (187 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_192_16_NN_NoSwizzle_128B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_192_16_NN_NoSwizzle_128B__half (321 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_192_16_TT_64B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_192_16_TT_64B_64B__bfloat (247 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_192_16_NN_64B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_192_16_NN_64B_NoSwizzle__bfloat (209 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_200_16_TN_32B_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_200_16_TN_32B_32B__half (219 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_200_16_NT_64B_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_200_16_NT_64B_32B__half (257 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_200_16_TT_128B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_200_16_TT_128B_32B__bfloat (270 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_200_16_NT_64B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_200_16_NT_64B_32B__bfloat (257 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_208_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_208_16_TT_NoSwizzle_NoSwizzle__half (196 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_208_16_TN_32B_64B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_208_16_TN_32B_64B__half (285 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_208_16_TT_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_208_16_TT_NoSwizzle_NoSwizzle__bfloat (196 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_208_16_TT_64B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_208_16_TT_64B_32B__bfloat (257 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_208_16_TN_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_208_16_TN_64B_128B__bfloat (382 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_208_16_NT_32B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_208_16_NT_32B_32B__bfloat (261 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_216_16_TT_128B_64B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_216_16_TT_128B_64B__half (286 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_216_16_TN_64B_64B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_216_16_TN_64B_64B__half (290 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_216_16_NN_64B_128B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_216_16_NN_64B_128B__half (389 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_216_16_TN_32B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_216_16_TN_32B_32B__bfloat (230 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_224_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_224_16_TT_NoSwizzle_NoSwizzle__half (205 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_224_16_NT_NoSwizzle_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_224_16_NT_NoSwizzle_32B__half (238 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_224_16_TT_128B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_224_16_TT_128B_NoSwizzle__bfloat (258 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_224_16_TN_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_224_16_TN_NoSwizzle_128B__bfloat (361 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_224_16_TN_32B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_224_16_TN_32B_128B__bfloat (401 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_224_16_NN_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_224_16_NN_64B_128B__bfloat (402 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_232_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_232_16_TT_NoSwizzle_NoSwizzle__half (223 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_232_16_TN_64B_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_232_16_TN_64B_32B__half (261 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_232_16_NT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_232_16_NT_NoSwizzle_NoSwizzle__half (231 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_232_16_NN_NoSwizzle_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_232_16_NN_NoSwizzle_32B__half (229 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_232_16_TT_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_232_16_TT_NoSwizzle_NoSwizzle__bfloat (224 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_232_16_TN_128B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_232_16_TN_128B_128B__bfloat (434 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_232_16_TN_32B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_232_16_TN_32B_NoSwizzle__bfloat (213 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_232_16_NN_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_232_16_NN_NoSwizzle_128B__bfloat (379 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_240_16_TT_32B_128B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_240_16_TT_32B_128B__half (258 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_240_16_NT_128B_64B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_240_16_NT_128B_64B__half (265 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_240_16_NT_64B_32B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_240_16_NT_64B_32B__half (277 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_240_16_NT_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_240_16_NT_NoSwizzle_NoSwizzle__bfloat (219 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_240_16_NN_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_240_16_NN_NoSwizzle_NoSwizzle__bfloat (203 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_240_16_NN_128B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_240_16_NN_128B_64B__bfloat (302 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_248_16_TT_128B_64B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_248_16_TT_128B_64B__half (309 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_248_16_TN_128B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_248_16_TN_128B_NoSwizzle__half (249 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_248_16_TN_32B_128B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_248_16_TN_32B_128B__half (436 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_248_16_NN_64B_64B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_248_16_NN_64B_64B__half (323 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_248_16_TN_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_248_16_TN_NoSwizzle_128B__bfloat (395 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_248_16_NT_64B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_248_16_NT_64B_NoSwizzle__bfloat (270 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_248_16_NN_32B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_248_16_NN_32B_64B__bfloat (329 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_256_16_TT_NoSwizzle_128B__half
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_256_16_TT_NoSwizzle_128B__half (245 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_256_16_TT_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_256_16_TT_NoSwizzle_128B__bfloat (243 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_256_16_TN_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_256_16_TN_NoSwizzle_128B__bfloat (406 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_256_16_NT_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_256_16_NT_NoSwizzle_NoSwizzle__bfloat (226 ms)
[ RUN      ] MmaTest/HopperSS.SingleTile/Hopper_64_256_16_NN_128B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTile/Hopper_64_256_16_NN_128B_32B__bfloat (254 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_8_16_TT_NoSwizzle_128B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_8_16_TT_NoSwizzle_128B__half (84 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_8_16_TT_128B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_8_16_TT_128B_NoSwizzle__half (86 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_8_16_TN_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_8_16_TN_NoSwizzle_NoSwizzle__half (78 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_8_16_NN_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_8_16_NN_NoSwizzle_NoSwizzle__half (79 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_8_16_TT_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_8_16_TT_NoSwizzle_NoSwizzle__bfloat (77 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_8_16_TN_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_8_16_TN_64B_128B__bfloat (101 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_8_16_NT_32B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_8_16_NT_32B_64B__bfloat (120 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_16_16_TT_NoSwizzle_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_16_16_TT_NoSwizzle_32B__half (101 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_16_16_TT_64B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_16_16_TT_64B_64B__bfloat (112 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_16_16_NN_128B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_16_16_NN_128B_32B__bfloat (111 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_24_16_TT_64B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_24_16_TT_64B_NoSwizzle__half (100 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_24_16_NN_128B_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_24_16_NN_128B_32B__half (109 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_24_16_TT_32B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_24_16_TT_32B_NoSwizzle__bfloat (102 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_24_16_TN_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_24_16_TN_NoSwizzle_128B__bfloat (95 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_24_16_TN_32B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_24_16_TN_32B_64B__bfloat (117 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_24_16_NT_32B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_24_16_NT_32B_64B__bfloat (131 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_24_16_NN_64B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_24_16_NN_64B_64B__bfloat (127 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_32_16_TT_128B_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_32_16_TT_128B_32B__half (127 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_32_16_TN_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_32_16_TN_NoSwizzle_NoSwizzle__half (86 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_32_16_NT_NoSwizzle_64B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_32_16_NT_NoSwizzle_64B__half (100 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_32_16_TT_128B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_32_16_TT_128B_64B__bfloat (107 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_32_16_NT_128B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_32_16_NT_128B_NoSwizzle__bfloat (97 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_32_16_NN_128B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_32_16_NN_128B_NoSwizzle__bfloat (94 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_40_16_TT_128B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_40_16_TT_128B_NoSwizzle__half (95 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_40_16_TN_128B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_40_16_TN_128B_NoSwizzle__half (96 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_40_16_NN_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_40_16_NN_NoSwizzle_NoSwizzle__half (86 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_40_16_TN_NoSwizzle_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_40_16_TN_NoSwizzle_32B__bfloat (101 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_40_16_NT_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_40_16_NT_64B_128B__bfloat (123 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_48_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_48_16_TT_NoSwizzle_NoSwizzle__half (86 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_48_16_NT_NoSwizzle_128B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_48_16_NT_NoSwizzle_128B__half (95 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_48_16_NN_64B_64B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_48_16_NN_64B_64B__half (126 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_48_16_TT_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_48_16_TT_64B_128B__bfloat (107 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_48_16_TN_128B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_48_16_TN_128B_32B__bfloat (111 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_48_16_NT_32B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_48_16_NT_32B_NoSwizzle__bfloat (119 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_56_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_56_16_TT_NoSwizzle_NoSwizzle__half (86 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_56_16_NN_128B_128B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_56_16_NN_128B_128B__half (102 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_56_16_TT_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_56_16_TT_64B_128B__bfloat (108 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_64_16_TT_128B_64B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_64_16_TT_128B_64B__half (122 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_64_16_NT_128B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_64_16_NT_128B_NoSwizzle__half (95 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_64_16_TT_32B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_64_16_TT_32B_NoSwizzle__bfloat (102 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_64_16_NN_32B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_64_16_NN_32B_32B__bfloat (133 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_72_16_TN_128B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_72_16_TN_128B_NoSwizzle__half (94 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_72_16_NT_32B_128B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_72_16_NT_32B_128B__half (139 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_72_16_NN_64B_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_72_16_NN_64B_32B__half (129 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_72_16_TT_32B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_72_16_TT_32B_128B__bfloat (122 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_72_16_NT_128B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_72_16_NT_128B_32B__bfloat (126 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_72_16_NN_64B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_72_16_NN_64B_32B__bfloat (131 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_80_16_TT_64B_64B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_80_16_TT_64B_64B__half (127 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_80_16_TT_64B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_80_16_TT_64B_64B__bfloat (127 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_80_16_TN_64B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_80_16_TN_64B_64B__bfloat (112 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_80_16_NN_NoSwizzle_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_80_16_NN_NoSwizzle_64B__bfloat (100 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_80_16_NN_32B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_80_16_NN_32B_128B__bfloat (126 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_88_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_88_16_TT_NoSwizzle_NoSwizzle__half (87 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_88_16_TN_64B_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_88_16_TN_64B_32B__half (120 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_88_16_NT_64B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_88_16_NT_64B_NoSwizzle__half (115 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_88_16_TT_128B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_88_16_TT_128B_128B__bfloat (115 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_88_16_NT_128B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_88_16_NT_128B_128B__bfloat (115 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_88_16_NN_128B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_88_16_NN_128B_NoSwizzle__bfloat (95 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_96_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_96_16_TT_NoSwizzle_NoSwizzle__half (87 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_96_16_TN_NoSwizzle_128B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_96_16_TN_NoSwizzle_128B__half (95 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_96_16_NT_128B_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_96_16_NT_128B_32B__half (126 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_96_16_NN_32B_128B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_96_16_NN_32B_128B__half (127 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_96_16_TT_128B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_96_16_TT_128B_64B__bfloat (122 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_96_16_NT_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_96_16_NT_NoSwizzle_128B__bfloat (107 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_104_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_104_16_TT_NoSwizzle_NoSwizzle__half (88 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_104_16_TT_64B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_104_16_TT_64B_NoSwizzle__half (100 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_104_16_TN_NoSwizzle_64B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_104_16_TN_NoSwizzle_64B__half (99 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_104_16_NT_64B_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_104_16_NT_64B_32B__half (146 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_104_16_TT_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_104_16_TT_NoSwizzle_128B__bfloat (107 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_104_16_NT_128B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_104_16_NT_128B_128B__bfloat (115 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_104_16_NN_32B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_104_16_NN_32B_128B__bfloat (127 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_112_16_TT_64B_128B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_112_16_TT_64B_128B__half (120 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_112_16_NT_NoSwizzle_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_112_16_NT_NoSwizzle_32B__half (119 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_112_16_TT_64B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_112_16_TT_64B_NoSwizzle__bfloat (100 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_112_16_TN_128B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_112_16_TN_128B_NoSwizzle__bfloat (95 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_112_16_NN_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_112_16_NN_64B_128B__bfloat (123 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_120_16_TT_64B_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_120_16_TT_64B_32B__half (132 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_120_16_NN_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_120_16_NN_NoSwizzle_NoSwizzle__half (87 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_120_16_TT_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_120_16_TT_64B_128B__bfloat (120 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_120_16_TN_32B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_120_16_TN_32B_NoSwizzle__bfloat (102 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_120_16_NN_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_120_16_NN_NoSwizzle_128B__bfloat (96 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_128_16_TN_NoSwizzle_128B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_128_16_TN_NoSwizzle_128B__half (95 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_128_16_TT_NoSwizzle_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_128_16_TT_NoSwizzle_64B__bfloat (114 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_128_16_TN_NoSwizzle_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_128_16_TN_NoSwizzle_32B__bfloat (102 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_128_16_NT_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_128_16_NT_64B_128B__bfloat (135 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_136_16_TT_NoSwizzle_64B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_136_16_TT_NoSwizzle_64B__half (115 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_136_16_TT_32B_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_136_16_TT_32B_32B__half (135 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_136_16_NT_64B_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_136_16_NT_64B_32B__half (147 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_136_16_NN_64B_128B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_136_16_NN_64B_128B__half (123 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_136_16_NT_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_136_16_NT_NoSwizzle_128B__bfloat (109 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_136_16_NN_32B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_136_16_NN_32B_32B__bfloat (136 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_144_16_TN_NoSwizzle_128B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_144_16_TN_NoSwizzle_128B__half (95 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_144_16_NT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_144_16_NT_NoSwizzle_NoSwizzle__half (88 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_144_16_NT_32B_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_144_16_NT_32B_32B__half (150 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_144_16_TT_128B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_144_16_TT_128B_NoSwizzle__bfloat (96 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_144_16_NT_32B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_144_16_NT_32B_NoSwizzle__bfloat (120 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_144_16_NN_32B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_144_16_NN_32B_128B__bfloat (128 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_152_16_TT_64B_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_152_16_TT_64B_32B__half (132 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_152_16_TN_64B_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_152_16_TN_64B_32B__half (118 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_152_16_NT_64B_128B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_152_16_NT_64B_128B__half (136 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_152_16_TT_64B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_152_16_TT_64B_32B__bfloat (132 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_152_16_TN_NoSwizzle_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_152_16_TN_NoSwizzle_32B__bfloat (102 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_152_16_NT_64B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_152_16_NT_64B_32B__bfloat (147 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_152_16_NN_32B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_152_16_NN_32B_128B__bfloat (127 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_160_16_TT_64B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_160_16_TT_64B_NoSwizzle__half (100 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_160_16_TN_32B_64B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_160_16_TN_32B_64B__half (118 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_160_16_NT_128B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_160_16_NT_128B_NoSwizzle__half (96 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_160_16_TT_128B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_160_16_TT_128B_NoSwizzle__bfloat (96 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_160_16_NT_64B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_160_16_NT_64B_64B__bfloat (142 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_160_16_NN_64B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_160_16_NN_64B_NoSwizzle__bfloat (115 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_168_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_168_16_TT_NoSwizzle_NoSwizzle__half (87 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_168_16_TT_128B_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_168_16_TT_128B_32B__half (128 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_168_16_TN_32B_128B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_168_16_TN_32B_128B__half (113 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_168_16_NN_NoSwizzle_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_168_16_NN_NoSwizzle_32B__half (103 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_168_16_NN_32B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_168_16_NN_32B_NoSwizzle__half (119 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_168_16_TN_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_168_16_TN_NoSwizzle_NoSwizzle__bfloat (87 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_168_16_NT_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_168_16_NT_NoSwizzle_NoSwizzle__bfloat (88 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_176_16_TT_128B_128B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_176_16_TT_128B_128B__half (115 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_176_16_NT_32B_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_176_16_NT_32B_32B__half (153 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_176_16_TT_NoSwizzle_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_176_16_TT_NoSwizzle_64B__bfloat (115 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_176_16_TT_32B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_176_16_TT_32B_32B__bfloat (134 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_176_16_NT_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_176_16_NT_64B_128B__bfloat (136 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_176_16_NN_32B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_176_16_NN_32B_NoSwizzle__bfloat (119 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_184_16_TT_128B_64B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_184_16_TT_128B_64B__half (123 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_184_16_TN_128B_128B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_184_16_TN_128B_128B__half (104 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_184_16_NN_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_184_16_NN_NoSwizzle_NoSwizzle__half (88 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_184_16_TN_128B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_184_16_TN_128B_128B__bfloat (104 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_184_16_TN_32B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_184_16_TN_32B_64B__bfloat (118 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_192_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_192_16_TT_NoSwizzle_NoSwizzle__half (88 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_192_16_NN_NoSwizzle_128B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_192_16_NN_NoSwizzle_128B__half (98 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_192_16_TT_64B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_192_16_TT_64B_64B__bfloat (129 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_192_16_NN_64B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_192_16_NN_64B_NoSwizzle__bfloat (116 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_200_16_TN_32B_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_200_16_TN_32B_32B__half (117 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_200_16_NT_64B_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_200_16_NT_64B_32B__half (148 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_200_16_TT_128B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_200_16_TT_128B_32B__bfloat (127 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_200_16_NT_64B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_200_16_NT_64B_32B__bfloat (147 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_208_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_208_16_TT_NoSwizzle_NoSwizzle__half (88 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_208_16_TN_32B_64B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_208_16_TN_32B_64B__half (119 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_208_16_TT_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_208_16_TT_NoSwizzle_NoSwizzle__bfloat (89 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_208_16_TT_64B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_208_16_TT_64B_32B__bfloat (135 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_208_16_TN_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_208_16_TN_64B_128B__bfloat (111 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_208_16_NT_32B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_208_16_NT_32B_32B__bfloat (152 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_216_16_TT_128B_64B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_216_16_TT_128B_64B__half (123 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_216_16_TN_64B_64B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_216_16_TN_64B_64B__half (113 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_216_16_NN_64B_128B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_216_16_NN_64B_128B__half (125 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_216_16_TN_32B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_216_16_TN_32B_32B__bfloat (119 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_224_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_224_16_TT_NoSwizzle_NoSwizzle__half (90 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_224_16_NT_NoSwizzle_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_224_16_NT_NoSwizzle_32B__half (120 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_224_16_TT_128B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_224_16_TT_128B_NoSwizzle__bfloat (97 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_224_16_TN_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_224_16_TN_NoSwizzle_128B__bfloat (96 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_224_16_TN_32B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_224_16_TN_32B_128B__bfloat (114 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_224_16_NN_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_224_16_NN_64B_128B__bfloat (124 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_232_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_232_16_TT_NoSwizzle_NoSwizzle__half (88 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_232_16_TN_64B_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_232_16_TN_64B_32B__half (119 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_232_16_NT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_232_16_NT_NoSwizzle_NoSwizzle__half (89 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_232_16_NN_NoSwizzle_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_232_16_NN_NoSwizzle_32B__half (103 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_232_16_TT_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_232_16_TT_NoSwizzle_NoSwizzle__bfloat (89 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_232_16_TN_128B_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_232_16_TN_128B_128B__bfloat (106 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_232_16_TN_32B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_232_16_TN_32B_NoSwizzle__bfloat (103 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_232_16_NN_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_232_16_NN_NoSwizzle_128B__bfloat (97 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_240_16_TT_32B_128B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_240_16_TT_32B_128B__half (123 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_240_16_NT_128B_64B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_240_16_NT_128B_64B__half (124 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_240_16_NT_64B_32B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_240_16_NT_64B_32B__half (148 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_240_16_NT_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_240_16_NT_NoSwizzle_NoSwizzle__bfloat (89 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_240_16_NN_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_240_16_NN_NoSwizzle_NoSwizzle__bfloat (88 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_240_16_NN_128B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_240_16_NN_128B_64B__bfloat (109 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_248_16_TT_128B_64B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_248_16_TT_128B_64B__half (123 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_248_16_TN_128B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_248_16_TN_128B_NoSwizzle__half (96 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_248_16_TN_32B_128B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_248_16_TN_32B_128B__half (114 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_248_16_NN_64B_64B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_248_16_NN_64B_64B__half (128 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_248_16_TN_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_248_16_TN_NoSwizzle_128B__bfloat (97 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_248_16_NT_64B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_248_16_NT_64B_NoSwizzle__bfloat (116 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_248_16_NN_32B_64B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_248_16_NN_32B_64B__bfloat (133 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_256_16_TT_NoSwizzle_128B__half
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_256_16_TT_NoSwizzle_128B__half (110 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_256_16_TT_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_256_16_TT_NoSwizzle_128B__bfloat (109 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_256_16_TN_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_256_16_TN_NoSwizzle_128B__bfloat (96 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_256_16_NT_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_256_16_NT_NoSwizzle_NoSwizzle__bfloat (89 ms)
[ RUN      ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_256_16_NN_128B_32B__bfloat
[       OK ] MmaTest/HopperSS.SingleTileTransposed/Hopper_64_256_16_NN_128B_32B__bfloat (111 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_8_16_TT_NoSwizzle_128B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_8_16_TT_NoSwizzle_128B__half (198 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_8_16_TT_128B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_8_16_TT_128B_NoSwizzle__half (231 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_8_16_TN_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_8_16_TN_NoSwizzle_NoSwizzle__half (170 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_8_16_NN_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_8_16_NN_NoSwizzle_NoSwizzle__half (171 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_8_16_TT_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_8_16_TT_NoSwizzle_NoSwizzle__bfloat (170 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_8_16_TN_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_8_16_TN_64B_128B__bfloat (209 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_8_16_NT_32B_64B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_8_16_NT_32B_64B__bfloat (227 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_16_16_TT_NoSwizzle_32B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_16_16_TT_NoSwizzle_32B__half (218 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_16_16_TT_64B_64B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_16_16_TT_64B_64B__bfloat (218 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_16_16_NN_128B_32B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_16_16_NN_128B_32B__bfloat (238 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_24_16_TT_64B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_24_16_TT_64B_NoSwizzle__half (219 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_24_16_NN_128B_32B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_24_16_NN_128B_32B__half (250 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_24_16_TT_32B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_24_16_TT_32B_NoSwizzle__bfloat (223 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_24_16_TN_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_24_16_TN_NoSwizzle_128B__bfloat (229 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_24_16_TN_32B_64B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_24_16_TN_32B_64B__bfloat (241 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_24_16_NT_32B_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_24_16_NT_32B_64B__bfloat (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_24_16_NN_64B_64B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_24_16_NN_64B_64B__bfloat (244 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_32_16_TT_128B_32B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_32_16_TT_128B_32B__half (308 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_32_16_TN_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_32_16_TN_NoSwizzle_NoSwizzle__half (207 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_32_16_NT_NoSwizzle_64B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_32_16_NT_NoSwizzle_64B__half (244 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_32_16_TT_128B_64B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_32_16_TT_128B_64B__bfloat (307 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_32_16_NT_128B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_32_16_NT_128B_NoSwizzle__bfloat (227 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_32_16_NN_128B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_32_16_NN_128B_NoSwizzle__bfloat (231 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_40_16_TT_128B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_40_16_TT_128B_NoSwizzle__half (290 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_40_16_TN_128B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_40_16_TN_128B_NoSwizzle__half (290 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_40_16_NN_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_40_16_NN_NoSwizzle_NoSwizzle__half (223 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_40_16_TN_NoSwizzle_32B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_40_16_TN_NoSwizzle_32B__bfloat (257 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_40_16_NT_64B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_40_16_NT_64B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_48_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_48_16_TT_NoSwizzle_NoSwizzle__half (239 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_48_16_NT_NoSwizzle_128B__half
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_48_16_NT_NoSwizzle_128B__half (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_48_16_NN_64B_64B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_48_16_NN_64B_64B__half (285 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_48_16_TT_64B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_48_16_TT_64B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_48_16_TN_128B_32B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_48_16_TN_128B_32B__bfloat (331 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_48_16_NT_32B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_48_16_NT_32B_NoSwizzle__bfloat (263 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_56_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_56_16_TT_NoSwizzle_NoSwizzle__half (252 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_56_16_NN_128B_128B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_56_16_NN_128B_128B__half (331 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_56_16_TT_64B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_56_16_TT_64B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_64_16_TT_128B_64B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_64_16_TT_128B_64B__half (375 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_64_16_NT_128B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_64_16_NT_128B_NoSwizzle__half (291 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_64_16_TT_32B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_64_16_TT_32B_NoSwizzle__bfloat (291 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_64_16_NN_32B_32B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_64_16_NN_32B_32B__bfloat (317 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_72_16_TN_128B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_72_16_TN_128B_NoSwizzle__half (358 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_72_16_NT_32B_128B__half
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_72_16_NT_32B_128B__half (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_72_16_NN_64B_32B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_72_16_NN_64B_32B__half (341 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_72_16_TT_32B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_72_16_TT_32B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_72_16_NT_128B_32B__bfloat
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_72_16_NT_128B_32B__bfloat (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_72_16_NN_64B_32B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_72_16_NN_64B_32B__bfloat (342 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_80_16_TT_64B_64B__half
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_80_16_TT_64B_64B__half (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_80_16_TT_64B_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_80_16_TT_64B_64B__bfloat (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_80_16_TN_64B_64B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_80_16_TN_64B_64B__bfloat (329 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_80_16_NN_NoSwizzle_64B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_80_16_NN_NoSwizzle_64B__bfloat (325 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_80_16_NN_32B_128B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_80_16_NN_32B_128B__bfloat (409 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_88_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_88_16_TT_NoSwizzle_NoSwizzle__half (315 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_88_16_TN_64B_32B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_88_16_TN_64B_32B__half (360 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_88_16_NT_64B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_88_16_NT_64B_NoSwizzle__half (353 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_88_16_TT_128B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_88_16_TT_128B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_88_16_NT_128B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_88_16_NT_128B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_88_16_NN_128B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_88_16_NN_128B_NoSwizzle__bfloat (347 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_96_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_96_16_TT_NoSwizzle_NoSwizzle__half (333 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_96_16_TN_NoSwizzle_128B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_96_16_TN_NoSwizzle_128B__half (458 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_96_16_NT_128B_32B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_96_16_NT_128B_32B__half (382 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_96_16_NN_32B_128B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_96_16_NN_32B_128B__half (469 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_96_16_TT_128B_64B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_96_16_TT_128B_64B__bfloat (457 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_96_16_NT_NoSwizzle_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_96_16_NT_NoSwizzle_128B__bfloat (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_104_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_104_16_TT_NoSwizzle_NoSwizzle__half (352 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_104_16_TT_64B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_104_16_TT_64B_NoSwizzle__half (376 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_104_16_TN_NoSwizzle_64B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_104_16_TN_NoSwizzle_64B__half (370 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_104_16_NT_64B_32B__half
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_104_16_NT_64B_32B__half (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_104_16_TT_NoSwizzle_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_104_16_TT_NoSwizzle_128B__bfloat (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_104_16_NT_128B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_104_16_NT_128B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_104_16_NN_32B_128B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_104_16_NN_32B_128B__bfloat (496 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_112_16_TT_64B_128B__half
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_112_16_TT_64B_128B__half (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_112_16_NT_NoSwizzle_32B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_112_16_NT_NoSwizzle_32B__half (393 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_112_16_TT_64B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_112_16_TT_64B_NoSwizzle__bfloat (405 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_112_16_TN_128B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_112_16_TN_128B_NoSwizzle__bfloat (456 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_112_16_NN_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_112_16_NN_64B_128B__bfloat (563 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_120_16_TT_64B_32B__half
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_120_16_TT_64B_32B__half (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_120_16_NN_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_120_16_NN_NoSwizzle_NoSwizzle__half (404 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_120_16_TT_64B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_120_16_TT_64B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_120_16_TN_32B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_120_16_TN_32B_NoSwizzle__bfloat (418 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_120_16_NN_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_120_16_NN_NoSwizzle_128B__bfloat (562 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_128_16_TN_NoSwizzle_128B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_128_16_TN_NoSwizzle_128B__half (598 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_128_16_TT_NoSwizzle_64B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_128_16_TT_NoSwizzle_64B__bfloat (457 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_128_16_TN_NoSwizzle_32B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_128_16_TN_NoSwizzle_32B__bfloat (430 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_128_16_NT_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_128_16_NT_64B_128B__bfloat (496 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_136_16_TT_NoSwizzle_64B__half
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_136_16_TT_NoSwizzle_64B__half (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_136_16_TT_32B_32B__half
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_136_16_TT_32B_32B__half (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_136_16_NT_64B_32B__half
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_136_16_NT_64B_32B__half (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_136_16_NN_64B_128B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_136_16_NN_64B_128B__half (674 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_136_16_NT_NoSwizzle_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_136_16_NT_NoSwizzle_128B__bfloat (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_136_16_NN_32B_32B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_136_16_NN_32B_32B__bfloat (481 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_144_16_TN_NoSwizzle_128B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_144_16_TN_NoSwizzle_128B__half (672 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_144_16_NT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_144_16_NT_NoSwizzle_NoSwizzle__half (463 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_144_16_NT_32B_32B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_144_16_NT_32B_32B__half (507 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_144_16_TT_128B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_144_16_TT_128B_NoSwizzle__bfloat (556 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_144_16_NT_32B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_144_16_NT_32B_NoSwizzle__bfloat (489 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_144_16_NN_32B_128B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_144_16_NN_32B_128B__bfloat (674 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_152_16_TT_64B_32B__half
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_152_16_TT_64B_32B__half (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_152_16_TN_64B_32B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_152_16_TN_64B_32B__half (520 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_152_16_NT_64B_128B__half
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_152_16_NT_64B_128B__half (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_152_16_TT_64B_32B__bfloat
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_152_16_TT_64B_32B__bfloat (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_152_16_TN_NoSwizzle_32B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_152_16_TN_NoSwizzle_32B__bfloat (499 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_152_16_NT_64B_32B__bfloat
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_152_16_NT_64B_32B__bfloat (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_152_16_NN_32B_128B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_152_16_NN_32B_128B__bfloat (710 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_160_16_TT_64B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_160_16_TT_64B_NoSwizzle__half (538 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_160_16_TN_32B_64B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_160_16_TN_32B_64B__half (573 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_160_16_NT_128B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_160_16_NT_128B_NoSwizzle__half (538 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_160_16_TT_128B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_160_16_TT_128B_NoSwizzle__bfloat (606 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_160_16_NT_64B_64B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_160_16_NT_64B_64B__bfloat (604 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_160_16_NN_64B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_160_16_NN_64B_NoSwizzle__bfloat (544 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_168_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_168_16_TT_NoSwizzle_NoSwizzle__half (535 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_168_16_TT_128B_32B__half
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_168_16_TT_128B_32B__half (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_168_16_TN_32B_128B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_168_16_TN_32B_128B__half (774 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_168_16_NN_NoSwizzle_32B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_168_16_NN_NoSwizzle_32B__half (552 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_168_16_NN_32B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_168_16_NN_32B_NoSwizzle__half (561 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_168_16_TN_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_168_16_TN_NoSwizzle_NoSwizzle__bfloat (532 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_168_16_NT_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_168_16_NT_NoSwizzle_NoSwizzle__bfloat (543 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_176_16_TT_128B_128B__half
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_176_16_TT_128B_128B__half (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_176_16_NT_32B_32B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_176_16_NT_32B_32B__half (572 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_176_16_TT_NoSwizzle_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_176_16_TT_NoSwizzle_64B__bfloat (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_176_16_TT_32B_32B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_176_16_TT_32B_32B__bfloat (566 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_176_16_NT_64B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_176_16_NT_64B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_176_16_NN_32B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_176_16_NN_32B_NoSwizzle__bfloat (588 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_184_16_TT_128B_64B__half
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_184_16_TT_128B_64B__half (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_184_16_TN_128B_128B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_184_16_TN_128B_128B__half (900 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_184_16_NN_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_184_16_NN_NoSwizzle_NoSwizzle__half (597 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_184_16_TN_128B_128B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_184_16_TN_128B_128B__bfloat (902 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_184_16_TN_32B_64B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_184_16_TN_32B_64B__bfloat (646 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_192_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_192_16_TT_NoSwizzle_NoSwizzle__half (620 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_192_16_NN_NoSwizzle_128B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_192_16_NN_NoSwizzle_128B__half (875 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_192_16_TT_64B_64B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_192_16_TT_64B_64B__bfloat (689 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_192_16_NN_64B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_192_16_NN_64B_NoSwizzle__bfloat (644 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_200_16_TN_32B_32B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_200_16_TN_32B_32B__half (638 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_200_16_NT_64B_32B__half
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_200_16_NT_64B_32B__half (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_200_16_TT_128B_32B__bfloat
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_200_16_TT_128B_32B__bfloat (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_200_16_NT_64B_32B__bfloat
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_200_16_NT_64B_32B__bfloat (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_208_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_208_16_TT_NoSwizzle_NoSwizzle__half (687 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_208_16_TN_32B_64B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_208_16_TN_32B_64B__half (744 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_208_16_TT_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_208_16_TT_NoSwizzle_NoSwizzle__bfloat (688 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_208_16_TT_64B_32B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_208_16_TT_64B_32B__bfloat (672 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_208_16_TN_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_208_16_TN_64B_128B__bfloat (957 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_208_16_NT_32B_32B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_208_16_NT_32B_32B__bfloat (667 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_216_16_TT_128B_64B__half
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_216_16_TT_128B_64B__half (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_216_16_TN_64B_64B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_216_16_TN_64B_64B__half (767 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_216_16_NN_64B_128B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_216_16_NN_64B_128B__half (957 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_216_16_TN_32B_32B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_216_16_TN_32B_32B__bfloat (689 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_224_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_224_16_TT_NoSwizzle_NoSwizzle__half (763 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_224_16_NT_NoSwizzle_32B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_224_16_NT_NoSwizzle_32B__half (705 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_224_16_TT_128B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_224_16_TT_128B_NoSwizzle__bfloat (855 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_224_16_TN_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_224_16_TN_NoSwizzle_128B__bfloat (965 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_224_16_TN_32B_128B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_224_16_TN_32B_128B__bfloat (961 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_224_16_NN_64B_128B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_224_16_NN_64B_128B__bfloat (1004 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_232_16_TT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_232_16_TT_NoSwizzle_NoSwizzle__half (788 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_232_16_TN_64B_32B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_232_16_TN_64B_32B__half (737 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_232_16_NT_NoSwizzle_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_232_16_NT_NoSwizzle_NoSwizzle__half (791 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_232_16_NN_NoSwizzle_32B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_232_16_NN_NoSwizzle_32B__half (725 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_232_16_TT_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_232_16_TT_NoSwizzle_NoSwizzle__bfloat (804 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_232_16_TN_128B_128B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_232_16_TN_128B_128B__bfloat (1115 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_232_16_TN_32B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_232_16_TN_32B_NoSwizzle__bfloat (800 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_232_16_NN_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_232_16_NN_NoSwizzle_128B__bfloat (1011 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_240_16_TT_32B_128B__half
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_240_16_TT_32B_128B__half (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_240_16_NT_128B_64B__half
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_240_16_NT_128B_64B__half (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_240_16_NT_64B_32B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_240_16_NT_64B_32B__half (802 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_240_16_NT_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_240_16_NT_NoSwizzle_NoSwizzle__bfloat (832 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_240_16_NN_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_240_16_NN_NoSwizzle_NoSwizzle__bfloat (823 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_240_16_NN_128B_64B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_240_16_NN_128B_64B__bfloat (897 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_248_16_TT_128B_64B__half
/opt/pytorch/nvfuser/tests/cpp/test_mma.cpp:987: Skipped
This test stores smem inputs on the inner dimension densely, which is not compatible with this macro and swizzle mode because TensorCore instructions span multiple swizzle patterns unevenly.

[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_248_16_TT_128B_64B__half (0 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_248_16_TN_128B_NoSwizzle__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_248_16_TN_128B_NoSwizzle__half (957 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_248_16_TN_32B_128B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_248_16_TN_32B_128B__half (1086 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_248_16_NN_64B_64B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_248_16_NN_64B_64B__half (1014 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_248_16_TN_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_248_16_TN_NoSwizzle_128B__bfloat (1095 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_248_16_NT_64B_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_248_16_NT_64B_NoSwizzle__bfloat (896 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_248_16_NN_32B_64B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_248_16_NN_32B_64B__bfloat (914 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_256_16_TT_NoSwizzle_128B__half
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_256_16_TT_NoSwizzle_128B__half (925 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_256_16_TT_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_256_16_TT_NoSwizzle_128B__bfloat (916 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_256_16_TN_NoSwizzle_128B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_256_16_TN_NoSwizzle_128B__bfloat (1124 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_256_16_NT_NoSwizzle_NoSwizzle__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_256_16_NT_NoSwizzle_NoSwizzle__bfloat (797 ms)
[ RUN      ] MmaTest/HopperSS.MultipleTile/Hopper_64_256_16_NN_128B_32B__bfloat
[       OK ] MmaTest/HopperSS.MultipleTile/Hopper_64_256_16_NN_128B_32B__bfloat (825 ms)
[----------] 534 tests from MmaTest/HopperSS (134601 ms total)

[----------] 170 tests from MmaTest/Blackwell1CTAM128SS
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_16_16_TT_NoSwizzle_128B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_16_16_TT_NoSwizzle_128B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_16_16_TT_128B_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_16_16_TT_128B_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_16_16_TN_NoSwizzle_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_16_16_TN_NoSwizzle_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_16_16_NN_NoSwizzle_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_16_16_NN_NoSwizzle_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_16_16_TT_NoSwizzle_NoSwizzle__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_16_16_TT_NoSwizzle_NoSwizzle__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_16_16_TN_64B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_16_16_TN_64B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_16_16_NT_32B_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_16_16_NT_32B_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_32_16_TT_NoSwizzle_32B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_32_16_TT_NoSwizzle_32B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_32_16_TT_64B_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_32_16_TT_64B_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_32_16_NN_128B_32B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_32_16_NN_128B_32B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_48_16_TT_64B_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_48_16_TT_64B_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_48_16_NN_128B_32B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_48_16_NN_128B_32B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_48_16_TT_32B_NoSwizzle__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_48_16_TT_32B_NoSwizzle__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_48_16_TN_NoSwizzle_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_48_16_TN_NoSwizzle_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_48_16_TN_32B_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_48_16_TN_32B_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_48_16_NT_32B_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_48_16_NT_32B_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_48_16_NN_64B_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_48_16_NN_64B_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_64_16_TT_128B_32B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_64_16_TT_128B_32B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_64_16_TN_NoSwizzle_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_64_16_TN_NoSwizzle_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_64_16_NT_NoSwizzle_64B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_64_16_NT_NoSwizzle_64B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_64_16_TT_128B_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_64_16_TT_128B_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_64_16_NT_128B_NoSwizzle__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_64_16_NT_128B_NoSwizzle__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_64_16_NN_128B_NoSwizzle__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_64_16_NN_128B_NoSwizzle__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_80_16_TT_128B_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_80_16_TT_128B_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_80_16_TN_128B_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_80_16_TN_128B_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_80_16_NN_NoSwizzle_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_80_16_NN_NoSwizzle_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_80_16_TN_NoSwizzle_32B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_80_16_TN_NoSwizzle_32B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_80_16_NT_64B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_80_16_NT_64B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_96_16_TT_NoSwizzle_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_96_16_TT_NoSwizzle_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_96_16_NT_NoSwizzle_128B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_96_16_NT_NoSwizzle_128B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_96_16_NN_64B_64B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_96_16_NN_64B_64B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_96_16_TT_64B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_96_16_TT_64B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_96_16_TN_128B_32B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_96_16_TN_128B_32B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_96_16_NT_32B_NoSwizzle__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_96_16_NT_32B_NoSwizzle__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_112_16_TT_NoSwizzle_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_112_16_TT_NoSwizzle_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_112_16_NN_128B_128B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_112_16_NN_128B_128B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_112_16_TT_64B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_112_16_TT_64B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_128_16_TT_128B_64B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_128_16_TT_128B_64B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_128_16_NT_128B_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_128_16_NT_128B_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_128_16_TT_32B_NoSwizzle__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_128_16_TT_32B_NoSwizzle__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_128_16_NN_32B_32B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_128_16_NN_32B_32B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_144_16_TN_128B_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_144_16_TN_128B_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_144_16_NT_32B_128B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_144_16_NT_32B_128B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_144_16_NN_64B_32B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_144_16_NN_64B_32B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_144_16_TT_32B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_144_16_TT_32B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_144_16_NT_128B_32B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_144_16_NT_128B_32B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_144_16_NN_64B_32B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_144_16_NN_64B_32B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_160_16_TT_64B_64B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_160_16_TT_64B_64B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_160_16_TT_64B_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_160_16_TT_64B_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_160_16_TN_64B_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_160_16_TN_64B_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_160_16_NN_NoSwizzle_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_160_16_NN_NoSwizzle_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_160_16_NN_32B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_160_16_NN_32B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_176_16_TT_NoSwizzle_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_176_16_TT_NoSwizzle_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_176_16_TN_64B_32B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_176_16_TN_64B_32B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_176_16_NT_64B_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_176_16_NT_64B_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_176_16_TT_128B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_176_16_TT_128B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_176_16_NT_128B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_176_16_NT_128B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_176_16_NN_128B_NoSwizzle__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_176_16_NN_128B_NoSwizzle__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_192_16_TT_NoSwizzle_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_192_16_TT_NoSwizzle_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_192_16_TN_NoSwizzle_128B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_192_16_TN_NoSwizzle_128B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_192_16_NT_128B_32B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_192_16_NT_128B_32B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_192_16_NN_32B_128B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_192_16_NN_32B_128B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_192_16_TT_128B_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_192_16_TT_128B_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_192_16_NT_NoSwizzle_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_192_16_NT_NoSwizzle_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_208_16_TT_NoSwizzle_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_208_16_TT_NoSwizzle_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_208_16_TT_64B_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_208_16_TT_64B_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_208_16_TN_NoSwizzle_64B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_208_16_TN_NoSwizzle_64B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_208_16_NT_64B_32B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_208_16_NT_64B_32B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_208_16_TT_NoSwizzle_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_208_16_TT_NoSwizzle_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_208_16_NT_128B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_208_16_NT_128B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_208_16_NN_32B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_208_16_NN_32B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_224_16_TT_64B_128B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_224_16_TT_64B_128B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_224_16_NT_NoSwizzle_32B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_224_16_NT_NoSwizzle_32B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_224_16_TT_64B_NoSwizzle__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_224_16_TT_64B_NoSwizzle__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_224_16_TN_128B_NoSwizzle__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_224_16_TN_128B_NoSwizzle__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_224_16_NN_64B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_224_16_NN_64B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_240_16_TT_64B_32B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_240_16_TT_64B_32B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_240_16_NN_NoSwizzle_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_240_16_NN_NoSwizzle_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_240_16_TT_64B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_240_16_TT_64B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_240_16_TN_32B_NoSwizzle__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_240_16_TN_32B_NoSwizzle__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_240_16_NN_NoSwizzle_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_240_16_NN_NoSwizzle_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_256_16_TN_NoSwizzle_128B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_256_16_TN_NoSwizzle_128B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_256_16_TT_NoSwizzle_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_256_16_TT_NoSwizzle_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_256_16_TN_NoSwizzle_32B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_256_16_TN_NoSwizzle_32B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_256_16_NT_64B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_256_16_NT_64B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_16_16_TT_NoSwizzle_128B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_16_16_TT_NoSwizzle_128B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_16_16_TT_128B_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_16_16_TT_128B_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_16_16_TN_NoSwizzle_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_16_16_TN_NoSwizzle_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_16_16_NN_NoSwizzle_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_16_16_NN_NoSwizzle_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_16_16_TT_NoSwizzle_NoSwizzle__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_16_16_TT_NoSwizzle_NoSwizzle__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_16_16_TN_64B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_16_16_TN_64B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_16_16_NT_32B_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_16_16_NT_32B_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_32_16_TT_NoSwizzle_32B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_32_16_TT_NoSwizzle_32B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_32_16_TT_64B_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_32_16_TT_64B_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_32_16_NN_128B_32B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_32_16_NN_128B_32B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_48_16_TT_64B_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_48_16_TT_64B_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_48_16_NN_128B_32B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_48_16_NN_128B_32B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_48_16_TT_32B_NoSwizzle__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_48_16_TT_32B_NoSwizzle__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_48_16_TN_NoSwizzle_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_48_16_TN_NoSwizzle_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_48_16_TN_32B_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_48_16_TN_32B_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_48_16_NT_32B_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_48_16_NT_32B_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_48_16_NN_64B_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_48_16_NN_64B_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_64_16_TT_128B_32B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_64_16_TT_128B_32B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_64_16_TN_NoSwizzle_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_64_16_TN_NoSwizzle_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_64_16_NT_NoSwizzle_64B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_64_16_NT_NoSwizzle_64B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_64_16_TT_128B_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_64_16_TT_128B_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_64_16_NT_128B_NoSwizzle__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_64_16_NT_128B_NoSwizzle__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_64_16_NN_128B_NoSwizzle__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_64_16_NN_128B_NoSwizzle__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_80_16_TT_128B_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_80_16_TT_128B_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_80_16_TN_128B_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_80_16_TN_128B_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_80_16_NN_NoSwizzle_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_80_16_NN_NoSwizzle_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_80_16_TN_NoSwizzle_32B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_80_16_TN_NoSwizzle_32B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_80_16_NT_64B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_80_16_NT_64B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_96_16_TT_NoSwizzle_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_96_16_TT_NoSwizzle_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_96_16_NT_NoSwizzle_128B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_96_16_NT_NoSwizzle_128B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_96_16_NN_64B_64B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_96_16_NN_64B_64B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_96_16_TT_64B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_96_16_TT_64B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_96_16_TN_128B_32B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_96_16_TN_128B_32B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_96_16_NT_32B_NoSwizzle__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_96_16_NT_32B_NoSwizzle__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_112_16_TT_NoSwizzle_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_112_16_TT_NoSwizzle_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_112_16_NN_128B_128B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_112_16_NN_128B_128B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_112_16_TT_64B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_112_16_TT_64B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_128_16_TT_128B_64B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_128_16_TT_128B_64B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_128_16_NT_128B_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_128_16_NT_128B_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_128_16_TT_32B_NoSwizzle__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_128_16_TT_32B_NoSwizzle__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_128_16_NN_32B_32B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_128_16_NN_32B_32B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_144_16_TN_128B_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_144_16_TN_128B_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_144_16_NT_32B_128B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_144_16_NT_32B_128B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_144_16_NN_64B_32B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_144_16_NN_64B_32B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_144_16_TT_32B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_144_16_TT_32B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_144_16_NT_128B_32B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_144_16_NT_128B_32B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_144_16_NN_64B_32B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_144_16_NN_64B_32B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_160_16_TT_64B_64B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_160_16_TT_64B_64B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_160_16_TT_64B_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_160_16_TT_64B_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_160_16_TN_64B_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_160_16_TN_64B_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_160_16_NN_NoSwizzle_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_160_16_NN_NoSwizzle_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_160_16_NN_32B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_160_16_NN_32B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_176_16_TT_NoSwizzle_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_176_16_TT_NoSwizzle_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_176_16_TN_64B_32B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_176_16_TN_64B_32B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_176_16_NT_64B_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_176_16_NT_64B_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_176_16_TT_128B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_176_16_TT_128B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_176_16_NT_128B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_176_16_NT_128B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_176_16_NN_128B_NoSwizzle__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_176_16_NN_128B_NoSwizzle__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_192_16_TT_NoSwizzle_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_192_16_TT_NoSwizzle_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_192_16_TN_NoSwizzle_128B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_192_16_TN_NoSwizzle_128B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_192_16_NT_128B_32B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_192_16_NT_128B_32B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_192_16_NN_32B_128B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_192_16_NN_32B_128B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_192_16_TT_128B_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_192_16_TT_128B_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_192_16_NT_NoSwizzle_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_192_16_NT_NoSwizzle_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_208_16_TT_NoSwizzle_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_208_16_TT_NoSwizzle_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_208_16_TT_64B_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_208_16_TT_64B_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_208_16_TN_NoSwizzle_64B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_208_16_TN_NoSwizzle_64B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_208_16_NT_64B_32B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_208_16_NT_64B_32B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_208_16_TT_NoSwizzle_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_208_16_TT_NoSwizzle_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_208_16_NT_128B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_208_16_NT_128B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_208_16_NN_32B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_208_16_NN_32B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_224_16_TT_64B_128B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_224_16_TT_64B_128B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_224_16_NT_NoSwizzle_32B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_224_16_NT_NoSwizzle_32B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_224_16_TT_64B_NoSwizzle__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_224_16_TT_64B_NoSwizzle__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_224_16_TN_128B_NoSwizzle__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_224_16_TN_128B_NoSwizzle__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_224_16_NN_64B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_224_16_NN_64B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_240_16_TT_64B_32B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_240_16_TT_64B_32B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_240_16_NN_NoSwizzle_NoSwizzle__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_240_16_NN_NoSwizzle_NoSwizzle__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_240_16_TT_64B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_240_16_TT_64B_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_240_16_TN_32B_NoSwizzle__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_240_16_TN_32B_NoSwizzle__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_240_16_NN_NoSwizzle_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_240_16_NN_NoSwizzle_128B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_256_16_TN_NoSwizzle_128B__half
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_256_16_TN_NoSwizzle_128B__half (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_256_16_TT_NoSwizzle_64B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_256_16_TT_NoSwizzle_64B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_256_16_TN_NoSwizzle_32B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_256_16_TN_NoSwizzle_32B__bfloat (0 ms)
[ RUN      ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_256_16_NT_64B_128B__bfloat
/opt/pytorch/nvfuser/tests/cpp/utils.h:474: Skipped
skipping tests on non-Blackwell GPUs (requires sm_100/sm_104, not sm_110+)

[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_256_16_NT_64B_128B__bfloat (0 ms)
[----------] 170 tests from MmaTest/Blackwell1CTAM128SS (1 ms total)

[----------] Global test environment tear-down
[==========] 958 tests from 30 test suites ran. (163255 ms total)
[  PASSED  ] 633 tests.
[  SKIPPED ] 325 tests, listed below:
[  SKIPPED ] CombineMulSumAsMmaTest.MulSumToMatmul_MultipleBroadcasts
[  SKIPPED ] MatmulTest.AmpereMatmulSmemEpiloguePromotionRequiredA100
[  SKIPPED ] MatmulTest.MultipleMDimsBatch
[  SKIPPED ] BlackwellMatmulTest.EpilogueSiluPersistentBroadcastInputs
[  SKIPPED ] MatmulSchedulerTest.BasicMatmulStrictCheckTT
[  SKIPPED ] MatmulSchedulerTest.EpilogueAlpha
[  SKIPPED ] MatmulSchedulerTest.EpilogueAlphaBeta
[  SKIPPED ] MatmulSchedulerTest.SegmentMatmulOpPrologue
[  SKIPPED ] MatmulSchedulerPluginTest.BasicMatmul
[  SKIPPED ] CutlassExecutorTest.Nvfp4BlockScaledGemmReLU
[  SKIPPED ] CombineMulSumAsMmaTestWithLayout.AmpereMulSumToMatmul_Schedule/TT
[  SKIPPED ] CombineMulSumAsMmaTestWithLayout.AmpereMulSumToMatmul_Schedule/NN
[  SKIPPED ] CombineMulSumAsMmaTestWithLayout.UseMatmulScheduler/NT
[  SKIPPED ] MatmulNodeTranslationTest.AutomaticSchedulerMatmulNode/3dA_2dB
[  SKIPPED ] LinearNodeTranslationTest.AutomaticSchedulerLinearNode/1dA_2dB
[  SKIPPED ] MatmulTestWithLayout.AmpereMatmul/TN
[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulBroadcastBatch/TT
[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulBroadcastBatch/NN
[  SKIPPED ] MatmulTestWithLayout.AmperePrologueFusionBroadcast/NT
[  SKIPPED ] MatmulTestWithLayout.AmpereProloguePointwise/TN
[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulBFloat16/TT
[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulBFloat16/NN
[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulPipelineGmem/NT
[  SKIPPED ] MatmulTestWithLayout.AmpereSwizzle/TN
[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulRegCircularBuffer/TT
[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulRegCircularBuffer/NN
[  SKIPPED ] MatmulTestWithLayout.TuringMatmul/NT
[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulLargeLoad/TN
[  SKIPPED ] MatmulTestWithLayout.TuringMatmulLargeLoad/TT
[  SKIPPED ] MatmulTestWithLayout.TuringMatmulLargeLoad/NN
[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulTileCheck4warp/NT
[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulTileCheck8warp/TN
[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulTileCheck6warp/TT
[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulTileCheck6warp/NN
[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulLargeLoadLargeK/NT
[  SKIPPED ] MatmulTestWithLayout.AmpereSplitKLikeStridedBatchedMatmul/TN
[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulSmemEpilogue/TT
[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulSmemEpilogue/NN
[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulSmemEpilogueCast/NT
[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulSmemEpilogueRelu/TN
[  SKIPPED ] MatmulTestWithLayout.FusionAmpereMatmulSplitK_CUDA/TT
[  SKIPPED ] MatmulTestWithLayout.FusionAmpereMatmulSplitK_CUDA/NN
[  SKIPPED ] MatmulTestWithLayout.FusionAmpereMatmulSplitKBias_CUDA/NT
[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulBatchSplitK/TN
[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulBatchSplitKBias/TT
[  SKIPPED ] MatmulTestWithLayout.AmpereMatmulBatchSplitKBias/NN
[  SKIPPED ] MatmulTestWithLayout.MisalignedVectorization/NT
[  SKIPPED ] MLPBenchmarkTest.FwdEpilogueFusion_BroadcastInputs/dataparallel_non_warpspec
[  SKIPPED ] MLPBenchmarkTest.FwdEpilogueFusion_BroadcastInputs/persistent_warpspec
[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueBias/HSS
[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueBias/TST
[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueRelu/TSS
[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueBiasRelu/HSH
[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueReluAux/HSS
[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueReluAux/TST
[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueBiasReluAux/TSS
[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueGelu/HSH
[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueGeluAux/HSS
[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueGeluAux/TST
[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueBiasGelu/TSS
[  SKIPPED ] MatmulSchedulerTest/PrecisionParametrizedTest.EpilogueBiasGeluAux/HSH
[  SKIPPED ] MatmulSchedulerTestWithLayout.BasicMatmulRelaxedCheck/TT
[  SKIPPED ] MatmulSchedulerTestWithLayout.BasicMatmulRelaxedCheck/NN
[  SKIPPED ] MatmulSchedulerTestWithLayout.StridedBatch/NT
[  SKIPPED ] MatmulSchedulerTestWithLayout.StridedBatchEpilogueAlphaBeta/TN
[  SKIPPED ] MatmulSchedulerTestWithLayout.StridedBatchEpilogueAlphaSingleBeta/TT
[  SKIPPED ] MatmulSchedulerTestWithLayout.StridedBatchEpilogueAlphaSingleBeta/NN
[  SKIPPED ] MatmulSchedulerTestWithLayout.StridedBatchEpilogueBias/NT
[  SKIPPED ] MatmulSchedulerTestWithLayout.StridedBatchEpilogueSingleBias/TN
[  SKIPPED ] MatmulSchedulerTestWithLayout.MisalignedVectorization/TT
[  SKIPPED ] MatmulSchedulerTestWithLayout.MisalignedVectorization/NN
[  SKIPPED ] MatmulSchedulerTestWithLayout.StridedInputs/NT
[  SKIPPED ] MatmulFusionTest.Llama2FFN/fuse_single
[  SKIPPED ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmulNoTranspose/0
[  SKIPPED ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmulNoTranspose/3
[  SKIPPED ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmulWithPrologueSet/2
[  SKIPPED ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmulWithPrologueSetCastSin/1
[  SKIPPED ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmulWithPrologueSetCastSinNoTranspose/0
[  SKIPPED ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmulWithPrologueSetCastSinNoTranspose/3
[  SKIPPED ] MatmulSchedulerTest/AllocationDomainTest.BasicMatmulWithPrologueSetCastSinSetNoTranspose/2
[  SKIPPED ] MatmulSchedulerTest/AllocationDomainTest.MatmulWithPrologueSetCastSinTranspose/1
[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MN_512_256_128_MmaMacro_m128_n128_k16_splitk_2
[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MK_512_256_128_MmaMacro_m128_n128_k16
[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KK_512_256_128_MmaMacro_m128_n128_k16_splitk_2
[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MN_512_256_128_MmaMacro_m128_n128_k16_tma_store
[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KN_512_256_128_MmaMacro_m128_n128_k16_tma_store_splitk_2
[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KK_512_256_128_MmaMacro_m128_n128_k16_tma_store
[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MK_512_256_128_MmaMacro_m128_n128_k16_splitk_2
[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KN_512_256_128_MmaMacro_m128_n128_k16
[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MN_512_256_128_MmaMacro_m128_n128_k16_tma_store_splitk_2
[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MK_512_256_128_MmaMacro_m128_n128_k16_tma_store
[  SKIPPED ] General/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KK_512_256_128_MmaMacro_m128_n128_k16_tma_store_splitk_2
[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MN_512_256_64_MmaMacro_m128_n256_k16_tma_store_128BSwizzle
[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MN_512_256_64_MmaMacro_m128_n32_k16_tma_store_64BSwizzle
[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/MK_512_256_64_MmaMacro_m128_n64_k16_tma_store_128BSwizzle
[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KN_512_256_64_MmaMacro_m128_n128_k16_tma_store_128BSwizzle
[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KN_512_256_64_MmaMacro_m128_n16_k16_tma_store_32BSwizzle
[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KK_512_256_64_MmaMacro_m128_n256_k16_tma_store_128BSwizzle
[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySum/KK_512_256_64_MmaMacro_m128_n32_k16_tma_store_64BSwizzle
[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MN_512_256_64_MmaMacro_m128_n64_k16_tma_store_128BSwizzle
[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MK_512_256_64_MmaMacro_m128_n128_k16_tma_store_128BSwizzle
[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/MK_512_256_64_MmaMacro_m128_n16_k16_tma_store_32BSwizzle
[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KN_512_256_64_MmaMacro_m128_n256_k16_tma_store_128BSwizzle
[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KN_512_256_64_MmaMacro_m128_n32_k16_tma_store_64BSwizzle
[  SKIPPED ] Swizzle/HopperPlusMatmulSchedulerTest.FusedMultiplySumBiasNeg/KK_512_256_64_MmaMacro_m128_n64_k16_tma_store_128BSwizzle
[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/2
[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/5
[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/14
[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/17
[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/26
[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/29
[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/38
[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/41
[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/50
[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/53
[  SKIPPED ] MmaTest/HopperRSStmatrix.SingleTileWithTMALoadStoreStMatrix/62
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_24_16_NT_32B_64B__bfloat
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_40_16_NT_64B_128B__bfloat
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_48_16_NT_NoSwizzle_128B__half
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_48_16_TT_64B_128B__bfloat
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_56_16_TT_64B_128B__bfloat
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_72_16_NT_32B_128B__half
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_72_16_TT_32B_128B__bfloat
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_72_16_NT_128B_32B__bfloat
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_80_16_TT_64B_64B__half
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_80_16_TT_64B_64B__bfloat
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_88_16_TT_128B_128B__bfloat
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_88_16_NT_128B_128B__bfloat
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_96_16_NT_NoSwizzle_128B__bfloat
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_104_16_NT_64B_32B__half
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_104_16_TT_NoSwizzle_128B__bfloat
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_104_16_NT_128B_128B__bfloat
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_112_16_TT_64B_128B__half
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_120_16_TT_64B_32B__half
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_120_16_TT_64B_128B__bfloat
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_136_16_TT_NoSwizzle_64B__half
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_136_16_TT_32B_32B__half
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_136_16_NT_64B_32B__half
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_136_16_NT_NoSwizzle_128B__bfloat
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_152_16_TT_64B_32B__half
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_152_16_NT_64B_128B__half
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_152_16_TT_64B_32B__bfloat
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_152_16_NT_64B_32B__bfloat
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_168_16_TT_128B_32B__half
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_176_16_TT_128B_128B__half
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_176_16_TT_NoSwizzle_64B__bfloat
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_176_16_NT_64B_128B__bfloat
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_184_16_TT_128B_64B__half
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_200_16_NT_64B_32B__half
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_200_16_TT_128B_32B__bfloat
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_200_16_NT_64B_32B__bfloat
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_216_16_TT_128B_64B__half
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_240_16_TT_32B_128B__half
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_240_16_NT_128B_64B__half
[  SKIPPED ] MmaTest/HopperSS.MultipleTile/Hopper_64_248_16_TT_128B_64B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_16_16_TT_NoSwizzle_128B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_16_16_TT_128B_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_16_16_TN_NoSwizzle_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_16_16_NN_NoSwizzle_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_16_16_TT_NoSwizzle_NoSwizzle__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_16_16_TN_64B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_16_16_NT_32B_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_32_16_TT_NoSwizzle_32B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_32_16_TT_64B_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_32_16_NN_128B_32B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_48_16_TT_64B_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_48_16_NN_128B_32B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_48_16_TT_32B_NoSwizzle__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_48_16_TN_NoSwizzle_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_48_16_TN_32B_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_48_16_NT_32B_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_48_16_NN_64B_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_64_16_TT_128B_32B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_64_16_TN_NoSwizzle_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_64_16_NT_NoSwizzle_64B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_64_16_TT_128B_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_64_16_NT_128B_NoSwizzle__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_64_16_NN_128B_NoSwizzle__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_80_16_TT_128B_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_80_16_TN_128B_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_80_16_NN_NoSwizzle_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_80_16_TN_NoSwizzle_32B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_80_16_NT_64B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_96_16_TT_NoSwizzle_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_96_16_NT_NoSwizzle_128B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_96_16_NN_64B_64B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_96_16_TT_64B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_96_16_TN_128B_32B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_96_16_NT_32B_NoSwizzle__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_112_16_TT_NoSwizzle_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_112_16_NN_128B_128B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_112_16_TT_64B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_128_16_TT_128B_64B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_128_16_NT_128B_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_128_16_TT_32B_NoSwizzle__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_128_16_NN_32B_32B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_144_16_TN_128B_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_144_16_NT_32B_128B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_144_16_NN_64B_32B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_144_16_TT_32B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_144_16_NT_128B_32B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_144_16_NN_64B_32B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_160_16_TT_64B_64B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_160_16_TT_64B_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_160_16_TN_64B_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_160_16_NN_NoSwizzle_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_160_16_NN_32B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_176_16_TT_NoSwizzle_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_176_16_TN_64B_32B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_176_16_NT_64B_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_176_16_TT_128B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_176_16_NT_128B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_176_16_NN_128B_NoSwizzle__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_192_16_TT_NoSwizzle_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_192_16_TN_NoSwizzle_128B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_192_16_NT_128B_32B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_192_16_NN_32B_128B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_192_16_TT_128B_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_192_16_NT_NoSwizzle_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_208_16_TT_NoSwizzle_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_208_16_TT_64B_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_208_16_TN_NoSwizzle_64B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_208_16_NT_64B_32B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_208_16_TT_NoSwizzle_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_208_16_NT_128B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_208_16_NN_32B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_224_16_TT_64B_128B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_224_16_NT_NoSwizzle_32B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_224_16_TT_64B_NoSwizzle__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_224_16_TN_128B_NoSwizzle__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_224_16_NN_64B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_240_16_TT_64B_32B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_240_16_NN_NoSwizzle_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_240_16_TT_64B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_240_16_TN_32B_NoSwizzle__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_240_16_NN_NoSwizzle_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_256_16_TN_NoSwizzle_128B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_256_16_TT_NoSwizzle_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_256_16_TN_NoSwizzle_32B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.SingleTile/Blackwell1CTA_128_256_16_NT_64B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_16_16_TT_NoSwizzle_128B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_16_16_TT_128B_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_16_16_TN_NoSwizzle_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_16_16_NN_NoSwizzle_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_16_16_TT_NoSwizzle_NoSwizzle__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_16_16_TN_64B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_16_16_NT_32B_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_32_16_TT_NoSwizzle_32B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_32_16_TT_64B_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_32_16_NN_128B_32B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_48_16_TT_64B_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_48_16_NN_128B_32B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_48_16_TT_32B_NoSwizzle__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_48_16_TN_NoSwizzle_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_48_16_TN_32B_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_48_16_NT_32B_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_48_16_NN_64B_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_64_16_TT_128B_32B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_64_16_TN_NoSwizzle_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_64_16_NT_NoSwizzle_64B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_64_16_TT_128B_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_64_16_NT_128B_NoSwizzle__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_64_16_NN_128B_NoSwizzle__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_80_16_TT_128B_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_80_16_TN_128B_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_80_16_NN_NoSwizzle_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_80_16_TN_NoSwizzle_32B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_80_16_NT_64B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_96_16_TT_NoSwizzle_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_96_16_NT_NoSwizzle_128B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_96_16_NN_64B_64B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_96_16_TT_64B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_96_16_TN_128B_32B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_96_16_NT_32B_NoSwizzle__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_112_16_TT_NoSwizzle_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_112_16_NN_128B_128B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_112_16_TT_64B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_128_16_TT_128B_64B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_128_16_NT_128B_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_128_16_TT_32B_NoSwizzle__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_128_16_NN_32B_32B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_144_16_TN_128B_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_144_16_NT_32B_128B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_144_16_NN_64B_32B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_144_16_TT_32B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_144_16_NT_128B_32B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_144_16_NN_64B_32B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_160_16_TT_64B_64B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_160_16_TT_64B_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_160_16_TN_64B_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_160_16_NN_NoSwizzle_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_160_16_NN_32B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_176_16_TT_NoSwizzle_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_176_16_TN_64B_32B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_176_16_NT_64B_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_176_16_TT_128B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_176_16_NT_128B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_176_16_NN_128B_NoSwizzle__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_192_16_TT_NoSwizzle_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_192_16_TN_NoSwizzle_128B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_192_16_NT_128B_32B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_192_16_NN_32B_128B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_192_16_TT_128B_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_192_16_NT_NoSwizzle_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_208_16_TT_NoSwizzle_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_208_16_TT_64B_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_208_16_TN_NoSwizzle_64B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_208_16_NT_64B_32B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_208_16_TT_NoSwizzle_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_208_16_NT_128B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_208_16_NN_32B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_224_16_TT_64B_128B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_224_16_NT_NoSwizzle_32B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_224_16_TT_64B_NoSwizzle__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_224_16_TN_128B_NoSwizzle__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_224_16_NN_64B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_240_16_TT_64B_32B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_240_16_NN_NoSwizzle_NoSwizzle__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_240_16_TT_64B_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_240_16_TN_32B_NoSwizzle__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_240_16_NN_NoSwizzle_128B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_256_16_TN_NoSwizzle_128B__half
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_256_16_TT_NoSwizzle_64B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_256_16_TN_NoSwizzle_32B__bfloat
[  SKIPPED ] MmaTest/Blackwell1CTAM128SS.MultipleTile/Blackwell1CTA_128_256_16_NT_64B_128B__bfloat

  YOU HAVE 3 DISABLED TESTS

+ rv=0
+ for f in __tmp_*
+ [[ __tmp_* == __tmp_c* ]]
+ fs='__tmp_*'
++ echo '__tmp_*'
++ sed s/__tmp_/__tmp_c10_/g
+ ft='__tmp_c10_*'
+ mv '__tmp_*' '__tmp_c10_*'
mv: cannot stat '__tmp_*': No such file or directory
+ counter=11
+ for f in $test_files
+ [[ /opt/pytorch/nvfuser/bin/test_reshape == *matmul* ]]
+ [[ /opt/pytorch/nvfuser/bin/test_reshape == *multidevice* ]]
+ executable=/opt/pytorch/nvfuser/bin/test_reshape
++ basename /opt/pytorch/nvfuser/bin/test_reshape
+ report_name=TEST-report-test_reshape.xml
+ sed_flag=s/__tmp_/__tmp_c11_/g
+ /opt/pytorch/nvfuser/bin/test_reshape --gtest_output=xml:TEST-report-test_reshape.xml
+ sed -u s/__tmp_/__tmp_c11_/g
Running main() from /opt/pytorch/nvfuser/third_party/googletest/googletest/src/gtest_main.cc
Note: This is test shard 2 of 3.
[==========] Running 60 tests from 4 test suites.
[----------] Global test environment set-up.
[----------] 23 tests from ReshapeTest
[ RUN      ] ReshapeTest.ViewDtypeFailMismatchSize
[       OK ] ReshapeTest.ViewDtypeFailMismatchSize (109 ms)
[ RUN      ] ReshapeTest.ReshapeOutput
[       OK ] ReshapeTest.ReshapeOutput (262 ms)
[ RUN      ] ReshapeTest.ReshapePersistentShmoo
[       OK ] ReshapeTest.ReshapePersistentShmoo (37933 ms)
[ RUN      ] ReshapeTest.ReshapeMerge
[       OK ] ReshapeTest.ReshapeMerge (210 ms)
[ RUN      ] ReshapeTest.ReshapeBinary
[       OK ] ReshapeTest.ReshapeBinary (302 ms)
[ RUN      ] ReshapeTest.ReshapeConcreteDomain3
[       OK ] ReshapeTest.ReshapeConcreteDomain3 (346 ms)
[ RUN      ] ReshapeTest.FlattenAfterUnsqueezeOutput
[       OK ] ReshapeTest.FlattenAfterUnsqueezeOutput (84 ms)
[ RUN      ] ReshapeTest.ExpandView1
[       OK ] ReshapeTest.ExpandView1 (94 ms)
[ RUN      ] ReshapeTest.ReshapeIdGraph
[       OK ] ReshapeTest.ReshapeIdGraph (1 ms)
[ RUN      ] ReshapeTest.IllegalReductionFlatten
[       OK ] ReshapeTest.IllegalReductionFlatten (0 ms)
[ RUN      ] ReshapeTest.SumViewSchedule
[       OK ] ReshapeTest.SumViewSchedule (1686 ms)
[ RUN      ] ReshapeTest.ReshapeMagicSchedule3
[       OK ] ReshapeTest.ReshapeMagicSchedule3 (148 ms)
[ RUN      ] ReshapeTest.ReshapeMagicSchedule6
[       OK ] ReshapeTest.ReshapeMagicSchedule6 (127 ms)
[ RUN      ] ReshapeTest.ReshapeMagicSchedule9
[       OK ] ReshapeTest.ReshapeMagicSchedule9 (343 ms)
[ RUN      ] ReshapeTest.ReshapeMapping
[       OK ] ReshapeTest.ReshapeMapping (349 ms)
[ RUN      ] ReshapeTest.Issue2076_v2
[       OK ] ReshapeTest.Issue2076_v2 (140 ms)
[ RUN      ] ReshapeTest.ReshapeZeroDimInputOutput
[       OK ] ReshapeTest.ReshapeZeroDimInputOutput (69 ms)
[ RUN      ] ReshapeTest.SplitMergePointwiseSplitMerge
[       OK ] ReshapeTest.SplitMergePointwiseSplitMerge (139 ms)
[ RUN      ] ReshapeTest.MismatchingReshape
[       OK ] ReshapeTest.MismatchingReshape (97 ms)
[ RUN      ] ReshapeTest.CyclicReshape
[       OK ] ReshapeTest.CyclicReshape (93 ms)
[ RUN      ] ReshapeTest.CompatibleReshapesDifferentDisjointSetsWithMerge
[       OK ] ReshapeTest.CompatibleReshapesDifferentDisjointSetsWithMerge (158 ms)
[ RUN      ] ReshapeTest.IncompatibleReshapesSameDisjointSetsMultiSteps
[       OK ] ReshapeTest.IncompatibleReshapesSameDisjointSetsMultiSteps (166 ms)
[ RUN      ] ReshapeTest.CompatibleReshapesDifferentDisjointSetsMultiSteps
[       OK ] ReshapeTest.CompatibleReshapesDifferentDisjointSetsMultiSteps (132 ms)
[----------] 23 tests from ReshapeTest (42998 ms total)

[----------] 32 tests from ReshapeReduction
[ RUN      ] ReshapeReduction.FusionReshapeReduction/0
[       OK ] ReshapeReduction.FusionReshapeReduction/0 (699 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/3
[       OK ] ReshapeReduction.FusionReshapeReduction/3 (226 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/6
[       OK ] ReshapeReduction.FusionReshapeReduction/6 (214 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/9
[       OK ] ReshapeReduction.FusionReshapeReduction/9 (117 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/12
[       OK ] ReshapeReduction.FusionReshapeReduction/12 (202 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/15
[       OK ] ReshapeReduction.FusionReshapeReduction/15 (582 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/18
[       OK ] ReshapeReduction.FusionReshapeReduction/18 (119 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/21
[       OK ] ReshapeReduction.FusionReshapeReduction/21 (253 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/24
[       OK ] ReshapeReduction.FusionReshapeReduction/24 (199 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/27
[       OK ] ReshapeReduction.FusionReshapeReduction/27 (179 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/30
[       OK ] ReshapeReduction.FusionReshapeReduction/30 (67 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/33
[       OK ] ReshapeReduction.FusionReshapeReduction/33 (265 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/36
[       OK ] ReshapeReduction.FusionReshapeReduction/36 (211 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/39
[       OK ] ReshapeReduction.FusionReshapeReduction/39 (136 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/42
[       OK ] ReshapeReduction.FusionReshapeReduction/42 (97 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/45
[       OK ] ReshapeReduction.FusionReshapeReduction/45 (193 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/48
[       OK ] ReshapeReduction.FusionReshapeReduction/48 (270 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/51
[       OK ] ReshapeReduction.FusionReshapeReduction/51 (92 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/54
[       OK ] ReshapeReduction.FusionReshapeReduction/54 (145 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/57
[       OK ] ReshapeReduction.FusionReshapeReduction/57 (173 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/60
[       OK ] ReshapeReduction.FusionReshapeReduction/60 (125 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/63
[       OK ] ReshapeReduction.FusionReshapeReduction/63 (52 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/66
[       OK ] ReshapeReduction.FusionReshapeReduction/66 (184 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/69
[       OK ] ReshapeReduction.FusionReshapeReduction/69 (232 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/72
[       OK ] ReshapeReduction.FusionReshapeReduction/72 (193 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/75
[       OK ] ReshapeReduction.FusionReshapeReduction/75 (176 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/78
[       OK ] ReshapeReduction.FusionReshapeReduction/78 (194 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/81
[       OK ] ReshapeReduction.FusionReshapeReduction/81 (201 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/84
[       OK ] ReshapeReduction.FusionReshapeReduction/84 (186 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/87
[       OK ] ReshapeReduction.FusionReshapeReduction/87 (86 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/90
[       OK ] ReshapeReduction.FusionReshapeReduction/90 (89 ms)
[ RUN      ] ReshapeReduction.FusionReshapeReduction/93
[       OK ] ReshapeReduction.FusionReshapeReduction/93 (175 ms)
[----------] 32 tests from ReshapeReduction (6346 ms total)

[----------] 2 tests from ViewReductionTest
[ RUN      ] ViewReductionTest.ReductionReshapeInputNoMergedIds/2
[       OK ] ViewReductionTest.ReductionReshapeInputNoMergedIds/2 (210 ms)
[ RUN      ] ViewReductionTest.ReductionReshapeInputNoMergedIds/5
[       OK ] ViewReductionTest.ReductionReshapeInputNoMergedIds/5 (154 ms)
[----------] 2 tests from ViewReductionTest (364 ms total)

[----------] 3 tests from ViewNormalizationTest
[ RUN      ] ViewNormalizationTest.NormalizationReshapeInputNoMergedIds/0
[       OK ] ViewNormalizationTest.NormalizationReshapeInputNoMergedIds/0 (162 ms)
[ RUN      ] ViewNormalizationTest.NormalizationReshapeInputNoMergedIds/3
[       OK ] ViewNormalizationTest.NormalizationReshapeInputNoMergedIds/3 (173 ms)
[ RUN      ] ViewNormalizationTest.NormalizationReshapeInputNoMergedIds/6
[       OK ] ViewNormalizationTest.NormalizationReshapeInputNoMergedIds/6 (261 ms)
[----------] 3 tests from ViewNormalizationTest (597 ms total)

[----------] Global test environment tear-down
[==========] 60 tests from 4 test suites ran. (50306 ms total)
[  PASSED  ] 60 tests.
+ rv=0
+ for f in __tmp_*
+ [[ __tmp_* == __tmp_c* ]]
+ fs='__tmp_*'
++ echo '__tmp_*'
++ sed s/__tmp_/__tmp_c11_/g
+ ft='__tmp_c11_*'
+ mv '__tmp_*' '__tmp_c11_*'
mv: cannot stat '__tmp_*': No such file or directory
+ counter=12
+ for f in $test_files
+ [[ /opt/pytorch/nvfuser/bin/test_cluster == *matmul* ]]
+ [[ /opt/pytorch/nvfuser/bin/test_cluster == *multidevice* ]]
+ executable=/opt/pytorch/nvfuser/bin/test_cluster
++ basename /opt/pytorch/nvfuser/bin/test_cluster
+ report_name=TEST-report-test_cluster.xml
+ sed_flag=s/__tmp_/__tmp_c12_/g
+ /opt/pytorch/nvfuser/bin/test_cluster --gtest_output=xml:TEST-report-test_cluster.xml
+ sed -u s/__tmp_/__tmp_c12_/g
Running main() from /opt/pytorch/nvfuser/third_party/googletest/googletest/src/gtest_main.cc
Note: This is test shard 2 of 3.
[==========] Running 38 tests from 4 test suites.
[----------] Global test environment set-up.
[----------] 30 tests from ClusterReductionTest
[ RUN      ] ClusterReductionTest.SimpleFusionAllReduce/cluster_2_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionAllReduce/cluster_2_dtype___bfloat (415 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionAllReduce/cluster_3_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionAllReduce/cluster_3_dtype___bfloat (133 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionAllReduce/cluster_4_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionAllReduce/cluster_4_dtype___bfloat (124 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionAllReduce/cluster_5_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionAllReduce/cluster_5_dtype___bfloat (124 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionAllReduce/cluster_6_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionAllReduce/cluster_6_dtype___bfloat (124 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionAllReduce/cluster_7_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionAllReduce/cluster_7_dtype___bfloat (124 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionAllReduce/cluster_8_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionAllReduce/cluster_8_dtype___bfloat (123 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionAllReduce/cluster_9_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionAllReduce/cluster_9_dtype___bfloat (124 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionAllReduce/cluster_10_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionAllReduce/cluster_10_dtype___bfloat (124 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionAllReduce/cluster_11_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionAllReduce/cluster_11_dtype___bfloat (124 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionAllReduce/cluster_12_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionAllReduce/cluster_12_dtype___bfloat (125 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionAllReduce/cluster_13_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionAllReduce/cluster_13_dtype___bfloat (125 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionAllReduce/cluster_14_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionAllReduce/cluster_14_dtype___bfloat (125 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionAllReduce/cluster_15_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionAllReduce/cluster_15_dtype___bfloat (124 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionAllReduce/cluster_16_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionAllReduce/cluster_16_dtype___bfloat (124 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_2_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_2_dtype___bfloat (110 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_3_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_3_dtype___bfloat (110 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_4_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_4_dtype___bfloat (109 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_5_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_5_dtype___bfloat (111 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_6_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_6_dtype___bfloat (111 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_7_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_7_dtype___bfloat (111 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_8_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_8_dtype___bfloat (110 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_9_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_9_dtype___bfloat (110 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_10_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_10_dtype___bfloat (110 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_11_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_11_dtype___bfloat (110 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_12_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_12_dtype___bfloat (111 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_13_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_13_dtype___bfloat (111 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_14_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_14_dtype___bfloat (111 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_15_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_15_dtype___bfloat (111 ms)
[ RUN      ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_16_dtype___bfloat
[       OK ] ClusterReductionTest.SimpleFusionNotAllReduce/cluster_16_dtype___bfloat (110 ms)
[----------] 30 tests from ClusterReductionTest (3832 ms total)

[----------] 1 test from NVFuserTest
[ RUN      ] NVFuserTest.GetMaxActiveClusters
[       OK ] NVFuserTest.GetMaxActiveClusters (0 ms)
[----------] 1 test from NVFuserTest (0 ms total)

[----------] 2 tests from ClusterDeviceFuncTest
[ RUN      ] ClusterDeviceFuncTest.BasicStoreSharedRemoteDouble
[       OK ] ClusterDeviceFuncTest.BasicStoreSharedRemoteDouble (1 ms)
[ RUN      ] ClusterDeviceFuncTest.ClusterReduceFloatNotAllReduce
[       OK ] ClusterDeviceFuncTest.ClusterReduceFloatNotAllReduce (0 ms)
[----------] 2 tests from ClusterDeviceFuncTest (1 ms total)

[----------] 5 tests from ClusterReductionTestAutoScheduler
[ RUN      ] ClusterReductionTestAutoScheduler.Softmax/_hidden_size_129280_dtype_float
[       OK ] ClusterReductionTestAutoScheduler.Softmax/_hidden_size_129280_dtype_float (361 ms)
[ RUN      ] ClusterReductionTestAutoScheduler.Softmax/_hidden_size_202048_dtype___bfloat
[       OK ] ClusterReductionTestAutoScheduler.Softmax/_hidden_size_202048_dtype___bfloat (560 ms)
[ RUN      ] ClusterReductionTestAutoScheduler.Softmax/_hidden_size_256000_dtype_float
[       OK ] ClusterReductionTestAutoScheduler.Softmax/_hidden_size_256000_dtype_float (323 ms)
[ RUN      ] ClusterReductionTestAutoScheduler.Softmax/_hidden_size_152064_dtype___bfloat
[       OK ] ClusterReductionTestAutoScheduler.Softmax/_hidden_size_152064_dtype___bfloat (452 ms)
[ RUN      ] ClusterReductionTestAutoScheduler.Softmax/_hidden_size_100352_dtype_float
[       OK ] ClusterReductionTestAutoScheduler.Softmax/_hidden_size_100352_dtype_float (288 ms)
[----------] 5 tests from ClusterReductionTestAutoScheduler (1986 ms total)

[----------] Global test environment tear-down
[==========] 38 tests from 4 test suites ran. (5821 ms total)
[  PASSED  ] 38 tests.
+ rv=0
+ for f in __tmp_*
+ [[ __tmp_* == __tmp_c* ]]
+ fs='__tmp_*'
++ echo '__tmp_*'
++ sed s/__tmp_/__tmp_c12_/g
+ ft='__tmp_c12_*'
+ mv '__tmp_*' '__tmp_c12_*'
mv: cannot stat '__tmp_*': No such file or directory
+ counter=13
+ for f in $test_files
+ [[ /opt/pytorch/nvfuser/bin/test_scan == *matmul* ]]
+ [[ /opt/pytorch/nvfuser/bin/test_scan == *multidevice* ]]
+ executable=/opt/pytorch/nvfuser/bin/test_scan
++ basename /opt/pytorch/nvfuser/bin/test_scan
+ report_name=TEST-report-test_scan.xml
+ sed_flag=s/__tmp_/__tmp_c13_/g
+ /opt/pytorch/nvfuser/bin/test_scan --gtest_output=xml:TEST-report-test_scan.xml
+ sed -u s/__tmp_/__tmp_c13_/g
Running main() from /opt/pytorch/nvfuser/third_party/googletest/googletest/src/gtest_main.cc
Note: This is test shard 2 of 3.
[==========] Running 36 tests from 4 test suites.
[----------] Global test environment set-up.
[----------] 6 tests from ScanTest
[ RUN      ] ScanTest.BasicScanMax
[       OK ] ScanTest.BasicScanMax (277 ms)
[ RUN      ] ScanTest.ScanDifferentDimensions
[       OK ] ScanTest.ScanDifferentDimensions (4 ms)
[ RUN      ] ScanTest.ScanWithArithmeticOps
[       OK ] ScanTest.ScanWithArithmeticOps (1209 ms)
[ RUN      ] ScanTest.KernelExecutorScanMin
[       OK ] ScanTest.KernelExecutorScanMin (1124 ms)
[ RUN      ] ScanTest.Predication
[       OK ] ScanTest.Predication (1120 ms)
[ RUN      ] ScanTest.OuterScanWithGrouping
[       OK ] ScanTest.OuterScanWithGrouping (1128 ms)
[----------] 6 tests from ScanTest (4865 ms total)

[----------] 3 tests from ScanDeviceFuncTest
[ RUN      ] ScanDeviceFuncTest.BasicScanAdd
[       OK ] ScanDeviceFuncTest.BasicScanAdd (0 ms)
[ RUN      ] ScanDeviceFuncTest.BasicScanMul
[       OK ] ScanDeviceFuncTest.BasicScanMul (0 ms)
[ RUN      ] ScanDeviceFuncTest.EdgeCases
[       OK ] ScanDeviceFuncTest.EdgeCases (0 ms)
[----------] 3 tests from ScanDeviceFuncTest (1 ms total)

[----------] 1 test from ScanTest/ScanCodeGenTest
[ RUN      ] ScanTest/ScanCodeGenTest.ParameterizedCodeGenExecution/Int
[       OK ] ScanTest/ScanCodeGenTest.ParameterizedCodeGenExecution/Int (1119 ms)
[----------] 1 test from ScanTest/ScanCodeGenTest (1119 ms total)

[----------] 26 tests from ScanParameterizedWithBlock
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/128_1_1_0
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/128_1_1_0 (1132 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/128_2_0_1
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/128_2_0_1 (1163 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/128_3_0_0
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/128_3_0_0 (1153 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/128_3_1_1
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/128_3_1_1 (1225 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/128_8_1_0
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/128_8_1_0 (1152 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/512_1_0_1
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/512_1_0_1 (1168 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/512_2_0_0
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/512_2_0_0 (1124 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/512_2_1_1
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/512_2_1_1 (1187 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/512_3_1_0
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/512_3_1_0 (1179 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/512_8_0_1
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/512_8_0_1 (1164 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/1024_1_0_0
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/1024_1_0_0 (1155 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/1024_1_1_1
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/1024_1_1_1 (1263 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/1024_2_1_0
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/1024_2_1_0 (1169 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/1024_3_0_1
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/1024_3_0_1 (1198 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/1024_8_0_0
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/1024_8_0_0 (1124 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/1024_8_1_1
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/1024_8_1_1 (1192 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/2048_1_1_0
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/2048_1_1_0 (0 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/2048_2_0_1
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/2048_2_0_1 (1232 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/2048_3_0_0
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/2048_3_0_0 (1174 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/2048_3_1_1
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/2048_3_1_1 (1304 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/2048_8_1_0
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/2048_8_1_0 (1163 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/4096_1_0_1
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/4096_1_0_1 (0 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/4096_2_0_0
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/4096_2_0_0 (0 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/4096_2_1_1
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/4096_2_1_1 (0 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/4096_3_1_0
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/4096_3_1_0 (0 ms)
[ RUN      ] ScanParameterizedWithBlock.SharedMemoryRequirement/4096_8_0_1
[       OK ] ScanParameterizedWithBlock.SharedMemoryRequirement/4096_8_0_1 (1191 ms)
[----------] 26 tests from ScanParameterizedWithBlock (24821 ms total)

[----------] Global test environment tear-down
[==========] 36 tests from 4 test suites ran. (30808 ms total)
[  PASSED  ] 36 tests.
+ rv=0
+ for f in __tmp_*
+ [[ __tmp_* == __tmp_c* ]]
+ fs='__tmp_*'
++ echo '__tmp_*'
++ sed s/__tmp_/__tmp_c13_/g
+ ft='__tmp_c13_*'
+ mv '__tmp_*' '__tmp_c13_*'
mv: cannot stat '__tmp_*': No such file or directory
+ counter=14
+ for f in $test_files
+ [[ /opt/pytorch/nvfuser/bin/test_mps == *matmul* ]]
+ [[ /opt/pytorch/nvfuser/bin/test_mps == *multidevice* ]]
+ executable=/opt/pytorch/nvfuser/bin/test_mps
++ basename /opt/pytorch/nvfuser/bin/test_mps
+ report_name=TEST-report-test_mps.xml
+ sed_flag=s/__tmp_/__tmp_c14_/g
+ /opt/pytorch/nvfuser/bin/test_mps --gtest_output=xml:TEST-report-test_mps.xml
+ sed -u s/__tmp_/__tmp_c14_/g
Running main() from /opt/pytorch/nvfuser/third_party/googletest/googletest/src/gtest_main.cc
Note: This is test shard 2 of 3.
[==========] Running 0 tests from 0 test suites.
[==========] 0 tests from 0 test suites ran. (0 ms total)
[  PASSED  ] 0 tests.
+ rv=0
+ for f in __tmp_*
+ [[ __tmp_* == __tmp_c* ]]
+ fs='__tmp_*'
++ echo '__tmp_*'
++ sed s/__tmp_/__tmp_c14_/g
+ ft='__tmp_c14_*'
+ mv '__tmp_*' '__tmp_c14_*'
mv: cannot stat '__tmp_*': No such file or directory
+ counter=15
+ for f in $test_files
+ [[ /opt/pytorch/nvfuser/bin/test_profiler == *matmul* ]]
+ [[ /opt/pytorch/nvfuser/bin/test_profiler == *multidevice* ]]
+ executable=/opt/pytorch/nvfuser/bin/test_profiler
++ basename /opt/pytorch/nvfuser/bin/test_profiler
+ report_name=TEST-report-test_profiler.xml
+ sed_flag=s/__tmp_/__tmp_c15_/g
+ /opt/pytorch/nvfuser/bin/test_profiler --gtest_output=xml:TEST-report-test_profiler.xml
+ sed -u s/__tmp_/__tmp_c15_/g
Running main() from /opt/pytorch/nvfuser/third_party/googletest/googletest/src/gtest_main.cc
Note: This is test shard 2 of 3.
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from FusionProfilerTest
[ RUN      ] FusionProfilerTest.ProfileNocupti1Segment
[       OK ] FusionProfilerTest.ProfileNocupti1Segment (315 ms)
[----------] 1 test from FusionProfilerTest (315 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (315 ms total)
[  PASSED  ] 1 test.
+ rv=0
+ for f in __tmp_*
+ [[ __tmp_* == __tmp_c* ]]
+ fs='__tmp_*'
++ echo '__tmp_*'
++ sed s/__tmp_/__tmp_c15_/g
+ ft='__tmp_c15_*'
+ mv '__tmp_*' '__tmp_c15_*'
mv: cannot stat '__tmp_*': No such file or directory
+ counter=16
+ for f in $test_files
+ [[ /opt/pytorch/nvfuser/bin/test_multidevice == *matmul* ]]
+ [[ /opt/pytorch/nvfuser/bin/test_multidevice == *multidevice* ]]
+ executable='mpirun -np 1 /opt/pytorch/nvfuser/bin/test_multidevice'
+ '[' '' = 1 ']'
++ basename /opt/pytorch/nvfuser/bin/test_multidevice
+ report_name=TEST-report-test_multidevice.xml
+ sed_flag=s/__tmp_/__tmp_c16_/g
+ mpirun -np 1 /opt/pytorch/nvfuser/bin/test_multidevice --gtest_output=xml:TEST-report-test_multidevice.xml
+ sed -u s/__tmp_/__tmp_c16_/g
[0;33mNote: This is test shard 2 of 3.
[m[0;32m[==========] [mRunning 197 tests from 31 test suites.
[0;32m[----------] [mGlobal test environment set-up.
[0;32m[----------] [m1 test from P2PCommHostIrTest
[0;32m[ RUN      ] [mP2PCommHostIrTest.CoalescedRingPairwiseExchange
[0;32m[       OK ] [mP2PCommHostIrTest.CoalescedRingPairwiseExchange (463 ms)
[0;32m[----------] [m1 test from P2PCommHostIrTest (463 ms total)

[0;32m[----------] [m9 tests from MultiDeviceTest
[0;32m[ RUN      ] [mMultiDeviceTest.Slice
[0;32m[       OK ] [mMultiDeviceTest.Slice (11 ms)
[0;32m[ RUN      ] [mMultiDeviceTest.LayerNorm
[0;32m[       OK ] [mMultiDeviceTest.LayerNorm (218 ms)
[0;32m[ RUN      ] [mMultiDeviceTest.LoopSplit
[0;32m[       OK ] [mMultiDeviceTest.LoopSplit (2 ms)
[0;32m[ RUN      ] [mMultiDeviceTest.ShardTensor_InnerSplit
[0;32m[       OK ] [mMultiDeviceTest.ShardTensor_InnerSplit (0 ms)
[0;32m[ RUN      ] [mMultiDeviceTest.ViewWithMerge
[0;32m[       OK ] [mMultiDeviceTest.ViewWithMerge (86 ms)
[0;32m[ RUN      ] [mMultiDeviceTest.LoopShardedMergeReshapeIds
[0;32m[       OK ] [mMultiDeviceTest.LoopShardedMergeReshapeIds (6 ms)
[0;32m[ RUN      ] [mMultiDeviceTest.DecomposeRowParallelLinearWithBias
[0;32m[       OK ] [mMultiDeviceTest.DecomposeRowParallelLinearWithBias (205 ms)
[0;32m[ RUN      ] [mMultiDeviceTest.PointwiseSchedulerReordering
[0;32m[       OK ] [mMultiDeviceTest.PointwiseSchedulerReordering (89 ms)
[0;32m[ RUN      ] [mMultiDeviceTest.MultipleIncompatibleReshapes
[0;32m[       OK ] [mMultiDeviceTest.MultipleIncompatibleReshapes (176 ms)
[0;32m[----------] [m9 tests from MultiDeviceTest (797 ms total)

[0;32m[----------] [m1 test from CollectiveBasedOverlapTest
[0;32m[ RUN      ] [mCollectiveBasedOverlapTest.ReduceScatterBasedPipeliningHostIrImplementation
[0;32m[       OK ] [mCollectiveBasedOverlapTest.ReduceScatterBasedPipeliningHostIrImplementation (29 ms)
[0;32m[----------] [m1 test from CollectiveBasedOverlapTest (29 ms total)

[0;32m[----------] [m1 test from AllgatherOverlapTest
[0;32m[ RUN      ] [mAllgatherOverlapTest.AllgatherBasedPipeliningATenImplementation
[0;32m[       OK ] [mAllgatherOverlapTest.AllgatherBasedPipeliningATenImplementation (4 ms)
[0;32m[----------] [m1 test from AllgatherOverlapTest (4 ms total)

[0;32m[----------] [m1 test from RingAllgatherOverlapTest
[0;32m[ RUN      ] [mRingAllgatherOverlapTest.RingAllgatherBasedPipeliningHostIRImplementationCudaIpc
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_host_ir_overlap.cpp:1101: Skipped
Skipping test for single device

[0;32m[  SKIPPED ] [mRingAllgatherOverlapTest.RingAllgatherBasedPipeliningHostIRImplementationCudaIpc (0 ms)
[0;32m[----------] [m1 test from RingAllgatherOverlapTest (0 ms total)

[0;32m[----------] [m2 tests from IpcTest
[0;32m[ RUN      ] [mIpcTest.IpcMemHandlePtrArithmeticAtReceiver
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_ipc.cpp:72: Skipped
Skipping test for single device

[0;32m[  SKIPPED ] [mIpcTest.IpcMemHandlePtrArithmeticAtReceiver (0 ms)
[0;32m[ RUN      ] [mIpcTest.IpcNvlsMulticastBroadcast
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_ipc.cpp:340: Skipped
Skipping test for single device

[0;32m[  SKIPPED ] [mIpcTest.IpcNvlsMulticastBroadcast (0 ms)
[0;32m[----------] [m2 tests from IpcTest (0 ms total)

[0;32m[----------] [m1 test from LowerCollectiveCudaTest
[0;32m[ RUN      ] [mLowerCollectiveCudaTest.Allgather
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication.cpp:790: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaTest.Allgather (0 ms)
[0;32m[----------] [m1 test from LowerCollectiveCudaTest (0 ms total)

[0;32m[----------] [m3 tests from DistributedMatmulTest
[0;32m[ RUN      ] [mDistributedMatmulTest.Matmul_LayoutTN_NoComms
[0;32m[       OK ] [mDistributedMatmulTest.Matmul_LayoutTN_NoComms (81 ms)
[0;32m[ RUN      ] [mDistributedMatmulTest.Matmul_LayoutNT_ReduceScatter
[0;32m[       OK ] [mDistributedMatmulTest.Matmul_LayoutNT_ReduceScatter (120 ms)
[0;32m[ RUN      ] [mDistributedMatmulTest.RowParallelLinear
[0;32m[       OK ] [mDistributedMatmulTest.RowParallelLinear (11 ms)
[0;32m[----------] [m3 tests from DistributedMatmulTest (213 ms total)

[0;32m[----------] [m2 tests from MultiDeviceStreamParallelTypeTest
[0;32m[ RUN      ] [mMultiDeviceStreamParallelTypeTest.Allreduce
[0;32m[       OK ] [mMultiDeviceStreamParallelTypeTest.Allreduce (9 ms)
[0;32m[ RUN      ] [mMultiDeviceStreamParallelTypeTest.matmul_RS_through_bcast
[0;32m[       OK ] [mMultiDeviceStreamParallelTypeTest.matmul_RS_through_bcast (98 ms)
[0;32m[----------] [m2 tests from MultiDeviceStreamParallelTypeTest (108 ms total)

[0;32m[----------] [m2 tests from SymmetricTensorTest
[0;32m[ RUN      ] [mSymmetricTensorTest.PreallocatedTensor
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_symmetric_tensor.cpp:59: Skipped
Skipping test for single device

[0;32m[  SKIPPED ] [mSymmetricTensorTest.PreallocatedTensor (0 ms)
[0;32m[ RUN      ] [mSymmetricTensorTest.SmallAllocation
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_symmetric_tensor.cpp:233: Skipped
Skipping test for single device

[0;32m[  SKIPPED ] [mSymmetricTensorTest.SmallAllocation (0 ms)
[0;32m[----------] [m2 tests from SymmetricTensorTest (0 ms total)

[0;32m[----------] [m3 tests from CommunicationTest
[0;32m[ RUN      ] [mCommunicationTest.Allgather/NCCL
[0;32m[       OK ] [mCommunicationTest.Allgather/NCCL (2 ms)
[0;32m[ RUN      ] [mCommunicationTest.SendRecv/NCCL
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_communications.cpp:217: Skipped
This test needs at least 2 GPUs and 2 ranks.

[0;32m[  SKIPPED ] [mCommunicationTest.SendRecv/NCCL (0 ms)
[0;32m[ RUN      ] [mCommunicationTest.Allreduce/NCCL
[0;32m[       OK ] [mCommunicationTest.Allreduce/NCCL (0 ms)
[0;32m[----------] [m3 tests from CommunicationTest (3 ms total)

[0;32m[----------] [m1 test from P2PCommunicationTest
[0;32m[ RUN      ] [mP2PCommunicationTest.CudaComm/Put
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_communications.cpp:432: Skipped
This test needs at least 2 GPUs and 2 ranks.

[0;32m[  SKIPPED ] [mP2PCommunicationTest.CudaComm/Put (0 ms)
[0;32m[----------] [m1 test from P2PCommunicationTest (0 ms total)

[0;32m[----------] [m2 tests from Manual/MultiDeviceHostIrTest
[0;32m[ RUN      ] [mManual/MultiDeviceHostIrTest.SingleFusionSingleComm/useFusionExecutorCache_withoutShardingAnnotations
[0;32m[       OK ] [mManual/MultiDeviceHostIrTest.SingleFusionSingleComm/useFusionExecutorCache_withoutShardingAnnotations (89 ms)
[0;32m[ RUN      ] [mManual/MultiDeviceHostIrTest.SingleCommTwoFusionAndWait/useFusionExecutor_withShardingAnnotations
[0;32m[       OK ] [mManual/MultiDeviceHostIrTest.SingleCommTwoFusionAndWait/useFusionExecutor_withShardingAnnotations (333 ms)
[0;32m[----------] [m2 tests from Manual/MultiDeviceHostIrTest (423 ms total)

[0;32m[----------] [m2 tests from LowerGatherTest
[0;32m[ RUN      ] [mLowerGatherTest.InMesh_0_1_OutMesh_0_NonHostIr
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication.cpp:74: Skipped
T0_g_float[ideviceIdx.x0{i0}, iS1{i2}] (DeviceMesh{0 1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mLowerGatherTest.InMesh_0_1_OutMesh_0_NonHostIr (0 ms)
[0;32m[ RUN      ] [mLowerGatherTest.InMesh_0_1_OutMesh_1_HostIr
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication.cpp:74: Skipped
T0_g_float[ideviceIdx.x0{i0}, iS1{i2}] (DeviceMesh{0 1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mLowerGatherTest.InMesh_0_1_OutMesh_1_HostIr (0 ms)
[0;32m[----------] [m2 tests from LowerGatherTest (0 ms total)

[0;32m[----------] [m2 tests from LowerScatterTest
[0;32m[ RUN      ] [mLowerScatterTest.InMesh_0_OutMesh_0_1_NonHostIr
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication.cpp:146: Skipped
T1_g_float[ideviceIdx.x2{i0}, iS3{i2}] (DeviceMesh{0 1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mLowerScatterTest.InMesh_0_OutMesh_0_1_NonHostIr (0 ms)
[0;32m[ RUN      ] [mLowerScatterTest.InMesh_1_OutMesh_0_1_HostIr
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication.cpp:146: Skipped
T0_g_float[iS0{i0}, iS1{i2}] (DeviceMesh{1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mLowerScatterTest.InMesh_1_OutMesh_0_1_HostIr (0 ms)
[0;32m[----------] [m2 tests from LowerScatterTest (0 ms total)

[0;32m[----------] [m3 tests from LowerSendRecvTest
[0;32m[ RUN      ] [mLowerSendRecvTest.InMesh_0_OutMesh_1_NonHostIr
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication.cpp:198: Skipped
T1_g_float[ideviceIdx.x2{i0}, iS3{i2}] (DeviceMesh{1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mLowerSendRecvTest.InMesh_0_OutMesh_1_NonHostIr (0 ms)
[0;32m[ RUN      ] [mLowerSendRecvTest.InMesh_1_OutMesh_0_HostIr
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication.cpp:198: Skipped
T0_g_float[ideviceIdx.x0{i0}, iS1{i2}] (DeviceMesh{1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mLowerSendRecvTest.InMesh_1_OutMesh_0_HostIr (0 ms)
[0;32m[ RUN      ] [mLowerSendRecvTest.InMesh_1_2_OutMesh_1_0_NonHostIr
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication.cpp:198: Skipped
T0_g_float[ideviceIdx.x0{i0}, iS1{i2}] (DeviceMesh{1 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mLowerSendRecvTest.InMesh_1_2_OutMesh_1_0_NonHostIr (0 ms)
[0;32m[----------] [m3 tests from LowerSendRecvTest (0 ms total)

[0;32m[----------] [m9 tests from LowerCollectiveTest
[0;32m[ RUN      ] [mLowerCollectiveTest.Allgather/NCCL_HostIr
[0;32m[       OK ] [mLowerCollectiveTest.Allgather/NCCL_HostIr (1 ms)
[0;32m[ RUN      ] [mLowerCollectiveTest.Broadcast/NCCL_NonHostIr
[0;32m[       OK ] [mLowerCollectiveTest.Broadcast/NCCL_NonHostIr (1 ms)
[0;32m[ RUN      ] [mLowerCollectiveTest.Reduce/NCCL_HostIr
[0;32m[       OK ] [mLowerCollectiveTest.Reduce/NCCL_HostIr (1 ms)
[0;32m[ RUN      ] [mLowerCollectiveTest.Allreduce_Concrete/NCCL_NonHostIr
[0;32m[       OK ] [mLowerCollectiveTest.Allreduce_Concrete/NCCL_NonHostIr (1 ms)
[0;32m[ RUN      ] [mLowerCollectiveTest.ReduceScatter/NCCL_HostIr
[0;32m[       OK ] [mLowerCollectiveTest.ReduceScatter/NCCL_HostIr (134 ms)
[0;32m[ RUN      ] [mLowerCollectiveTest.ReduceScatter_Allgather/NCCL_NonHostIr
[0;32m[       OK ] [mLowerCollectiveTest.ReduceScatter_Allgather/NCCL_NonHostIr (14 ms)
[0;32m[ RUN      ] [mLowerCollectiveTest.ReduceScatterNoncontig/NCCL_HostIr
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication.cpp:549: Skipped
This test exercises ReorderShardedAxisPass, and requires at least 2 devices.

[0;32m[  SKIPPED ] [mLowerCollectiveTest.ReduceScatterNoncontig/NCCL_HostIr (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveTest.Allgather_CompliantAllocation/NCCL_NonHostIr
[0;32m[       OK ] [mLowerCollectiveTest.Allgather_CompliantAllocation/NCCL_NonHostIr (1 ms)
[0;32m[ RUN      ] [mLowerCollectiveTest.Allgather_NonCompliantAllocation/NCCL_HostIr
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication.cpp:675: Skipped
Should pass with one GPU, but doesn't.

[0;32m[  SKIPPED ] [mLowerCollectiveTest.Allgather_NonCompliantAllocation/NCCL_HostIr (0 ms)
[0;32m[----------] [m9 tests from LowerCollectiveTest (156 ms total)

[0;32m[----------] [m32 tests from LowerCollectiveCudaAndNcclTest
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Allgather/memcpy_128KB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:187: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/memcpy_128KB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Allgather/batch_memcpy_128KB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:187: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/batch_memcpy_128KB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Allgather/multimem_256KB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:187: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/multimem_256KB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Allgather/nccl_512KB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:187: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/nccl_512KB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Allgather/memcpy_1MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:187: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/memcpy_1MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Allgather/batch_memcpy_1MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:187: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/batch_memcpy_1MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Allgather/multimem_2MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:187: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/multimem_2MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Allgather/nccl_4MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:187: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/nccl_4MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Allgather/memcpy_8MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:187: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/memcpy_8MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Allgather/batch_memcpy_8MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:187: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/batch_memcpy_8MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Allgather/multimem_16MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:187: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/multimem_16MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Allgather/nccl_32MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:187: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/nccl_32MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Allgather/memcpy_64MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:187: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/memcpy_64MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Allgather/batch_memcpy_64MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:187: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/batch_memcpy_64MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Allgather/multimem_128MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:187: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/multimem_128MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Allgather/nccl_256MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:187: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/nccl_256MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Broadcast/memcpy_128KB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:255: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/memcpy_128KB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Broadcast/batch_memcpy_128KB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:255: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/batch_memcpy_128KB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Broadcast/multimem_256KB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:255: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/multimem_256KB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Broadcast/nccl_512KB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:255: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/nccl_512KB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Broadcast/memcpy_1MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:255: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/memcpy_1MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Broadcast/batch_memcpy_1MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:255: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/batch_memcpy_1MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Broadcast/multimem_2MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:255: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/multimem_2MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Broadcast/nccl_4MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:255: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/nccl_4MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Broadcast/memcpy_8MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:255: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/memcpy_8MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Broadcast/batch_memcpy_8MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:255: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/batch_memcpy_8MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Broadcast/multimem_16MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:255: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/multimem_16MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Broadcast/nccl_32MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:255: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/nccl_32MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Broadcast/memcpy_64MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:255: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/memcpy_64MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Broadcast/batch_memcpy_64MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:255: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/batch_memcpy_64MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Broadcast/multimem_128MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:255: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/multimem_128MB (0 ms)
[0;32m[ RUN      ] [mLowerCollectiveCudaAndNcclTest.Broadcast/nccl_256MB
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_lower_communication_cuda.cpp:255: Skipped
This test needs at least 2 ranks.

[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/nccl_256MB (0 ms)
[0;32m[----------] [m32 tests from LowerCollectiveCudaAndNcclTest (0 ms total)

[0;32m[----------] [m24 tests from Gather/PipelineTestTwoStages
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/0
[0;32m[       OK ] [mGather/PipelineTestTwoStages.Communication/0 (101 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/3
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T2_l_float[bS8{1}, iS9{2}, iS10{3}] (DeviceMesh{1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/3 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/6
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T2_l_float[bS8{1}, iS9{2}, iS10{3}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/6 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/9
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T2_l_float[bS8{1}, iS9{2}, iS10{3}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/9 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/12
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[bdeviceIdx.x0{1}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/12 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/15
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[bdeviceIdx.x0{1}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/15 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/18
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[bdeviceIdx.x0{1}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/18 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/21
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[bdeviceIdx.x0{1}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/21 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/24
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{4}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 1 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/24 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/27
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{4}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 1 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/27 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/30
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{4}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 1 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/30 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/33
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{4}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 1 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/33 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/36
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/36 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/39
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/39 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/42
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/42 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/45
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/45 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/48
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/48 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/51
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/51 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/54
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/54 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/57
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/57 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/60
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{2}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/60 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/63
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{2}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/63 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/66
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{2}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/66 (0 ms)
[0;32m[ RUN      ] [mGather/PipelineTestTwoStages.Communication/69
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{2}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/69 (0 ms)
[0;32m[----------] [m24 tests from Gather/PipelineTestTwoStages (104 ms total)

[0;32m[----------] [m4 tests from Scatter/PipelineTestTwoStages
[0;32m[ RUN      ] [mScatter/PipelineTestTwoStages.Communication/0
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T2_l_float[ideviceIdx.x8{4}, iS9{2}, iS10{3}] (DeviceMesh{0 1 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mScatter/PipelineTestTwoStages.Communication/0 (0 ms)
[0;32m[ RUN      ] [mScatter/PipelineTestTwoStages.Communication/3
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T2_l_float[ideviceIdx.x8{3}, iS9{2}, iS10{3}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mScatter/PipelineTestTwoStages.Communication/3 (0 ms)
[0;32m[ RUN      ] [mScatter/PipelineTestTwoStages.Communication/6
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{4}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mScatter/PipelineTestTwoStages.Communication/6 (0 ms)
[0;32m[ RUN      ] [mScatter/PipelineTestTwoStages.Communication/9
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mScatter/PipelineTestTwoStages.Communication/9 (0 ms)
[0;32m[----------] [m4 tests from Scatter/PipelineTestTwoStages (0 ms total)

[0;32m[----------] [m48 tests from Bcast/PipelineTestTwoStages
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/0
[0;32m[       OK ] [mBcast/PipelineTestTwoStages.Communication/0 (103 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/3
[0;32m[       OK ] [mBcast/PipelineTestTwoStages.Communication/3 (72 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/6
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T2_l_float[iS8{3}, iS9{2}, iS10{3}] (DeviceMesh{1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/6 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/9
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T2_l_float[iS8{3}, iS9{2}, iS10{3}] (DeviceMesh{0 1 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/9 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/12
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T2_l_float[iS8{3}, iS9{2}, iS10{3}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/12 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/15
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T2_l_float[iS8{3}, iS9{2}, iS10{3}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/15 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/18
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T2_l_float[iS8{3}, iS9{2}, iS10{3}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/18 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/21
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T2_l_float[iS8{3}, iS9{2}, iS10{3}] (DeviceMesh{1 0})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/21 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/24
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/24 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/27
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/27 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/30
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/30 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/33
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/33 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/36
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/36 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/39
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/39 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/42
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/42 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/45
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/45 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/48
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 1 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/48 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/51
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 1 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/51 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/54
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 1 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/54 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/57
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 1 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/57 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/60
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 1 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/60 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/63
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 1 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/63 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/66
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 1 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/66 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/69
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 1 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/69 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/72
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/72 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/75
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/75 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/78
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/78 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/81
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/81 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/84
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/84 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/87
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/87 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/90
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/90 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/93
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/93 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/96
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/96 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/99
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/99 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/102
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/102 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/105
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/105 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/108
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/108 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/111
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/111 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/114
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/114 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/117
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/117 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/120
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/120 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/123
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/123 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/126
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/126 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/129
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/129 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/132
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/132 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/135
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/135 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/138
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/138 (0 ms)
[0;32m[ RUN      ] [mBcast/PipelineTestTwoStages.Communication/141
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/141 (0 ms)
[0;32m[----------] [m48 tests from Bcast/PipelineTestTwoStages (181 ms total)

[0;32m[----------] [m6 tests from Bcast_sharded/PipelineTestTwoStages
[0;32m[ RUN      ] [mBcast_sharded/PipelineTestTwoStages.Communication/0
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast_sharded/PipelineTestTwoStages.Communication/0 (0 ms)
[0;32m[ RUN      ] [mBcast_sharded/PipelineTestTwoStages.Communication/3
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, ideviceIdx.x1{3}, iS2{3}, iS3{5}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast_sharded/PipelineTestTwoStages.Communication/3 (0 ms)
[0;32m[ RUN      ] [mBcast_sharded/PipelineTestTwoStages.Communication/6
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, ideviceIdx.x1{3}, iS2{3}, iS3{5}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast_sharded/PipelineTestTwoStages.Communication/6 (0 ms)
[0;32m[ RUN      ] [mBcast_sharded/PipelineTestTwoStages.Communication/9
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast_sharded/PipelineTestTwoStages.Communication/9 (0 ms)
[0;32m[ RUN      ] [mBcast_sharded/PipelineTestTwoStages.Communication/12
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast_sharded/PipelineTestTwoStages.Communication/12 (0 ms)
[0;32m[ RUN      ] [mBcast_sharded/PipelineTestTwoStages.Communication/15
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, ideviceIdx.x1{3}, iS2{3}, iS3{5}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast_sharded/PipelineTestTwoStages.Communication/15 (0 ms)
[0;32m[----------] [m6 tests from Bcast_sharded/PipelineTestTwoStages (0 ms total)

[0;32m[----------] [m2 tests from Bcast_sharded_same_mesh/PipelineTestTwoStages
[0;32m[ RUN      ] [mBcast_sharded_same_mesh/PipelineTestTwoStages.Communication/2
[0;32m[       OK ] [mBcast_sharded_same_mesh/PipelineTestTwoStages.Communication/2 (102 ms)
[0;32m[ RUN      ] [mBcast_sharded_same_mesh/PipelineTestTwoStages.Communication/5
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[bdeviceIdx.x0{1}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mBcast_sharded_same_mesh/PipelineTestTwoStages.Communication/5 (0 ms)
[0;32m[----------] [m2 tests from Bcast_sharded_same_mesh/PipelineTestTwoStages (103 ms total)

[0;32m[----------] [m16 tests from Reduce/PipelineTestTwoStages
[0;32m[ RUN      ] [mReduce/PipelineTestTwoStages.Communication/0
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{4}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 1 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/0 (0 ms)
[0;32m[ RUN      ] [mReduce/PipelineTestTwoStages.Communication/3
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, ideviceIdx.x1{4}, iS2{3}, iS3{5}] (DeviceMesh{0 1 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/3 (0 ms)
[0;32m[ RUN      ] [mReduce/PipelineTestTwoStages.Communication/6
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{4}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 1 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/6 (0 ms)
[0;32m[ RUN      ] [mReduce/PipelineTestTwoStages.Communication/9
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, ideviceIdx.x1{4}, iS2{3}, iS3{5}] (DeviceMesh{0 1 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/9 (0 ms)
[0;32m[ RUN      ] [mReduce/PipelineTestTwoStages.Communication/12
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/12 (0 ms)
[0;32m[ RUN      ] [mReduce/PipelineTestTwoStages.Communication/15
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, ideviceIdx.x1{3}, iS2{3}, iS3{5}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/15 (0 ms)
[0;32m[ RUN      ] [mReduce/PipelineTestTwoStages.Communication/18
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/18 (0 ms)
[0;32m[ RUN      ] [mReduce/PipelineTestTwoStages.Communication/21
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, ideviceIdx.x1{3}, iS2{3}, iS3{5}] (DeviceMesh{0 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/21 (0 ms)
[0;32m[ RUN      ] [mReduce/PipelineTestTwoStages.Communication/24
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/24 (0 ms)
[0;32m[ RUN      ] [mReduce/PipelineTestTwoStages.Communication/27
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, ideviceIdx.x1{3}, iS2{3}, iS3{5}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/27 (0 ms)
[0;32m[ RUN      ] [mReduce/PipelineTestTwoStages.Communication/30
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{3}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/30 (0 ms)
[0;32m[ RUN      ] [mReduce/PipelineTestTwoStages.Communication/33
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, ideviceIdx.x1{3}, iS2{3}, iS3{5}] (DeviceMesh{1 0 2})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/33 (0 ms)
[0;32m[ RUN      ] [mReduce/PipelineTestTwoStages.Communication/36
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{2}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/36 (0 ms)
[0;32m[ RUN      ] [mReduce/PipelineTestTwoStages.Communication/39
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, ideviceIdx.x1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/39 (0 ms)
[0;32m[ RUN      ] [mReduce/PipelineTestTwoStages.Communication/42
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{2}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/42 (0 ms)
[0;32m[ RUN      ] [mReduce/PipelineTestTwoStages.Communication/45
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[iS0{3}, ideviceIdx.x1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/45 (0 ms)
[0;32m[----------] [m16 tests from Reduce/PipelineTestTwoStages (1 ms total)

[0;32m[----------] [m2 tests from ReduceScatter/PipelineTestTwoStages
[0;32m[ RUN      ] [mReduceScatter/PipelineTestTwoStages.Communication/0
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{4}, iS1{4}, iS2{3}, iS3{5}] (DeviceMesh{0 1 2 3})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mReduceScatter/PipelineTestTwoStages.Communication/0 (0 ms)
[0;32m[ RUN      ] [mReduceScatter/PipelineTestTwoStages.Communication/3
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_pipeline.cpp:342: Skipped
T0_g_float[ideviceIdx.x0{2}, iS1{2}, iS2{3}, iS3{5}] (DeviceMesh{1 0})) requires more than 1 devices.

[0;32m[  SKIPPED ] [mReduceScatter/PipelineTestTwoStages.Communication/3 (0 ms)
[0;32m[----------] [m2 tests from ReduceScatter/PipelineTestTwoStages (0 ms total)

[0;32m[----------] [m2 tests from MultiDeviceReductionTest
[0;32m[ RUN      ] [mMultiDeviceReductionTest.UnshardedInput_ShardedOutput/concrete_sharded_along_dim_0
[0;32m[       OK ] [mMultiDeviceReductionTest.UnshardedInput_ShardedOutput/concrete_sharded_along_dim_0 (103 ms)
[0;32m[ RUN      ] [mMultiDeviceReductionTest.ShardedInput_ReplicatedOutput/symbolic_sharded_along_dim_1
[0;32m[       OK ] [mMultiDeviceReductionTest.ShardedInput_ReplicatedOutput/symbolic_sharded_along_dim_1 (98 ms)
[0;32m[----------] [m2 tests from MultiDeviceReductionTest (202 ms total)

[0;32m[----------] [m2 tests from MultiDeviceBroadcastTest
[0;32m[ RUN      ] [mMultiDeviceBroadcastTest.NotExpanded/0
[0;32m[       OK ] [mMultiDeviceBroadcastTest.NotExpanded/0 (1 ms)
[0;32m[ RUN      ] [mMultiDeviceBroadcastTest.Expanded/1
[0;32m[       OK ] [mMultiDeviceBroadcastTest.Expanded/1 (2 ms)
[0;32m[----------] [m2 tests from MultiDeviceBroadcastTest (3 ms total)

[0;32m[----------] [m2 tests from InsertReshardingTest
[0;32m[ RUN      ] [mInsertReshardingTest.Execute/2
[0;32m[       OK ] [mInsertReshardingTest.Execute/2 (249 ms)
[0;32m[ RUN      ] [mInsertReshardingTest.Execute/5
[0;32m[       OK ] [mInsertReshardingTest.Execute/5 (258 ms)
[0;32m[----------] [m2 tests from InsertReshardingTest (507 ms total)

[0;32m[----------] [m1 test from AGMatmulTest
[0;32m[ RUN      ] [mAGMatmulTest.CollectiveBasedPipeline/AG_before_Matmul
[0;32m[       OK ] [mAGMatmulTest.CollectiveBasedPipeline/AG_before_Matmul (120 ms)
[0;32m[----------] [m1 test from AGMatmulTest (120 ms total)

[0;32m[----------] [m3 tests from StreamParallelBackendTest
[0;32m[ RUN      ] [mStreamParallelBackendTest.AllgatherP2p/Broadcast_Cuda
[0;32m[       OK ] [mStreamParallelBackendTest.AllgatherP2p/Broadcast_Cuda (8 ms)
[0;32m[ RUN      ] [mStreamParallelBackendTest.AG_matmul_P2p/Broadcast_Nccl
[0;32m[       OK ] [mStreamParallelBackendTest.AG_matmul_P2p/Broadcast_Nccl (123 ms)
[0;32m[ RUN      ] [mStreamParallelBackendTest.AG_matmul_P2p/p2p_Cuda
[0;32m[       OK ] [mStreamParallelBackendTest.AG_matmul_P2p/p2p_Cuda (130 ms)
[0;32m[----------] [m3 tests from StreamParallelBackendTest (262 ms total)

[0;32m[----------] [m8 tests from DistributedTransformerTest
[0;32m[ RUN      ] [mDistributedTransformerTest.MLP_Layer/__half
[0;32m[       OK ] [mDistributedTransformerTest.MLP_Layer/__half (442 ms)
[0;32m[ RUN      ] [mDistributedTransformerTest.Sequence_Parallel_MLP_Layer/__bfloat
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_transformer.cpp:289: Skipped
Requires >1 devices, D=1

[0;32m[  SKIPPED ] [mDistributedTransformerTest.Sequence_Parallel_MLP_Layer/__bfloat (0 ms)
[0;32m[ RUN      ] [mDistributedTransformerTest.MultiheadAttention_SP/__half
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_transformer.cpp:434: Skipped
Requires >1 devices, D=1

[0;32m[  SKIPPED ] [mDistributedTransformerTest.MultiheadAttention_SP/__half (0 ms)
[0;32m[ RUN      ] [mDistributedTransformerTest.MLP_Backward/__bfloat
[0;32m[       OK ] [mDistributedTransformerTest.MLP_Backward/__bfloat (2858 ms)
[0;32m[ RUN      ] [mDistributedTransformerTest.Forward_SP/__half
/opt/pytorch/nvfuser/tests/cpp/test_multidevice_transformer.cpp:676: Skipped
Requires >1 devices, D=1

[0;32m[  SKIPPED ] [mDistributedTransformerTest.Forward_SP/__half (0 ms)
[0;32m[ RUN      ] [mDistributedTransformerTest.Forward/__bfloat
[0;32m[       OK ] [mDistributedTransformerTest.Forward/__bfloat (1826 ms)
[0;32m[ RUN      ] [mDistributedTransformerTest.LoopSplitMLP/__half
[0;32m[       OK ] [mDistributedTransformerTest.LoopSplitMLP/__half (381 ms)
[0;32m[ RUN      ] [mDistributedTransformerTest.LoopSplitMHAFwd/__bfloat
[0;32m[       OK ] [mDistributedTransformerTest.LoopSplitMHAFwd/__bfloat (671 ms)
[0;32m[----------] [m8 tests from DistributedTransformerTest (6179 ms total)

[0;32m[----------] [mGlobal test environment tear-down
[0;32m[==========] [m197 tests from 31 test suites ran. (9931 ms total)
[0;32m[  PASSED  ] [m47 tests.
[0;32m[  SKIPPED ] [m150 tests, listed below:
[0;32m[  SKIPPED ] [mRingAllgatherOverlapTest.RingAllgatherBasedPipeliningHostIRImplementationCudaIpc
[0;32m[  SKIPPED ] [mIpcTest.IpcMemHandlePtrArithmeticAtReceiver
[0;32m[  SKIPPED ] [mIpcTest.IpcNvlsMulticastBroadcast
[0;32m[  SKIPPED ] [mLowerCollectiveCudaTest.Allgather
[0;32m[  SKIPPED ] [mSymmetricTensorTest.PreallocatedTensor
[0;32m[  SKIPPED ] [mSymmetricTensorTest.SmallAllocation
[0;32m[  SKIPPED ] [mCommunicationTest.SendRecv/NCCL
[0;32m[  SKIPPED ] [mP2PCommunicationTest.CudaComm/Put
[0;32m[  SKIPPED ] [mLowerGatherTest.InMesh_0_1_OutMesh_0_NonHostIr
[0;32m[  SKIPPED ] [mLowerGatherTest.InMesh_0_1_OutMesh_1_HostIr
[0;32m[  SKIPPED ] [mLowerScatterTest.InMesh_0_OutMesh_0_1_NonHostIr
[0;32m[  SKIPPED ] [mLowerScatterTest.InMesh_1_OutMesh_0_1_HostIr
[0;32m[  SKIPPED ] [mLowerSendRecvTest.InMesh_0_OutMesh_1_NonHostIr
[0;32m[  SKIPPED ] [mLowerSendRecvTest.InMesh_1_OutMesh_0_HostIr
[0;32m[  SKIPPED ] [mLowerSendRecvTest.InMesh_1_2_OutMesh_1_0_NonHostIr
[0;32m[  SKIPPED ] [mLowerCollectiveTest.ReduceScatterNoncontig/NCCL_HostIr
[0;32m[  SKIPPED ] [mLowerCollectiveTest.Allgather_NonCompliantAllocation/NCCL_HostIr
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/memcpy_128KB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/batch_memcpy_128KB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/multimem_256KB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/nccl_512KB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/memcpy_1MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/batch_memcpy_1MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/multimem_2MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/nccl_4MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/memcpy_8MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/batch_memcpy_8MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/multimem_16MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/nccl_32MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/memcpy_64MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/batch_memcpy_64MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/multimem_128MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Allgather/nccl_256MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/memcpy_128KB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/batch_memcpy_128KB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/multimem_256KB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/nccl_512KB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/memcpy_1MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/batch_memcpy_1MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/multimem_2MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/nccl_4MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/memcpy_8MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/batch_memcpy_8MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/multimem_16MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/nccl_32MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/memcpy_64MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/batch_memcpy_64MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/multimem_128MB
[0;32m[  SKIPPED ] [mLowerCollectiveCudaAndNcclTest.Broadcast/nccl_256MB
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/3
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/6
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/9
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/12
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/15
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/18
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/21
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/24
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/27
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/30
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/33
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/36
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/39
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/42
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/45
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/48
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/51
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/54
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/57
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/60
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/63
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/66
[0;32m[  SKIPPED ] [mGather/PipelineTestTwoStages.Communication/69
[0;32m[  SKIPPED ] [mScatter/PipelineTestTwoStages.Communication/0
[0;32m[  SKIPPED ] [mScatter/PipelineTestTwoStages.Communication/3
[0;32m[  SKIPPED ] [mScatter/PipelineTestTwoStages.Communication/6
[0;32m[  SKIPPED ] [mScatter/PipelineTestTwoStages.Communication/9
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/6
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/9
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/12
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/15
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/18
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/21
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/24
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/27
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/30
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/33
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/36
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/39
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/42
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/45
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/48
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/51
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/54
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/57
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/60
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/63
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/66
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/69
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/72
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/75
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/78
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/81
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/84
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/87
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/90
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/93
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/96
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/99
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/102
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/105
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/108
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/111
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/114
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/117
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/120
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/123
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/126
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/129
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/132
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/135
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/138
[0;32m[  SKIPPED ] [mBcast/PipelineTestTwoStages.Communication/141
[0;32m[  SKIPPED ] [mBcast_sharded/PipelineTestTwoStages.Communication/0
[0;32m[  SKIPPED ] [mBcast_sharded/PipelineTestTwoStages.Communication/3
[0;32m[  SKIPPED ] [mBcast_sharded/PipelineTestTwoStages.Communication/6
[0;32m[  SKIPPED ] [mBcast_sharded/PipelineTestTwoStages.Communication/9
[0;32m[  SKIPPED ] [mBcast_sharded/PipelineTestTwoStages.Communication/12
[0;32m[  SKIPPED ] [mBcast_sharded/PipelineTestTwoStages.Communication/15
[0;32m[  SKIPPED ] [mBcast_sharded_same_mesh/PipelineTestTwoStages.Communication/5
[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/0
[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/3
[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/6
[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/9
[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/12
[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/15
[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/18
[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/21
[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/24
[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/27
[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/30
[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/33
[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/36
[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/39
[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/42
[0;32m[  SKIPPED ] [mReduce/PipelineTestTwoStages.Communication/45
[0;32m[  SKIPPED ] [mReduceScatter/PipelineTestTwoStages.Communication/0
[0;32m[  SKIPPED ] [mReduceScatter/PipelineTestTwoStages.Communication/3
[0;32m[  SKIPPED ] [mDistributedTransformerTest.Sequence_Parallel_MLP_Layer/__bfloat
[0;32m[  SKIPPED ] [mDistributedTransformerTest.MultiheadAttention_SP/__half
[0;32m[  SKIPPED ] [mDistributedTransformerTest.Forward_SP/__half
+ rv=0
+ for f in __tmp_*
+ [[ __tmp_* == __tmp_c* ]]
+ fs='__tmp_*'
++ sed s/__tmp_/__tmp_c16_/g
++ echo '__tmp_*'
+ ft='__tmp_c16_*'
+ mv '__tmp_*' '__tmp_c16_*'
mv: cannot stat '__tmp_*': No such file or directory
+ counter=17
+ for f in $test_files
+ [[ /opt/pytorch/nvfuser/bin/test_greedy == *matmul* ]]
+ [[ /opt/pytorch/nvfuser/bin/test_greedy == *multidevice* ]]
+ executable=/opt/pytorch/nvfuser/bin/test_greedy
++ basename /opt/pytorch/nvfuser/bin/test_greedy
+ report_name=TEST-report-test_greedy.xml
+ sed_flag=s/__tmp_/__tmp_c17_/g
+ /opt/pytorch/nvfuser/bin/test_greedy --gtest_output=xml:TEST-report-test_greedy.xml
+ sed -u s/__tmp_/__tmp_c17_/g
Running main() from /opt/pytorch/nvfuser/third_party/googletest/googletest/src/gtest_main.cc
Note: This is test shard 2 of 3.
[==========] Running 18 tests from 3 test suites.
[----------] Global test environment set-up.
[----------] 7 tests from GreedySchedulerTest
[ RUN      ] GreedySchedulerTest.ScanPad3D
[       OK ] GreedySchedulerTest.ScanPad3D (1452 ms)
[ RUN      ] GreedySchedulerTest.ScanPad3DReshape3
[       OK ] GreedySchedulerTest.ScanPad3DReshape3 (1306 ms)
[ RUN      ] GreedySchedulerTest.ArgsortPadScan
[       OK ] GreedySchedulerTest.ArgsortPadScan (1394 ms)
[ RUN      ] GreedySchedulerTest.Scatter
[       OK ] GreedySchedulerTest.Scatter (77 ms)
[ RUN      ] GreedySchedulerTest.TopKWithPartialTidParticipation
[       OK ] GreedySchedulerTest.TopKWithPartialTidParticipation (1293 ms)
[ RUN      ] GreedySchedulerTest.ConstrainedIDAndSqueeze
[       OK ] GreedySchedulerTest.ConstrainedIDAndSqueeze (1268 ms)
[ RUN      ] GreedySchedulerTest.TranslateScatterAndReductionToScatterAccumulate
[       OK ] GreedySchedulerTest.TranslateScatterAndReductionToScatterAccumulate (91 ms)
[----------] 7 tests from GreedySchedulerTest (6884 ms total)

[----------] 5 tests from GreedySchedulerTestConstraintSize
[ RUN      ] GreedySchedulerTestConstraintSize.ArgsortLargeConstrainedIDs/2048
[       OK ] GreedySchedulerTestConstraintSize.ArgsortLargeConstrainedIDs/2048 (1340 ms)
[ RUN      ] GreedySchedulerTestConstraintSize.ScanLargeConstrainedIDs/2048
[       OK ] GreedySchedulerTestConstraintSize.ScanLargeConstrainedIDs/2048 (1168 ms)
[ RUN      ] GreedySchedulerTestConstraintSize.ScatterLargeConstrainedIDs/2048
[       OK ] GreedySchedulerTestConstraintSize.ScatterLargeConstrainedIDs/2048 (83 ms)
[ RUN      ] GreedySchedulerTestConstraintSize.ArgsortArgsort/2048
[       OK ] GreedySchedulerTestConstraintSize.ArgsortArgsort/2048 (1392 ms)
[ RUN      ] GreedySchedulerTestConstraintSize.TopKLargeConstrainedIDs/2048
[       OK ] GreedySchedulerTestConstraintSize.TopKLargeConstrainedIDs/2048 (1330 ms)
[----------] 5 tests from GreedySchedulerTestConstraintSize (5315 ms total)

[----------] 6 tests from GreedySchedulerTestShmemSize
[ RUN      ] GreedySchedulerTestShmemSize.Argsort/256
[       OK ] GreedySchedulerTestShmemSize.Argsort/256 (1334 ms)
[ RUN      ] GreedySchedulerTestShmemSize.Argsort/2048
[       OK ] GreedySchedulerTestShmemSize.Argsort/2048 (3868 ms)
[ RUN      ] GreedySchedulerTestShmemSize.TopK/256
[       OK ] GreedySchedulerTestShmemSize.TopK/256 (1339 ms)
[ RUN      ] GreedySchedulerTestShmemSize.TopK/2048
[       OK ] GreedySchedulerTestShmemSize.TopK/2048 (0 ms)
[ RUN      ] GreedySchedulerTestShmemSize.Scan/256
[       OK ] GreedySchedulerTestShmemSize.Scan/256 (1176 ms)
[ RUN      ] GreedySchedulerTestShmemSize.Scan/2048
[       OK ] GreedySchedulerTestShmemSize.Scan/2048 (1287 ms)
[----------] 6 tests from GreedySchedulerTestShmemSize (9005 ms total)

[----------] Global test environment tear-down
[==========] 18 tests from 3 test suites ran. (21205 ms total)
[  PASSED  ] 18 tests.
+ rv=0
+ for f in __tmp_*
+ [[ __tmp_* == __tmp_c* ]]
+ fs='__tmp_*'
++ echo '__tmp_*'
++ sed s/__tmp_/__tmp_c17_/g
+ ft='__tmp_c17_*'
+ mv '__tmp_*' '__tmp_c17_*'
mv: cannot stat '__tmp_*': No such file or directory
+ counter=18
+ for f in $test_files
+ [[ /opt/pytorch/nvfuser/bin/test_external_src == *matmul* ]]
+ [[ /opt/pytorch/nvfuser/bin/test_external_src == *multidevice* ]]
+ executable=/opt/pytorch/nvfuser/bin/test_external_src
++ basename /opt/pytorch/nvfuser/bin/test_external_src
+ report_name=TEST-report-test_external_src.xml
+ sed_flag=s/__tmp_/__tmp_c18_/g
+ /opt/pytorch/nvfuser/bin/test_external_src --gtest_output=xml:TEST-report-test_external_src.xml
+ sed -u s/__tmp_/__tmp_c18_/g
Running main() from /opt/pytorch/nvfuser/third_party/googletest/googletest/src/gtest_main.cc
Note: This is test shard 2 of 3.
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from ExternalSrcExample
[ RUN      ] ExternalSrcExample.Matmul_CUDA
[       OK ] ExternalSrcExample.Matmul_CUDA (109 ms)
[----------] 1 test from ExternalSrcExample (109 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test suite ran. (109 ms total)
[  PASSED  ] 1 test.
+ rv=0
+ for f in __tmp_*
+ [[ __tmp_* == __tmp_c* ]]
+ fs='__tmp_*'
++ echo '__tmp_*'
++ sed s/__tmp_/__tmp_c18_/g
+ ft='__tmp_c18_*'
+ mv '__tmp_*' '__tmp_c18_*'
mv: cannot stat '__tmp_*': No such file or directory
+ counter=19
+ mv '__tmp_*' /opt/pytorch/nvfuser
mv: cannot stat '__tmp_*': No such file or directory
+ mkdir -p /opt/pytorch/nvfuser/test/test-reports
+ mv TEST-report-test_argsort.xml TEST-report-test_cluster.xml TEST-report-test_external_src.xml TEST-report-test_greedy.xml TEST-report-test_host_ir.xml TEST-report-test_host_ir_jit.xml TEST-report-test_layout_op.xml TEST-report-test_matmul.xml TEST-report-test_moe.xml TEST-report-test_mps.xml TEST-report-test_multidevice.xml TEST-report-test_multidevice_tutorial.xml TEST-report-test_nvfuser.xml TEST-report-test_profiler.xml TEST-report-test_reshape.xml TEST-report-test_rng.xml TEST-report-test_scan.xml TEST-report-test_topk.xml TEST-report-test_tutorial.xml /opt/pytorch/nvfuser/test/test-reports
+ exit 0
