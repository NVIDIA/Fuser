op0: SchedulerNode(ComputedBuffer)
op0.writes = [MemoryDep('buf0', c0, {c0: 262144}, None)]
op0.unmet_dependencies = []
op0.met_dependencies = [MemoryDep('arg0_1', c0, {c0: 67108864}, None)]
op0.outputs = [
    buf0: ComputedBuffer
    buf0.layout = FixedLayout('cuda', torch.float16, size=[1024, 16, 16], stride=[256, 16, 1])
    buf0.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]
]
op0.group.device = cuda:0
op0.group.iteration = (262144, 256)
op0.sizes = ([262144], [256])
arg0_1_layout = FixedLayout('cuda', torch.float16, size=[1024, 16, 16, 256], stride=[65536, 4096, 256, 1])
buf0_layout = FixedLayout('cuda', torch.float16, size=[1024, 16, 16], stride=[256, 16, 1])
class op0_loop_body:
    var_ranges = {z0: 262144, z1: 256}
    index0 = 256*z0 + z1
    index1 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('arg0_1', get_index)
        reduction = ops.reduction(torch.float16, torch.float16, 'sum', load)
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf0', get_index_1, reduction)
        return store_reduction
op0 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.persistent_reduction(
        size_hints=[262144, 256],
        reduction_hint=ReductionHint.INNER,
        filename=__file__,
        triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=80, major=8, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=108), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': True, 'num_load': 1, 'num_reduction': 1, 'backend_hash': '7B49AAF3A75F29DE52C7456271F58F952C64B9DD540A4F3FC92DA148F5BB4A91', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False}
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
        xnumel = 262144
        XBLOCK: tl.constexpr = 1
        rnumel = 256
        RBLOCK: tl.constexpr = 256
        xoffset = tl.program_id(0) * XBLOCK
        xindex = tl.full([1], xoffset, tl.int32)
        xmask = tl.full([RBLOCK], True, tl.int1)
        rindex = tl.arange(0, RBLOCK)[:]
        roffset = 0
        rmask = tl.full([RBLOCK], True, tl.int1)
        r1 = rindex
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (r1 + (256*x0)), None).to(tl.float32)
        tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
        tmp3 = triton_helpers.promote_to_tensor(tl.sum(tmp1, 0))
        tl.store(out_ptr0 + (x0), tmp3, None)


