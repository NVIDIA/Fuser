{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is based on LitGPT:\n",
    "# https://github.com/Lightning-AI/litgpt/blob/main/litgpt/model.py\n",
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "import thunder\n",
    "import nvfuser\n",
    "\n",
    "\n",
    "# Config for Mistral 7B\n",
    "# https://github.com/Lightning-AI/litgpt/blob/e60f21a49a435efd215cb858200396f7b24baf17/litgpt/config.py#L1375-L1389\n",
    "@dataclass\n",
    "class Config:\n",
    "    name = \"Mistral-7B-v0.1\"\n",
    "    n_layer = 1  # NOTE: Use just one transformer block! Should be 32 for real 7B model\n",
    "    ### n_embd, intermediate_size, n_query_groups, n_head, head_size have impact on the matmul sizes\n",
    "    n_embd = 4096\n",
    "    intermediate_size = 14336\n",
    "    n_query_groups = 8\n",
    "    n_head = 32\n",
    "    head_size = 128\n",
    "    ###\n",
    "    norm_eps = 1e-05\n",
    "    bias = False\n",
    "    lm_head_bias = False\n",
    "    block_size = 4096\n",
    "    padded_vocab_size = 32000\n",
    "    rope_n_elem = head_size\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.lm_head = nn.Linear(\n",
    "            config.n_embd, config.padded_vocab_size, bias=config.lm_head_bias\n",
    "        )\n",
    "        self.wte = nn.Embedding(config.padded_vocab_size, config.n_embd)\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            Block(config) for _ in range(config.n_layer)\n",
    "        )\n",
    "        self.ln_f = RMSNorm(config.n_embd, eps=config.norm_eps)\n",
    "        self.max_seq_length = self.config.block_size\n",
    "\n",
    "        cos, sin = self.rope_cache()\n",
    "        self.register_buffer(\"cos\", cos, persistent=False)\n",
    "        self.register_buffer(\"sin\", sin, persistent=False)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        T = idx.shape[1]\n",
    "        if self.max_seq_length < T:\n",
    "            raise ValueError(\n",
    "                f\"Cannot forward sequence of length {T}, max seq length is only {self.max_seq_length}.\"\n",
    "            )\n",
    "\n",
    "        cos = self.cos[:T]\n",
    "        sin = self.sin[:T]\n",
    "\n",
    "        x = self.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
    "\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, cos, sin)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        return self.lm_head(x)  # (b, t, vocab_size)\n",
    "\n",
    "    def rope_cache(\n",
    "        self, device: Optional[torch.device] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return build_rope_cache(\n",
    "            seq_len=self.max_seq_length,\n",
    "            n_elem=self.config.rope_n_elem,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def get_num_params(self):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        return n_params\n",
    "\n",
    "\n",
    "def build_rope_cache(\n",
    "    seq_len: int,\n",
    "    n_elem: int,\n",
    "    device: Optional[torch.device] = None,\n",
    "    base: int = 10000,\n",
    "    condense_ratio: int = 1,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Enhanced Transformer with Rotary Position Embedding.\n",
    "    Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/\n",
    "    transformers/rope/__init__.py. MIT License:\n",
    "    https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.\n",
    "    \"\"\"\n",
    "    # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n",
    "    theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, device=device).float() / n_elem))\n",
    "    # Create position indexes `[0, 1, ..., seq_len - 1]`\n",
    "    seq_idx = torch.arange(seq_len, device=device) / condense_ratio\n",
    "    # Calculate the product of position index and $\\theta_i$\n",
    "    idx_theta = torch.outer(seq_idx, theta).repeat(1, 2)\n",
    "    return torch.cos(idx_theta), torch.sin(idx_theta)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_1 = RMSNorm(config.n_embd, eps=config.norm_eps)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.norm_2 = RMSNorm(config.n_embd, eps=config.norm_eps)\n",
    "        self.mlp = LLaMAMLP(config)\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        cos: torch.Tensor,\n",
    "        sin: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        x_normed = self.norm_1(x)\n",
    "        attention_output = self.attn(x_normed, cos, sin)\n",
    "        x = attention_output + x\n",
    "        x = self.mlp(self.norm_2(x)) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "def apply_rope(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
    "    head_size = x.size(-1)\n",
    "    x1 = x[..., : head_size // 2]  # (B, nh, T, hs/2)\n",
    "    x2 = x[..., head_size // 2 :]  # (B, nh, T, hs/2)\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)  # (B, nh, T, hs)\n",
    "    roped = (x * cos) + (rotated * sin)\n",
    "    return roped.to(dtype=x.dtype)\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        shape = (config.n_head + 2 * config.n_query_groups) * config.head_size\n",
    "\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.attn = nn.Linear(config.n_embd, shape, bias=config.bias)\n",
    "\n",
    "        # output projection\n",
    "        # if `head_size` is explicitly specified in the config, `n_emd` might not be equal to `head_size * n_head`\n",
    "        self.proj = nn.Linear(\n",
    "            config.head_size * config.n_head, config.n_embd, bias=config.bias\n",
    "        )\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        cos: torch.Tensor,\n",
    "        sin: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        (\n",
    "            B,\n",
    "            T,\n",
    "            C,\n",
    "        ) = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        qkv = self.attn(x)\n",
    "\n",
    "        # assemble into a number of query groups to support MHA, MQA and GQA together (see `config.n_query_groups`)\n",
    "        q_per_kv = self.config.n_head // self.config.n_query_groups\n",
    "        total_qkv = q_per_kv + 2  # each group has 1+ queries, 1 key, and 1 value\n",
    "        qkv = qkv.view(\n",
    "            B, T, self.config.n_query_groups, total_qkv, self.config.head_size\n",
    "        )\n",
    "        qkv = qkv.permute(0, 2, 3, 1, 4)  # (B, n_query_groups, total_qkv, T, hs)\n",
    "\n",
    "        # split batched computation into three\n",
    "        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)\n",
    "\n",
    "        # maybe repeat k and v if for the non multi-head attention cases\n",
    "        # training: flash attention requires it\n",
    "        if (\n",
    "            self.config.n_query_groups != self.config.n_head\n",
    "            and self.config.n_query_groups != 1\n",
    "        ):\n",
    "            k = k.expand(\n",
    "                B, self.config.n_query_groups, q_per_kv, T, self.config.head_size\n",
    "            )\n",
    "            v = v.expand(\n",
    "                B, self.config.n_query_groups, q_per_kv, T, self.config.head_size\n",
    "            )\n",
    "\n",
    "        q = q.reshape(B, -1, T, self.config.head_size)  # (B, nh_q, T, hs)\n",
    "        k = k.reshape(B, -1, T, self.config.head_size)  # (B, nh_k, T, hs)\n",
    "        v = v.reshape(B, -1, T, self.config.head_size)  # (B, nh_v, T, hs)\n",
    "        q_roped = apply_rope(q[..., : self.config.rope_n_elem], cos, sin)\n",
    "        k_roped = apply_rope(k[..., : self.config.rope_n_elem], cos, sin)\n",
    "        q = torch.cat((q_roped, q[..., self.config.rope_n_elem :]), dim=-1)\n",
    "        k = torch.cat((k_roped, k[..., self.config.rope_n_elem :]), dim=-1)\n",
    "\n",
    "        y = self.scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "        y = y.reshape(\n",
    "            B, T, self.config.head_size * self.config.n_head\n",
    "        )  # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        return self.proj(y)\n",
    "\n",
    "    # def scaled_dot_product_attention(\n",
    "    #     self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor\n",
    "    # ) -> torch.Tensor:\n",
    "    #     scale = 1.0 / math.sqrt(self.config.head_size)\n",
    "    #     y = torch.nn.functional.scaled_dot_product_attention(\n",
    "    #         q, k, v, dropout_p=0.0, scale=scale, is_causal=True\n",
    "    #     )\n",
    "    #     return y.transpose(1, 2)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, query, key, value, dropout_p=0.0) -> torch.Tensor:\n",
    "        scale = 1.0 / math.sqrt(self.config.head_size)\n",
    "        L, S = query.size(-2), key.size(-2)\n",
    "        # attn_bias = torch.zeros(L, S, dtype=query.dtype)\n",
    "        # temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
    "        # attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "        # attn_bias.to(query.dtype)\n",
    "        attn_weight = query @ key.transpose(-2, -1) * scale\n",
    "        # attn_weight += attn_bias\n",
    "        attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "        # attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "        return (attn_weight @ value).transpose(1, 2)\n",
    "\n",
    "\n",
    "class LLaMAMLP(nn.Module):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.fc_1 = nn.Linear(config.n_embd, config.intermediate_size, bias=config.bias)\n",
    "        self.fc_2 = nn.Linear(config.n_embd, config.intermediate_size, bias=config.bias)\n",
    "        self.proj = nn.Linear(config.intermediate_size, config.n_embd, bias=config.bias)\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_fc_1 = self.fc_1(x)\n",
    "        x_fc_2 = self.fc_2(x)\n",
    "        x = torch.nn.functional.silu(x_fc_1) * x_fc_2\n",
    "        return self.proj(x)\n",
    "\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization.\n",
    "    Derived from https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py. BSD 3-Clause License:\n",
    "    https://github.com/bzhangGo/rmsnorm/blob/master/LICENSE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size: int, dim: int = -1, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.ones(size))\n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        dtype = x.dtype\n",
    "        x = x.float()\n",
    "        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)\n",
    "        x_normed = x * torch.rsqrt(norm_x + self.eps)\n",
    "        x_normed = x_normed.to(dtype=dtype)\n",
    "        return x_normed * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "init_device = torch.device(\"cuda\")\n",
    "with init_device:\n",
    "    model = GPT(config).to(dtype=torch.bfloat16)\n",
    "\n",
    "input_shape = (BATCH_SIZE, config.block_size)\n",
    "x = torch.randint(\n",
    "    0, config.padded_vocab_size, input_shape, dtype=torch.int64, device=\"cuda\"\n",
    ")\n",
    "\n",
    "jit_model = thunder.jit(model, nv_enable_linear=True, nv_enable_matmul=True, nv_enable_bookend=False)\n",
    "\n",
    "out = jit_model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_trace = thunder.last_traces(jit_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Constructed by Delete Last Used (took 0 milliseconds)\n",
       "import torch\n",
       "import torch.nn.functional\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast\n",
       "def augmented_forward_fn(idx, tos1, t_lm_head_weight, t_ln_f_weight, t_sin, t_transformer_blocks_0_attn_attn_weight, t_transformer_blocks_0_attn_proj_weight, t_transformer_blocks_0_mlp_fc_1_weight, t_transformer_blocks_0_mlp_fc_2_weight, t_transformer_blocks_0_mlp_proj_weight, t_transformer_blocks_0_norm_1_weight, t_transformer_blocks_0_norm_2_weight, t_wte_weight):\n",
       "  # idx: \"cuda:0 i64[2, 4096]\"\n",
       "  # tos1: \"cuda:0 bf16[4096, 128]\"\n",
       "  # t_lm_head_weight: \"cuda:0 bf16[32000, 4096]\"\n",
       "  # t_ln_f_weight: \"cuda:0 bf16[4096]\"\n",
       "  # t_sin: \"cuda:0 bf16[4096, 128]\"\n",
       "  # t_transformer_blocks_0_attn_attn_weight: \"cuda:0 bf16[6144, 4096]\"\n",
       "  # t_transformer_blocks_0_attn_proj_weight: \"cuda:0 bf16[4096, 4096]\"\n",
       "  # t_transformer_blocks_0_mlp_fc_1_weight: \"cuda:0 bf16[14336, 4096]\"\n",
       "  # t_transformer_blocks_0_mlp_fc_2_weight: \"cuda:0 bf16[14336, 4096]\"\n",
       "  # t_transformer_blocks_0_mlp_proj_weight: \"cuda:0 bf16[4096, 14336]\"\n",
       "  # t_transformer_blocks_0_norm_1_weight: \"cuda:0 bf16[4096]\"\n",
       "  # t_transformer_blocks_0_norm_2_weight: \"cuda:0 bf16[4096]\"\n",
       "  # t_wte_weight: \"cuda:0 bf16[32000, 4096]\"\n",
       "  t4 = torch.nn.functional.embedding(idx, t_wte_weight, None, None, 2.0, False, False)  # t4: \"cuda:0 bf16[2, 4096, 4096]\"\n",
       "    # t4 = ltorch.embedding(idx, t_wte_weight, None, None, 2.0, False, False)  # t4: \"cuda:0 bf16[2, 4096, 4096]\"\n",
       "      # t176 = ltorch.reshape(idx, [8192])  # t176: \"cuda:0 i64[8192]\"\n",
       "        # t176 = prims.reshape(idx, (8192,))  # t176: \"cuda:0 i64[8192]\"\n",
       "      # t177 = prims.take(t_wte_weight, t176, 0)  # t177: \"cuda:0 bf16[8192, 4096]\"\n",
       "      # t4 = ltorch.reshape(t177, [2, 4096, 4096])  # t4: \"cuda:0 bf16[2, 4096, 4096]\"\n",
       "        # t4 = prims.reshape(t177, (2, 4096, 4096))  # t4: \"cuda:0 bf16[2, 4096, 4096]\"\n",
       "  [t109, t11, t127, t166, t175] = nvFusion0(t4, t_lm_head_weight, t_ln_f_weight, t_sin, t_transformer_blocks_0_attn_attn_weight, t_transformer_blocks_0_attn_proj_weight, t_transformer_blocks_0_mlp_fc_1_weight, t_transformer_blocks_0_mlp_fc_2_weight, t_transformer_blocks_0_mlp_proj_weight, t_transformer_blocks_0_norm_1_weight, t_transformer_blocks_0_norm_2_weight, tos1)\n",
       "    # t0 = prims.slice_prim(tos1, [0, 0], [4096, 128], [1, 1])  # t0: \"cuda:0 bf16[4096, 128]\"\n",
       "    # t1 = prims.slice_prim(t_sin, [0, 0], [4096, 128], [1, 1])  # t1: \"cuda:0 bf16[4096, 128]\"\n",
       "    # t5 = prims.convert_element_type(t4, dtypes.float32)  # t5: \"cuda:0 f32[2, 4096, 4096]\"\n",
       "    # t6 = prims.mul(t5, t5)  # t6: \"cuda:0 f32[2, 4096, 4096]\"\n",
       "    # t7 = prims.sum(t6, (2,))  # t7: \"cuda:0 f32[2, 4096]\"\n",
       "    # t8 = prims.broadcast_in_dim(t7, [2, 4096, 1], [0, 1])  # t8: \"cuda:0 f32[2, 4096, 1]\"\n",
       "    # t9 = prims.div(t8, 4096.0)  # t9: \"cuda:0 f32[2, 4096, 1]\"\n",
       "    # t10 = prims.add(t9, 1e-05)  # t10: \"cuda:0 f32[2, 4096, 1]\"\n",
       "    # t11 = prims.rsqrt(t10)  # t11: \"cuda:0 f32[2, 4096, 1]\"\n",
       "    # t12 = prims.broadcast_in_dim(t11, (2, 4096, 4096), (0, 1, 2))  # t12: \"cuda:0 f32[2, 4096, 4096]\"\n",
       "    # t13 = prims.mul(t5, t12)  # t13: \"cuda:0 f32[2, 4096, 4096]\"\n",
       "    # t15 = prims.broadcast_in_dim(t_transformer_blocks_0_norm_1_weight, (2, 4096, 4096), (2,))  # t15: \"cuda:0 bf16[2, 4096, 4096]\"\n",
       "    # t17 = prims.convert_element_type(t15, dtypes.float32)  # t17: \"cuda:0 f32[2, 4096, 4096]\"\n",
       "    # t18 = prims.mul(t13, t17)  # t18: \"cuda:0 f32[2, 4096, 4096]\"\n",
       "    # t19 = prims.convert_element_type(t18, dtypes.bfloat16)  # t19: \"cuda:0 bf16[2, 4096, 4096]\"\n",
       "    # t20 = prims.linear(t19, t_transformer_blocks_0_attn_attn_weight, None)  # t20: \"cuda:0 bf16[2, 4096, 6144]\"\n",
       "    # t21 = prims.reshape(t20, (2, 4096, 8, 6, 128))  # t21: \"cuda:0 bf16[2, 4096, 8, 6, 128]\"\n",
       "    # t22 = prims.transpose(t21, (0, 2, 3, 1, 4))  # t22: \"cuda:0 bf16[2, 8, 6, 4096, 128]\"\n",
       "    # t23 = prims.slice_prim(t22, [0, 0, 0, 0, 0], [2, 8, 4, 4096, 128], [1, 1, 1, 1, 1])  # t23: \"cuda:0 bf16[2, 8, 4, 4096, 128]\"\n",
       "    # t24 = prims.slice_prim(t22, [0, 0, 4, 0, 0], [2, 8, 5, 4096, 128], [1, 1, 1, 1, 1])  # t24: \"cuda:0 bf16[2, 8, 1, 4096, 128]\"\n",
       "    # t25 = prims.slice_prim(t22, [0, 0, 5, 0, 0], [2, 8, 6, 4096, 128], [1, 1, 1, 1, 1])  # t25: \"cuda:0 bf16[2, 8, 1, 4096, 128]\"\n",
       "    # t26 = prims.broadcast_in_dim(t24, (2, 8, 4, 4096, 128), (0, 1, 2, 3, 4))  # t26: \"cuda:0 bf16[2, 8, 4, 4096, 128]\"\n",
       "    # t32 = prims.broadcast_in_dim(t25, (2, 8, 4, 4096, 128), (0, 1, 2, 3, 4))  # t32: \"cuda:0 bf16[2, 8, 4, 4096, 128]\"\n",
       "    # t33 = prims.reshape(t23, (2, 32, 4096, 128))  # t33: \"cuda:0 bf16[2, 32, 4096, 128]\"\n",
       "    # t39 = prims.reshape(t26, (2, 32, 4096, 128))  # t39: \"cuda:0 bf16[2, 32, 4096, 128]\"\n",
       "    # t45 = prims.reshape(t32, (2, 32, 4096, 128))  # t45: \"cuda:0 bf16[2, 32, 4096, 128]\"\n",
       "    # t46 = prims.slice_prim(t33, [0, 0, 0, 0], [2, 32, 4096, 128], [1, 1, 1, 1])  # t46: \"cuda:0 bf16[2, 32, 4096, 128]\"\n",
       "    # t47 = prims.slice_prim(t46, [0, 0, 0, 0], [2, 32, 4096, 64], [1, 1, 1, 1])  # t47: \"cuda:0 bf16[2, 32, 4096, 64]\"\n",
       "    # t48 = prims.slice_prim(t46, [0, 0, 0, 64], [2, 32, 4096, 128], [1, 1, 1, 1])  # t48: \"cuda:0 bf16[2, 32, 4096, 64]\"\n",
       "    # t49 = prims.convert_element_type(t48, dtypes.float32)  # t49: \"cuda:0 f32[2, 32, 4096, 64]\"\n",
       "    # t50 = prims.neg(t49)  # t50: \"cuda:0 f32[2, 32, 4096, 64]\"\n",
       "    # t51 = prims.convert_element_type(t50, dtypes.bfloat16)  # t51: \"cuda:0 bf16[2, 32, 4096, 64]\"\n",
       "    # t52 = prims.cat((t51, t47), -1)  # t52: \"cuda:0 bf16[2, 32, 4096, 128]\"\n",
       "    # t53 = prims.broadcast_in_dim(t0, (2, 32, 4096, 128), (2, 3))  # t53: \"cuda:0 bf16[2, 32, 4096, 128]\"\n",
       "    # t54 = prims.convert_element_type(t46, dtypes.float32)  # t54: \"cuda:0 f32[2, 32, 4096, 128]\"\n",
       "    # t55 = prims.convert_element_type(t53, dtypes.float32)  # t55: \"cuda:0 f32[2, 32, 4096, 128]\"\n",
       "    # t56 = prims.mul(t54, t55)  # t56: \"cuda:0 f32[2, 32, 4096, 128]\"\n",
       "    # t58 = prims.broadcast_in_dim(t1, (2, 32, 4096, 128), (2, 3))  # t58: \"cuda:0 bf16[2, 32, 4096, 128]\"\n",
       "    # t59 = prims.convert_element_type(t52, dtypes.float32)  # t59: \"cuda:0 f32[2, 32, 4096, 128]\"\n",
       "    # t60 = prims.convert_element_type(t58, dtypes.float32)  # t60: \"cuda:0 f32[2, 32, 4096, 128]\"\n",
       "    # t61 = prims.mul(t59, t60)  # t61: \"cuda:0 f32[2, 32, 4096, 128]\"\n",
       "    # t65 = prims.add(t56, t61)  # t65: \"cuda:0 f32[2, 32, 4096, 128]\"\n",
       "    # t66 = prims.convert_element_type(t65, dtypes.bfloat16)  # t66: \"cuda:0 bf16[2, 32, 4096, 128]\"\n",
       "    # t67 = prims.slice_prim(t39, [0, 0, 0, 0], [2, 32, 4096, 128], [1, 1, 1, 1])  # t67: \"cuda:0 bf16[2, 32, 4096, 128]\"\n",
       "    # t68 = prims.slice_prim(t67, [0, 0, 0, 0], [2, 32, 4096, 64], [1, 1, 1, 1])  # t68: \"cuda:0 bf16[2, 32, 4096, 64]\"\n",
       "    # t69 = prims.slice_prim(t67, [0, 0, 0, 64], [2, 32, 4096, 128], [1, 1, 1, 1])  # t69: \"cuda:0 bf16[2, 32, 4096, 64]\"\n",
       "    # t70 = prims.convert_element_type(t69, dtypes.float32)  # t70: \"cuda:0 f32[2, 32, 4096, 64]\"\n",
       "    # t71 = prims.neg(t70)  # t71: \"cuda:0 f32[2, 32, 4096, 64]\"\n",
       "    # t72 = prims.convert_element_type(t71, dtypes.bfloat16)  # t72: \"cuda:0 bf16[2, 32, 4096, 64]\"\n",
       "    # t74 = prims.cat((t72, t68), -1)  # t74: \"cuda:0 bf16[2, 32, 4096, 128]\"\n",
       "    # t76 = prims.convert_element_type(t67, dtypes.float32)  # t76: \"cuda:0 f32[2, 32, 4096, 128]\"\n",
       "    # t78 = prims.mul(t76, t55)  # t78: \"cuda:0 f32[2, 32, 4096, 128]\"\n",
       "    # t81 = prims.convert_element_type(t74, dtypes.float32)  # t81: \"cuda:0 f32[2, 32, 4096, 128]\"\n",
       "    # t83 = prims.mul(t81, t60)  # t83: \"cuda:0 f32[2, 32, 4096, 128]\"\n",
       "    # t87 = prims.add(t78, t83)  # t87: \"cuda:0 f32[2, 32, 4096, 128]\"\n",
       "    # t88 = prims.convert_element_type(t87, dtypes.bfloat16)  # t88: \"cuda:0 bf16[2, 32, 4096, 128]\"\n",
       "    # t89 = prims.slice_prim(t33, [0, 0, 0, 0], [2, 32, 4096, 0], [1, 1, 1, 1])  # t89: \"cuda:0 bf16[2, 32, 4096, 0]\"\n",
       "    # t90 = prims.cat((t66, t89), -1)  # t90: \"cuda:0 bf16[2, 32, 4096, 128]\"\n",
       "    # t91 = prims.slice_prim(t39, [0, 0, 0, 0], [2, 32, 4096, 0], [1, 1, 1, 1])  # t91: \"cuda:0 bf16[2, 32, 4096, 0]\"\n",
       "    # t93 = prims.cat((t88, t91), -1)  # t93: \"cuda:0 bf16[2, 32, 4096, 128]\"\n",
       "    # t94 = prims.transpose(t93, (0, 1, 3, 2))  # t94: \"cuda:0 bf16[2, 32, 128, 4096]\"\n",
       "    # t95 = prims.matmul(t90, t94)  # t95: \"cuda:0 bf16[2, 32, 4096, 4096]\"\n",
       "    # t96 = prims.convert_element_type(t95, dtypes.float32)  # t96: \"cuda:0 f32[2, 32, 4096, 4096]\"\n",
       "    # t97 = prims.mul(t96, 0.08838834764831843)  # t97: \"cuda:0 f32[2, 32, 4096, 4096]\"\n",
       "    # t100 = prims.amax(t97, (3,))  # t100: \"cuda:0 f32[2, 32, 4096]\"\n",
       "    # t101 = prims.broadcast_in_dim(t100, [2, 32, 4096, 1], [0, 1, 2])  # t101: \"cuda:0 f32[2, 32, 4096, 1]\"\n",
       "    # t102 = prims.broadcast_in_dim(t101, (2, 32, 4096, 4096), (0, 1, 2, 3))  # t102: \"cuda:0 f32[2, 32, 4096, 4096]\"\n",
       "    # t103 = prims.sub(t97, t102)  # t103: \"cuda:0 f32[2, 32, 4096, 4096]\"\n",
       "    # t104 = prims.exp(t103)  # t104: \"cuda:0 f32[2, 32, 4096, 4096]\"\n",
       "    # t105 = prims.sum(t104, (3,))  # t105: \"cuda:0 f32[2, 32, 4096]\"\n",
       "    # t106 = prims.broadcast_in_dim(t105, [2, 32, 4096, 1], [0, 1, 2])  # t106: \"cuda:0 f32[2, 32, 4096, 1]\"\n",
       "    # t107 = prims.broadcast_in_dim(t106, (2, 32, 4096, 4096), (0, 1, 2, 3))  # t107: \"cuda:0 f32[2, 32, 4096, 4096]\"\n",
       "    # t108 = prims.div(t104, t107)  # t108: \"cuda:0 f32[2, 32, 4096, 4096]\"\n",
       "    # t109 = prims.convert_element_type(t108, dtypes.bfloat16)  # t109: \"cuda:0 bf16[2, 32, 4096, 4096]\"\n",
       "    # t110 = prims.matmul(t109, t45)  # t110: \"cuda:0 bf16[2, 32, 4096, 128]\"\n",
       "    # t111 = prims.transpose(t110, (0, 2, 1, 3))  # t111: \"cuda:0 bf16[2, 4096, 32, 128]\"\n",
       "    # t112 = prims.reshape(t111, (2, 4096, 4096))  # t112: \"cuda:0 bf16[2, 4096, 4096]\"\n",
       "    # t113 = prims.linear(t112, t_transformer_blocks_0_attn_proj_weight, None)  # t113: \"cuda:0 bf16[2, 4096, 4096]\"\n",
       "    # t114 = prims.convert_element_type(t113, dtypes.float32)  # t114: \"cuda:0 f32[2, 4096, 4096]\"\n",
       "    # t116 = prims.add(t114, t5)  # t116: \"cuda:0 f32[2, 4096, 4096]\"\n",
       "    # t119 = prims.mul(t116, t116)  # t119: \"cuda:0 f32[2, 4096, 4096]\"\n",
       "    # t121 = prims.sum(t119, (2,))  # t121: \"cuda:0 f32[2, 4096]\"\n",
       "    # t122 = prims.broadcast_in_dim(t121, [2, 4096, 1], [0, 1])  # t122: \"cuda:0 f32[2, 4096, 1]\"\n",
       "    # t124 = prims.div(t122, 4096.0)  # t124: \"cuda:0 f32[2, 4096, 1]\"\n",
       "    # t126 = prims.add(t124, 1e-05)  # t126: \"cuda:0 f32[2, 4096, 1]\"\n",
       "    # t127 = prims.rsqrt(t126)  # t127: \"cuda:0 f32[2, 4096, 1]\"\n",
       "    # t128 = prims.broadcast_in_dim(t127, (2, 4096, 4096), (0, 1, 2))  # t128: \"cuda:0 f32[2, 4096, 4096]\"\n",
       "    # t129 = prims.mul(t116, t128)  # t129: \"cuda:0 f32[2, 4096, 4096]\"\n",
       "    # t131 = prims.broadcast_in_dim(t_transformer_blocks_0_norm_2_weight, (2, 4096, 4096), (2,))  # t131: \"cuda:0 bf16[2, 4096, 4096]\"\n",
       "    # t133 = prims.convert_element_type(t131, dtypes.float32)  # t133: \"cuda:0 f32[2, 4096, 4096]\"\n",
       "    # t134 = prims.mul(t129, t133)  # t134: \"cuda:0 f32[2, 4096, 4096]\"\n",
       "    # t135 = prims.convert_element_type(t134, dtypes.bfloat16)  # t135: \"cuda:0 bf16[2, 4096, 4096]\"\n",
       "    # t136 = prims.linear(t135, t_transformer_blocks_0_mlp_fc_1_weight, None)  # t136: \"cuda:0 bf16[2, 4096, 14336]\"\n",
       "    # t137 = prims.linear(t135, t_transformer_blocks_0_mlp_fc_2_weight, None)  # t137: \"cuda:0 bf16[2, 4096, 14336]\"\n",
       "    # t138 = prims.convert_element_type(t136, dtypes.float32)  # t138: \"cuda:0 f32[2, 4096, 14336]\"\n",
       "    # t139 = prims.neg(t138)  # t139: \"cuda:0 f32[2, 4096, 14336]\"\n",
       "    # t140 = prims.exp(t139)  # t140: \"cuda:0 f32[2, 4096, 14336]\"\n",
       "    # t141 = prims.add(1.0, t140)  # t141: \"cuda:0 f32[2, 4096, 14336]\"\n",
       "    # t142 = prims.reciprocal(t141)  # t142: \"cuda:0 f32[2, 4096, 14336]\"\n",
       "    # t146 = prims.mul(t138, t142)  # t146: \"cuda:0 f32[2, 4096, 14336]\"\n",
       "    # t149 = prims.convert_element_type(t137, dtypes.float32)  # t149: \"cuda:0 f32[2, 4096, 14336]\"\n",
       "    # t150 = prims.mul(t146, t149)  # t150: \"cuda:0 f32[2, 4096, 14336]\"\n",
       "    # t151 = prims.convert_element_type(t150, dtypes.bfloat16)  # t151: \"cuda:0 bf16[2, 4096, 14336]\"\n",
       "    # t152 = prims.linear(t151, t_transformer_blocks_0_mlp_proj_weight, None)  # t152: \"cuda:0 bf16[2, 4096, 4096]\"\n",
       "    # t153 = prims.convert_element_type(t152, dtypes.float32)  # t153: \"cuda:0 f32[2, 4096, 4096]\"\n",
       "    # t155 = prims.add(t153, t116)  # t155: \"cuda:0 f32[2, 4096, 4096]\"\n",
       "    # t158 = prims.mul(t155, t155)  # t158: \"cuda:0 f32[2, 4096, 4096]\"\n",
       "    # t160 = prims.sum(t158, (2,))  # t160: \"cuda:0 f32[2, 4096]\"\n",
       "    # t161 = prims.broadcast_in_dim(t160, [2, 4096, 1], [0, 1])  # t161: \"cuda:0 f32[2, 4096, 1]\"\n",
       "    # t163 = prims.div(t161, 4096.0)  # t163: \"cuda:0 f32[2, 4096, 1]\"\n",
       "    # t165 = prims.add(t163, 1e-05)  # t165: \"cuda:0 f32[2, 4096, 1]\"\n",
       "    # t166 = prims.rsqrt(t165)  # t166: \"cuda:0 f32[2, 4096, 1]\"\n",
       "    # t167 = prims.broadcast_in_dim(t166, (2, 4096, 4096), (0, 1, 2))  # t167: \"cuda:0 f32[2, 4096, 4096]\"\n",
       "    # t168 = prims.mul(t155, t167)  # t168: \"cuda:0 f32[2, 4096, 4096]\"\n",
       "    # t170 = prims.broadcast_in_dim(t_ln_f_weight, (2, 4096, 4096), (2,))  # t170: \"cuda:0 bf16[2, 4096, 4096]\"\n",
       "    # t172 = prims.convert_element_type(t170, dtypes.float32)  # t172: \"cuda:0 f32[2, 4096, 4096]\"\n",
       "    # t173 = prims.mul(t168, t172)  # t173: \"cuda:0 f32[2, 4096, 4096]\"\n",
       "    # t174 = prims.convert_element_type(t173, dtypes.bfloat16)  # t174: \"cuda:0 bf16[2, 4096, 4096]\"\n",
       "    # t175 = prims.linear(t174, t_lm_head_weight, None)  # t175: \"cuda:0 bf16[2, 4096, 32000]\"\n",
       "  return {'output': t175, 'flat_args': [idx, tos1, t_lm_head_weight, t_ln_f_weight, t_sin, t_transformer_blocks_0_attn_attn_weight, t_transformer_blocks_0_attn_proj_weight, t_transformer_blocks_0_mlp_fc_1_weight, t_transformer_blocks_0_mlp_fc_2_weight, t_transformer_blocks_0_mlp_proj_weight, t_transformer_blocks_0_norm_1_weight, t_transformer_blocks_0_norm_2_weight, t_wte_weight], 'flat_output': (t175,)}, ((idx, t109, t11, t127, t166, t4, t_lm_head_weight, t_ln_f_weight, t_sin, t_transformer_blocks_0_attn_attn_weight, t_transformer_blocks_0_attn_proj_weight, t_transformer_blocks_0_mlp_fc_1_weight, t_transformer_blocks_0_mlp_fc_2_weight, t_transformer_blocks_0_mlp_proj_weight, t_transformer_blocks_0_norm_1_weight, t_transformer_blocks_0_norm_2_weight, tos1), (False, False, 0.08838834764831843, 4096.0, 4096.0, 4096.0, 32000, 2, -1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fwd_trace[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'torch.nn.functional': <module 'torch.nn.functional' from '/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py'>,\n",
       " 'torch': <module 'torch' from '/usr/local/lib/python3.10/dist-packages/torch/__init__.py'>,\n",
       " 'nvFusion0': FusionDefinitionWrapper(nvFusion0: (slice_prim, slice_prim, convert_element_type, mul, sum, broadcast_in_dim, div, add, rsqrt, broadcast_in_dim, mul, broadcast_in_dim, convert_element_type, mul, convert_element_type, linear, reshape, transpose, slice_prim, slice_prim, slice_prim, broadcast_in_dim, broadcast_in_dim, reshape, reshape, reshape, slice_prim, slice_prim, slice_prim, convert_element_type, neg, convert_element_type, cat, broadcast_in_dim, convert_element_type, convert_element_type, mul, broadcast_in_dim, convert_element_type, convert_element_type, mul, add, convert_element_type, slice_prim, slice_prim, slice_prim, convert_element_type, neg, convert_element_type, cat, convert_element_type, mul, convert_element_type, mul, add, convert_element_type, slice_prim, cat, slice_prim, cat, transpose, matmul, convert_element_type, mul, amax, broadcast_in_dim, broadcast_in_dim, sub, exp, sum, broadcast_in_dim, broadcast_in_dim, div, convert_element_type, matmul, transpose, reshape, linear, convert_element_type, add, mul, sum, broadcast_in_dim, div, add, rsqrt, broadcast_in_dim, mul, broadcast_in_dim, convert_element_type, mul, convert_element_type, linear, linear, convert_element_type, neg, exp, add, reciprocal, mul, convert_element_type, mul, convert_element_type, linear, convert_element_type, add, mul, sum, broadcast_in_dim, div, add, rsqrt, broadcast_in_dim, mul, broadcast_in_dim, convert_element_type, mul, convert_element_type, linear))}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fwd_trace[-1].python_ctx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
