// clang-format off
/*
 * SPDX-FileCopyrightText: Copyright (c) 2023-present NVIDIA CORPORATION & AFFILIATES.
 * All rights reserved.
 * SPDX-License-Identifier: BSD-3-Clause
 */
// clang-format on
//
#include <dispatch.h>
#include <python_frontend/translation.h>
#include <python_frontend/translation_utils.h>

#include <vector>

namespace nvfuser::python_frontend {

namespace {

// Given a CPP Fusion and an empty python_frontend FusionDefinition
// FusionTranslator adds the appropriate RecordFunctors corresponding to the
// CPP values and expressions.
//
// Rather than create a new FusionDefinition from the CPP Fusion, we add
// RecordFunctors to a blank FusionDefinition. This is a design decision because
// of the FusionDefinition python class, which inherits from the
// _C._FusionDefinition class created by pybind11. It is easier to operate on
// the child class directly than to create a new child instance from parent
// instance.
class FusionTranslator : public OptInConstDispatch {
 public:
  static void translate(Fusion* fusion, FusionDefinition* fd) {
    NVF_ERROR(
        !fd->completed(),
        "Expected an incomplete definition before fusion translation!");
    FusionTranslator translator(fusion, fd);
    translator.translate();
  }

 private:
  FusionTranslator(Fusion* fusion, FusionDefinition* fd)
      : fusion_(fusion), fd_(fd) {}

  // The new shape for view operation can be dynamic. Check that all dynamic
  // scalar dependencies are handled before the ViewOp.
  bool checkViewShapeDependency(const ViewOp* vop) {
    const std::vector<IterDomain*>& logical_out_domain =
        vop->out()->as<TensorView>()->domain()->logical();
    std::vector<Val*> logical_domain_extents;
    std::transform(
        logical_out_domain.begin(),
        logical_out_domain.end(),
        std::back_inserter(logical_domain_extents),
        [](IterDomain* id) { return id->extent(); });
    return std::all_of(
        logical_domain_extents.begin(),
        logical_domain_extents.end(),
        [&](Val* v) {
          return v->definition() == nullptr ||
              map_val_to_fd_index_.count(v) > 0;
        });
  }

  // Check that all of the expression's inputs are defined in FusionDefinition.
  bool checkExpressionDependencies(Expr* e) {
    bool check_view_dependency =
        !e->isA<ViewOp>() || checkViewShapeDependency(e->as<ViewOp>());
    return check_view_dependency &&
        std::all_of(e->inputs().begin(), e->inputs().end(), [&](const Val* v) {
             return map_val_to_fd_index_.count(v) > 0;
           });
  }

  void translate() {
    fd_->setupDefinition();

    // Add Fusion inputs to FusionDefinition
    for (nvfuser::Val* v : fusion_->inputs()) {
      OptOutConstDispatch::dispatch(v);
    }

    // Gather all expressions in CPP Fusion.
    std::deque<nvfuser::Expr*> to_visit = fusion_->deterministic_exprs();

    // Topological search of Fusion expressions
    size_t skip_count = 0;
    std::unordered_set<nvfuser::Expr*> visited;
    while (!to_visit.empty()) {
      Expr* e = to_visit.front();
      to_visit.pop_front();

      NVF_ERROR(
          skip_count <= to_visit.size(),
          "Cycle detected: None of the expressions can be processed!");

      // short-circuit: skip if already visited
      if (visited.count(e) > 0) {
        continue;
      }

      // Handle scalars and constants not generated by separate expression.
      std::vector<Val*> scalars;
      std::copy_if(
          e->inputs().begin(),
          e->inputs().end(),
          std::back_inserter(scalars),
          [](Val* v) { return v->isScalar(); });
      std::for_each(scalars.begin(), scalars.end(), [this](const Val* v) {
        OptOutConstDispatch::dispatch(v);
      });

      // short-circuit: add to back of stack if not all of the expression's
      // dependencies are satisfied.
      if (!checkExpressionDependencies(e)) {
        ++skip_count;
        to_visit.push_back(e);
        continue;
      }

      // Create RecordFunctor given inputs, outputs, and attributes.
      visited.insert(e);
      OptOutConstDispatch::dispatch(e);
      skip_count = 0;
    }

    // Outputs and Aliasing
    for (nvfuser::Val* v : fusion_->outputs()) {
      // TODO Add support for aliased outputs
      // Handle only TensorViews
      NVF_ERROR(v->isA<TensorView>());
      handleOutput(v->as<TensorView>());
    }

    fd_->finalizeDefinition();
  }

  // =================================================================================
  // Handle define_scalar, define_vector, define_tensor variants

  // Add scalar value to Fusion Definition
  void handle(const Val* v) final {
    // short-circuit: value already exists in FusionDefinition
    if (map_val_to_fd_index_.count(v) > 0) {
      return;
    }

    // short-circuit: scalar definition has a definition
    if (v->definition() != nullptr) {
      return;
    }

    // DataType::Index does not exist in python_frontend, so convert to
    // DataType::Int
    DataType scalar_dtype =
        (v->dtype() == DataType::Index) ? DataType::Int : v->dtype();

    Scalar output = fd_->defineScalar();
    fd_->defineRecord(new ScalarRecord(
        {fd_->recordingState(output())},
        v->value(),
        std::get<PrimDataType>(scalar_dtype.type)));
    map_val_to_fd_index_.emplace(v, output());
  }

  // Create python_frontend Vector from a vector of CPP scalar values.
  Vector createVector(std::vector<Val*> scalars) {
    // Add CPP values to Fusion Definition if necessary
    std::for_each(scalars.begin(), scalars.end(), [this](const Val* v) {
      OptOutConstDispatch::dispatch(v);
    });

    // Get corresponding index for CPP values
    std::vector<State> inputs;
    std::transform(
        scalars.begin(),
        scalars.end(),
        std::back_inserter(inputs),
        [&](Val* v) {
          return fd_->recordingState(map_val_to_fd_index_.at(v));
        });

    // NOTE There is not an equivalent CPP class for python-frontend vector,
    // so we do not add it to map_val_to_fd_index_.
    Vector output = fd_->defineVector(inputs.size());
    fd_->defineRecord(new VectorRecord(
        inputs, {fd_->recordingState(output())}, DataType::Int));
    return output;
  }

  // Add Tensor value to Fusion Definition
  void handle(const TensorView* tv) final {
    // short-circuit: value already exists in FusionDefinition
    if (map_val_to_fd_index_.count(tv) > 0) {
      return;
    }

    Tensor output = fd_->defineTensor(tv->nDims());

    std::vector<int64_t> shape;
    std::transform(
        tv->domain()->logical().begin(),
        tv->domain()->logical().end(),
        std::back_inserter(shape),
        [](IterDomain* id) {
          return (id->extent()->isConstScalar())
              ? id->extent()->evaluate().as<int64_t>()
              : -1;
        });

    fd_->defineRecord(new TensorRecord(
        {fd_->recordingState(output())},
        shape,
        tv->domain()->contiguity(),
        std::get<PrimDataType>(tv->dtype().type),
        tv->isCpuScalar(),
        tv->domain()->strideOrder()));

    map_val_to_fd_index_.emplace(tv, output());
  }

  // =================================================================================
  // Handle add_output variants

  // Add Tensor output to FusionDefinition
  void handleOutput(const TensorView* tv) {
    size_t output_index = map_val_to_fd_index_.at(tv);
    fd_->defineRecord(new OutputRecord<TensorView>(
        {fd_->recordingState(output_index)}, serde::RecordType::OutputTv));
  }

  // =================================================================================
  // Map CPP Expression classes to corresponding RecordFunctors in
  // python_frontend

  // Add Broadcast operation to FusionDefinition
  void handle(const BroadcastOp* bcast_op) final {
    Tensor output =
        fd_->defineTensor(bcast_op->out()->as<TensorView>()->nDims());
    fd_->defineRecord(new BroadcastOpRecord(
        {fd_->recordingState(map_val_to_fd_index_.at(bcast_op->in()))},
        {fd_->recordingState(output())},
        "ops.broadcast",
        bcast_op->getBroadcastDimFlags()));
    map_val_to_fd_index_.emplace(bcast_op->out(), output());
  }

  // A generic function to map UnaryOp, BinaryOp, and TernaryOp to
  // python_frontend OpRecord
  template <typename ExprType, typename ResultType, typename... ArgTypes>
  void handleOpRecord(
      const Expr* e,
      serde::RecordType record_type,
      ResultType result,
      ArgTypes... args) {
    NVF_ERROR(e->isA<ExprType>());
    std::vector<State> argument_states;
    std::transform(
        e->inputs().begin(),
        e->inputs().end(),
        std::back_inserter(argument_states),
        [&](auto arg) {
          return fd_->recordingState(map_val_to_fd_index_.at(arg));
        });

    fd_->defineRecord(new OpRecord<ResultType, ArgTypes...>(
        argument_states,
        {fd_->recordingState(map_val_to_fd_index_.at(result))},
        "ops." + getString(e->as<ExprType>()),
        record_type,
        getFunction<ResultType, ArgTypes...>(e->as<ExprType>())));
  }

  void handle(const UnaryOp* uop) final {
    // short-circuit: Handle cast operation separately
    if (uop->getUnaryOpType() == UnaryOpType::Cast) {
      return handleCastOp(uop);
    }

    // Map remaining UnaryOp to python_frontend OpRecord
    if (uop->in()->isA<TensorView>()) {
      Tensor output = fd_->defineTensor(uop->out()->as<TensorView>()->nDims());
      map_val_to_fd_index_.emplace(uop->out(), output());
      handleOpRecord<nvfuser::UnaryOp>(
          uop,
          serde::RecordType::Unary_TV,
          uop->out()->as<TensorView>(),
          uop->in()->as<TensorView>());
    } else {
      Scalar output = fd_->defineScalar();
      map_val_to_fd_index_.emplace(uop->out(), output());
      handleOpRecord<nvfuser::UnaryOp>(
          uop, serde::RecordType::Unary_VAL, uop->out(), uop->in());
    }
  }

  // If input and output values share the same type, a LoadStoreOp will be
  // created instead of a CastOp.
  void handle(const LoadStoreOp* lsop) final {
    handleCastOp(lsop);
  }

  // Map cast UnaryOp to CastOpRecord
  void handleCastOp(const Expr* op) {
    bool is_cast_op = op->isA<UnaryOp>() &&
        op->as<UnaryOp>()->getUnaryOpType() == UnaryOpType::Cast;
    NVF_ERROR(op->isA<LoadStoreOp>() || is_cast_op);

    size_t input_fd_index = map_val_to_fd_index_.at(op->input(0));

    if (op->input(0)->isA<TensorView>()) {
      Tensor output =
          fd_->defineTensor(op->output(0)->as<TensorView>()->nDims());
      map_val_to_fd_index_.emplace(op->output(0), output());
      fd_->defineRecord(new CastOpRecord<TensorView*, TensorView*>(
          {fd_->recordingState(input_fd_index)},
          {fd_->recordingState(output())},
          "ops.cast",
          serde::RecordType::CastTv,
          static_cast<TensorView* (*)(DataType, TensorView*)>(castOp),
          std::get<PrimDataType>(op->output(0)->dtype().type)));
    } else {
      Scalar output = fd_->defineScalar();
      map_val_to_fd_index_.emplace(op->output(0), output());
      fd_->defineRecord(new CastOpRecord<Val*, Val*>(
          {fd_->recordingState(input_fd_index)},
          {fd_->recordingState(output())},
          "ops.cast",
          serde::RecordType::CastVal,
          static_cast<Val* (*)(DataType, Val*)>(castOp),
          std::get<PrimDataType>(op->output(0)->dtype().type)));
    }
  }

  // Map BinaryOp to python_frontend OpRecord
  void handle(const BinaryOp* bop) final {
    bool is_lhs_tv = bop->lhs()->isA<TensorView>();
    bool is_rhs_tv = bop->rhs()->isA<TensorView>();

    if (is_lhs_tv || is_rhs_tv) {
      Tensor output = fd_->defineTensor(bop->out()->as<TensorView>()->nDims());
      map_val_to_fd_index_.emplace(bop->out(), output());

      if (is_lhs_tv && is_rhs_tv) {
        handleOpRecord<nvfuser::BinaryOp>(
            bop,
            serde::RecordType::Binary_TV,
            bop->out()->as<TensorView>(),
            bop->lhs()->as<TensorView>(),
            bop->rhs()->as<TensorView>());
      } else if (is_lhs_tv && !is_rhs_tv) {
        handleOpRecord<nvfuser::BinaryOp>(
            bop,
            serde::RecordType::Binary_TV_VAL,
            bop->out()->as<TensorView>(),
            bop->lhs()->as<TensorView>(),
            bop->rhs());
      } else {
        handleOpRecord<nvfuser::BinaryOp>(
            bop,
            serde::RecordType::Binary_VAL_TV,
            bop->out()->as<TensorView>(),
            bop->lhs(),
            bop->rhs()->as<TensorView>());
      }
    } else {
      Scalar output = fd_->defineScalar();
      map_val_to_fd_index_.emplace(bop->out(), output());
      handleOpRecord<nvfuser::BinaryOp>(
          bop,
          serde::RecordType::Binary_VAL,
          bop->out(),
          bop->lhs(),
          bop->rhs());
    }
  }

  // Map TernaryOp to python frontend
  void handle(const TernaryOp* top) final {
    bool is_in1_tv = top->in1()->isA<TensorView>();
    bool is_in2_tv = top->in2()->isA<TensorView>();
    bool is_in3_tv = top->in3()->isA<TensorView>();

    if (is_in1_tv || is_in2_tv || is_in3_tv) {
      Tensor output = fd_->defineTensor(top->out()->as<TensorView>()->nDims());
      map_val_to_fd_index_.emplace(top->out(), output());

      if (is_in1_tv && is_in2_tv && is_in3_tv) {
        handleOpRecord<nvfuser::TernaryOp>(
            top,
            serde::RecordType::Ternary_TV,
            top->out()->as<TensorView>(),
            top->in1()->as<TensorView>(),
            top->in2()->as<TensorView>(),
            top->in3()->as<TensorView>());
      } else if (is_in1_tv && is_in2_tv && !is_in3_tv) {
        handleOpRecord<nvfuser::TernaryOp>(
            top,
            serde::RecordType::Ternary_TV_TV_VAL,
            top->out()->as<TensorView>(),
            top->in1()->as<TensorView>(),
            top->in2()->as<TensorView>(),
            top->in3());
      } else if (is_in1_tv && !is_in2_tv && is_in3_tv) {
        handleOpRecord<nvfuser::TernaryOp>(
            top,
            serde::RecordType::Ternary_TV_VAL_TV,
            top->out()->as<TensorView>(),
            top->in1()->as<TensorView>(),
            top->in2(),
            top->in3()->as<TensorView>());
      } else if (is_in1_tv && !is_in2_tv && !is_in3_tv) {
        handleOpRecord<nvfuser::TernaryOp>(
            top,
            serde::RecordType::Ternary_TV_VAL_VAL,
            top->out()->as<TensorView>(),
            top->in1()->as<TensorView>(),
            top->in2(),
            top->in3());
      } else if (!is_in1_tv && is_in2_tv && is_in3_tv) {
        handleOpRecord<nvfuser::TernaryOp>(
            top,
            serde::RecordType::Ternary_VAL_TV_TV,
            top->out()->as<TensorView>(),
            top->in1(),
            top->in2()->as<TensorView>(),
            top->in3()->as<TensorView>());
      } else if (!is_in1_tv && is_in2_tv && !is_in3_tv) {
        handleOpRecord<nvfuser::TernaryOp>(
            top,
            serde::RecordType::Ternary_VAL_TV_VAL,
            top->out()->as<TensorView>(),
            top->in1(),
            top->in2()->as<TensorView>(),
            top->in3());
      } else if (!is_in1_tv && !is_in2_tv && is_in3_tv) {
        handleOpRecord<nvfuser::TernaryOp>(
            top,
            serde::RecordType::Ternary_VAL_VAL_TV,
            top->out()->as<TensorView>(),
            top->in1(),
            top->in2(),
            top->in3()->as<TensorView>());
      }
    } else {
      Scalar output = fd_->defineScalar();
      map_val_to_fd_index_.emplace(top->out(), output());
      handleOpRecord<nvfuser::TernaryOp>(
          top,
          serde::RecordType::Ternary_VAL,
          top->out(),
          top->in1(),
          top->in2(),
          top->in3());
    }
  }

  // Map ReductionOp to python frontend
  void handle(const ReductionOp* rop) final {
    TensorView* out_tv = rop->out()->as<TensorView>();

    std::vector<int64_t> axes;
    const std::vector<IterDomain*>& logical_domain =
        out_tv->domain()->logical();
    for (int64_t dim : c10::irange((int64_t)logical_domain.size())) {
      if (logical_domain.at(dim)->isReduction()) {
        axes.push_back(dim);
      }
    }

    Tensor output = fd_->defineTensor(out_tv->nDims());
    map_val_to_fd_index_.emplace(rop->out(), output());
    fd_->defineRecord(new ReductionOpRecord(
        {fd_->recordingState(map_val_to_fd_index_.at(rop->in()))},
        {fd_->recordingState(output())},
        "ops." + getString(rop),
        getSerdeType(rop),
        getFunction<
            TensorView*,
            TensorView*,
            const std::vector<int64_t>&,
            bool,
            DataType>(rop),
        axes,
        /*keep_dim=*/false,
        std::get<PrimDataType>(rop->out()->dtype().type)));
  }

  // Map SqueezeOp to python frontend
  void handle(const SqueezeOp* sop) final {
    std::vector<int64_t> squeeze_dims;
    const std::vector<bool>& is_squeeze_dims = sop->getSqueezeDimFlags();
    for (int64_t dim : c10::irange((int64_t)is_squeeze_dims.size())) {
      if (is_squeeze_dims.at(dim)) {
        squeeze_dims.push_back(dim);
      }
    }

    Tensor output = fd_->defineTensor(sop->out()->as<TensorView>()->nDims());
    map_val_to_fd_index_.emplace(sop->out(), output());
    fd_->defineRecord(new SqueezeOpRecord(
        {fd_->recordingState(map_val_to_fd_index_.at(sop->in()))},
        {fd_->recordingState(output())},
        squeeze_dims));
  }

  // Map ViewOp to python frontend
  void handle(const ViewOp* vop) final {
    // Get extent's for output's logical domain
    TensorView* out_tv = vop->out()->as<TensorView>();
    const std::vector<IterDomain*>& logical_out_domain =
        out_tv->domain()->logical();
    std::vector<Val*> logical_domain_extents;
    std::transform(
        logical_out_domain.begin(),
        logical_out_domain.end(),
        std::back_inserter(logical_domain_extents),
        [](IterDomain* id) { return id->extent(); });
    Vector new_shape = createVector(logical_domain_extents);

    Tensor output = fd_->defineTensor(out_tv->nDims());
    map_val_to_fd_index_.emplace(out_tv, output());
    fd_->defineRecord(new ReshapeOpRecord(
        {fd_->recordingState(map_val_to_fd_index_.at(vop->in())),
         fd_->recordingState(new_shape())},
        {fd_->recordingState(output())}));
  }

 private:
  //! The reference CPP fusion to be translated.
  Fusion* fusion_ = nullptr;
  //! The blank FusionDefinition that receives the RecordFunctors for
  //! translated CPP values and expressions.
  FusionDefinition* fd_ = nullptr;
  //! Map nvfuser Val to FusionDefinition index.
  std::unordered_map<const nvfuser::Val*, size_t> map_val_to_fd_index_;
};

} // namespace

void translate(Fusion* fusion, FusionDefinition* fd) {
  FusionTranslator::translate(fusion, fd);
}

} // namespace nvfuser::python_frontend
