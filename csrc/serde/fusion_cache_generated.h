// automatically generated by the FlatBuffers compiler, do not modify


#ifndef FLATBUFFERS_GENERATED_FUSIONCACHE_NVFUSER_SERDE_H_
#define FLATBUFFERS_GENERATED_FUSIONCACHE_NVFUSER_SERDE_H_

#include "flatbuffers/flatbuffers.h"

// Ensure the included flatbuffers.h is the same version as when this file was
// generated, otherwise it may not be compatible.
static_assert(FLATBUFFERS_VERSION_MAJOR == 23 &&
              FLATBUFFERS_VERSION_MINOR == 3 &&
              FLATBUFFERS_VERSION_REVISION == 3,
             "Non-compatible flatbuffers version included");

namespace nvfuser {
namespace serde {

struct State;

struct Scalar;
struct ScalarBuilder;

struct Symbolic;
struct SymbolicBuilder;

struct NamedScalar;
struct NamedScalarBuilder;

struct UnaryOp;
struct UnaryOpBuilder;

struct BinaryOp;
struct BinaryOpBuilder;

struct Split;
struct SplitBuilder;

struct Merge;
struct MergeBuilder;

struct Swizzle2D;
struct Swizzle2DBuilder;

struct Resize;
struct ResizeBuilder;

struct Instruction;
struct InstructionBuilder;

struct NaiveValueGenerator;
struct NaiveValueGeneratorBuilder;

struct IterationDomain;
struct IterationDomainBuilder;

struct Domain;
struct DomainBuilder;

struct SymbolicTensor;
struct SymbolicTensorBuilder;

struct AllocateBuffer;
struct AllocateBufferBuilder;

struct ScalarCpu;
struct ScalarCpuBuilder;

struct TensorArg;
struct TensorArgBuilder;

struct PolymorphicValue;
struct PolymorphicValueBuilder;

struct KernelArgumentHolder;
struct KernelArgumentHolderBuilder;

struct TensorShape;
struct TensorShapeBuilder;

struct LaunchParams;
struct LaunchParamsBuilder;

struct GlobalBufferInfo;
struct GlobalBufferInfoBuilder;

struct ExecutorEntry;
struct ExecutorEntryBuilder;

struct At;
struct AtBuilder;

struct BatchNorm;
struct BatchNormBuilder;

struct Broadcast;
struct BroadcastBuilder;

struct BroadcastInDim;
struct BroadcastInDimBuilder;

struct BroadcastInDimSymbolic;
struct BroadcastInDimSymbolicBuilder;

struct Dtype;
struct DtypeBuilder;

struct Dimension;
struct DimensionBuilder;

struct Norm;
struct NormBuilder;

struct Output;
struct OutputBuilder;

struct Pad;
struct PadBuilder;

struct Permute;
struct PermuteBuilder;

struct Reduction;
struct ReductionBuilder;

struct Reshape;
struct ReshapeBuilder;

struct Size;
struct SizeBuilder;

struct Slice;
struct SliceBuilder;

struct Squeeze;
struct SqueezeBuilder;

struct Tensor;
struct TensorBuilder;

struct TensorCreation;
struct TensorCreationBuilder;

struct TensorCreationSymbolic;
struct TensorCreationSymbolicBuilder;

struct Vector;
struct VectorBuilder;

struct KernelSummary;
struct KernelSummaryBuilder;

struct FusionExecutor;
struct FusionExecutorBuilder;

struct FusionKernelRuntime;
struct FusionKernelRuntimeBuilder;

struct EncodingEntry;

struct InputsIdLookup;
struct InputsIdLookupBuilder;

struct KernelRuntimeState;
struct KernelRuntimeStateBuilder;

struct FusionExecutorCache;
struct FusionExecutorCacheBuilder;

struct RecordFunctor;
struct RecordFunctorBuilder;

struct TrieNode;
struct TrieNodeBuilder;

struct FusionCache;
struct FusionCacheBuilder;

enum DataType : int32_t {
  DataType_None = 0,
  DataType_Double = 1,
  DataType_Float = 2,
  DataType_Half = 3,
  DataType_Int = 4,
  DataType_Int32 = 5,
  DataType_Bool = 6,
  DataType_BFloat16 = 7,
  DataType_ComplexFloat = 8,
  DataType_ComplexDouble = 9,
  DataType_MIN = DataType_None,
  DataType_MAX = DataType_ComplexDouble
};

inline const DataType (&EnumValuesDataType())[10] {
  static const DataType values[] = {
    DataType_None,
    DataType_Double,
    DataType_Float,
    DataType_Half,
    DataType_Int,
    DataType_Int32,
    DataType_Bool,
    DataType_BFloat16,
    DataType_ComplexFloat,
    DataType_ComplexDouble
  };
  return values;
}

inline const char * const *EnumNamesDataType() {
  static const char * const names[11] = {
    "None",
    "Double",
    "Float",
    "Half",
    "Int",
    "Int32",
    "Bool",
    "BFloat16",
    "ComplexFloat",
    "ComplexDouble",
    nullptr
  };
  return names;
}

inline const char *EnumNameDataType(DataType e) {
  if (::flatbuffers::IsOutRange(e, DataType_None, DataType_ComplexDouble)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesDataType()[index];
}

enum StateType : int32_t {
  StateType_None = 0,
  StateType_Scalar = 1,
  StateType_Vector = 2,
  StateType_Tensor = 3,
  StateType_MIN = StateType_None,
  StateType_MAX = StateType_Tensor
};

inline const StateType (&EnumValuesStateType())[4] {
  static const StateType values[] = {
    StateType_None,
    StateType_Scalar,
    StateType_Vector,
    StateType_Tensor
  };
  return values;
}

inline const char * const *EnumNamesStateType() {
  static const char * const names[5] = {
    "None",
    "Scalar",
    "Vector",
    "Tensor",
    nullptr
  };
  return names;
}

inline const char *EnumNameStateType(StateType e) {
  if (::flatbuffers::IsOutRange(e, StateType_None, StateType_Tensor)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesStateType()[index];
}

enum Contiguity : int32_t {
  Contiguity_None = 0,
  Contiguity_Strided = 1,
  Contiguity_Contiguous = 2,
  Contiguity_MIN = Contiguity_None,
  Contiguity_MAX = Contiguity_Contiguous
};

inline const Contiguity (&EnumValuesContiguity())[3] {
  static const Contiguity values[] = {
    Contiguity_None,
    Contiguity_Strided,
    Contiguity_Contiguous
  };
  return values;
}

inline const char * const *EnumNamesContiguity() {
  static const char * const names[4] = {
    "None",
    "Strided",
    "Contiguous",
    nullptr
  };
  return names;
}

inline const char *EnumNameContiguity(Contiguity e) {
  if (::flatbuffers::IsOutRange(e, Contiguity_None, Contiguity_Contiguous)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesContiguity()[index];
}

enum RecordType : int32_t {
  RecordType_Base = 0,
  RecordType_AtOp = 1,
  RecordType_BatchNormOp = 2,
  RecordType_BroadcastOp = 3,
  RecordType_BroadcastInDim = 4,
  RecordType_BroadcastInDimSymbolic = 5,
  RecordType_CastTv = 6,
  RecordType_CastVal = 7,
  RecordType_CatOp = 8,
  RecordType_End = 9,
  RecordType_FullOp = 10,
  RecordType_IotaOp = 11,
  RecordType_IndexSelectOp = 12,
  RecordType_TorchGatherOp = 13,
  RecordType_TakeAlongAxisOp = 14,
  RecordType_Unary_TV = 15,
  RecordType_Unary_VAL = 16,
  RecordType_Binary_TV = 17,
  RecordType_Binary_VAL = 18,
  RecordType_Binary_TV_VAL = 19,
  RecordType_Binary_VAL_TV = 20,
  RecordType_Ternary_TV = 21,
  RecordType_Ternary_VAL = 22,
  RecordType_Ternary_TV_TV_VAL = 23,
  RecordType_Ternary_TV_VAL_TV = 24,
  RecordType_Ternary_VAL_TV_TV = 25,
  RecordType_Ternary_VAL_VAL_TV = 26,
  RecordType_Ternary_TV_VAL_VAL = 27,
  RecordType_Ternary_VAL_TV_VAL = 28,
  RecordType_Ternary_Alpha_TV = 29,
  RecordType_Ternary_Alpha_VAL = 30,
  RecordType_Ternary_Alpha_TV_TV_VAL = 31,
  RecordType_Ternary_Alpha_TV_VAL_TV = 32,
  RecordType_Ternary_Alpha_VAL_TV_TV = 33,
  RecordType_Ternary_Alpha_VAL_VAL_TV = 34,
  RecordType_Ternary_Alpha_TV_VAL_VAL = 35,
  RecordType_Ternary_Alpha_VAL_TV_VAL = 36,
  RecordType_OutputTv = 37,
  RecordType_OutputVal = 38,
  RecordType_PadOp = 39,
  RecordType_PermuteOp = 40,
  RecordType_RandomOp = 41,
  RecordType_ReductionMax = 42,
  RecordType_ReductionMin = 43,
  RecordType_ReductionProd = 44,
  RecordType_ReductionSum = 45,
  RecordType_ReshapeOp = 46,
  RecordType_Scalar = 47,
  RecordType_ShapeOp = 48,
  RecordType_SizeOp = 49,
  RecordType_SliceOp = 50,
  RecordType_SqueezeOp = 51,
  RecordType_Start = 52,
  RecordType_Tensor = 53,
  RecordType_TensorSizes = 54,
  RecordType_VarianceOp = 55,
  RecordType_VarianceMeanOp = 56,
  RecordType_Vector = 57,
  RecordType_MIN = RecordType_Base,
  RecordType_MAX = RecordType_Vector
};

inline const RecordType (&EnumValuesRecordType())[58] {
  static const RecordType values[] = {
    RecordType_Base,
    RecordType_AtOp,
    RecordType_BatchNormOp,
    RecordType_BroadcastOp,
    RecordType_BroadcastInDim,
    RecordType_BroadcastInDimSymbolic,
    RecordType_CastTv,
    RecordType_CastVal,
    RecordType_CatOp,
    RecordType_End,
    RecordType_FullOp,
    RecordType_IotaOp,
    RecordType_IndexSelectOp,
    RecordType_TorchGatherOp,
    RecordType_TakeAlongAxisOp,
    RecordType_Unary_TV,
    RecordType_Unary_VAL,
    RecordType_Binary_TV,
    RecordType_Binary_VAL,
    RecordType_Binary_TV_VAL,
    RecordType_Binary_VAL_TV,
    RecordType_Ternary_TV,
    RecordType_Ternary_VAL,
    RecordType_Ternary_TV_TV_VAL,
    RecordType_Ternary_TV_VAL_TV,
    RecordType_Ternary_VAL_TV_TV,
    RecordType_Ternary_VAL_VAL_TV,
    RecordType_Ternary_TV_VAL_VAL,
    RecordType_Ternary_VAL_TV_VAL,
    RecordType_Ternary_Alpha_TV,
    RecordType_Ternary_Alpha_VAL,
    RecordType_Ternary_Alpha_TV_TV_VAL,
    RecordType_Ternary_Alpha_TV_VAL_TV,
    RecordType_Ternary_Alpha_VAL_TV_TV,
    RecordType_Ternary_Alpha_VAL_VAL_TV,
    RecordType_Ternary_Alpha_TV_VAL_VAL,
    RecordType_Ternary_Alpha_VAL_TV_VAL,
    RecordType_OutputTv,
    RecordType_OutputVal,
    RecordType_PadOp,
    RecordType_PermuteOp,
    RecordType_RandomOp,
    RecordType_ReductionMax,
    RecordType_ReductionMin,
    RecordType_ReductionProd,
    RecordType_ReductionSum,
    RecordType_ReshapeOp,
    RecordType_Scalar,
    RecordType_ShapeOp,
    RecordType_SizeOp,
    RecordType_SliceOp,
    RecordType_SqueezeOp,
    RecordType_Start,
    RecordType_Tensor,
    RecordType_TensorSizes,
    RecordType_VarianceOp,
    RecordType_VarianceMeanOp,
    RecordType_Vector
  };
  return values;
}

inline const char * const *EnumNamesRecordType() {
  static const char * const names[59] = {
    "Base",
    "AtOp",
    "BatchNormOp",
    "BroadcastOp",
    "BroadcastInDim",
    "BroadcastInDimSymbolic",
    "CastTv",
    "CastVal",
    "CatOp",
    "End",
    "FullOp",
    "IotaOp",
    "IndexSelectOp",
    "TorchGatherOp",
    "TakeAlongAxisOp",
    "Unary_TV",
    "Unary_VAL",
    "Binary_TV",
    "Binary_VAL",
    "Binary_TV_VAL",
    "Binary_VAL_TV",
    "Ternary_TV",
    "Ternary_VAL",
    "Ternary_TV_TV_VAL",
    "Ternary_TV_VAL_TV",
    "Ternary_VAL_TV_TV",
    "Ternary_VAL_VAL_TV",
    "Ternary_TV_VAL_VAL",
    "Ternary_VAL_TV_VAL",
    "Ternary_Alpha_TV",
    "Ternary_Alpha_VAL",
    "Ternary_Alpha_TV_TV_VAL",
    "Ternary_Alpha_TV_VAL_TV",
    "Ternary_Alpha_VAL_TV_TV",
    "Ternary_Alpha_VAL_VAL_TV",
    "Ternary_Alpha_TV_VAL_VAL",
    "Ternary_Alpha_VAL_TV_VAL",
    "OutputTv",
    "OutputVal",
    "PadOp",
    "PermuteOp",
    "RandomOp",
    "ReductionMax",
    "ReductionMin",
    "ReductionProd",
    "ReductionSum",
    "ReshapeOp",
    "Scalar",
    "ShapeOp",
    "SizeOp",
    "SliceOp",
    "SqueezeOp",
    "Start",
    "Tensor",
    "TensorSizes",
    "VarianceOp",
    "VarianceMeanOp",
    "Vector",
    nullptr
  };
  return names;
}

inline const char *EnumNameRecordType(RecordType e) {
  if (::flatbuffers::IsOutRange(e, RecordType_Base, RecordType_Vector)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesRecordType()[index];
}

enum UnaryOpType : int32_t {
  UnaryOpType_None = 0,
  UnaryOpType_Cast = 1,
  UnaryOpType_Neg = 2,
  UnaryOpType_MIN = UnaryOpType_None,
  UnaryOpType_MAX = UnaryOpType_Neg
};

inline const UnaryOpType (&EnumValuesUnaryOpType())[3] {
  static const UnaryOpType values[] = {
    UnaryOpType_None,
    UnaryOpType_Cast,
    UnaryOpType_Neg
  };
  return values;
}

inline const char * const *EnumNamesUnaryOpType() {
  static const char * const names[4] = {
    "None",
    "Cast",
    "Neg",
    nullptr
  };
  return names;
}

inline const char *EnumNameUnaryOpType(UnaryOpType e) {
  if (::flatbuffers::IsOutRange(e, UnaryOpType_None, UnaryOpType_Neg)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesUnaryOpType()[index];
}

enum BinaryOpType : int32_t {
  BinaryOpType_None = 0,
  BinaryOpType_Add = 1,
  BinaryOpType_CeilDiv = 2,
  BinaryOpType_Div = 3,
  BinaryOpType_Mod = 4,
  BinaryOpType_Mul = 5,
  BinaryOpType_Sub = 6,
  BinaryOpType_MIN = BinaryOpType_None,
  BinaryOpType_MAX = BinaryOpType_Sub
};

inline const BinaryOpType (&EnumValuesBinaryOpType())[7] {
  static const BinaryOpType values[] = {
    BinaryOpType_None,
    BinaryOpType_Add,
    BinaryOpType_CeilDiv,
    BinaryOpType_Div,
    BinaryOpType_Mod,
    BinaryOpType_Mul,
    BinaryOpType_Sub
  };
  return values;
}

inline const char * const *EnumNamesBinaryOpType() {
  static const char * const names[8] = {
    "None",
    "Add",
    "CeilDiv",
    "Div",
    "Mod",
    "Mul",
    "Sub",
    nullptr
  };
  return names;
}

inline const char *EnumNameBinaryOpType(BinaryOpType e) {
  if (::flatbuffers::IsOutRange(e, BinaryOpType_None, BinaryOpType_Sub)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesBinaryOpType()[index];
}

enum Swizzle2DType : int32_t {
  Swizzle2DType_None = 0,
  Swizzle2DType_ZShape = 1,
  Swizzle2DType_Xor = 2,
  Swizzle2DType_Shift = 3,
  Swizzle2DType_MIN = Swizzle2DType_None,
  Swizzle2DType_MAX = Swizzle2DType_Shift
};

inline const Swizzle2DType (&EnumValuesSwizzle2DType())[4] {
  static const Swizzle2DType values[] = {
    Swizzle2DType_None,
    Swizzle2DType_ZShape,
    Swizzle2DType_Xor,
    Swizzle2DType_Shift
  };
  return values;
}

inline const char * const *EnumNamesSwizzle2DType() {
  static const char * const names[5] = {
    "None",
    "ZShape",
    "Xor",
    "Shift",
    nullptr
  };
  return names;
}

inline const char *EnumNameSwizzle2DType(Swizzle2DType e) {
  if (::flatbuffers::IsOutRange(e, Swizzle2DType_None, Swizzle2DType_Shift)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesSwizzle2DType()[index];
}

enum SwizzleMode : int32_t {
  SwizzleMode_None = 0,
  SwizzleMode_Data = 1,
  SwizzleMode_Loop = 2,
  SwizzleMode_MIN = SwizzleMode_None,
  SwizzleMode_MAX = SwizzleMode_Loop
};

inline const SwizzleMode (&EnumValuesSwizzleMode())[3] {
  static const SwizzleMode values[] = {
    SwizzleMode_None,
    SwizzleMode_Data,
    SwizzleMode_Loop
  };
  return values;
}

inline const char * const *EnumNamesSwizzleMode() {
  static const char * const names[4] = {
    "None",
    "Data",
    "Loop",
    nullptr
  };
  return names;
}

inline const char *EnumNameSwizzleMode(SwizzleMode e) {
  if (::flatbuffers::IsOutRange(e, SwizzleMode_None, SwizzleMode_Loop)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesSwizzleMode()[index];
}

enum IterType : int32_t {
  IterType_Iteration = 0,
  IterType_Reduction = 1,
  IterType_Broadcast = 2,
  IterType_Gather = 3,
  IterType_Stride = 4,
  IterType_GatherScatter = 5,
  IterType_VectorComponent = 6,
  IterType_Symbolic = 7,
  IterType_MIN = IterType_Iteration,
  IterType_MAX = IterType_Symbolic
};

inline const IterType (&EnumValuesIterType())[8] {
  static const IterType values[] = {
    IterType_Iteration,
    IterType_Reduction,
    IterType_Broadcast,
    IterType_Gather,
    IterType_Stride,
    IterType_GatherScatter,
    IterType_VectorComponent,
    IterType_Symbolic
  };
  return values;
}

inline const char * const *EnumNamesIterType() {
  static const char * const names[9] = {
    "Iteration",
    "Reduction",
    "Broadcast",
    "Gather",
    "Stride",
    "GatherScatter",
    "VectorComponent",
    "Symbolic",
    nullptr
  };
  return names;
}

inline const char *EnumNameIterType(IterType e) {
  if (::flatbuffers::IsOutRange(e, IterType_Iteration, IterType_Symbolic)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesIterType()[index];
}

enum RecordData : uint8_t {
  RecordData_NONE = 0,
  RecordData_At = 1,
  RecordData_BatchNorm = 2,
  RecordData_Broadcast = 3,
  RecordData_BroadcastInDim = 4,
  RecordData_BroadcastInDimSymbolic = 5,
  RecordData_Dimension = 6,
  RecordData_Dtype = 7,
  RecordData_Norm = 8,
  RecordData_Output = 9,
  RecordData_Pad = 10,
  RecordData_Permute = 11,
  RecordData_Slice = 12,
  RecordData_Squeeze = 13,
  RecordData_Reduction = 14,
  RecordData_Reshape = 15,
  RecordData_Scalar = 16,
  RecordData_Size = 17,
  RecordData_Tensor = 18,
  RecordData_TensorCreation = 19,
  RecordData_TensorCreationSymbolic = 20,
  RecordData_Vector = 21,
  RecordData_MIN = RecordData_NONE,
  RecordData_MAX = RecordData_Vector
};

inline const RecordData (&EnumValuesRecordData())[22] {
  static const RecordData values[] = {
    RecordData_NONE,
    RecordData_At,
    RecordData_BatchNorm,
    RecordData_Broadcast,
    RecordData_BroadcastInDim,
    RecordData_BroadcastInDimSymbolic,
    RecordData_Dimension,
    RecordData_Dtype,
    RecordData_Norm,
    RecordData_Output,
    RecordData_Pad,
    RecordData_Permute,
    RecordData_Slice,
    RecordData_Squeeze,
    RecordData_Reduction,
    RecordData_Reshape,
    RecordData_Scalar,
    RecordData_Size,
    RecordData_Tensor,
    RecordData_TensorCreation,
    RecordData_TensorCreationSymbolic,
    RecordData_Vector
  };
  return values;
}

inline const char * const *EnumNamesRecordData() {
  static const char * const names[23] = {
    "NONE",
    "At",
    "BatchNorm",
    "Broadcast",
    "BroadcastInDim",
    "BroadcastInDimSymbolic",
    "Dimension",
    "Dtype",
    "Norm",
    "Output",
    "Pad",
    "Permute",
    "Slice",
    "Squeeze",
    "Reduction",
    "Reshape",
    "Scalar",
    "Size",
    "Tensor",
    "TensorCreation",
    "TensorCreationSymbolic",
    "Vector",
    nullptr
  };
  return names;
}

inline const char *EnumNameRecordData(RecordData e) {
  if (::flatbuffers::IsOutRange(e, RecordData_NONE, RecordData_Vector)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesRecordData()[index];
}

template<typename T> struct RecordDataTraits {
  static const RecordData enum_value = RecordData_NONE;
};

template<> struct RecordDataTraits<nvfuser::serde::At> {
  static const RecordData enum_value = RecordData_At;
};

template<> struct RecordDataTraits<nvfuser::serde::BatchNorm> {
  static const RecordData enum_value = RecordData_BatchNorm;
};

template<> struct RecordDataTraits<nvfuser::serde::Broadcast> {
  static const RecordData enum_value = RecordData_Broadcast;
};

template<> struct RecordDataTraits<nvfuser::serde::BroadcastInDim> {
  static const RecordData enum_value = RecordData_BroadcastInDim;
};

template<> struct RecordDataTraits<nvfuser::serde::BroadcastInDimSymbolic> {
  static const RecordData enum_value = RecordData_BroadcastInDimSymbolic;
};

template<> struct RecordDataTraits<nvfuser::serde::Dimension> {
  static const RecordData enum_value = RecordData_Dimension;
};

template<> struct RecordDataTraits<nvfuser::serde::Dtype> {
  static const RecordData enum_value = RecordData_Dtype;
};

template<> struct RecordDataTraits<nvfuser::serde::Norm> {
  static const RecordData enum_value = RecordData_Norm;
};

template<> struct RecordDataTraits<nvfuser::serde::Output> {
  static const RecordData enum_value = RecordData_Output;
};

template<> struct RecordDataTraits<nvfuser::serde::Pad> {
  static const RecordData enum_value = RecordData_Pad;
};

template<> struct RecordDataTraits<nvfuser::serde::Permute> {
  static const RecordData enum_value = RecordData_Permute;
};

template<> struct RecordDataTraits<nvfuser::serde::Slice> {
  static const RecordData enum_value = RecordData_Slice;
};

template<> struct RecordDataTraits<nvfuser::serde::Squeeze> {
  static const RecordData enum_value = RecordData_Squeeze;
};

template<> struct RecordDataTraits<nvfuser::serde::Reduction> {
  static const RecordData enum_value = RecordData_Reduction;
};

template<> struct RecordDataTraits<nvfuser::serde::Reshape> {
  static const RecordData enum_value = RecordData_Reshape;
};

template<> struct RecordDataTraits<nvfuser::serde::Scalar> {
  static const RecordData enum_value = RecordData_Scalar;
};

template<> struct RecordDataTraits<nvfuser::serde::Size> {
  static const RecordData enum_value = RecordData_Size;
};

template<> struct RecordDataTraits<nvfuser::serde::Tensor> {
  static const RecordData enum_value = RecordData_Tensor;
};

template<> struct RecordDataTraits<nvfuser::serde::TensorCreation> {
  static const RecordData enum_value = RecordData_TensorCreation;
};

template<> struct RecordDataTraits<nvfuser::serde::TensorCreationSymbolic> {
  static const RecordData enum_value = RecordData_TensorCreationSymbolic;
};

template<> struct RecordDataTraits<nvfuser::serde::Vector> {
  static const RecordData enum_value = RecordData_Vector;
};

bool VerifyRecordData(::flatbuffers::Verifier &verifier, const void *obj, RecordData type);
bool VerifyRecordDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<uint8_t> *types);

enum PolymorphicValueData : uint8_t {
  PolymorphicValueData_NONE = 0,
  PolymorphicValueData_Scalar = 1,
  PolymorphicValueData_ScalarCpu = 2,
  PolymorphicValueData_TensorArg = 3,
  PolymorphicValueData_MIN = PolymorphicValueData_NONE,
  PolymorphicValueData_MAX = PolymorphicValueData_TensorArg
};

inline const PolymorphicValueData (&EnumValuesPolymorphicValueData())[4] {
  static const PolymorphicValueData values[] = {
    PolymorphicValueData_NONE,
    PolymorphicValueData_Scalar,
    PolymorphicValueData_ScalarCpu,
    PolymorphicValueData_TensorArg
  };
  return values;
}

inline const char * const *EnumNamesPolymorphicValueData() {
  static const char * const names[5] = {
    "NONE",
    "Scalar",
    "ScalarCpu",
    "TensorArg",
    nullptr
  };
  return names;
}

inline const char *EnumNamePolymorphicValueData(PolymorphicValueData e) {
  if (::flatbuffers::IsOutRange(e, PolymorphicValueData_NONE, PolymorphicValueData_TensorArg)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesPolymorphicValueData()[index];
}

template<typename T> struct PolymorphicValueDataTraits {
  static const PolymorphicValueData enum_value = PolymorphicValueData_NONE;
};

template<> struct PolymorphicValueDataTraits<nvfuser::serde::Scalar> {
  static const PolymorphicValueData enum_value = PolymorphicValueData_Scalar;
};

template<> struct PolymorphicValueDataTraits<nvfuser::serde::ScalarCpu> {
  static const PolymorphicValueData enum_value = PolymorphicValueData_ScalarCpu;
};

template<> struct PolymorphicValueDataTraits<nvfuser::serde::TensorArg> {
  static const PolymorphicValueData enum_value = PolymorphicValueData_TensorArg;
};

bool VerifyPolymorphicValueData(::flatbuffers::Verifier &verifier, const void *obj, PolymorphicValueData type);
bool VerifyPolymorphicValueDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<uint8_t> *types);

enum InstructionData : uint8_t {
  InstructionData_NONE = 0,
  InstructionData_BinaryOp = 1,
  InstructionData_Symbolic = 2,
  InstructionData_Scalar = 3,
  InstructionData_Merge = 4,
  InstructionData_NamedScalar = 5,
  InstructionData_Resize = 6,
  InstructionData_Split = 7,
  InstructionData_Swizzle2D = 8,
  InstructionData_UnaryOp = 9,
  InstructionData_MIN = InstructionData_NONE,
  InstructionData_MAX = InstructionData_UnaryOp
};

inline const InstructionData (&EnumValuesInstructionData())[10] {
  static const InstructionData values[] = {
    InstructionData_NONE,
    InstructionData_BinaryOp,
    InstructionData_Symbolic,
    InstructionData_Scalar,
    InstructionData_Merge,
    InstructionData_NamedScalar,
    InstructionData_Resize,
    InstructionData_Split,
    InstructionData_Swizzle2D,
    InstructionData_UnaryOp
  };
  return values;
}

inline const char * const *EnumNamesInstructionData() {
  static const char * const names[11] = {
    "NONE",
    "BinaryOp",
    "Symbolic",
    "Scalar",
    "Merge",
    "NamedScalar",
    "Resize",
    "Split",
    "Swizzle2D",
    "UnaryOp",
    nullptr
  };
  return names;
}

inline const char *EnumNameInstructionData(InstructionData e) {
  if (::flatbuffers::IsOutRange(e, InstructionData_NONE, InstructionData_UnaryOp)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesInstructionData()[index];
}

template<typename T> struct InstructionDataTraits {
  static const InstructionData enum_value = InstructionData_NONE;
};

template<> struct InstructionDataTraits<nvfuser::serde::BinaryOp> {
  static const InstructionData enum_value = InstructionData_BinaryOp;
};

template<> struct InstructionDataTraits<nvfuser::serde::Symbolic> {
  static const InstructionData enum_value = InstructionData_Symbolic;
};

template<> struct InstructionDataTraits<nvfuser::serde::Scalar> {
  static const InstructionData enum_value = InstructionData_Scalar;
};

template<> struct InstructionDataTraits<nvfuser::serde::Merge> {
  static const InstructionData enum_value = InstructionData_Merge;
};

template<> struct InstructionDataTraits<nvfuser::serde::NamedScalar> {
  static const InstructionData enum_value = InstructionData_NamedScalar;
};

template<> struct InstructionDataTraits<nvfuser::serde::Resize> {
  static const InstructionData enum_value = InstructionData_Resize;
};

template<> struct InstructionDataTraits<nvfuser::serde::Split> {
  static const InstructionData enum_value = InstructionData_Split;
};

template<> struct InstructionDataTraits<nvfuser::serde::Swizzle2D> {
  static const InstructionData enum_value = InstructionData_Swizzle2D;
};

template<> struct InstructionDataTraits<nvfuser::serde::UnaryOp> {
  static const InstructionData enum_value = InstructionData_UnaryOp;
};

bool VerifyInstructionData(::flatbuffers::Verifier &verifier, const void *obj, InstructionData type);
bool VerifyInstructionDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<uint8_t> *types);

FLATBUFFERS_MANUALLY_ALIGNED_STRUCT(4) State FLATBUFFERS_FINAL_CLASS {
 private:
  int32_t index_;
  int32_t type_;

 public:
  State()
      : index_(0),
        type_(0) {
  }
  State(int32_t _index, nvfuser::serde::StateType _type)
      : index_(::flatbuffers::EndianScalar(_index)),
        type_(::flatbuffers::EndianScalar(static_cast<int32_t>(_type))) {
  }
  int32_t index() const {
    return ::flatbuffers::EndianScalar(index_);
  }
  nvfuser::serde::StateType type() const {
    return static_cast<nvfuser::serde::StateType>(::flatbuffers::EndianScalar(type_));
  }
};
FLATBUFFERS_STRUCT_END(State, 8);

FLATBUFFERS_MANUALLY_ALIGNED_STRUCT(8) EncodingEntry FLATBUFFERS_FINAL_CLASS {
 private:
  uint64_t id_;
  uint64_t lru_iter_;

 public:
  EncodingEntry()
      : id_(0),
        lru_iter_(0) {
  }
  EncodingEntry(uint64_t _id, uint64_t _lru_iter)
      : id_(::flatbuffers::EndianScalar(_id)),
        lru_iter_(::flatbuffers::EndianScalar(_lru_iter)) {
  }
  uint64_t id() const {
    return ::flatbuffers::EndianScalar(id_);
  }
  uint64_t lru_iter() const {
    return ::flatbuffers::EndianScalar(lru_iter_);
  }
};
FLATBUFFERS_STRUCT_END(EncodingEntry, 16);

struct Scalar FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ScalarBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DTYPE = 4,
    VT_HAS_VALUE = 6,
    VT_VALUE_TYPE = 8,
    VT_BOOL_VALUE = 10,
    VT_LONG_VALUE = 12,
    VT_DOUBLE_VALUE = 14,
    VT_REAL_VALUE = 16,
    VT_IMAG_VALUE = 18
  };
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool has_value() const {
    return GetField<uint8_t>(VT_HAS_VALUE, 0) != 0;
  }
  nvfuser::serde::DataType value_type() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_VALUE_TYPE, 0));
  }
  bool bool_value() const {
    return GetField<uint8_t>(VT_BOOL_VALUE, 0) != 0;
  }
  int64_t long_value() const {
    return GetField<int64_t>(VT_LONG_VALUE, 0);
  }
  double double_value() const {
    return GetField<double>(VT_DOUBLE_VALUE, 0.0);
  }
  double real_value() const {
    return GetField<double>(VT_REAL_VALUE, 0.0);
  }
  double imag_value() const {
    return GetField<double>(VT_IMAG_VALUE, 0.0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_HAS_VALUE, 1) &&
           VerifyField<int32_t>(verifier, VT_VALUE_TYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_BOOL_VALUE, 1) &&
           VerifyField<int64_t>(verifier, VT_LONG_VALUE, 8) &&
           VerifyField<double>(verifier, VT_DOUBLE_VALUE, 8) &&
           VerifyField<double>(verifier, VT_REAL_VALUE, 8) &&
           VerifyField<double>(verifier, VT_IMAG_VALUE, 8) &&
           verifier.EndTable();
  }
};

struct ScalarBuilder {
  typedef Scalar Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(Scalar::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  void add_has_value(bool has_value) {
    fbb_.AddElement<uint8_t>(Scalar::VT_HAS_VALUE, static_cast<uint8_t>(has_value), 0);
  }
  void add_value_type(nvfuser::serde::DataType value_type) {
    fbb_.AddElement<int32_t>(Scalar::VT_VALUE_TYPE, static_cast<int32_t>(value_type), 0);
  }
  void add_bool_value(bool bool_value) {
    fbb_.AddElement<uint8_t>(Scalar::VT_BOOL_VALUE, static_cast<uint8_t>(bool_value), 0);
  }
  void add_long_value(int64_t long_value) {
    fbb_.AddElement<int64_t>(Scalar::VT_LONG_VALUE, long_value, 0);
  }
  void add_double_value(double double_value) {
    fbb_.AddElement<double>(Scalar::VT_DOUBLE_VALUE, double_value, 0.0);
  }
  void add_real_value(double real_value) {
    fbb_.AddElement<double>(Scalar::VT_REAL_VALUE, real_value, 0.0);
  }
  void add_imag_value(double imag_value) {
    fbb_.AddElement<double>(Scalar::VT_IMAG_VALUE, imag_value, 0.0);
  }
  explicit ScalarBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Scalar> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Scalar>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Scalar> CreateScalar(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None,
    bool has_value = false,
    nvfuser::serde::DataType value_type = nvfuser::serde::DataType_None,
    bool bool_value = false,
    int64_t long_value = 0,
    double double_value = 0.0,
    double real_value = 0.0,
    double imag_value = 0.0) {
  ScalarBuilder builder_(_fbb);
  builder_.add_imag_value(imag_value);
  builder_.add_real_value(real_value);
  builder_.add_double_value(double_value);
  builder_.add_long_value(long_value);
  builder_.add_value_type(value_type);
  builder_.add_dtype(dtype);
  builder_.add_bool_value(bool_value);
  builder_.add_has_value(has_value);
  return builder_.Finish();
}

struct Symbolic FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SymbolicBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SRC0 = 4,
    VT_NAME = 6
  };
  int64_t src0() const {
    return GetField<int64_t>(VT_SRC0, 0);
  }
  const ::flatbuffers::String *name() const {
    return GetPointer<const ::flatbuffers::String *>(VT_NAME);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_SRC0, 8) &&
           VerifyOffset(verifier, VT_NAME) &&
           verifier.VerifyString(name()) &&
           verifier.EndTable();
  }
};

struct SymbolicBuilder {
  typedef Symbolic Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_src0(int64_t src0) {
    fbb_.AddElement<int64_t>(Symbolic::VT_SRC0, src0, 0);
  }
  void add_name(::flatbuffers::Offset<::flatbuffers::String> name) {
    fbb_.AddOffset(Symbolic::VT_NAME, name);
  }
  explicit SymbolicBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Symbolic> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Symbolic>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Symbolic> CreateSymbolic(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t src0 = 0,
    ::flatbuffers::Offset<::flatbuffers::String> name = 0) {
  SymbolicBuilder builder_(_fbb);
  builder_.add_src0(src0);
  builder_.add_name(name);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Symbolic> CreateSymbolicDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t src0 = 0,
    const char *name = nullptr) {
  auto name__ = name ? _fbb.CreateString(name) : 0;
  return nvfuser::serde::CreateSymbolic(
      _fbb,
      src0,
      name__);
}

struct NamedScalar FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef NamedScalarBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_NAME = 4
  };
  const ::flatbuffers::String *name() const {
    return GetPointer<const ::flatbuffers::String *>(VT_NAME);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_NAME) &&
           verifier.VerifyString(name()) &&
           verifier.EndTable();
  }
};

struct NamedScalarBuilder {
  typedef NamedScalar Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_name(::flatbuffers::Offset<::flatbuffers::String> name) {
    fbb_.AddOffset(NamedScalar::VT_NAME, name);
  }
  explicit NamedScalarBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<NamedScalar> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<NamedScalar>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<NamedScalar> CreateNamedScalar(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::String> name = 0) {
  NamedScalarBuilder builder_(_fbb);
  builder_.add_name(name);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<NamedScalar> CreateNamedScalarDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const char *name = nullptr) {
  auto name__ = name ? _fbb.CreateString(name) : 0;
  return nvfuser::serde::CreateNamedScalar(
      _fbb,
      name__);
}

struct UnaryOp FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef UnaryOpBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_UNARY_TYPE = 4,
    VT_DATA_TYPE = 6,
    VT_SRC0 = 8,
    VT_DEST = 10,
    VT_NAME = 12
  };
  nvfuser::serde::UnaryOpType unary_type() const {
    return static_cast<nvfuser::serde::UnaryOpType>(GetField<int32_t>(VT_UNARY_TYPE, 0));
  }
  nvfuser::serde::DataType data_type() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DATA_TYPE, 0));
  }
  int64_t src0() const {
    return GetField<int64_t>(VT_SRC0, 0);
  }
  int64_t dest() const {
    return GetField<int64_t>(VT_DEST, 0);
  }
  const ::flatbuffers::String *name() const {
    return GetPointer<const ::flatbuffers::String *>(VT_NAME);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_UNARY_TYPE, 4) &&
           VerifyField<int32_t>(verifier, VT_DATA_TYPE, 4) &&
           VerifyField<int64_t>(verifier, VT_SRC0, 8) &&
           VerifyField<int64_t>(verifier, VT_DEST, 8) &&
           VerifyOffset(verifier, VT_NAME) &&
           verifier.VerifyString(name()) &&
           verifier.EndTable();
  }
};

struct UnaryOpBuilder {
  typedef UnaryOp Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_unary_type(nvfuser::serde::UnaryOpType unary_type) {
    fbb_.AddElement<int32_t>(UnaryOp::VT_UNARY_TYPE, static_cast<int32_t>(unary_type), 0);
  }
  void add_data_type(nvfuser::serde::DataType data_type) {
    fbb_.AddElement<int32_t>(UnaryOp::VT_DATA_TYPE, static_cast<int32_t>(data_type), 0);
  }
  void add_src0(int64_t src0) {
    fbb_.AddElement<int64_t>(UnaryOp::VT_SRC0, src0, 0);
  }
  void add_dest(int64_t dest) {
    fbb_.AddElement<int64_t>(UnaryOp::VT_DEST, dest, 0);
  }
  void add_name(::flatbuffers::Offset<::flatbuffers::String> name) {
    fbb_.AddOffset(UnaryOp::VT_NAME, name);
  }
  explicit UnaryOpBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<UnaryOp> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<UnaryOp>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<UnaryOp> CreateUnaryOp(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::UnaryOpType unary_type = nvfuser::serde::UnaryOpType_None,
    nvfuser::serde::DataType data_type = nvfuser::serde::DataType_None,
    int64_t src0 = 0,
    int64_t dest = 0,
    ::flatbuffers::Offset<::flatbuffers::String> name = 0) {
  UnaryOpBuilder builder_(_fbb);
  builder_.add_dest(dest);
  builder_.add_src0(src0);
  builder_.add_name(name);
  builder_.add_data_type(data_type);
  builder_.add_unary_type(unary_type);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<UnaryOp> CreateUnaryOpDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::UnaryOpType unary_type = nvfuser::serde::UnaryOpType_None,
    nvfuser::serde::DataType data_type = nvfuser::serde::DataType_None,
    int64_t src0 = 0,
    int64_t dest = 0,
    const char *name = nullptr) {
  auto name__ = name ? _fbb.CreateString(name) : 0;
  return nvfuser::serde::CreateUnaryOp(
      _fbb,
      unary_type,
      data_type,
      src0,
      dest,
      name__);
}

struct BinaryOp FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef BinaryOpBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_BINARY_TYPE = 4,
    VT_SRC0 = 6,
    VT_SRC1 = 8,
    VT_DEST = 10,
    VT_NAME = 12
  };
  nvfuser::serde::BinaryOpType binary_type() const {
    return static_cast<nvfuser::serde::BinaryOpType>(GetField<int32_t>(VT_BINARY_TYPE, 0));
  }
  int64_t src0() const {
    return GetField<int64_t>(VT_SRC0, 0);
  }
  int64_t src1() const {
    return GetField<int64_t>(VT_SRC1, 0);
  }
  int64_t dest() const {
    return GetField<int64_t>(VT_DEST, 0);
  }
  const ::flatbuffers::String *name() const {
    return GetPointer<const ::flatbuffers::String *>(VT_NAME);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_BINARY_TYPE, 4) &&
           VerifyField<int64_t>(verifier, VT_SRC0, 8) &&
           VerifyField<int64_t>(verifier, VT_SRC1, 8) &&
           VerifyField<int64_t>(verifier, VT_DEST, 8) &&
           VerifyOffset(verifier, VT_NAME) &&
           verifier.VerifyString(name()) &&
           verifier.EndTable();
  }
};

struct BinaryOpBuilder {
  typedef BinaryOp Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_binary_type(nvfuser::serde::BinaryOpType binary_type) {
    fbb_.AddElement<int32_t>(BinaryOp::VT_BINARY_TYPE, static_cast<int32_t>(binary_type), 0);
  }
  void add_src0(int64_t src0) {
    fbb_.AddElement<int64_t>(BinaryOp::VT_SRC0, src0, 0);
  }
  void add_src1(int64_t src1) {
    fbb_.AddElement<int64_t>(BinaryOp::VT_SRC1, src1, 0);
  }
  void add_dest(int64_t dest) {
    fbb_.AddElement<int64_t>(BinaryOp::VT_DEST, dest, 0);
  }
  void add_name(::flatbuffers::Offset<::flatbuffers::String> name) {
    fbb_.AddOffset(BinaryOp::VT_NAME, name);
  }
  explicit BinaryOpBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<BinaryOp> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<BinaryOp>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<BinaryOp> CreateBinaryOp(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::BinaryOpType binary_type = nvfuser::serde::BinaryOpType_None,
    int64_t src0 = 0,
    int64_t src1 = 0,
    int64_t dest = 0,
    ::flatbuffers::Offset<::flatbuffers::String> name = 0) {
  BinaryOpBuilder builder_(_fbb);
  builder_.add_dest(dest);
  builder_.add_src1(src1);
  builder_.add_src0(src0);
  builder_.add_name(name);
  builder_.add_binary_type(binary_type);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<BinaryOp> CreateBinaryOpDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::BinaryOpType binary_type = nvfuser::serde::BinaryOpType_None,
    int64_t src0 = 0,
    int64_t src1 = 0,
    int64_t dest = 0,
    const char *name = nullptr) {
  auto name__ = name ? _fbb.CreateString(name) : 0;
  return nvfuser::serde::CreateBinaryOp(
      _fbb,
      binary_type,
      src0,
      src1,
      dest,
      name__);
}

struct Split FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SplitBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_IN = 4,
    VT_FACTOR = 6,
    VT_INNER = 8,
    VT_OUTER = 10,
    VT_INNER_SPLIT = 12,
    VT_TRIM_OUT_OF_BOUNDS = 14
  };
  int64_t in() const {
    return GetField<int64_t>(VT_IN, 0);
  }
  int64_t factor() const {
    return GetField<int64_t>(VT_FACTOR, 0);
  }
  int64_t inner() const {
    return GetField<int64_t>(VT_INNER, 0);
  }
  int64_t outer() const {
    return GetField<int64_t>(VT_OUTER, 0);
  }
  bool inner_split() const {
    return GetField<uint8_t>(VT_INNER_SPLIT, 0) != 0;
  }
  bool trim_out_of_bounds() const {
    return GetField<uint8_t>(VT_TRIM_OUT_OF_BOUNDS, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_IN, 8) &&
           VerifyField<int64_t>(verifier, VT_FACTOR, 8) &&
           VerifyField<int64_t>(verifier, VT_INNER, 8) &&
           VerifyField<int64_t>(verifier, VT_OUTER, 8) &&
           VerifyField<uint8_t>(verifier, VT_INNER_SPLIT, 1) &&
           VerifyField<uint8_t>(verifier, VT_TRIM_OUT_OF_BOUNDS, 1) &&
           verifier.EndTable();
  }
};

struct SplitBuilder {
  typedef Split Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_in(int64_t in) {
    fbb_.AddElement<int64_t>(Split::VT_IN, in, 0);
  }
  void add_factor(int64_t factor) {
    fbb_.AddElement<int64_t>(Split::VT_FACTOR, factor, 0);
  }
  void add_inner(int64_t inner) {
    fbb_.AddElement<int64_t>(Split::VT_INNER, inner, 0);
  }
  void add_outer(int64_t outer) {
    fbb_.AddElement<int64_t>(Split::VT_OUTER, outer, 0);
  }
  void add_inner_split(bool inner_split) {
    fbb_.AddElement<uint8_t>(Split::VT_INNER_SPLIT, static_cast<uint8_t>(inner_split), 0);
  }
  void add_trim_out_of_bounds(bool trim_out_of_bounds) {
    fbb_.AddElement<uint8_t>(Split::VT_TRIM_OUT_OF_BOUNDS, static_cast<uint8_t>(trim_out_of_bounds), 0);
  }
  explicit SplitBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Split> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Split>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Split> CreateSplit(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t in = 0,
    int64_t factor = 0,
    int64_t inner = 0,
    int64_t outer = 0,
    bool inner_split = false,
    bool trim_out_of_bounds = false) {
  SplitBuilder builder_(_fbb);
  builder_.add_outer(outer);
  builder_.add_inner(inner);
  builder_.add_factor(factor);
  builder_.add_in(in);
  builder_.add_trim_out_of_bounds(trim_out_of_bounds);
  builder_.add_inner_split(inner_split);
  return builder_.Finish();
}

struct Merge FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef MergeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INNER = 4,
    VT_OUTER = 6,
    VT_OUT = 8
  };
  int64_t inner() const {
    return GetField<int64_t>(VT_INNER, 0);
  }
  int64_t outer() const {
    return GetField<int64_t>(VT_OUTER, 0);
  }
  int64_t out() const {
    return GetField<int64_t>(VT_OUT, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_INNER, 8) &&
           VerifyField<int64_t>(verifier, VT_OUTER, 8) &&
           VerifyField<int64_t>(verifier, VT_OUT, 8) &&
           verifier.EndTable();
  }
};

struct MergeBuilder {
  typedef Merge Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_inner(int64_t inner) {
    fbb_.AddElement<int64_t>(Merge::VT_INNER, inner, 0);
  }
  void add_outer(int64_t outer) {
    fbb_.AddElement<int64_t>(Merge::VT_OUTER, outer, 0);
  }
  void add_out(int64_t out) {
    fbb_.AddElement<int64_t>(Merge::VT_OUT, out, 0);
  }
  explicit MergeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Merge> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Merge>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Merge> CreateMerge(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t inner = 0,
    int64_t outer = 0,
    int64_t out = 0) {
  MergeBuilder builder_(_fbb);
  builder_.add_out(out);
  builder_.add_outer(outer);
  builder_.add_inner(inner);
  return builder_.Finish();
}

struct Swizzle2D FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef Swizzle2DBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_IN_X = 4,
    VT_IN_Y = 6,
    VT_SWIZZLE_TYPE = 8,
    VT_SWIZZLE_MODE = 10,
    VT_OUT_X = 12,
    VT_OUT_Y = 14
  };
  int64_t in_x() const {
    return GetField<int64_t>(VT_IN_X, 0);
  }
  int64_t in_y() const {
    return GetField<int64_t>(VT_IN_Y, 0);
  }
  nvfuser::serde::Swizzle2DType swizzle_type() const {
    return static_cast<nvfuser::serde::Swizzle2DType>(GetField<int32_t>(VT_SWIZZLE_TYPE, 0));
  }
  nvfuser::serde::SwizzleMode swizzle_mode() const {
    return static_cast<nvfuser::serde::SwizzleMode>(GetField<int32_t>(VT_SWIZZLE_MODE, 0));
  }
  int64_t out_x() const {
    return GetField<int64_t>(VT_OUT_X, 0);
  }
  int64_t out_y() const {
    return GetField<int64_t>(VT_OUT_Y, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_IN_X, 8) &&
           VerifyField<int64_t>(verifier, VT_IN_Y, 8) &&
           VerifyField<int32_t>(verifier, VT_SWIZZLE_TYPE, 4) &&
           VerifyField<int32_t>(verifier, VT_SWIZZLE_MODE, 4) &&
           VerifyField<int64_t>(verifier, VT_OUT_X, 8) &&
           VerifyField<int64_t>(verifier, VT_OUT_Y, 8) &&
           verifier.EndTable();
  }
};

struct Swizzle2DBuilder {
  typedef Swizzle2D Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_in_x(int64_t in_x) {
    fbb_.AddElement<int64_t>(Swizzle2D::VT_IN_X, in_x, 0);
  }
  void add_in_y(int64_t in_y) {
    fbb_.AddElement<int64_t>(Swizzle2D::VT_IN_Y, in_y, 0);
  }
  void add_swizzle_type(nvfuser::serde::Swizzle2DType swizzle_type) {
    fbb_.AddElement<int32_t>(Swizzle2D::VT_SWIZZLE_TYPE, static_cast<int32_t>(swizzle_type), 0);
  }
  void add_swizzle_mode(nvfuser::serde::SwizzleMode swizzle_mode) {
    fbb_.AddElement<int32_t>(Swizzle2D::VT_SWIZZLE_MODE, static_cast<int32_t>(swizzle_mode), 0);
  }
  void add_out_x(int64_t out_x) {
    fbb_.AddElement<int64_t>(Swizzle2D::VT_OUT_X, out_x, 0);
  }
  void add_out_y(int64_t out_y) {
    fbb_.AddElement<int64_t>(Swizzle2D::VT_OUT_Y, out_y, 0);
  }
  explicit Swizzle2DBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Swizzle2D> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Swizzle2D>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Swizzle2D> CreateSwizzle2D(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t in_x = 0,
    int64_t in_y = 0,
    nvfuser::serde::Swizzle2DType swizzle_type = nvfuser::serde::Swizzle2DType_None,
    nvfuser::serde::SwizzleMode swizzle_mode = nvfuser::serde::SwizzleMode_None,
    int64_t out_x = 0,
    int64_t out_y = 0) {
  Swizzle2DBuilder builder_(_fbb);
  builder_.add_out_y(out_y);
  builder_.add_out_x(out_x);
  builder_.add_in_y(in_y);
  builder_.add_in_x(in_x);
  builder_.add_swizzle_mode(swizzle_mode);
  builder_.add_swizzle_type(swizzle_type);
  return builder_.Finish();
}

struct Resize FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ResizeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_IN = 4,
    VT_LEFT_EXPANSION = 6,
    VT_RIGHT_EXPANSION = 8,
    VT_OUT = 10
  };
  int64_t in() const {
    return GetField<int64_t>(VT_IN, 0);
  }
  int64_t left_expansion() const {
    return GetField<int64_t>(VT_LEFT_EXPANSION, 0);
  }
  int64_t right_expansion() const {
    return GetField<int64_t>(VT_RIGHT_EXPANSION, 0);
  }
  int64_t out() const {
    return GetField<int64_t>(VT_OUT, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_IN, 8) &&
           VerifyField<int64_t>(verifier, VT_LEFT_EXPANSION, 8) &&
           VerifyField<int64_t>(verifier, VT_RIGHT_EXPANSION, 8) &&
           VerifyField<int64_t>(verifier, VT_OUT, 8) &&
           verifier.EndTable();
  }
};

struct ResizeBuilder {
  typedef Resize Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_in(int64_t in) {
    fbb_.AddElement<int64_t>(Resize::VT_IN, in, 0);
  }
  void add_left_expansion(int64_t left_expansion) {
    fbb_.AddElement<int64_t>(Resize::VT_LEFT_EXPANSION, left_expansion, 0);
  }
  void add_right_expansion(int64_t right_expansion) {
    fbb_.AddElement<int64_t>(Resize::VT_RIGHT_EXPANSION, right_expansion, 0);
  }
  void add_out(int64_t out) {
    fbb_.AddElement<int64_t>(Resize::VT_OUT, out, 0);
  }
  explicit ResizeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Resize> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Resize>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Resize> CreateResize(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t in = 0,
    int64_t left_expansion = 0,
    int64_t right_expansion = 0,
    int64_t out = 0) {
  ResizeBuilder builder_(_fbb);
  builder_.add_out(out);
  builder_.add_right_expansion(right_expansion);
  builder_.add_left_expansion(left_expansion);
  builder_.add_in(in);
  return builder_.Finish();
}

struct Instruction FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef InstructionBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DATA_TYPE = 4,
    VT_DATA = 6
  };
  nvfuser::serde::InstructionData data_type() const {
    return static_cast<nvfuser::serde::InstructionData>(GetField<uint8_t>(VT_DATA_TYPE, 0));
  }
  const void *data() const {
    return GetPointer<const void *>(VT_DATA);
  }
  template<typename T> const T *data_as() const;
  const nvfuser::serde::BinaryOp *data_as_BinaryOp() const {
    return data_type() == nvfuser::serde::InstructionData_BinaryOp ? static_cast<const nvfuser::serde::BinaryOp *>(data()) : nullptr;
  }
  const nvfuser::serde::Symbolic *data_as_Symbolic() const {
    return data_type() == nvfuser::serde::InstructionData_Symbolic ? static_cast<const nvfuser::serde::Symbolic *>(data()) : nullptr;
  }
  const nvfuser::serde::Scalar *data_as_Scalar() const {
    return data_type() == nvfuser::serde::InstructionData_Scalar ? static_cast<const nvfuser::serde::Scalar *>(data()) : nullptr;
  }
  const nvfuser::serde::Merge *data_as_Merge() const {
    return data_type() == nvfuser::serde::InstructionData_Merge ? static_cast<const nvfuser::serde::Merge *>(data()) : nullptr;
  }
  const nvfuser::serde::NamedScalar *data_as_NamedScalar() const {
    return data_type() == nvfuser::serde::InstructionData_NamedScalar ? static_cast<const nvfuser::serde::NamedScalar *>(data()) : nullptr;
  }
  const nvfuser::serde::Resize *data_as_Resize() const {
    return data_type() == nvfuser::serde::InstructionData_Resize ? static_cast<const nvfuser::serde::Resize *>(data()) : nullptr;
  }
  const nvfuser::serde::Split *data_as_Split() const {
    return data_type() == nvfuser::serde::InstructionData_Split ? static_cast<const nvfuser::serde::Split *>(data()) : nullptr;
  }
  const nvfuser::serde::Swizzle2D *data_as_Swizzle2D() const {
    return data_type() == nvfuser::serde::InstructionData_Swizzle2D ? static_cast<const nvfuser::serde::Swizzle2D *>(data()) : nullptr;
  }
  const nvfuser::serde::UnaryOp *data_as_UnaryOp() const {
    return data_type() == nvfuser::serde::InstructionData_UnaryOp ? static_cast<const nvfuser::serde::UnaryOp *>(data()) : nullptr;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_DATA_TYPE, 1) &&
           VerifyOffset(verifier, VT_DATA) &&
           VerifyInstructionData(verifier, data(), data_type()) &&
           verifier.EndTable();
  }
};

template<> inline const nvfuser::serde::BinaryOp *Instruction::data_as<nvfuser::serde::BinaryOp>() const {
  return data_as_BinaryOp();
}

template<> inline const nvfuser::serde::Symbolic *Instruction::data_as<nvfuser::serde::Symbolic>() const {
  return data_as_Symbolic();
}

template<> inline const nvfuser::serde::Scalar *Instruction::data_as<nvfuser::serde::Scalar>() const {
  return data_as_Scalar();
}

template<> inline const nvfuser::serde::Merge *Instruction::data_as<nvfuser::serde::Merge>() const {
  return data_as_Merge();
}

template<> inline const nvfuser::serde::NamedScalar *Instruction::data_as<nvfuser::serde::NamedScalar>() const {
  return data_as_NamedScalar();
}

template<> inline const nvfuser::serde::Resize *Instruction::data_as<nvfuser::serde::Resize>() const {
  return data_as_Resize();
}

template<> inline const nvfuser::serde::Split *Instruction::data_as<nvfuser::serde::Split>() const {
  return data_as_Split();
}

template<> inline const nvfuser::serde::Swizzle2D *Instruction::data_as<nvfuser::serde::Swizzle2D>() const {
  return data_as_Swizzle2D();
}

template<> inline const nvfuser::serde::UnaryOp *Instruction::data_as<nvfuser::serde::UnaryOp>() const {
  return data_as_UnaryOp();
}

struct InstructionBuilder {
  typedef Instruction Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_data_type(nvfuser::serde::InstructionData data_type) {
    fbb_.AddElement<uint8_t>(Instruction::VT_DATA_TYPE, static_cast<uint8_t>(data_type), 0);
  }
  void add_data(::flatbuffers::Offset<void> data) {
    fbb_.AddOffset(Instruction::VT_DATA, data);
  }
  explicit InstructionBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Instruction> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Instruction>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Instruction> CreateInstruction(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::InstructionData data_type = nvfuser::serde::InstructionData_NONE,
    ::flatbuffers::Offset<void> data = 0) {
  InstructionBuilder builder_(_fbb);
  builder_.add_data(data);
  builder_.add_data_type(data_type);
  return builder_.Finish();
}

struct NaiveValueGenerator FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef NaiveValueGeneratorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INSTRUCTIONS = 4
  };
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::Instruction>> *instructions() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::Instruction>> *>(VT_INSTRUCTIONS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_INSTRUCTIONS) &&
           verifier.VerifyVector(instructions()) &&
           verifier.VerifyVectorOfTables(instructions()) &&
           verifier.EndTable();
  }
};

struct NaiveValueGeneratorBuilder {
  typedef NaiveValueGenerator Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_instructions(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::Instruction>>> instructions) {
    fbb_.AddOffset(NaiveValueGenerator::VT_INSTRUCTIONS, instructions);
  }
  explicit NaiveValueGeneratorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<NaiveValueGenerator> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<NaiveValueGenerator>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<NaiveValueGenerator> CreateNaiveValueGenerator(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::Instruction>>> instructions = 0) {
  NaiveValueGeneratorBuilder builder_(_fbb);
  builder_.add_instructions(instructions);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<NaiveValueGenerator> CreateNaiveValueGeneratorDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::Instruction>> *instructions = nullptr) {
  auto instructions__ = instructions ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::Instruction>>(*instructions) : 0;
  return nvfuser::serde::CreateNaiveValueGenerator(
      _fbb,
      instructions__);
}

struct IterationDomain FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef IterationDomainBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_EXTENT = 4
  };
  int64_t extent() const {
    return GetField<int64_t>(VT_EXTENT, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_EXTENT, 8) &&
           verifier.EndTable();
  }
};

struct IterationDomainBuilder {
  typedef IterationDomain Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_extent(int64_t extent) {
    fbb_.AddElement<int64_t>(IterationDomain::VT_EXTENT, extent, 0);
  }
  explicit IterationDomainBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<IterationDomain> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<IterationDomain>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<IterationDomain> CreateIterationDomain(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t extent = 0) {
  IterationDomainBuilder builder_(_fbb);
  builder_.add_extent(extent);
  return builder_.Finish();
}

struct Domain FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef DomainBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIMS = 4
  };
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::IterationDomain>> *dims() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::IterationDomain>> *>(VT_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_DIMS) &&
           verifier.VerifyVector(dims()) &&
           verifier.VerifyVectorOfTables(dims()) &&
           verifier.EndTable();
  }
};

struct DomainBuilder {
  typedef Domain Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dims(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::IterationDomain>>> dims) {
    fbb_.AddOffset(Domain::VT_DIMS, dims);
  }
  explicit DomainBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Domain> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Domain>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Domain> CreateDomain(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::IterationDomain>>> dims = 0) {
  DomainBuilder builder_(_fbb);
  builder_.add_dims(dims);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Domain> CreateDomainDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::IterationDomain>> *dims = nullptr) {
  auto dims__ = dims ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::IterationDomain>>(*dims) : 0;
  return nvfuser::serde::CreateDomain(
      _fbb,
      dims__);
}

struct SymbolicTensor FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SymbolicTensorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DTYPE = 4,
    VT_ROOT = 6,
    VT_RFACTOR = 8,
    VT_ALLOCATE = 10,
    VT_LEAF = 12
  };
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  const nvfuser::serde::Domain *root() const {
    return GetPointer<const nvfuser::serde::Domain *>(VT_ROOT);
  }
  const nvfuser::serde::Domain *rfactor() const {
    return GetPointer<const nvfuser::serde::Domain *>(VT_RFACTOR);
  }
  const nvfuser::serde::Domain *allocate() const {
    return GetPointer<const nvfuser::serde::Domain *>(VT_ALLOCATE);
  }
  const nvfuser::serde::Domain *leaf() const {
    return GetPointer<const nvfuser::serde::Domain *>(VT_LEAF);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           VerifyOffset(verifier, VT_ROOT) &&
           verifier.VerifyTable(root()) &&
           VerifyOffset(verifier, VT_RFACTOR) &&
           verifier.VerifyTable(rfactor()) &&
           VerifyOffset(verifier, VT_ALLOCATE) &&
           verifier.VerifyTable(allocate()) &&
           VerifyOffset(verifier, VT_LEAF) &&
           verifier.VerifyTable(leaf()) &&
           verifier.EndTable();
  }
};

struct SymbolicTensorBuilder {
  typedef SymbolicTensor Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(SymbolicTensor::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  void add_root(::flatbuffers::Offset<nvfuser::serde::Domain> root) {
    fbb_.AddOffset(SymbolicTensor::VT_ROOT, root);
  }
  void add_rfactor(::flatbuffers::Offset<nvfuser::serde::Domain> rfactor) {
    fbb_.AddOffset(SymbolicTensor::VT_RFACTOR, rfactor);
  }
  void add_allocate(::flatbuffers::Offset<nvfuser::serde::Domain> allocate) {
    fbb_.AddOffset(SymbolicTensor::VT_ALLOCATE, allocate);
  }
  void add_leaf(::flatbuffers::Offset<nvfuser::serde::Domain> leaf) {
    fbb_.AddOffset(SymbolicTensor::VT_LEAF, leaf);
  }
  explicit SymbolicTensorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<SymbolicTensor> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<SymbolicTensor>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<SymbolicTensor> CreateSymbolicTensor(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None,
    ::flatbuffers::Offset<nvfuser::serde::Domain> root = 0,
    ::flatbuffers::Offset<nvfuser::serde::Domain> rfactor = 0,
    ::flatbuffers::Offset<nvfuser::serde::Domain> allocate = 0,
    ::flatbuffers::Offset<nvfuser::serde::Domain> leaf = 0) {
  SymbolicTensorBuilder builder_(_fbb);
  builder_.add_leaf(leaf);
  builder_.add_allocate(allocate);
  builder_.add_rfactor(rfactor);
  builder_.add_root(root);
  builder_.add_dtype(dtype);
  return builder_.Finish();
}

struct AllocateBuffer FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef AllocateBufferBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TV = 4,
    VT_SHAPE = 6,
    VT_ZERO_INIT = 8
  };
  const nvfuser::serde::SymbolicTensor *tv() const {
    return GetPointer<const nvfuser::serde::SymbolicTensor *>(VT_TV);
  }
  const ::flatbuffers::Vector<int64_t> *shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SHAPE);
  }
  bool zero_init() const {
    return GetField<uint8_t>(VT_ZERO_INIT, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_TV) &&
           verifier.VerifyTable(tv()) &&
           VerifyOffset(verifier, VT_SHAPE) &&
           verifier.VerifyVector(shape()) &&
           VerifyField<uint8_t>(verifier, VT_ZERO_INIT, 1) &&
           verifier.EndTable();
  }
};

struct AllocateBufferBuilder {
  typedef AllocateBuffer Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_tv(::flatbuffers::Offset<nvfuser::serde::SymbolicTensor> tv) {
    fbb_.AddOffset(AllocateBuffer::VT_TV, tv);
  }
  void add_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape) {
    fbb_.AddOffset(AllocateBuffer::VT_SHAPE, shape);
  }
  void add_zero_init(bool zero_init) {
    fbb_.AddElement<uint8_t>(AllocateBuffer::VT_ZERO_INIT, static_cast<uint8_t>(zero_init), 0);
  }
  explicit AllocateBufferBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<AllocateBuffer> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<AllocateBuffer>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<AllocateBuffer> CreateAllocateBuffer(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::SymbolicTensor> tv = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape = 0,
    bool zero_init = false) {
  AllocateBufferBuilder builder_(_fbb);
  builder_.add_shape(shape);
  builder_.add_tv(tv);
  builder_.add_zero_init(zero_init);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<AllocateBuffer> CreateAllocateBufferDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::SymbolicTensor> tv = 0,
    const std::vector<int64_t> *shape = nullptr,
    bool zero_init = false) {
  auto shape__ = shape ? _fbb.CreateVector<int64_t>(*shape) : 0;
  return nvfuser::serde::CreateAllocateBuffer(
      _fbb,
      tv,
      shape__,
      zero_init);
}

struct ScalarCpu FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ScalarCpuBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SCALAR_VALUE = 4
  };
  const nvfuser::serde::Scalar *scalar_value() const {
    return GetPointer<const nvfuser::serde::Scalar *>(VT_SCALAR_VALUE);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SCALAR_VALUE) &&
           verifier.VerifyTable(scalar_value()) &&
           verifier.EndTable();
  }
};

struct ScalarCpuBuilder {
  typedef ScalarCpu Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_scalar_value(::flatbuffers::Offset<nvfuser::serde::Scalar> scalar_value) {
    fbb_.AddOffset(ScalarCpu::VT_SCALAR_VALUE, scalar_value);
  }
  explicit ScalarCpuBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<ScalarCpu> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<ScalarCpu>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<ScalarCpu> CreateScalarCpu(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::Scalar> scalar_value = 0) {
  ScalarCpuBuilder builder_(_fbb);
  builder_.add_scalar_value(scalar_value);
  return builder_.Finish();
}

struct TensorArg FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorArgBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_PTR = 4,
    VT_SIZES = 6,
    VT_STRIDES = 8,
    VT_DTYPE = 10
  };
  uint64_t ptr() const {
    return GetField<uint64_t>(VT_PTR, 0);
  }
  const ::flatbuffers::Vector<int64_t> *sizes() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SIZES);
  }
  const ::flatbuffers::Vector<int64_t> *strides() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_STRIDES);
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_PTR, 8) &&
           VerifyOffset(verifier, VT_SIZES) &&
           verifier.VerifyVector(sizes()) &&
           VerifyOffset(verifier, VT_STRIDES) &&
           verifier.VerifyVector(strides()) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
};

struct TensorArgBuilder {
  typedef TensorArg Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_ptr(uint64_t ptr) {
    fbb_.AddElement<uint64_t>(TensorArg::VT_PTR, ptr, 0);
  }
  void add_sizes(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes) {
    fbb_.AddOffset(TensorArg::VT_SIZES, sizes);
  }
  void add_strides(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides) {
    fbb_.AddOffset(TensorArg::VT_STRIDES, strides);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(TensorArg::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit TensorArgBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TensorArg> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TensorArg>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TensorArg> CreateTensorArg(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t ptr = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides = 0,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None) {
  TensorArgBuilder builder_(_fbb);
  builder_.add_ptr(ptr);
  builder_.add_dtype(dtype);
  builder_.add_strides(strides);
  builder_.add_sizes(sizes);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TensorArg> CreateTensorArgDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t ptr = 0,
    const std::vector<int64_t> *sizes = nullptr,
    const std::vector<int64_t> *strides = nullptr,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None) {
  auto sizes__ = sizes ? _fbb.CreateVector<int64_t>(*sizes) : 0;
  auto strides__ = strides ? _fbb.CreateVector<int64_t>(*strides) : 0;
  return nvfuser::serde::CreateTensorArg(
      _fbb,
      ptr,
      sizes__,
      strides__,
      dtype);
}

struct PolymorphicValue FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef PolymorphicValueBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DATA_TYPE = 4,
    VT_DATA = 6
  };
  nvfuser::serde::PolymorphicValueData data_type() const {
    return static_cast<nvfuser::serde::PolymorphicValueData>(GetField<uint8_t>(VT_DATA_TYPE, 0));
  }
  const void *data() const {
    return GetPointer<const void *>(VT_DATA);
  }
  template<typename T> const T *data_as() const;
  const nvfuser::serde::Scalar *data_as_Scalar() const {
    return data_type() == nvfuser::serde::PolymorphicValueData_Scalar ? static_cast<const nvfuser::serde::Scalar *>(data()) : nullptr;
  }
  const nvfuser::serde::ScalarCpu *data_as_ScalarCpu() const {
    return data_type() == nvfuser::serde::PolymorphicValueData_ScalarCpu ? static_cast<const nvfuser::serde::ScalarCpu *>(data()) : nullptr;
  }
  const nvfuser::serde::TensorArg *data_as_TensorArg() const {
    return data_type() == nvfuser::serde::PolymorphicValueData_TensorArg ? static_cast<const nvfuser::serde::TensorArg *>(data()) : nullptr;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_DATA_TYPE, 1) &&
           VerifyOffset(verifier, VT_DATA) &&
           VerifyPolymorphicValueData(verifier, data(), data_type()) &&
           verifier.EndTable();
  }
};

template<> inline const nvfuser::serde::Scalar *PolymorphicValue::data_as<nvfuser::serde::Scalar>() const {
  return data_as_Scalar();
}

template<> inline const nvfuser::serde::ScalarCpu *PolymorphicValue::data_as<nvfuser::serde::ScalarCpu>() const {
  return data_as_ScalarCpu();
}

template<> inline const nvfuser::serde::TensorArg *PolymorphicValue::data_as<nvfuser::serde::TensorArg>() const {
  return data_as_TensorArg();
}

struct PolymorphicValueBuilder {
  typedef PolymorphicValue Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_data_type(nvfuser::serde::PolymorphicValueData data_type) {
    fbb_.AddElement<uint8_t>(PolymorphicValue::VT_DATA_TYPE, static_cast<uint8_t>(data_type), 0);
  }
  void add_data(::flatbuffers::Offset<void> data) {
    fbb_.AddOffset(PolymorphicValue::VT_DATA, data);
  }
  explicit PolymorphicValueBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<PolymorphicValue> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<PolymorphicValue>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<PolymorphicValue> CreatePolymorphicValue(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::PolymorphicValueData data_type = nvfuser::serde::PolymorphicValueData_NONE,
    ::flatbuffers::Offset<void> data = 0) {
  PolymorphicValueBuilder builder_(_fbb);
  builder_.add_data(data);
  builder_.add_data_type(data_type);
  return builder_.Finish();
}

struct KernelArgumentHolder FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef KernelArgumentHolderBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ARGUMENTS = 4,
    VT_DEVICE_INDEX = 6,
    VT_CACHE_ID = 8
  };
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>> *arguments() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>> *>(VT_ARGUMENTS);
  }
  int8_t device_index() const {
    return GetField<int8_t>(VT_DEVICE_INDEX, 0);
  }
  uint64_t cache_id() const {
    return GetField<uint64_t>(VT_CACHE_ID, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ARGUMENTS) &&
           verifier.VerifyVector(arguments()) &&
           verifier.VerifyVectorOfTables(arguments()) &&
           VerifyField<int8_t>(verifier, VT_DEVICE_INDEX, 1) &&
           VerifyField<uint64_t>(verifier, VT_CACHE_ID, 8) &&
           verifier.EndTable();
  }
};

struct KernelArgumentHolderBuilder {
  typedef KernelArgumentHolder Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_arguments(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>>> arguments) {
    fbb_.AddOffset(KernelArgumentHolder::VT_ARGUMENTS, arguments);
  }
  void add_device_index(int8_t device_index) {
    fbb_.AddElement<int8_t>(KernelArgumentHolder::VT_DEVICE_INDEX, device_index, 0);
  }
  void add_cache_id(uint64_t cache_id) {
    fbb_.AddElement<uint64_t>(KernelArgumentHolder::VT_CACHE_ID, cache_id, 0);
  }
  explicit KernelArgumentHolderBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<KernelArgumentHolder> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<KernelArgumentHolder>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<KernelArgumentHolder> CreateKernelArgumentHolder(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>>> arguments = 0,
    int8_t device_index = 0,
    uint64_t cache_id = 0) {
  KernelArgumentHolderBuilder builder_(_fbb);
  builder_.add_cache_id(cache_id);
  builder_.add_arguments(arguments);
  builder_.add_device_index(device_index);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<KernelArgumentHolder> CreateKernelArgumentHolderDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>> *arguments = nullptr,
    int8_t device_index = 0,
    uint64_t cache_id = 0) {
  auto arguments__ = arguments ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>>(*arguments) : 0;
  return nvfuser::serde::CreateKernelArgumentHolder(
      _fbb,
      arguments__,
      device_index,
      cache_id);
}

struct TensorShape FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorShapeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SHAPE = 4
  };
  const ::flatbuffers::Vector<int64_t> *shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SHAPE);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SHAPE) &&
           verifier.VerifyVector(shape()) &&
           verifier.EndTable();
  }
};

struct TensorShapeBuilder {
  typedef TensorShape Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape) {
    fbb_.AddOffset(TensorShape::VT_SHAPE, shape);
  }
  explicit TensorShapeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TensorShape> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TensorShape>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TensorShape> CreateTensorShape(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape = 0) {
  TensorShapeBuilder builder_(_fbb);
  builder_.add_shape(shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TensorShape> CreateTensorShapeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *shape = nullptr) {
  auto shape__ = shape ? _fbb.CreateVector<int64_t>(*shape) : 0;
  return nvfuser::serde::CreateTensorShape(
      _fbb,
      shape__);
}

struct LaunchParams FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef LaunchParamsBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_GDIMX = 4,
    VT_GDIMY = 6,
    VT_GDIMZ = 8,
    VT_BDIMX = 10,
    VT_BDIMY = 12,
    VT_BDIMZ = 14,
    VT_SMEM = 16,
    VT_OUTPUT_SIZES = 18
  };
  int64_t gdimx() const {
    return GetField<int64_t>(VT_GDIMX, 0);
  }
  int64_t gdimy() const {
    return GetField<int64_t>(VT_GDIMY, 0);
  }
  int64_t gdimz() const {
    return GetField<int64_t>(VT_GDIMZ, 0);
  }
  int64_t bdimx() const {
    return GetField<int64_t>(VT_BDIMX, 0);
  }
  int64_t bdimy() const {
    return GetField<int64_t>(VT_BDIMY, 0);
  }
  int64_t bdimz() const {
    return GetField<int64_t>(VT_BDIMZ, 0);
  }
  int64_t smem() const {
    return GetField<int64_t>(VT_SMEM, 0);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>> *output_sizes() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>> *>(VT_OUTPUT_SIZES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_GDIMX, 8) &&
           VerifyField<int64_t>(verifier, VT_GDIMY, 8) &&
           VerifyField<int64_t>(verifier, VT_GDIMZ, 8) &&
           VerifyField<int64_t>(verifier, VT_BDIMX, 8) &&
           VerifyField<int64_t>(verifier, VT_BDIMY, 8) &&
           VerifyField<int64_t>(verifier, VT_BDIMZ, 8) &&
           VerifyField<int64_t>(verifier, VT_SMEM, 8) &&
           VerifyOffset(verifier, VT_OUTPUT_SIZES) &&
           verifier.VerifyVector(output_sizes()) &&
           verifier.VerifyVectorOfTables(output_sizes()) &&
           verifier.EndTable();
  }
};

struct LaunchParamsBuilder {
  typedef LaunchParams Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_gdimx(int64_t gdimx) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_GDIMX, gdimx, 0);
  }
  void add_gdimy(int64_t gdimy) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_GDIMY, gdimy, 0);
  }
  void add_gdimz(int64_t gdimz) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_GDIMZ, gdimz, 0);
  }
  void add_bdimx(int64_t bdimx) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_BDIMX, bdimx, 0);
  }
  void add_bdimy(int64_t bdimy) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_BDIMY, bdimy, 0);
  }
  void add_bdimz(int64_t bdimz) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_BDIMZ, bdimz, 0);
  }
  void add_smem(int64_t smem) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_SMEM, smem, 0);
  }
  void add_output_sizes(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>>> output_sizes) {
    fbb_.AddOffset(LaunchParams::VT_OUTPUT_SIZES, output_sizes);
  }
  explicit LaunchParamsBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<LaunchParams> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<LaunchParams>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<LaunchParams> CreateLaunchParams(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t gdimx = 0,
    int64_t gdimy = 0,
    int64_t gdimz = 0,
    int64_t bdimx = 0,
    int64_t bdimy = 0,
    int64_t bdimz = 0,
    int64_t smem = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>>> output_sizes = 0) {
  LaunchParamsBuilder builder_(_fbb);
  builder_.add_smem(smem);
  builder_.add_bdimz(bdimz);
  builder_.add_bdimy(bdimy);
  builder_.add_bdimx(bdimx);
  builder_.add_gdimz(gdimz);
  builder_.add_gdimy(gdimy);
  builder_.add_gdimx(gdimx);
  builder_.add_output_sizes(output_sizes);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<LaunchParams> CreateLaunchParamsDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t gdimx = 0,
    int64_t gdimy = 0,
    int64_t gdimz = 0,
    int64_t bdimx = 0,
    int64_t bdimy = 0,
    int64_t bdimz = 0,
    int64_t smem = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>> *output_sizes = nullptr) {
  auto output_sizes__ = output_sizes ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::TensorShape>>(*output_sizes) : 0;
  return nvfuser::serde::CreateLaunchParams(
      _fbb,
      gdimx,
      gdimy,
      gdimz,
      bdimx,
      bdimy,
      bdimz,
      smem,
      output_sizes__);
}

struct GlobalBufferInfo FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef GlobalBufferInfoBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TV = 4,
    VT_SIZES = 6,
    VT_STRIDES = 8,
    VT_DTYPE = 10,
    VT_ZERO_INIT = 12,
    VT_IS_PROFILE_BUFFER = 14,
    VT_IS_FUSION_OUTPUT = 16
  };
  int64_t tv() const {
    return GetField<int64_t>(VT_TV, -1LL);
  }
  const ::flatbuffers::Vector<int64_t> *sizes() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SIZES);
  }
  const ::flatbuffers::Vector<int64_t> *strides() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_STRIDES);
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool zero_init() const {
    return GetField<uint8_t>(VT_ZERO_INIT, 0) != 0;
  }
  bool is_profile_buffer() const {
    return GetField<uint8_t>(VT_IS_PROFILE_BUFFER, 0) != 0;
  }
  bool is_fusion_output() const {
    return GetField<uint8_t>(VT_IS_FUSION_OUTPUT, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_TV, 8) &&
           VerifyOffset(verifier, VT_SIZES) &&
           verifier.VerifyVector(sizes()) &&
           VerifyOffset(verifier, VT_STRIDES) &&
           verifier.VerifyVector(strides()) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_ZERO_INIT, 1) &&
           VerifyField<uint8_t>(verifier, VT_IS_PROFILE_BUFFER, 1) &&
           VerifyField<uint8_t>(verifier, VT_IS_FUSION_OUTPUT, 1) &&
           verifier.EndTable();
  }
};

struct GlobalBufferInfoBuilder {
  typedef GlobalBufferInfo Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_tv(int64_t tv) {
    fbb_.AddElement<int64_t>(GlobalBufferInfo::VT_TV, tv, -1LL);
  }
  void add_sizes(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes) {
    fbb_.AddOffset(GlobalBufferInfo::VT_SIZES, sizes);
  }
  void add_strides(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides) {
    fbb_.AddOffset(GlobalBufferInfo::VT_STRIDES, strides);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(GlobalBufferInfo::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  void add_zero_init(bool zero_init) {
    fbb_.AddElement<uint8_t>(GlobalBufferInfo::VT_ZERO_INIT, static_cast<uint8_t>(zero_init), 0);
  }
  void add_is_profile_buffer(bool is_profile_buffer) {
    fbb_.AddElement<uint8_t>(GlobalBufferInfo::VT_IS_PROFILE_BUFFER, static_cast<uint8_t>(is_profile_buffer), 0);
  }
  void add_is_fusion_output(bool is_fusion_output) {
    fbb_.AddElement<uint8_t>(GlobalBufferInfo::VT_IS_FUSION_OUTPUT, static_cast<uint8_t>(is_fusion_output), 0);
  }
  explicit GlobalBufferInfoBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<GlobalBufferInfo> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<GlobalBufferInfo>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<GlobalBufferInfo> CreateGlobalBufferInfo(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t tv = -1LL,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides = 0,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None,
    bool zero_init = false,
    bool is_profile_buffer = false,
    bool is_fusion_output = false) {
  GlobalBufferInfoBuilder builder_(_fbb);
  builder_.add_tv(tv);
  builder_.add_dtype(dtype);
  builder_.add_strides(strides);
  builder_.add_sizes(sizes);
  builder_.add_is_fusion_output(is_fusion_output);
  builder_.add_is_profile_buffer(is_profile_buffer);
  builder_.add_zero_init(zero_init);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<GlobalBufferInfo> CreateGlobalBufferInfoDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t tv = -1LL,
    const std::vector<int64_t> *sizes = nullptr,
    const std::vector<int64_t> *strides = nullptr,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None,
    bool zero_init = false,
    bool is_profile_buffer = false,
    bool is_fusion_output = false) {
  auto sizes__ = sizes ? _fbb.CreateVector<int64_t>(*sizes) : 0;
  auto strides__ = strides ? _fbb.CreateVector<int64_t>(*strides) : 0;
  return nvfuser::serde::CreateGlobalBufferInfo(
      _fbb,
      tv,
      sizes__,
      strides__,
      dtype,
      zero_init,
      is_profile_buffer,
      is_fusion_output);
}

struct ExecutorEntry FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ExecutorEntryBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INIT = 4,
    VT_LAUNCH_PARAMS = 6,
    VT_OUTPUT_ALIASES = 8,
    VT_INPUT_ALIASES = 10,
    VT_OUTPUTS = 12,
    VT_INTERMEDIATES = 14
  };
  bool init() const {
    return GetField<uint8_t>(VT_INIT, 0) != 0;
  }
  const nvfuser::serde::LaunchParams *launch_params() const {
    return GetPointer<const nvfuser::serde::LaunchParams *>(VT_LAUNCH_PARAMS);
  }
  const ::flatbuffers::Vector<int32_t> *output_aliases() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_OUTPUT_ALIASES);
  }
  const ::flatbuffers::Vector<int32_t> *input_aliases() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_INPUT_ALIASES);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *outputs() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *>(VT_OUTPUTS);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *intermediates() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *>(VT_INTERMEDIATES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_INIT, 1) &&
           VerifyOffset(verifier, VT_LAUNCH_PARAMS) &&
           verifier.VerifyTable(launch_params()) &&
           VerifyOffset(verifier, VT_OUTPUT_ALIASES) &&
           verifier.VerifyVector(output_aliases()) &&
           VerifyOffset(verifier, VT_INPUT_ALIASES) &&
           verifier.VerifyVector(input_aliases()) &&
           VerifyOffset(verifier, VT_OUTPUTS) &&
           verifier.VerifyVector(outputs()) &&
           verifier.VerifyVectorOfTables(outputs()) &&
           VerifyOffset(verifier, VT_INTERMEDIATES) &&
           verifier.VerifyVector(intermediates()) &&
           verifier.VerifyVectorOfTables(intermediates()) &&
           verifier.EndTable();
  }
};

struct ExecutorEntryBuilder {
  typedef ExecutorEntry Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_init(bool init) {
    fbb_.AddElement<uint8_t>(ExecutorEntry::VT_INIT, static_cast<uint8_t>(init), 0);
  }
  void add_launch_params(::flatbuffers::Offset<nvfuser::serde::LaunchParams> launch_params) {
    fbb_.AddOffset(ExecutorEntry::VT_LAUNCH_PARAMS, launch_params);
  }
  void add_output_aliases(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> output_aliases) {
    fbb_.AddOffset(ExecutorEntry::VT_OUTPUT_ALIASES, output_aliases);
  }
  void add_input_aliases(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> input_aliases) {
    fbb_.AddOffset(ExecutorEntry::VT_INPUT_ALIASES, input_aliases);
  }
  void add_outputs(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> outputs) {
    fbb_.AddOffset(ExecutorEntry::VT_OUTPUTS, outputs);
  }
  void add_intermediates(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> intermediates) {
    fbb_.AddOffset(ExecutorEntry::VT_INTERMEDIATES, intermediates);
  }
  explicit ExecutorEntryBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<ExecutorEntry> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<ExecutorEntry>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<ExecutorEntry> CreateExecutorEntry(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool init = false,
    ::flatbuffers::Offset<nvfuser::serde::LaunchParams> launch_params = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> output_aliases = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> input_aliases = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> outputs = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> intermediates = 0) {
  ExecutorEntryBuilder builder_(_fbb);
  builder_.add_intermediates(intermediates);
  builder_.add_outputs(outputs);
  builder_.add_input_aliases(input_aliases);
  builder_.add_output_aliases(output_aliases);
  builder_.add_launch_params(launch_params);
  builder_.add_init(init);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<ExecutorEntry> CreateExecutorEntryDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool init = false,
    ::flatbuffers::Offset<nvfuser::serde::LaunchParams> launch_params = 0,
    const std::vector<int32_t> *output_aliases = nullptr,
    const std::vector<int32_t> *input_aliases = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *outputs = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *intermediates = nullptr) {
  auto output_aliases__ = output_aliases ? _fbb.CreateVector<int32_t>(*output_aliases) : 0;
  auto input_aliases__ = input_aliases ? _fbb.CreateVector<int32_t>(*input_aliases) : 0;
  auto outputs__ = outputs ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>(*outputs) : 0;
  auto intermediates__ = intermediates ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>(*intermediates) : 0;
  return nvfuser::serde::CreateExecutorEntry(
      _fbb,
      init,
      launch_params,
      output_aliases__,
      input_aliases__,
      outputs__,
      intermediates__);
}

struct At FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef AtBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INDEX = 4
  };
  int64_t index() const {
    return GetField<int64_t>(VT_INDEX, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_INDEX, 8) &&
           verifier.EndTable();
  }
};

struct AtBuilder {
  typedef At Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_index(int64_t index) {
    fbb_.AddElement<int64_t>(At::VT_INDEX, index, 0);
  }
  explicit AtBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<At> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<At>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<At> CreateAt(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t index = 0) {
  AtBuilder builder_(_fbb);
  builder_.add_index(index);
  return builder_.Finish();
}

struct BatchNorm FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef BatchNormBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TRAINING = 4,
    VT_CHANNELS_LAST = 6
  };
  bool training() const {
    return GetField<uint8_t>(VT_TRAINING, 0) != 0;
  }
  bool channels_last() const {
    return GetField<uint8_t>(VT_CHANNELS_LAST, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_TRAINING, 1) &&
           VerifyField<uint8_t>(verifier, VT_CHANNELS_LAST, 1) &&
           verifier.EndTable();
  }
};

struct BatchNormBuilder {
  typedef BatchNorm Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_training(bool training) {
    fbb_.AddElement<uint8_t>(BatchNorm::VT_TRAINING, static_cast<uint8_t>(training), 0);
  }
  void add_channels_last(bool channels_last) {
    fbb_.AddElement<uint8_t>(BatchNorm::VT_CHANNELS_LAST, static_cast<uint8_t>(channels_last), 0);
  }
  explicit BatchNormBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<BatchNorm> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<BatchNorm>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<BatchNorm> CreateBatchNorm(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool training = false,
    bool channels_last = false) {
  BatchNormBuilder builder_(_fbb);
  builder_.add_channels_last(channels_last);
  builder_.add_training(training);
  return builder_.Finish();
}

struct Broadcast FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef BroadcastBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_BROADCAST_DIMS = 4
  };
  const ::flatbuffers::Vector<uint8_t> *broadcast_dims() const {
    return GetPointer<const ::flatbuffers::Vector<uint8_t> *>(VT_BROADCAST_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_BROADCAST_DIMS) &&
           verifier.VerifyVector(broadcast_dims()) &&
           verifier.EndTable();
  }
};

struct BroadcastBuilder {
  typedef Broadcast Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_broadcast_dims(::flatbuffers::Offset<::flatbuffers::Vector<uint8_t>> broadcast_dims) {
    fbb_.AddOffset(Broadcast::VT_BROADCAST_DIMS, broadcast_dims);
  }
  explicit BroadcastBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Broadcast> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Broadcast>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Broadcast> CreateBroadcast(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint8_t>> broadcast_dims = 0) {
  BroadcastBuilder builder_(_fbb);
  builder_.add_broadcast_dims(broadcast_dims);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Broadcast> CreateBroadcastDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<uint8_t> *broadcast_dims = nullptr) {
  auto broadcast_dims__ = broadcast_dims ? _fbb.CreateVector<uint8_t>(*broadcast_dims) : 0;
  return nvfuser::serde::CreateBroadcast(
      _fbb,
      broadcast_dims__);
}

struct BroadcastInDim FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef BroadcastInDimBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OUTPUT_SHAPE = 4,
    VT_BROADCAST_DIMS = 6
  };
  const ::flatbuffers::Vector<int64_t> *output_shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_OUTPUT_SHAPE);
  }
  const ::flatbuffers::Vector<int64_t> *broadcast_dims() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_BROADCAST_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_OUTPUT_SHAPE) &&
           verifier.VerifyVector(output_shape()) &&
           VerifyOffset(verifier, VT_BROADCAST_DIMS) &&
           verifier.VerifyVector(broadcast_dims()) &&
           verifier.EndTable();
  }
};

struct BroadcastInDimBuilder {
  typedef BroadcastInDim Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_output_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> output_shape) {
    fbb_.AddOffset(BroadcastInDim::VT_OUTPUT_SHAPE, output_shape);
  }
  void add_broadcast_dims(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> broadcast_dims) {
    fbb_.AddOffset(BroadcastInDim::VT_BROADCAST_DIMS, broadcast_dims);
  }
  explicit BroadcastInDimBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<BroadcastInDim> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<BroadcastInDim>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<BroadcastInDim> CreateBroadcastInDim(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> output_shape = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> broadcast_dims = 0) {
  BroadcastInDimBuilder builder_(_fbb);
  builder_.add_broadcast_dims(broadcast_dims);
  builder_.add_output_shape(output_shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<BroadcastInDim> CreateBroadcastInDimDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *output_shape = nullptr,
    const std::vector<int64_t> *broadcast_dims = nullptr) {
  auto output_shape__ = output_shape ? _fbb.CreateVector<int64_t>(*output_shape) : 0;
  auto broadcast_dims__ = broadcast_dims ? _fbb.CreateVector<int64_t>(*broadcast_dims) : 0;
  return nvfuser::serde::CreateBroadcastInDim(
      _fbb,
      output_shape__,
      broadcast_dims__);
}

struct BroadcastInDimSymbolic FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef BroadcastInDimSymbolicBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OUTPUT_SHAPE = 4,
    VT_BROADCAST_DIMS = 6
  };
  const ::flatbuffers::Vector<const nvfuser::serde::State *> *output_shape() const {
    return GetPointer<const ::flatbuffers::Vector<const nvfuser::serde::State *> *>(VT_OUTPUT_SHAPE);
  }
  const ::flatbuffers::Vector<int64_t> *broadcast_dims() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_BROADCAST_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_OUTPUT_SHAPE) &&
           verifier.VerifyVector(output_shape()) &&
           VerifyOffset(verifier, VT_BROADCAST_DIMS) &&
           verifier.VerifyVector(broadcast_dims()) &&
           verifier.EndTable();
  }
};

struct BroadcastInDimSymbolicBuilder {
  typedef BroadcastInDimSymbolic Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_output_shape(::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> output_shape) {
    fbb_.AddOffset(BroadcastInDimSymbolic::VT_OUTPUT_SHAPE, output_shape);
  }
  void add_broadcast_dims(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> broadcast_dims) {
    fbb_.AddOffset(BroadcastInDimSymbolic::VT_BROADCAST_DIMS, broadcast_dims);
  }
  explicit BroadcastInDimSymbolicBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<BroadcastInDimSymbolic> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<BroadcastInDimSymbolic>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<BroadcastInDimSymbolic> CreateBroadcastInDimSymbolic(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> output_shape = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> broadcast_dims = 0) {
  BroadcastInDimSymbolicBuilder builder_(_fbb);
  builder_.add_broadcast_dims(broadcast_dims);
  builder_.add_output_shape(output_shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<BroadcastInDimSymbolic> CreateBroadcastInDimSymbolicDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<nvfuser::serde::State> *output_shape = nullptr,
    const std::vector<int64_t> *broadcast_dims = nullptr) {
  auto output_shape__ = output_shape ? _fbb.CreateVectorOfStructs<nvfuser::serde::State>(*output_shape) : 0;
  auto broadcast_dims__ = broadcast_dims ? _fbb.CreateVector<int64_t>(*broadcast_dims) : 0;
  return nvfuser::serde::CreateBroadcastInDimSymbolic(
      _fbb,
      output_shape__,
      broadcast_dims__);
}

struct Dtype FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef DtypeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DTYPE = 4
  };
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
};

struct DtypeBuilder {
  typedef Dtype Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(Dtype::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit DtypeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Dtype> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Dtype>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Dtype> CreateDtype(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None) {
  DtypeBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  return builder_.Finish();
}

struct Dimension FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef DimensionBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIM = 4
  };
  int64_t dim() const {
    return GetField<int64_t>(VT_DIM, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DIM, 8) &&
           verifier.EndTable();
  }
};

struct DimensionBuilder {
  typedef Dimension Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dim(int64_t dim) {
    fbb_.AddElement<int64_t>(Dimension::VT_DIM, dim, 0);
  }
  explicit DimensionBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Dimension> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Dimension>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Dimension> CreateDimension(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t dim = 0) {
  DimensionBuilder builder_(_fbb);
  builder_.add_dim(dim);
  return builder_.Finish();
}

struct Norm FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef NormBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_AXES = 4,
    VT_CORRECTION = 6,
    VT_KEEP_DIM = 8
  };
  const ::flatbuffers::Vector<int32_t> *axes() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_AXES);
  }
  int64_t correction() const {
    return GetField<int64_t>(VT_CORRECTION, 0);
  }
  bool keep_dim() const {
    return GetField<uint8_t>(VT_KEEP_DIM, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_AXES) &&
           verifier.VerifyVector(axes()) &&
           VerifyField<int64_t>(verifier, VT_CORRECTION, 8) &&
           VerifyField<uint8_t>(verifier, VT_KEEP_DIM, 1) &&
           verifier.EndTable();
  }
};

struct NormBuilder {
  typedef Norm Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_axes(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> axes) {
    fbb_.AddOffset(Norm::VT_AXES, axes);
  }
  void add_correction(int64_t correction) {
    fbb_.AddElement<int64_t>(Norm::VT_CORRECTION, correction, 0);
  }
  void add_keep_dim(bool keep_dim) {
    fbb_.AddElement<uint8_t>(Norm::VT_KEEP_DIM, static_cast<uint8_t>(keep_dim), 0);
  }
  explicit NormBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Norm> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Norm>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Norm> CreateNorm(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> axes = 0,
    int64_t correction = 0,
    bool keep_dim = false) {
  NormBuilder builder_(_fbb);
  builder_.add_correction(correction);
  builder_.add_axes(axes);
  builder_.add_keep_dim(keep_dim);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Norm> CreateNormDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int32_t> *axes = nullptr,
    int64_t correction = 0,
    bool keep_dim = false) {
  auto axes__ = axes ? _fbb.CreateVector<int32_t>(*axes) : 0;
  return nvfuser::serde::CreateNorm(
      _fbb,
      axes__,
      correction,
      keep_dim);
}

struct Output FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef OutputBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_STRIDE_ORDER = 4
  };
  const ::flatbuffers::Vector<int64_t> *stride_order() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_STRIDE_ORDER);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_STRIDE_ORDER) &&
           verifier.VerifyVector(stride_order()) &&
           verifier.EndTable();
  }
};

struct OutputBuilder {
  typedef Output Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_stride_order(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> stride_order) {
    fbb_.AddOffset(Output::VT_STRIDE_ORDER, stride_order);
  }
  explicit OutputBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Output> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Output>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Output> CreateOutput(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> stride_order = 0) {
  OutputBuilder builder_(_fbb);
  builder_.add_stride_order(stride_order);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Output> CreateOutputDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *stride_order = nullptr) {
  auto stride_order__ = stride_order ? _fbb.CreateVector<int64_t>(*stride_order) : 0;
  return nvfuser::serde::CreateOutput(
      _fbb,
      stride_order__);
}

struct Pad FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef PadBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_PAD_WIDTHS = 4
  };
  const ::flatbuffers::Vector<int64_t> *pad_widths() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_PAD_WIDTHS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_PAD_WIDTHS) &&
           verifier.VerifyVector(pad_widths()) &&
           verifier.EndTable();
  }
};

struct PadBuilder {
  typedef Pad Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_pad_widths(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> pad_widths) {
    fbb_.AddOffset(Pad::VT_PAD_WIDTHS, pad_widths);
  }
  explicit PadBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Pad> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Pad>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Pad> CreatePad(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> pad_widths = 0) {
  PadBuilder builder_(_fbb);
  builder_.add_pad_widths(pad_widths);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Pad> CreatePadDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *pad_widths = nullptr) {
  auto pad_widths__ = pad_widths ? _fbb.CreateVector<int64_t>(*pad_widths) : 0;
  return nvfuser::serde::CreatePad(
      _fbb,
      pad_widths__);
}

struct Permute FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef PermuteBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIMS = 4
  };
  const ::flatbuffers::Vector<int64_t> *dims() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_DIMS) &&
           verifier.VerifyVector(dims()) &&
           verifier.EndTable();
  }
};

struct PermuteBuilder {
  typedef Permute Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dims(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> dims) {
    fbb_.AddOffset(Permute::VT_DIMS, dims);
  }
  explicit PermuteBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Permute> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Permute>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Permute> CreatePermute(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> dims = 0) {
  PermuteBuilder builder_(_fbb);
  builder_.add_dims(dims);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Permute> CreatePermuteDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *dims = nullptr) {
  auto dims__ = dims ? _fbb.CreateVector<int64_t>(*dims) : 0;
  return nvfuser::serde::CreatePermute(
      _fbb,
      dims__);
}

struct Reduction FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ReductionBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_AXES = 4,
    VT_KEEP_DIM = 6,
    VT_DTYPE = 8
  };
  const ::flatbuffers::Vector<int32_t> *axes() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_AXES);
  }
  bool keep_dim() const {
    return GetField<uint8_t>(VT_KEEP_DIM, 0) != 0;
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_AXES) &&
           verifier.VerifyVector(axes()) &&
           VerifyField<uint8_t>(verifier, VT_KEEP_DIM, 1) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
};

struct ReductionBuilder {
  typedef Reduction Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_axes(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> axes) {
    fbb_.AddOffset(Reduction::VT_AXES, axes);
  }
  void add_keep_dim(bool keep_dim) {
    fbb_.AddElement<uint8_t>(Reduction::VT_KEEP_DIM, static_cast<uint8_t>(keep_dim), 0);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(Reduction::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit ReductionBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Reduction> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Reduction>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Reduction> CreateReduction(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> axes = 0,
    bool keep_dim = false,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None) {
  ReductionBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  builder_.add_axes(axes);
  builder_.add_keep_dim(keep_dim);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Reduction> CreateReductionDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int32_t> *axes = nullptr,
    bool keep_dim = false,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None) {
  auto axes__ = axes ? _fbb.CreateVector<int32_t>(*axes) : 0;
  return nvfuser::serde::CreateReduction(
      _fbb,
      axes__,
      keep_dim,
      dtype);
}

struct Reshape FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ReshapeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ORIGINAL_SHAPE = 4,
    VT_NEW_SHAPE = 6
  };
  const ::flatbuffers::Vector<int64_t> *original_shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_ORIGINAL_SHAPE);
  }
  const ::flatbuffers::Vector<int64_t> *new_shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_NEW_SHAPE);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ORIGINAL_SHAPE) &&
           verifier.VerifyVector(original_shape()) &&
           VerifyOffset(verifier, VT_NEW_SHAPE) &&
           verifier.VerifyVector(new_shape()) &&
           verifier.EndTable();
  }
};

struct ReshapeBuilder {
  typedef Reshape Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_original_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> original_shape) {
    fbb_.AddOffset(Reshape::VT_ORIGINAL_SHAPE, original_shape);
  }
  void add_new_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> new_shape) {
    fbb_.AddOffset(Reshape::VT_NEW_SHAPE, new_shape);
  }
  explicit ReshapeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Reshape> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Reshape>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Reshape> CreateReshape(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> original_shape = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> new_shape = 0) {
  ReshapeBuilder builder_(_fbb);
  builder_.add_new_shape(new_shape);
  builder_.add_original_shape(original_shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Reshape> CreateReshapeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *original_shape = nullptr,
    const std::vector<int64_t> *new_shape = nullptr) {
  auto original_shape__ = original_shape ? _fbb.CreateVector<int64_t>(*original_shape) : 0;
  auto new_shape__ = new_shape ? _fbb.CreateVector<int64_t>(*new_shape) : 0;
  return nvfuser::serde::CreateReshape(
      _fbb,
      original_shape__,
      new_shape__);
}

struct Size FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SizeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIM = 4
  };
  int64_t dim() const {
    return GetField<int64_t>(VT_DIM, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DIM, 8) &&
           verifier.EndTable();
  }
};

struct SizeBuilder {
  typedef Size Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dim(int64_t dim) {
    fbb_.AddElement<int64_t>(Size::VT_DIM, dim, 0);
  }
  explicit SizeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Size> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Size>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Size> CreateSize(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t dim = 0) {
  SizeBuilder builder_(_fbb);
  builder_.add_dim(dim);
  return builder_.Finish();
}

struct Slice FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SliceBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_START_INDICES = 4,
    VT_END_INDICES = 6,
    VT_STRIDES = 8
  };
  const ::flatbuffers::Vector<int64_t> *start_indices() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_START_INDICES);
  }
  const ::flatbuffers::Vector<int64_t> *end_indices() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_END_INDICES);
  }
  const ::flatbuffers::Vector<int64_t> *strides() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_STRIDES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_START_INDICES) &&
           verifier.VerifyVector(start_indices()) &&
           VerifyOffset(verifier, VT_END_INDICES) &&
           verifier.VerifyVector(end_indices()) &&
           VerifyOffset(verifier, VT_STRIDES) &&
           verifier.VerifyVector(strides()) &&
           verifier.EndTable();
  }
};

struct SliceBuilder {
  typedef Slice Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_start_indices(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> start_indices) {
    fbb_.AddOffset(Slice::VT_START_INDICES, start_indices);
  }
  void add_end_indices(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> end_indices) {
    fbb_.AddOffset(Slice::VT_END_INDICES, end_indices);
  }
  void add_strides(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides) {
    fbb_.AddOffset(Slice::VT_STRIDES, strides);
  }
  explicit SliceBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Slice> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Slice>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Slice> CreateSlice(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> start_indices = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> end_indices = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides = 0) {
  SliceBuilder builder_(_fbb);
  builder_.add_strides(strides);
  builder_.add_end_indices(end_indices);
  builder_.add_start_indices(start_indices);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Slice> CreateSliceDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *start_indices = nullptr,
    const std::vector<int64_t> *end_indices = nullptr,
    const std::vector<int64_t> *strides = nullptr) {
  auto start_indices__ = start_indices ? _fbb.CreateVector<int64_t>(*start_indices) : 0;
  auto end_indices__ = end_indices ? _fbb.CreateVector<int64_t>(*end_indices) : 0;
  auto strides__ = strides ? _fbb.CreateVector<int64_t>(*strides) : 0;
  return nvfuser::serde::CreateSlice(
      _fbb,
      start_indices__,
      end_indices__,
      strides__);
}

struct Squeeze FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SqueezeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ORIGINAL_SHAPE = 4,
    VT_SQUEEZE_DIMS = 6
  };
  const ::flatbuffers::Vector<int64_t> *original_shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_ORIGINAL_SHAPE);
  }
  const ::flatbuffers::Vector<int64_t> *squeeze_dims() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SQUEEZE_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ORIGINAL_SHAPE) &&
           verifier.VerifyVector(original_shape()) &&
           VerifyOffset(verifier, VT_SQUEEZE_DIMS) &&
           verifier.VerifyVector(squeeze_dims()) &&
           verifier.EndTable();
  }
};

struct SqueezeBuilder {
  typedef Squeeze Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_original_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> original_shape) {
    fbb_.AddOffset(Squeeze::VT_ORIGINAL_SHAPE, original_shape);
  }
  void add_squeeze_dims(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> squeeze_dims) {
    fbb_.AddOffset(Squeeze::VT_SQUEEZE_DIMS, squeeze_dims);
  }
  explicit SqueezeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Squeeze> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Squeeze>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Squeeze> CreateSqueeze(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> original_shape = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> squeeze_dims = 0) {
  SqueezeBuilder builder_(_fbb);
  builder_.add_squeeze_dims(squeeze_dims);
  builder_.add_original_shape(original_shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Squeeze> CreateSqueezeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *original_shape = nullptr,
    const std::vector<int64_t> *squeeze_dims = nullptr) {
  auto original_shape__ = original_shape ? _fbb.CreateVector<int64_t>(*original_shape) : 0;
  auto squeeze_dims__ = squeeze_dims ? _fbb.CreateVector<int64_t>(*squeeze_dims) : 0;
  return nvfuser::serde::CreateSqueeze(
      _fbb,
      original_shape__,
      squeeze_dims__);
}

struct Tensor FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SIZES = 4,
    VT_CONTIGUITY = 6,
    VT_DTYPE = 8,
    VT_IS_CPU = 10
  };
  const ::flatbuffers::Vector<int64_t> *sizes() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SIZES);
  }
  const ::flatbuffers::Vector<int32_t> *contiguity() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_CONTIGUITY);
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool is_cpu() const {
    return GetField<uint8_t>(VT_IS_CPU, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SIZES) &&
           verifier.VerifyVector(sizes()) &&
           VerifyOffset(verifier, VT_CONTIGUITY) &&
           verifier.VerifyVector(contiguity()) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_IS_CPU, 1) &&
           verifier.EndTable();
  }
};

struct TensorBuilder {
  typedef Tensor Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_sizes(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes) {
    fbb_.AddOffset(Tensor::VT_SIZES, sizes);
  }
  void add_contiguity(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> contiguity) {
    fbb_.AddOffset(Tensor::VT_CONTIGUITY, contiguity);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(Tensor::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  void add_is_cpu(bool is_cpu) {
    fbb_.AddElement<uint8_t>(Tensor::VT_IS_CPU, static_cast<uint8_t>(is_cpu), 0);
  }
  explicit TensorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Tensor> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Tensor>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Tensor> CreateTensor(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> contiguity = 0,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None,
    bool is_cpu = false) {
  TensorBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  builder_.add_contiguity(contiguity);
  builder_.add_sizes(sizes);
  builder_.add_is_cpu(is_cpu);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Tensor> CreateTensorDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *sizes = nullptr,
    const std::vector<int32_t> *contiguity = nullptr,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None,
    bool is_cpu = false) {
  auto sizes__ = sizes ? _fbb.CreateVector<int64_t>(*sizes) : 0;
  auto contiguity__ = contiguity ? _fbb.CreateVector<int32_t>(*contiguity) : 0;
  return nvfuser::serde::CreateTensor(
      _fbb,
      sizes__,
      contiguity__,
      dtype,
      is_cpu);
}

struct TensorCreation FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorCreationBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SHAPE = 4,
    VT_DTYPE = 6
  };
  const ::flatbuffers::Vector<int64_t> *shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SHAPE);
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SHAPE) &&
           verifier.VerifyVector(shape()) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
};

struct TensorCreationBuilder {
  typedef TensorCreation Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape) {
    fbb_.AddOffset(TensorCreation::VT_SHAPE, shape);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(TensorCreation::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit TensorCreationBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TensorCreation> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TensorCreation>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TensorCreation> CreateTensorCreation(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape = 0,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None) {
  TensorCreationBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  builder_.add_shape(shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TensorCreation> CreateTensorCreationDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *shape = nullptr,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None) {
  auto shape__ = shape ? _fbb.CreateVector<int64_t>(*shape) : 0;
  return nvfuser::serde::CreateTensorCreation(
      _fbb,
      shape__,
      dtype);
}

struct TensorCreationSymbolic FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorCreationSymbolicBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SHAPE = 4,
    VT_DTYPE = 6
  };
  const ::flatbuffers::Vector<const nvfuser::serde::State *> *shape() const {
    return GetPointer<const ::flatbuffers::Vector<const nvfuser::serde::State *> *>(VT_SHAPE);
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SHAPE) &&
           verifier.VerifyVector(shape()) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
};

struct TensorCreationSymbolicBuilder {
  typedef TensorCreationSymbolic Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_shape(::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> shape) {
    fbb_.AddOffset(TensorCreationSymbolic::VT_SHAPE, shape);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(TensorCreationSymbolic::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit TensorCreationSymbolicBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TensorCreationSymbolic> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TensorCreationSymbolic>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TensorCreationSymbolic> CreateTensorCreationSymbolic(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> shape = 0,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None) {
  TensorCreationSymbolicBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  builder_.add_shape(shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TensorCreationSymbolic> CreateTensorCreationSymbolicDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<nvfuser::serde::State> *shape = nullptr,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None) {
  auto shape__ = shape ? _fbb.CreateVectorOfStructs<nvfuser::serde::State>(*shape) : 0;
  return nvfuser::serde::CreateTensorCreationSymbolic(
      _fbb,
      shape__,
      dtype);
}

struct Vector FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef VectorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DTYPE = 4
  };
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
};

struct VectorBuilder {
  typedef Vector Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(Vector::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit VectorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Vector> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Vector>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Vector> CreateVector(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_None) {
  VectorBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  return builder_.Finish();
}

struct KernelSummary FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef KernelSummaryBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_HAS_COOPERATIVE_GRID_REDUCTION = 4,
    VT_HAS_DYNAMIC_LOCAL_MEMORY_ALLOCATIONS = 6,
    VT_HAS_BLOCK_REDUCTIONS = 8,
    VT_HAS_GRID_REDUCTIONS = 10,
    VT_HAS_BLOCK_BROADCASTS = 12,
    VT_HAS_GRID_BROADCASTS = 14,
    VT_HAS_BLOCK_WELFORD = 16,
    VT_HAS_GRID_WELFORD = 18,
    VT_HAS_OUTER_GROUPED_GRID_WELFORD = 20,
    VT_LARGEST_SMEM_DATA_TYPE = 22,
    VT_OUTER_GROUPED_GRID_WELFORD_LARGEST_SMEM_SIZE = 24,
    VT_GENERATOR = 26,
    VT_GLOBAL_ALLOCATIONS = 28,
    VT_DYNAMIC_SMEM_ALLOCATIONS = 30
  };
  bool has_cooperative_grid_reduction() const {
    return GetField<uint8_t>(VT_HAS_COOPERATIVE_GRID_REDUCTION, 0) != 0;
  }
  bool has_dynamic_local_memory_allocations() const {
    return GetField<uint8_t>(VT_HAS_DYNAMIC_LOCAL_MEMORY_ALLOCATIONS, 0) != 0;
  }
  bool has_block_reductions() const {
    return GetField<uint8_t>(VT_HAS_BLOCK_REDUCTIONS, 0) != 0;
  }
  bool has_grid_reductions() const {
    return GetField<uint8_t>(VT_HAS_GRID_REDUCTIONS, 0) != 0;
  }
  bool has_block_broadcasts() const {
    return GetField<uint8_t>(VT_HAS_BLOCK_BROADCASTS, 0) != 0;
  }
  bool has_grid_broadcasts() const {
    return GetField<uint8_t>(VT_HAS_GRID_BROADCASTS, 0) != 0;
  }
  bool has_block_welford() const {
    return GetField<uint8_t>(VT_HAS_BLOCK_WELFORD, 0) != 0;
  }
  bool has_grid_welford() const {
    return GetField<uint8_t>(VT_HAS_GRID_WELFORD, 0) != 0;
  }
  bool has_outer_grouped_grid_welford() const {
    return GetField<uint8_t>(VT_HAS_OUTER_GROUPED_GRID_WELFORD, 0) != 0;
  }
  nvfuser::serde::DataType largest_smem_data_type() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_LARGEST_SMEM_DATA_TYPE, 0));
  }
  int32_t outer_grouped_grid_welford_largest_smem_size() const {
    return GetField<int32_t>(VT_OUTER_GROUPED_GRID_WELFORD_LARGEST_SMEM_SIZE, 0);
  }
  const nvfuser::serde::NaiveValueGenerator *generator() const {
    return GetPointer<const nvfuser::serde::NaiveValueGenerator *>(VT_GENERATOR);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::AllocateBuffer>> *global_allocations() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::AllocateBuffer>> *>(VT_GLOBAL_ALLOCATIONS);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::AllocateBuffer>> *dynamic_smem_allocations() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::AllocateBuffer>> *>(VT_DYNAMIC_SMEM_ALLOCATIONS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_HAS_COOPERATIVE_GRID_REDUCTION, 1) &&
           VerifyField<uint8_t>(verifier, VT_HAS_DYNAMIC_LOCAL_MEMORY_ALLOCATIONS, 1) &&
           VerifyField<uint8_t>(verifier, VT_HAS_BLOCK_REDUCTIONS, 1) &&
           VerifyField<uint8_t>(verifier, VT_HAS_GRID_REDUCTIONS, 1) &&
           VerifyField<uint8_t>(verifier, VT_HAS_BLOCK_BROADCASTS, 1) &&
           VerifyField<uint8_t>(verifier, VT_HAS_GRID_BROADCASTS, 1) &&
           VerifyField<uint8_t>(verifier, VT_HAS_BLOCK_WELFORD, 1) &&
           VerifyField<uint8_t>(verifier, VT_HAS_GRID_WELFORD, 1) &&
           VerifyField<uint8_t>(verifier, VT_HAS_OUTER_GROUPED_GRID_WELFORD, 1) &&
           VerifyField<int32_t>(verifier, VT_LARGEST_SMEM_DATA_TYPE, 4) &&
           VerifyField<int32_t>(verifier, VT_OUTER_GROUPED_GRID_WELFORD_LARGEST_SMEM_SIZE, 4) &&
           VerifyOffset(verifier, VT_GENERATOR) &&
           verifier.VerifyTable(generator()) &&
           VerifyOffset(verifier, VT_GLOBAL_ALLOCATIONS) &&
           verifier.VerifyVector(global_allocations()) &&
           verifier.VerifyVectorOfTables(global_allocations()) &&
           VerifyOffset(verifier, VT_DYNAMIC_SMEM_ALLOCATIONS) &&
           verifier.VerifyVector(dynamic_smem_allocations()) &&
           verifier.VerifyVectorOfTables(dynamic_smem_allocations()) &&
           verifier.EndTable();
  }
};

struct KernelSummaryBuilder {
  typedef KernelSummary Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_has_cooperative_grid_reduction(bool has_cooperative_grid_reduction) {
    fbb_.AddElement<uint8_t>(KernelSummary::VT_HAS_COOPERATIVE_GRID_REDUCTION, static_cast<uint8_t>(has_cooperative_grid_reduction), 0);
  }
  void add_has_dynamic_local_memory_allocations(bool has_dynamic_local_memory_allocations) {
    fbb_.AddElement<uint8_t>(KernelSummary::VT_HAS_DYNAMIC_LOCAL_MEMORY_ALLOCATIONS, static_cast<uint8_t>(has_dynamic_local_memory_allocations), 0);
  }
  void add_has_block_reductions(bool has_block_reductions) {
    fbb_.AddElement<uint8_t>(KernelSummary::VT_HAS_BLOCK_REDUCTIONS, static_cast<uint8_t>(has_block_reductions), 0);
  }
  void add_has_grid_reductions(bool has_grid_reductions) {
    fbb_.AddElement<uint8_t>(KernelSummary::VT_HAS_GRID_REDUCTIONS, static_cast<uint8_t>(has_grid_reductions), 0);
  }
  void add_has_block_broadcasts(bool has_block_broadcasts) {
    fbb_.AddElement<uint8_t>(KernelSummary::VT_HAS_BLOCK_BROADCASTS, static_cast<uint8_t>(has_block_broadcasts), 0);
  }
  void add_has_grid_broadcasts(bool has_grid_broadcasts) {
    fbb_.AddElement<uint8_t>(KernelSummary::VT_HAS_GRID_BROADCASTS, static_cast<uint8_t>(has_grid_broadcasts), 0);
  }
  void add_has_block_welford(bool has_block_welford) {
    fbb_.AddElement<uint8_t>(KernelSummary::VT_HAS_BLOCK_WELFORD, static_cast<uint8_t>(has_block_welford), 0);
  }
  void add_has_grid_welford(bool has_grid_welford) {
    fbb_.AddElement<uint8_t>(KernelSummary::VT_HAS_GRID_WELFORD, static_cast<uint8_t>(has_grid_welford), 0);
  }
  void add_has_outer_grouped_grid_welford(bool has_outer_grouped_grid_welford) {
    fbb_.AddElement<uint8_t>(KernelSummary::VT_HAS_OUTER_GROUPED_GRID_WELFORD, static_cast<uint8_t>(has_outer_grouped_grid_welford), 0);
  }
  void add_largest_smem_data_type(nvfuser::serde::DataType largest_smem_data_type) {
    fbb_.AddElement<int32_t>(KernelSummary::VT_LARGEST_SMEM_DATA_TYPE, static_cast<int32_t>(largest_smem_data_type), 0);
  }
  void add_outer_grouped_grid_welford_largest_smem_size(int32_t outer_grouped_grid_welford_largest_smem_size) {
    fbb_.AddElement<int32_t>(KernelSummary::VT_OUTER_GROUPED_GRID_WELFORD_LARGEST_SMEM_SIZE, outer_grouped_grid_welford_largest_smem_size, 0);
  }
  void add_generator(::flatbuffers::Offset<nvfuser::serde::NaiveValueGenerator> generator) {
    fbb_.AddOffset(KernelSummary::VT_GENERATOR, generator);
  }
  void add_global_allocations(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::AllocateBuffer>>> global_allocations) {
    fbb_.AddOffset(KernelSummary::VT_GLOBAL_ALLOCATIONS, global_allocations);
  }
  void add_dynamic_smem_allocations(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::AllocateBuffer>>> dynamic_smem_allocations) {
    fbb_.AddOffset(KernelSummary::VT_DYNAMIC_SMEM_ALLOCATIONS, dynamic_smem_allocations);
  }
  explicit KernelSummaryBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<KernelSummary> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<KernelSummary>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<KernelSummary> CreateKernelSummary(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool has_cooperative_grid_reduction = false,
    bool has_dynamic_local_memory_allocations = false,
    bool has_block_reductions = false,
    bool has_grid_reductions = false,
    bool has_block_broadcasts = false,
    bool has_grid_broadcasts = false,
    bool has_block_welford = false,
    bool has_grid_welford = false,
    bool has_outer_grouped_grid_welford = false,
    nvfuser::serde::DataType largest_smem_data_type = nvfuser::serde::DataType_None,
    int32_t outer_grouped_grid_welford_largest_smem_size = 0,
    ::flatbuffers::Offset<nvfuser::serde::NaiveValueGenerator> generator = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::AllocateBuffer>>> global_allocations = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::AllocateBuffer>>> dynamic_smem_allocations = 0) {
  KernelSummaryBuilder builder_(_fbb);
  builder_.add_dynamic_smem_allocations(dynamic_smem_allocations);
  builder_.add_global_allocations(global_allocations);
  builder_.add_generator(generator);
  builder_.add_outer_grouped_grid_welford_largest_smem_size(outer_grouped_grid_welford_largest_smem_size);
  builder_.add_largest_smem_data_type(largest_smem_data_type);
  builder_.add_has_outer_grouped_grid_welford(has_outer_grouped_grid_welford);
  builder_.add_has_grid_welford(has_grid_welford);
  builder_.add_has_block_welford(has_block_welford);
  builder_.add_has_grid_broadcasts(has_grid_broadcasts);
  builder_.add_has_block_broadcasts(has_block_broadcasts);
  builder_.add_has_grid_reductions(has_grid_reductions);
  builder_.add_has_block_reductions(has_block_reductions);
  builder_.add_has_dynamic_local_memory_allocations(has_dynamic_local_memory_allocations);
  builder_.add_has_cooperative_grid_reduction(has_cooperative_grid_reduction);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<KernelSummary> CreateKernelSummaryDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool has_cooperative_grid_reduction = false,
    bool has_dynamic_local_memory_allocations = false,
    bool has_block_reductions = false,
    bool has_grid_reductions = false,
    bool has_block_broadcasts = false,
    bool has_grid_broadcasts = false,
    bool has_block_welford = false,
    bool has_grid_welford = false,
    bool has_outer_grouped_grid_welford = false,
    nvfuser::serde::DataType largest_smem_data_type = nvfuser::serde::DataType_None,
    int32_t outer_grouped_grid_welford_largest_smem_size = 0,
    ::flatbuffers::Offset<nvfuser::serde::NaiveValueGenerator> generator = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::AllocateBuffer>> *global_allocations = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::AllocateBuffer>> *dynamic_smem_allocations = nullptr) {
  auto global_allocations__ = global_allocations ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::AllocateBuffer>>(*global_allocations) : 0;
  auto dynamic_smem_allocations__ = dynamic_smem_allocations ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::AllocateBuffer>>(*dynamic_smem_allocations) : 0;
  return nvfuser::serde::CreateKernelSummary(
      _fbb,
      has_cooperative_grid_reduction,
      has_dynamic_local_memory_allocations,
      has_block_reductions,
      has_grid_reductions,
      has_block_broadcasts,
      has_grid_broadcasts,
      has_block_welford,
      has_grid_welford,
      has_outer_grouped_grid_welford,
      largest_smem_data_type,
      outer_grouped_grid_welford_largest_smem_size,
      generator,
      global_allocations__,
      dynamic_smem_allocations__);
}

struct FusionExecutor FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef FusionExecutorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DEVICE_SMEM_LIMIT = 4,
    VT_BLOCK_SIZE_HIGH_WATER_MARK = 6,
    VT_MAXRREGCOUNT_HIGH_WATER_MARK = 8,
    VT_WARP_SIZE = 10,
    VT_FUSION_ID = 12,
    VT_FUSION_ID_COUNTER = 14,
    VT_KERNEL_CODE = 16,
    VT_EXECUTOR_ENTRY_LOOKUP_KEYS = 18,
    VT_EXECUTOR_ENTRY_LOOKUP_VALUES = 20,
    VT_INDEX_TYPE = 22,
    VT_SUMMARY = 24
  };
  int64_t device_smem_limit() const {
    return GetField<int64_t>(VT_DEVICE_SMEM_LIMIT, 0);
  }
  int64_t block_size_high_water_mark() const {
    return GetField<int64_t>(VT_BLOCK_SIZE_HIGH_WATER_MARK, 0);
  }
  int64_t maxrregcount_high_water_mark() const {
    return GetField<int64_t>(VT_MAXRREGCOUNT_HIGH_WATER_MARK, 0);
  }
  int64_t warp_size() const {
    return GetField<int64_t>(VT_WARP_SIZE, 0);
  }
  int64_t fusion_id() const {
    return GetField<int64_t>(VT_FUSION_ID, 0);
  }
  int64_t fusion_id_counter() const {
    return GetField<int64_t>(VT_FUSION_ID_COUNTER, 0);
  }
  const ::flatbuffers::String *kernel_code() const {
    return GetPointer<const ::flatbuffers::String *>(VT_KERNEL_CODE);
  }
  const ::flatbuffers::Vector<uint64_t> *executor_entry_lookup_keys() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_EXECUTOR_ENTRY_LOOKUP_KEYS);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>> *executor_entry_lookup_values() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>> *>(VT_EXECUTOR_ENTRY_LOOKUP_VALUES);
  }
  nvfuser::serde::DataType index_type() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_INDEX_TYPE, 0));
  }
  const nvfuser::serde::KernelSummary *summary() const {
    return GetPointer<const nvfuser::serde::KernelSummary *>(VT_SUMMARY);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DEVICE_SMEM_LIMIT, 8) &&
           VerifyField<int64_t>(verifier, VT_BLOCK_SIZE_HIGH_WATER_MARK, 8) &&
           VerifyField<int64_t>(verifier, VT_MAXRREGCOUNT_HIGH_WATER_MARK, 8) &&
           VerifyField<int64_t>(verifier, VT_WARP_SIZE, 8) &&
           VerifyField<int64_t>(verifier, VT_FUSION_ID, 8) &&
           VerifyField<int64_t>(verifier, VT_FUSION_ID_COUNTER, 8) &&
           VerifyOffset(verifier, VT_KERNEL_CODE) &&
           verifier.VerifyString(kernel_code()) &&
           VerifyOffset(verifier, VT_EXECUTOR_ENTRY_LOOKUP_KEYS) &&
           verifier.VerifyVector(executor_entry_lookup_keys()) &&
           VerifyOffset(verifier, VT_EXECUTOR_ENTRY_LOOKUP_VALUES) &&
           verifier.VerifyVector(executor_entry_lookup_values()) &&
           verifier.VerifyVectorOfTables(executor_entry_lookup_values()) &&
           VerifyField<int32_t>(verifier, VT_INDEX_TYPE, 4) &&
           VerifyOffset(verifier, VT_SUMMARY) &&
           verifier.VerifyTable(summary()) &&
           verifier.EndTable();
  }
};

struct FusionExecutorBuilder {
  typedef FusionExecutor Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_device_smem_limit(int64_t device_smem_limit) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_DEVICE_SMEM_LIMIT, device_smem_limit, 0);
  }
  void add_block_size_high_water_mark(int64_t block_size_high_water_mark) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_BLOCK_SIZE_HIGH_WATER_MARK, block_size_high_water_mark, 0);
  }
  void add_maxrregcount_high_water_mark(int64_t maxrregcount_high_water_mark) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_MAXRREGCOUNT_HIGH_WATER_MARK, maxrregcount_high_water_mark, 0);
  }
  void add_warp_size(int64_t warp_size) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_WARP_SIZE, warp_size, 0);
  }
  void add_fusion_id(int64_t fusion_id) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_FUSION_ID, fusion_id, 0);
  }
  void add_fusion_id_counter(int64_t fusion_id_counter) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_FUSION_ID_COUNTER, fusion_id_counter, 0);
  }
  void add_kernel_code(::flatbuffers::Offset<::flatbuffers::String> kernel_code) {
    fbb_.AddOffset(FusionExecutor::VT_KERNEL_CODE, kernel_code);
  }
  void add_executor_entry_lookup_keys(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> executor_entry_lookup_keys) {
    fbb_.AddOffset(FusionExecutor::VT_EXECUTOR_ENTRY_LOOKUP_KEYS, executor_entry_lookup_keys);
  }
  void add_executor_entry_lookup_values(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>>> executor_entry_lookup_values) {
    fbb_.AddOffset(FusionExecutor::VT_EXECUTOR_ENTRY_LOOKUP_VALUES, executor_entry_lookup_values);
  }
  void add_index_type(nvfuser::serde::DataType index_type) {
    fbb_.AddElement<int32_t>(FusionExecutor::VT_INDEX_TYPE, static_cast<int32_t>(index_type), 0);
  }
  void add_summary(::flatbuffers::Offset<nvfuser::serde::KernelSummary> summary) {
    fbb_.AddOffset(FusionExecutor::VT_SUMMARY, summary);
  }
  explicit FusionExecutorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<FusionExecutor> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<FusionExecutor>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<FusionExecutor> CreateFusionExecutor(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t device_smem_limit = 0,
    int64_t block_size_high_water_mark = 0,
    int64_t maxrregcount_high_water_mark = 0,
    int64_t warp_size = 0,
    int64_t fusion_id = 0,
    int64_t fusion_id_counter = 0,
    ::flatbuffers::Offset<::flatbuffers::String> kernel_code = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> executor_entry_lookup_keys = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>>> executor_entry_lookup_values = 0,
    nvfuser::serde::DataType index_type = nvfuser::serde::DataType_None,
    ::flatbuffers::Offset<nvfuser::serde::KernelSummary> summary = 0) {
  FusionExecutorBuilder builder_(_fbb);
  builder_.add_fusion_id_counter(fusion_id_counter);
  builder_.add_fusion_id(fusion_id);
  builder_.add_warp_size(warp_size);
  builder_.add_maxrregcount_high_water_mark(maxrregcount_high_water_mark);
  builder_.add_block_size_high_water_mark(block_size_high_water_mark);
  builder_.add_device_smem_limit(device_smem_limit);
  builder_.add_summary(summary);
  builder_.add_index_type(index_type);
  builder_.add_executor_entry_lookup_values(executor_entry_lookup_values);
  builder_.add_executor_entry_lookup_keys(executor_entry_lookup_keys);
  builder_.add_kernel_code(kernel_code);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<FusionExecutor> CreateFusionExecutorDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t device_smem_limit = 0,
    int64_t block_size_high_water_mark = 0,
    int64_t maxrregcount_high_water_mark = 0,
    int64_t warp_size = 0,
    int64_t fusion_id = 0,
    int64_t fusion_id_counter = 0,
    const char *kernel_code = nullptr,
    const std::vector<uint64_t> *executor_entry_lookup_keys = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>> *executor_entry_lookup_values = nullptr,
    nvfuser::serde::DataType index_type = nvfuser::serde::DataType_None,
    ::flatbuffers::Offset<nvfuser::serde::KernelSummary> summary = 0) {
  auto kernel_code__ = kernel_code ? _fbb.CreateString(kernel_code) : 0;
  auto executor_entry_lookup_keys__ = executor_entry_lookup_keys ? _fbb.CreateVector<uint64_t>(*executor_entry_lookup_keys) : 0;
  auto executor_entry_lookup_values__ = executor_entry_lookup_values ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>>(*executor_entry_lookup_values) : 0;
  return nvfuser::serde::CreateFusionExecutor(
      _fbb,
      device_smem_limit,
      block_size_high_water_mark,
      maxrregcount_high_water_mark,
      warp_size,
      fusion_id,
      fusion_id_counter,
      kernel_code__,
      executor_entry_lookup_keys__,
      executor_entry_lookup_values__,
      index_type,
      summary);
}

struct FusionKernelRuntime FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef FusionKernelRuntimeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ARGS = 4,
    VT_EXECUTORS = 6
  };
  const nvfuser::serde::KernelArgumentHolder *args() const {
    return GetPointer<const nvfuser::serde::KernelArgumentHolder *>(VT_ARGS);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>> *executors() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>> *>(VT_EXECUTORS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ARGS) &&
           verifier.VerifyTable(args()) &&
           VerifyOffset(verifier, VT_EXECUTORS) &&
           verifier.VerifyVector(executors()) &&
           verifier.VerifyVectorOfTables(executors()) &&
           verifier.EndTable();
  }
};

struct FusionKernelRuntimeBuilder {
  typedef FusionKernelRuntime Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_args(::flatbuffers::Offset<nvfuser::serde::KernelArgumentHolder> args) {
    fbb_.AddOffset(FusionKernelRuntime::VT_ARGS, args);
  }
  void add_executors(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>>> executors) {
    fbb_.AddOffset(FusionKernelRuntime::VT_EXECUTORS, executors);
  }
  explicit FusionKernelRuntimeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<FusionKernelRuntime> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<FusionKernelRuntime>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<FusionKernelRuntime> CreateFusionKernelRuntime(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::KernelArgumentHolder> args = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>>> executors = 0) {
  FusionKernelRuntimeBuilder builder_(_fbb);
  builder_.add_executors(executors);
  builder_.add_args(args);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<FusionKernelRuntime> CreateFusionKernelRuntimeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::KernelArgumentHolder> args = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>> *executors = nullptr) {
  auto executors__ = executors ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>>(*executors) : 0;
  return nvfuser::serde::CreateFusionKernelRuntime(
      _fbb,
      args,
      executors__);
}

struct InputsIdLookup FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef InputsIdLookupBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_MAX_CACHE_SIZE = 4,
    VT_CURRENT_ID = 6,
    VT_LRU_CACHE = 8,
    VT_ENCODING_LOOKUP_KEYS = 10,
    VT_ENCODING_LOOKUP_VALUES = 12
  };
  uint64_t max_cache_size() const {
    return GetField<uint64_t>(VT_MAX_CACHE_SIZE, 0);
  }
  uint64_t current_id() const {
    return GetField<uint64_t>(VT_CURRENT_ID, 0);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>> *lru_cache() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>> *>(VT_LRU_CACHE);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>> *encoding_lookup_keys() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>> *>(VT_ENCODING_LOOKUP_KEYS);
  }
  const ::flatbuffers::Vector<const nvfuser::serde::EncodingEntry *> *encoding_lookup_values() const {
    return GetPointer<const ::flatbuffers::Vector<const nvfuser::serde::EncodingEntry *> *>(VT_ENCODING_LOOKUP_VALUES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_MAX_CACHE_SIZE, 8) &&
           VerifyField<uint64_t>(verifier, VT_CURRENT_ID, 8) &&
           VerifyOffset(verifier, VT_LRU_CACHE) &&
           verifier.VerifyVector(lru_cache()) &&
           verifier.VerifyVectorOfStrings(lru_cache()) &&
           VerifyOffset(verifier, VT_ENCODING_LOOKUP_KEYS) &&
           verifier.VerifyVector(encoding_lookup_keys()) &&
           verifier.VerifyVectorOfStrings(encoding_lookup_keys()) &&
           VerifyOffset(verifier, VT_ENCODING_LOOKUP_VALUES) &&
           verifier.VerifyVector(encoding_lookup_values()) &&
           verifier.EndTable();
  }
};

struct InputsIdLookupBuilder {
  typedef InputsIdLookup Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_max_cache_size(uint64_t max_cache_size) {
    fbb_.AddElement<uint64_t>(InputsIdLookup::VT_MAX_CACHE_SIZE, max_cache_size, 0);
  }
  void add_current_id(uint64_t current_id) {
    fbb_.AddElement<uint64_t>(InputsIdLookup::VT_CURRENT_ID, current_id, 0);
  }
  void add_lru_cache(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>>> lru_cache) {
    fbb_.AddOffset(InputsIdLookup::VT_LRU_CACHE, lru_cache);
  }
  void add_encoding_lookup_keys(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>>> encoding_lookup_keys) {
    fbb_.AddOffset(InputsIdLookup::VT_ENCODING_LOOKUP_KEYS, encoding_lookup_keys);
  }
  void add_encoding_lookup_values(::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::EncodingEntry *>> encoding_lookup_values) {
    fbb_.AddOffset(InputsIdLookup::VT_ENCODING_LOOKUP_VALUES, encoding_lookup_values);
  }
  explicit InputsIdLookupBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<InputsIdLookup> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<InputsIdLookup>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<InputsIdLookup> CreateInputsIdLookup(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t max_cache_size = 0,
    uint64_t current_id = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>>> lru_cache = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>>> encoding_lookup_keys = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::EncodingEntry *>> encoding_lookup_values = 0) {
  InputsIdLookupBuilder builder_(_fbb);
  builder_.add_current_id(current_id);
  builder_.add_max_cache_size(max_cache_size);
  builder_.add_encoding_lookup_values(encoding_lookup_values);
  builder_.add_encoding_lookup_keys(encoding_lookup_keys);
  builder_.add_lru_cache(lru_cache);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<InputsIdLookup> CreateInputsIdLookupDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t max_cache_size = 0,
    uint64_t current_id = 0,
    const std::vector<::flatbuffers::Offset<::flatbuffers::String>> *lru_cache = nullptr,
    const std::vector<::flatbuffers::Offset<::flatbuffers::String>> *encoding_lookup_keys = nullptr,
    const std::vector<nvfuser::serde::EncodingEntry> *encoding_lookup_values = nullptr) {
  auto lru_cache__ = lru_cache ? _fbb.CreateVector<::flatbuffers::Offset<::flatbuffers::String>>(*lru_cache) : 0;
  auto encoding_lookup_keys__ = encoding_lookup_keys ? _fbb.CreateVector<::flatbuffers::Offset<::flatbuffers::String>>(*encoding_lookup_keys) : 0;
  auto encoding_lookup_values__ = encoding_lookup_values ? _fbb.CreateVectorOfStructs<nvfuser::serde::EncodingEntry>(*encoding_lookup_values) : 0;
  return nvfuser::serde::CreateInputsIdLookup(
      _fbb,
      max_cache_size,
      current_id,
      lru_cache__,
      encoding_lookup_keys__,
      encoding_lookup_values__);
}

struct KernelRuntimeState FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef KernelRuntimeStateBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DEVICE_ID = 4,
    VT_HAS_DYNAMIC_TRANSFORM_INFO = 6,
    VT_RUNTIMES = 8
  };
  uint64_t device_id() const {
    return GetField<uint64_t>(VT_DEVICE_ID, 0);
  }
  bool has_dynamic_transform_info() const {
    return GetField<uint8_t>(VT_HAS_DYNAMIC_TRANSFORM_INFO, 0) != 0;
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>> *runtimes() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>> *>(VT_RUNTIMES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_DEVICE_ID, 8) &&
           VerifyField<uint8_t>(verifier, VT_HAS_DYNAMIC_TRANSFORM_INFO, 1) &&
           VerifyOffset(verifier, VT_RUNTIMES) &&
           verifier.VerifyVector(runtimes()) &&
           verifier.VerifyVectorOfTables(runtimes()) &&
           verifier.EndTable();
  }
};

struct KernelRuntimeStateBuilder {
  typedef KernelRuntimeState Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_device_id(uint64_t device_id) {
    fbb_.AddElement<uint64_t>(KernelRuntimeState::VT_DEVICE_ID, device_id, 0);
  }
  void add_has_dynamic_transform_info(bool has_dynamic_transform_info) {
    fbb_.AddElement<uint8_t>(KernelRuntimeState::VT_HAS_DYNAMIC_TRANSFORM_INFO, static_cast<uint8_t>(has_dynamic_transform_info), 0);
  }
  void add_runtimes(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>>> runtimes) {
    fbb_.AddOffset(KernelRuntimeState::VT_RUNTIMES, runtimes);
  }
  explicit KernelRuntimeStateBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<KernelRuntimeState> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<KernelRuntimeState>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<KernelRuntimeState> CreateKernelRuntimeState(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t device_id = 0,
    bool has_dynamic_transform_info = false,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>>> runtimes = 0) {
  KernelRuntimeStateBuilder builder_(_fbb);
  builder_.add_device_id(device_id);
  builder_.add_runtimes(runtimes);
  builder_.add_has_dynamic_transform_info(has_dynamic_transform_info);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<KernelRuntimeState> CreateKernelRuntimeStateDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t device_id = 0,
    bool has_dynamic_transform_info = false,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>> *runtimes = nullptr) {
  auto runtimes__ = runtimes ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>>(*runtimes) : 0;
  return nvfuser::serde::CreateKernelRuntimeState(
      _fbb,
      device_id,
      has_dynamic_transform_info,
      runtimes__);
}

struct FusionExecutorCache FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef FusionExecutorCacheBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INPUTS_CACHE = 4,
    VT_KERNEL_RUNTIMES_MAP = 6,
    VT_KERNEL_CACHE_KEYS = 8,
    VT_KERNEL_CACHE_VALUES = 10
  };
  const nvfuser::serde::InputsIdLookup *inputs_cache() const {
    return GetPointer<const nvfuser::serde::InputsIdLookup *>(VT_INPUTS_CACHE);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>> *kernel_runtimes_map() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>> *>(VT_KERNEL_RUNTIMES_MAP);
  }
  const ::flatbuffers::Vector<uint64_t> *kernel_cache_keys() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_KERNEL_CACHE_KEYS);
  }
  const ::flatbuffers::Vector<uint64_t> *kernel_cache_values() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_KERNEL_CACHE_VALUES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_INPUTS_CACHE) &&
           verifier.VerifyTable(inputs_cache()) &&
           VerifyOffset(verifier, VT_KERNEL_RUNTIMES_MAP) &&
           verifier.VerifyVector(kernel_runtimes_map()) &&
           verifier.VerifyVectorOfTables(kernel_runtimes_map()) &&
           VerifyOffset(verifier, VT_KERNEL_CACHE_KEYS) &&
           verifier.VerifyVector(kernel_cache_keys()) &&
           VerifyOffset(verifier, VT_KERNEL_CACHE_VALUES) &&
           verifier.VerifyVector(kernel_cache_values()) &&
           verifier.EndTable();
  }
};

struct FusionExecutorCacheBuilder {
  typedef FusionExecutorCache Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_inputs_cache(::flatbuffers::Offset<nvfuser::serde::InputsIdLookup> inputs_cache) {
    fbb_.AddOffset(FusionExecutorCache::VT_INPUTS_CACHE, inputs_cache);
  }
  void add_kernel_runtimes_map(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>>> kernel_runtimes_map) {
    fbb_.AddOffset(FusionExecutorCache::VT_KERNEL_RUNTIMES_MAP, kernel_runtimes_map);
  }
  void add_kernel_cache_keys(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> kernel_cache_keys) {
    fbb_.AddOffset(FusionExecutorCache::VT_KERNEL_CACHE_KEYS, kernel_cache_keys);
  }
  void add_kernel_cache_values(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> kernel_cache_values) {
    fbb_.AddOffset(FusionExecutorCache::VT_KERNEL_CACHE_VALUES, kernel_cache_values);
  }
  explicit FusionExecutorCacheBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<FusionExecutorCache> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<FusionExecutorCache>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<FusionExecutorCache> CreateFusionExecutorCache(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::InputsIdLookup> inputs_cache = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>>> kernel_runtimes_map = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> kernel_cache_keys = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> kernel_cache_values = 0) {
  FusionExecutorCacheBuilder builder_(_fbb);
  builder_.add_kernel_cache_values(kernel_cache_values);
  builder_.add_kernel_cache_keys(kernel_cache_keys);
  builder_.add_kernel_runtimes_map(kernel_runtimes_map);
  builder_.add_inputs_cache(inputs_cache);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<FusionExecutorCache> CreateFusionExecutorCacheDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::InputsIdLookup> inputs_cache = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>> *kernel_runtimes_map = nullptr,
    const std::vector<uint64_t> *kernel_cache_keys = nullptr,
    const std::vector<uint64_t> *kernel_cache_values = nullptr) {
  auto kernel_runtimes_map__ = kernel_runtimes_map ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>>(*kernel_runtimes_map) : 0;
  auto kernel_cache_keys__ = kernel_cache_keys ? _fbb.CreateVector<uint64_t>(*kernel_cache_keys) : 0;
  auto kernel_cache_values__ = kernel_cache_values ? _fbb.CreateVector<uint64_t>(*kernel_cache_values) : 0;
  return nvfuser::serde::CreateFusionExecutorCache(
      _fbb,
      inputs_cache,
      kernel_runtimes_map__,
      kernel_cache_keys__,
      kernel_cache_values__);
}

struct RecordFunctor FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef RecordFunctorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ARGS = 4,
    VT_OUTPUTS = 6,
    VT_NAME = 8,
    VT_TYPE = 10,
    VT_DATA_TYPE = 12,
    VT_DATA = 14
  };
  const ::flatbuffers::Vector<const nvfuser::serde::State *> *args() const {
    return GetPointer<const ::flatbuffers::Vector<const nvfuser::serde::State *> *>(VT_ARGS);
  }
  const ::flatbuffers::Vector<const nvfuser::serde::State *> *outputs() const {
    return GetPointer<const ::flatbuffers::Vector<const nvfuser::serde::State *> *>(VT_OUTPUTS);
  }
  const ::flatbuffers::String *name() const {
    return GetPointer<const ::flatbuffers::String *>(VT_NAME);
  }
  nvfuser::serde::RecordType type() const {
    return static_cast<nvfuser::serde::RecordType>(GetField<int32_t>(VT_TYPE, 0));
  }
  nvfuser::serde::RecordData data_type() const {
    return static_cast<nvfuser::serde::RecordData>(GetField<uint8_t>(VT_DATA_TYPE, 0));
  }
  const void *data() const {
    return GetPointer<const void *>(VT_DATA);
  }
  template<typename T> const T *data_as() const;
  const nvfuser::serde::At *data_as_At() const {
    return data_type() == nvfuser::serde::RecordData_At ? static_cast<const nvfuser::serde::At *>(data()) : nullptr;
  }
  const nvfuser::serde::BatchNorm *data_as_BatchNorm() const {
    return data_type() == nvfuser::serde::RecordData_BatchNorm ? static_cast<const nvfuser::serde::BatchNorm *>(data()) : nullptr;
  }
  const nvfuser::serde::Broadcast *data_as_Broadcast() const {
    return data_type() == nvfuser::serde::RecordData_Broadcast ? static_cast<const nvfuser::serde::Broadcast *>(data()) : nullptr;
  }
  const nvfuser::serde::BroadcastInDim *data_as_BroadcastInDim() const {
    return data_type() == nvfuser::serde::RecordData_BroadcastInDim ? static_cast<const nvfuser::serde::BroadcastInDim *>(data()) : nullptr;
  }
  const nvfuser::serde::BroadcastInDimSymbolic *data_as_BroadcastInDimSymbolic() const {
    return data_type() == nvfuser::serde::RecordData_BroadcastInDimSymbolic ? static_cast<const nvfuser::serde::BroadcastInDimSymbolic *>(data()) : nullptr;
  }
  const nvfuser::serde::Dimension *data_as_Dimension() const {
    return data_type() == nvfuser::serde::RecordData_Dimension ? static_cast<const nvfuser::serde::Dimension *>(data()) : nullptr;
  }
  const nvfuser::serde::Dtype *data_as_Dtype() const {
    return data_type() == nvfuser::serde::RecordData_Dtype ? static_cast<const nvfuser::serde::Dtype *>(data()) : nullptr;
  }
  const nvfuser::serde::Norm *data_as_Norm() const {
    return data_type() == nvfuser::serde::RecordData_Norm ? static_cast<const nvfuser::serde::Norm *>(data()) : nullptr;
  }
  const nvfuser::serde::Output *data_as_Output() const {
    return data_type() == nvfuser::serde::RecordData_Output ? static_cast<const nvfuser::serde::Output *>(data()) : nullptr;
  }
  const nvfuser::serde::Pad *data_as_Pad() const {
    return data_type() == nvfuser::serde::RecordData_Pad ? static_cast<const nvfuser::serde::Pad *>(data()) : nullptr;
  }
  const nvfuser::serde::Permute *data_as_Permute() const {
    return data_type() == nvfuser::serde::RecordData_Permute ? static_cast<const nvfuser::serde::Permute *>(data()) : nullptr;
  }
  const nvfuser::serde::Slice *data_as_Slice() const {
    return data_type() == nvfuser::serde::RecordData_Slice ? static_cast<const nvfuser::serde::Slice *>(data()) : nullptr;
  }
  const nvfuser::serde::Squeeze *data_as_Squeeze() const {
    return data_type() == nvfuser::serde::RecordData_Squeeze ? static_cast<const nvfuser::serde::Squeeze *>(data()) : nullptr;
  }
  const nvfuser::serde::Reduction *data_as_Reduction() const {
    return data_type() == nvfuser::serde::RecordData_Reduction ? static_cast<const nvfuser::serde::Reduction *>(data()) : nullptr;
  }
  const nvfuser::serde::Reshape *data_as_Reshape() const {
    return data_type() == nvfuser::serde::RecordData_Reshape ? static_cast<const nvfuser::serde::Reshape *>(data()) : nullptr;
  }
  const nvfuser::serde::Scalar *data_as_Scalar() const {
    return data_type() == nvfuser::serde::RecordData_Scalar ? static_cast<const nvfuser::serde::Scalar *>(data()) : nullptr;
  }
  const nvfuser::serde::Size *data_as_Size() const {
    return data_type() == nvfuser::serde::RecordData_Size ? static_cast<const nvfuser::serde::Size *>(data()) : nullptr;
  }
  const nvfuser::serde::Tensor *data_as_Tensor() const {
    return data_type() == nvfuser::serde::RecordData_Tensor ? static_cast<const nvfuser::serde::Tensor *>(data()) : nullptr;
  }
  const nvfuser::serde::TensorCreation *data_as_TensorCreation() const {
    return data_type() == nvfuser::serde::RecordData_TensorCreation ? static_cast<const nvfuser::serde::TensorCreation *>(data()) : nullptr;
  }
  const nvfuser::serde::TensorCreationSymbolic *data_as_TensorCreationSymbolic() const {
    return data_type() == nvfuser::serde::RecordData_TensorCreationSymbolic ? static_cast<const nvfuser::serde::TensorCreationSymbolic *>(data()) : nullptr;
  }
  const nvfuser::serde::Vector *data_as_Vector() const {
    return data_type() == nvfuser::serde::RecordData_Vector ? static_cast<const nvfuser::serde::Vector *>(data()) : nullptr;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ARGS) &&
           verifier.VerifyVector(args()) &&
           VerifyOffset(verifier, VT_OUTPUTS) &&
           verifier.VerifyVector(outputs()) &&
           VerifyOffset(verifier, VT_NAME) &&
           verifier.VerifyString(name()) &&
           VerifyField<int32_t>(verifier, VT_TYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_DATA_TYPE, 1) &&
           VerifyOffset(verifier, VT_DATA) &&
           VerifyRecordData(verifier, data(), data_type()) &&
           verifier.EndTable();
  }
};

template<> inline const nvfuser::serde::At *RecordFunctor::data_as<nvfuser::serde::At>() const {
  return data_as_At();
}

template<> inline const nvfuser::serde::BatchNorm *RecordFunctor::data_as<nvfuser::serde::BatchNorm>() const {
  return data_as_BatchNorm();
}

template<> inline const nvfuser::serde::Broadcast *RecordFunctor::data_as<nvfuser::serde::Broadcast>() const {
  return data_as_Broadcast();
}

template<> inline const nvfuser::serde::BroadcastInDim *RecordFunctor::data_as<nvfuser::serde::BroadcastInDim>() const {
  return data_as_BroadcastInDim();
}

template<> inline const nvfuser::serde::BroadcastInDimSymbolic *RecordFunctor::data_as<nvfuser::serde::BroadcastInDimSymbolic>() const {
  return data_as_BroadcastInDimSymbolic();
}

template<> inline const nvfuser::serde::Dimension *RecordFunctor::data_as<nvfuser::serde::Dimension>() const {
  return data_as_Dimension();
}

template<> inline const nvfuser::serde::Dtype *RecordFunctor::data_as<nvfuser::serde::Dtype>() const {
  return data_as_Dtype();
}

template<> inline const nvfuser::serde::Norm *RecordFunctor::data_as<nvfuser::serde::Norm>() const {
  return data_as_Norm();
}

template<> inline const nvfuser::serde::Output *RecordFunctor::data_as<nvfuser::serde::Output>() const {
  return data_as_Output();
}

template<> inline const nvfuser::serde::Pad *RecordFunctor::data_as<nvfuser::serde::Pad>() const {
  return data_as_Pad();
}

template<> inline const nvfuser::serde::Permute *RecordFunctor::data_as<nvfuser::serde::Permute>() const {
  return data_as_Permute();
}

template<> inline const nvfuser::serde::Slice *RecordFunctor::data_as<nvfuser::serde::Slice>() const {
  return data_as_Slice();
}

template<> inline const nvfuser::serde::Squeeze *RecordFunctor::data_as<nvfuser::serde::Squeeze>() const {
  return data_as_Squeeze();
}

template<> inline const nvfuser::serde::Reduction *RecordFunctor::data_as<nvfuser::serde::Reduction>() const {
  return data_as_Reduction();
}

template<> inline const nvfuser::serde::Reshape *RecordFunctor::data_as<nvfuser::serde::Reshape>() const {
  return data_as_Reshape();
}

template<> inline const nvfuser::serde::Scalar *RecordFunctor::data_as<nvfuser::serde::Scalar>() const {
  return data_as_Scalar();
}

template<> inline const nvfuser::serde::Size *RecordFunctor::data_as<nvfuser::serde::Size>() const {
  return data_as_Size();
}

template<> inline const nvfuser::serde::Tensor *RecordFunctor::data_as<nvfuser::serde::Tensor>() const {
  return data_as_Tensor();
}

template<> inline const nvfuser::serde::TensorCreation *RecordFunctor::data_as<nvfuser::serde::TensorCreation>() const {
  return data_as_TensorCreation();
}

template<> inline const nvfuser::serde::TensorCreationSymbolic *RecordFunctor::data_as<nvfuser::serde::TensorCreationSymbolic>() const {
  return data_as_TensorCreationSymbolic();
}

template<> inline const nvfuser::serde::Vector *RecordFunctor::data_as<nvfuser::serde::Vector>() const {
  return data_as_Vector();
}

struct RecordFunctorBuilder {
  typedef RecordFunctor Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_args(::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> args) {
    fbb_.AddOffset(RecordFunctor::VT_ARGS, args);
  }
  void add_outputs(::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> outputs) {
    fbb_.AddOffset(RecordFunctor::VT_OUTPUTS, outputs);
  }
  void add_name(::flatbuffers::Offset<::flatbuffers::String> name) {
    fbb_.AddOffset(RecordFunctor::VT_NAME, name);
  }
  void add_type(nvfuser::serde::RecordType type) {
    fbb_.AddElement<int32_t>(RecordFunctor::VT_TYPE, static_cast<int32_t>(type), 0);
  }
  void add_data_type(nvfuser::serde::RecordData data_type) {
    fbb_.AddElement<uint8_t>(RecordFunctor::VT_DATA_TYPE, static_cast<uint8_t>(data_type), 0);
  }
  void add_data(::flatbuffers::Offset<void> data) {
    fbb_.AddOffset(RecordFunctor::VT_DATA, data);
  }
  explicit RecordFunctorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<RecordFunctor> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<RecordFunctor>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<RecordFunctor> CreateRecordFunctor(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> args = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> outputs = 0,
    ::flatbuffers::Offset<::flatbuffers::String> name = 0,
    nvfuser::serde::RecordType type = nvfuser::serde::RecordType_Base,
    nvfuser::serde::RecordData data_type = nvfuser::serde::RecordData_NONE,
    ::flatbuffers::Offset<void> data = 0) {
  RecordFunctorBuilder builder_(_fbb);
  builder_.add_data(data);
  builder_.add_type(type);
  builder_.add_name(name);
  builder_.add_outputs(outputs);
  builder_.add_args(args);
  builder_.add_data_type(data_type);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<RecordFunctor> CreateRecordFunctorDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<nvfuser::serde::State> *args = nullptr,
    const std::vector<nvfuser::serde::State> *outputs = nullptr,
    const char *name = nullptr,
    nvfuser::serde::RecordType type = nvfuser::serde::RecordType_Base,
    nvfuser::serde::RecordData data_type = nvfuser::serde::RecordData_NONE,
    ::flatbuffers::Offset<void> data = 0) {
  auto args__ = args ? _fbb.CreateVectorOfStructs<nvfuser::serde::State>(*args) : 0;
  auto outputs__ = outputs ? _fbb.CreateVectorOfStructs<nvfuser::serde::State>(*outputs) : 0;
  auto name__ = name ? _fbb.CreateString(name) : 0;
  return nvfuser::serde::CreateRecordFunctor(
      _fbb,
      args__,
      outputs__,
      name__,
      type,
      data_type,
      data);
}

struct TrieNode FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TrieNodeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_RECORD = 4,
    VT_CHILDREN = 6,
    VT_FUSION_ID = 8,
    VT_VISITS = 10,
    VT_IS_TERMINAL = 12
  };
  const nvfuser::serde::RecordFunctor *record() const {
    return GetPointer<const nvfuser::serde::RecordFunctor *>(VT_RECORD);
  }
  const ::flatbuffers::Vector<uint64_t> *children() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_CHILDREN);
  }
  uint64_t fusion_id() const {
    return GetField<uint64_t>(VT_FUSION_ID, 0);
  }
  uint64_t visits() const {
    return GetField<uint64_t>(VT_VISITS, 0);
  }
  bool is_terminal() const {
    return GetField<uint8_t>(VT_IS_TERMINAL, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_RECORD) &&
           verifier.VerifyTable(record()) &&
           VerifyOffset(verifier, VT_CHILDREN) &&
           verifier.VerifyVector(children()) &&
           VerifyField<uint64_t>(verifier, VT_FUSION_ID, 8) &&
           VerifyField<uint64_t>(verifier, VT_VISITS, 8) &&
           VerifyField<uint8_t>(verifier, VT_IS_TERMINAL, 1) &&
           verifier.EndTable();
  }
};

struct TrieNodeBuilder {
  typedef TrieNode Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_record(::flatbuffers::Offset<nvfuser::serde::RecordFunctor> record) {
    fbb_.AddOffset(TrieNode::VT_RECORD, record);
  }
  void add_children(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> children) {
    fbb_.AddOffset(TrieNode::VT_CHILDREN, children);
  }
  void add_fusion_id(uint64_t fusion_id) {
    fbb_.AddElement<uint64_t>(TrieNode::VT_FUSION_ID, fusion_id, 0);
  }
  void add_visits(uint64_t visits) {
    fbb_.AddElement<uint64_t>(TrieNode::VT_VISITS, visits, 0);
  }
  void add_is_terminal(bool is_terminal) {
    fbb_.AddElement<uint8_t>(TrieNode::VT_IS_TERMINAL, static_cast<uint8_t>(is_terminal), 0);
  }
  explicit TrieNodeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TrieNode> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TrieNode>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TrieNode> CreateTrieNode(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::RecordFunctor> record = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> children = 0,
    uint64_t fusion_id = 0,
    uint64_t visits = 0,
    bool is_terminal = false) {
  TrieNodeBuilder builder_(_fbb);
  builder_.add_visits(visits);
  builder_.add_fusion_id(fusion_id);
  builder_.add_children(children);
  builder_.add_record(record);
  builder_.add_is_terminal(is_terminal);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TrieNode> CreateTrieNodeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::RecordFunctor> record = 0,
    const std::vector<uint64_t> *children = nullptr,
    uint64_t fusion_id = 0,
    uint64_t visits = 0,
    bool is_terminal = false) {
  auto children__ = children ? _fbb.CreateVector<uint64_t>(*children) : 0;
  return nvfuser::serde::CreateTrieNode(
      _fbb,
      record,
      children__,
      fusion_id,
      visits,
      is_terminal);
}

struct FusionCache FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef FusionCacheBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_MAX_FUSIONS = 4,
    VT_STRUCTURE = 6,
    VT_TERMINAL_NODES = 8,
    VT_AUTO_GEN_SCHEDULES = 10
  };
  uint64_t max_fusions() const {
    return GetField<uint64_t>(VT_MAX_FUSIONS, 0);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>> *structure() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>> *>(VT_STRUCTURE);
  }
  const ::flatbuffers::Vector<uint64_t> *terminal_nodes() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_TERMINAL_NODES);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>> *auto_gen_schedules() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>> *>(VT_AUTO_GEN_SCHEDULES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_MAX_FUSIONS, 8) &&
           VerifyOffset(verifier, VT_STRUCTURE) &&
           verifier.VerifyVector(structure()) &&
           verifier.VerifyVectorOfTables(structure()) &&
           VerifyOffset(verifier, VT_TERMINAL_NODES) &&
           verifier.VerifyVector(terminal_nodes()) &&
           VerifyOffset(verifier, VT_AUTO_GEN_SCHEDULES) &&
           verifier.VerifyVector(auto_gen_schedules()) &&
           verifier.VerifyVectorOfTables(auto_gen_schedules()) &&
           verifier.EndTable();
  }
};

struct FusionCacheBuilder {
  typedef FusionCache Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_max_fusions(uint64_t max_fusions) {
    fbb_.AddElement<uint64_t>(FusionCache::VT_MAX_FUSIONS, max_fusions, 0);
  }
  void add_structure(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>>> structure) {
    fbb_.AddOffset(FusionCache::VT_STRUCTURE, structure);
  }
  void add_terminal_nodes(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> terminal_nodes) {
    fbb_.AddOffset(FusionCache::VT_TERMINAL_NODES, terminal_nodes);
  }
  void add_auto_gen_schedules(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>>> auto_gen_schedules) {
    fbb_.AddOffset(FusionCache::VT_AUTO_GEN_SCHEDULES, auto_gen_schedules);
  }
  explicit FusionCacheBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<FusionCache> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<FusionCache>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<FusionCache> CreateFusionCache(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t max_fusions = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>>> structure = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> terminal_nodes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>>> auto_gen_schedules = 0) {
  FusionCacheBuilder builder_(_fbb);
  builder_.add_max_fusions(max_fusions);
  builder_.add_auto_gen_schedules(auto_gen_schedules);
  builder_.add_terminal_nodes(terminal_nodes);
  builder_.add_structure(structure);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<FusionCache> CreateFusionCacheDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t max_fusions = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>> *structure = nullptr,
    const std::vector<uint64_t> *terminal_nodes = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>> *auto_gen_schedules = nullptr) {
  auto structure__ = structure ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::TrieNode>>(*structure) : 0;
  auto terminal_nodes__ = terminal_nodes ? _fbb.CreateVector<uint64_t>(*terminal_nodes) : 0;
  auto auto_gen_schedules__ = auto_gen_schedules ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>>(*auto_gen_schedules) : 0;
  return nvfuser::serde::CreateFusionCache(
      _fbb,
      max_fusions,
      structure__,
      terminal_nodes__,
      auto_gen_schedules__);
}

inline bool VerifyRecordData(::flatbuffers::Verifier &verifier, const void *obj, RecordData type) {
  switch (type) {
    case RecordData_NONE: {
      return true;
    }
    case RecordData_At: {
      auto ptr = reinterpret_cast<const nvfuser::serde::At *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_BatchNorm: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BatchNorm *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Broadcast: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Broadcast *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_BroadcastInDim: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BroadcastInDim *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_BroadcastInDimSymbolic: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BroadcastInDimSymbolic *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Dimension: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Dimension *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Dtype: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Dtype *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Norm: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Norm *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Output: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Output *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Pad: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Pad *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Permute: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Permute *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Slice: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Slice *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Squeeze: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Squeeze *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Reduction: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Reduction *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Reshape: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Reshape *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Scalar *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Size: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Size *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Tensor: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Tensor *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_TensorCreation: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorCreation *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_TensorCreationSymbolic: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorCreationSymbolic *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Vector: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Vector *>(obj);
      return verifier.VerifyTable(ptr);
    }
    default: return true;
  }
}

inline bool VerifyRecordDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<uint8_t> *types) {
  if (!values || !types) return !values && !types;
  if (values->size() != types->size()) return false;
  for (::flatbuffers::uoffset_t i = 0; i < values->size(); ++i) {
    if (!VerifyRecordData(
        verifier,  values->Get(i), types->GetEnum<RecordData>(i))) {
      return false;
    }
  }
  return true;
}

inline bool VerifyPolymorphicValueData(::flatbuffers::Verifier &verifier, const void *obj, PolymorphicValueData type) {
  switch (type) {
    case PolymorphicValueData_NONE: {
      return true;
    }
    case PolymorphicValueData_Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Scalar *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case PolymorphicValueData_ScalarCpu: {
      auto ptr = reinterpret_cast<const nvfuser::serde::ScalarCpu *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case PolymorphicValueData_TensorArg: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorArg *>(obj);
      return verifier.VerifyTable(ptr);
    }
    default: return true;
  }
}

inline bool VerifyPolymorphicValueDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<uint8_t> *types) {
  if (!values || !types) return !values && !types;
  if (values->size() != types->size()) return false;
  for (::flatbuffers::uoffset_t i = 0; i < values->size(); ++i) {
    if (!VerifyPolymorphicValueData(
        verifier,  values->Get(i), types->GetEnum<PolymorphicValueData>(i))) {
      return false;
    }
  }
  return true;
}

inline bool VerifyInstructionData(::flatbuffers::Verifier &verifier, const void *obj, InstructionData type) {
  switch (type) {
    case InstructionData_NONE: {
      return true;
    }
    case InstructionData_BinaryOp: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BinaryOp *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case InstructionData_Symbolic: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Symbolic *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case InstructionData_Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Scalar *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case InstructionData_Merge: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Merge *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case InstructionData_NamedScalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::NamedScalar *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case InstructionData_Resize: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Resize *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case InstructionData_Split: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Split *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case InstructionData_Swizzle2D: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Swizzle2D *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case InstructionData_UnaryOp: {
      auto ptr = reinterpret_cast<const nvfuser::serde::UnaryOp *>(obj);
      return verifier.VerifyTable(ptr);
    }
    default: return true;
  }
}

inline bool VerifyInstructionDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<uint8_t> *types) {
  if (!values || !types) return !values && !types;
  if (values->size() != types->size()) return false;
  for (::flatbuffers::uoffset_t i = 0; i < values->size(); ++i) {
    if (!VerifyInstructionData(
        verifier,  values->Get(i), types->GetEnum<InstructionData>(i))) {
      return false;
    }
  }
  return true;
}

inline const nvfuser::serde::FusionCache *GetFusionCache(const void *buf) {
  return ::flatbuffers::GetRoot<nvfuser::serde::FusionCache>(buf);
}

inline const nvfuser::serde::FusionCache *GetSizePrefixedFusionCache(const void *buf) {
  return ::flatbuffers::GetSizePrefixedRoot<nvfuser::serde::FusionCache>(buf);
}

inline const char *FusionCacheIdentifier() {
  return "NV00";
}

inline bool FusionCacheBufferHasIdentifier(const void *buf) {
  return ::flatbuffers::BufferHasIdentifier(
      buf, FusionCacheIdentifier());
}

inline bool SizePrefixedFusionCacheBufferHasIdentifier(const void *buf) {
  return ::flatbuffers::BufferHasIdentifier(
      buf, FusionCacheIdentifier(), true);
}

inline bool VerifyFusionCacheBuffer(
    ::flatbuffers::Verifier &verifier) {
  return verifier.VerifyBuffer<nvfuser::serde::FusionCache>(FusionCacheIdentifier());
}

inline bool VerifySizePrefixedFusionCacheBuffer(
    ::flatbuffers::Verifier &verifier) {
  return verifier.VerifySizePrefixedBuffer<nvfuser::serde::FusionCache>(FusionCacheIdentifier());
}

inline void FinishFusionCacheBuffer(
    ::flatbuffers::FlatBufferBuilder &fbb,
    ::flatbuffers::Offset<nvfuser::serde::FusionCache> root) {
  fbb.Finish(root, FusionCacheIdentifier());
}

inline void FinishSizePrefixedFusionCacheBuffer(
    ::flatbuffers::FlatBufferBuilder &fbb,
    ::flatbuffers::Offset<nvfuser::serde::FusionCache> root) {
  fbb.FinishSizePrefixed(root, FusionCacheIdentifier());
}

}  // namespace serde
}  // namespace nvfuser

#endif  // FLATBUFFERS_GENERATED_FUSIONCACHE_NVFUSER_SERDE_H_
