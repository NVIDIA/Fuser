// automatically generated by the FlatBuffers compiler, do not modify


#ifndef FLATBUFFERS_GENERATED_FUSIONCACHE_NVFUSER_SERDE_H_
#define FLATBUFFERS_GENERATED_FUSIONCACHE_NVFUSER_SERDE_H_

#include "flatbuffers/flatbuffers.h"

// Ensure the included flatbuffers.h is the same version as when this file was
// generated, otherwise it may not be compatible.
static_assert(FLATBUFFERS_VERSION_MAJOR == 23 &&
              FLATBUFFERS_VERSION_MINOR == 3 &&
              FLATBUFFERS_VERSION_REVISION == 3,
             "Non-compatible flatbuffers version included");

namespace nvfuser {
namespace serde {

struct At;
struct AtBuilder;

struct State;

struct Scalar;
struct ScalarBuilder;

struct TensorShape;
struct TensorShapeBuilder;

struct Size;
struct SizeBuilder;

struct PhiloxCudaState;
struct PhiloxCudaStateBuilder;

struct ScalarCpu;
struct ScalarCpuBuilder;

struct TensorArg;
struct TensorArgBuilder;

struct ArgAbstract;
struct ArgAbstractBuilder;

struct KernelArgumentHolder;
struct KernelArgumentHolderBuilder;

struct LaunchParams;
struct LaunchParamsBuilder;

struct GlobalBufferInfo;
struct GlobalBufferInfoBuilder;

struct ExecutorEntry;
struct ExecutorEntryBuilder;

struct BatchNorm;
struct BatchNormBuilder;

struct Broadcast;
struct BroadcastBuilder;

struct BroadcastInDim;
struct BroadcastInDimBuilder;

struct Dtype;
struct DtypeBuilder;

struct Dimension;
struct DimensionBuilder;

struct Norm;
struct NormBuilder;

struct Output;
struct OutputBuilder;

struct Pad;
struct PadBuilder;

struct Permute;
struct PermuteBuilder;

struct Reduction;
struct ReductionBuilder;

struct Reshape;
struct ReshapeBuilder;

struct Slice;
struct SliceBuilder;

struct Squeeze;
struct SqueezeBuilder;

struct Tensor;
struct TensorBuilder;

struct TensorCreation;
struct TensorCreationBuilder;

struct TensorCreationSymbolic;
struct TensorCreationSymbolicBuilder;

struct Vector;
struct VectorBuilder;

struct FusionExecutor;
struct FusionExecutorBuilder;

struct FusionKernelRuntime;
struct FusionKernelRuntimeBuilder;

struct EncodingEntry;

struct InputsIdLookup;
struct InputsIdLookupBuilder;

struct KernelRuntimes;
struct KernelRuntimesBuilder;

struct FusionExecutorCache;
struct FusionExecutorCacheBuilder;

struct RecordFunctor;
struct RecordFunctorBuilder;

struct TrieNode;
struct TrieNodeBuilder;

struct FusionCache;
struct FusionCacheBuilder;

enum DataType : int32_t {
  DataType_Double = 0,
  DataType_Float = 1,
  DataType_Half = 2,
  DataType_Int = 3,
  DataType_Int32 = 4,
  DataType_Bool = 5,
  DataType_BFloat16 = 6,
  DataType_ComplexFloat = 7,
  DataType_ComplexDouble = 8,
  DataType_None = 9,
  DataType_MIN = DataType_Double,
  DataType_MAX = DataType_None
};

inline const DataType (&EnumValuesDataType())[10] {
  static const DataType values[] = {
    DataType_Double,
    DataType_Float,
    DataType_Half,
    DataType_Int,
    DataType_Int32,
    DataType_Bool,
    DataType_BFloat16,
    DataType_ComplexFloat,
    DataType_ComplexDouble,
    DataType_None
  };
  return values;
}

inline const char * const *EnumNamesDataType() {
  static const char * const names[11] = {
    "Double",
    "Float",
    "Half",
    "Int",
    "Int32",
    "Bool",
    "BFloat16",
    "ComplexFloat",
    "ComplexDouble",
    "None",
    nullptr
  };
  return names;
}

inline const char *EnumNameDataType(DataType e) {
  if (::flatbuffers::IsOutRange(e, DataType_Double, DataType_None)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesDataType()[index];
}

enum StateType : int32_t {
  StateType_Tensor = 0,
  StateType_Scalar = 1,
  StateType_Vector = 2,
  StateType_None = 3,
  StateType_MIN = StateType_Tensor,
  StateType_MAX = StateType_None
};

inline const StateType (&EnumValuesStateType())[4] {
  static const StateType values[] = {
    StateType_Tensor,
    StateType_Scalar,
    StateType_Vector,
    StateType_None
  };
  return values;
}

inline const char * const *EnumNamesStateType() {
  static const char * const names[5] = {
    "Tensor",
    "Scalar",
    "Vector",
    "None",
    nullptr
  };
  return names;
}

inline const char *EnumNameStateType(StateType e) {
  if (::flatbuffers::IsOutRange(e, StateType_Tensor, StateType_None)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesStateType()[index];
}

enum Contiguity : int32_t {
  Contiguity_Strided = 0,
  Contiguity_Contiguous = 1,
  Contiguity_None = 2,
  Contiguity_MIN = Contiguity_Strided,
  Contiguity_MAX = Contiguity_None
};

inline const Contiguity (&EnumValuesContiguity())[3] {
  static const Contiguity values[] = {
    Contiguity_Strided,
    Contiguity_Contiguous,
    Contiguity_None
  };
  return values;
}

inline const char * const *EnumNamesContiguity() {
  static const char * const names[4] = {
    "Strided",
    "Contiguous",
    "None",
    nullptr
  };
  return names;
}

inline const char *EnumNameContiguity(Contiguity e) {
  if (::flatbuffers::IsOutRange(e, Contiguity_Strided, Contiguity_None)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesContiguity()[index];
}

enum RecordType : int32_t {
  RecordType_Base = 0,
  RecordType_AtOp = 1,
  RecordType_BatchNormOp = 2,
  RecordType_BroadcastOp = 3,
  RecordType_BroadcastInDim = 4,
  RecordType_CastTv = 5,
  RecordType_CastVal = 6,
  RecordType_CatOp = 7,
  RecordType_End = 8,
  RecordType_FullOp = 9,
  RecordType_IotaOp = 10,
  RecordType_IndexSelectOp = 11,
  RecordType_TorchGatherOp = 12,
  RecordType_TakeAlongAxisOp = 13,
  RecordType_Unary_TV = 14,
  RecordType_Unary_VAL = 15,
  RecordType_Binary_TV = 16,
  RecordType_Binary_VAL = 17,
  RecordType_Binary_TV_VAL = 18,
  RecordType_Binary_VAL_TV = 19,
  RecordType_Ternary_TV = 20,
  RecordType_Ternary_VAL = 21,
  RecordType_Ternary_TV_TV_VAL = 22,
  RecordType_Ternary_TV_VAL_TV = 23,
  RecordType_Ternary_VAL_TV_TV = 24,
  RecordType_Ternary_VAL_VAL_TV = 25,
  RecordType_Ternary_TV_VAL_VAL = 26,
  RecordType_Ternary_VAL_TV_VAL = 27,
  RecordType_Ternary_Alpha_TV = 28,
  RecordType_Ternary_Alpha_VAL = 29,
  RecordType_Ternary_Alpha_TV_TV_VAL = 30,
  RecordType_Ternary_Alpha_TV_VAL_TV = 31,
  RecordType_Ternary_Alpha_VAL_TV_TV = 32,
  RecordType_Ternary_Alpha_VAL_VAL_TV = 33,
  RecordType_Ternary_Alpha_TV_VAL_VAL = 34,
  RecordType_Ternary_Alpha_VAL_TV_VAL = 35,
  RecordType_OutputTv = 36,
  RecordType_OutputVal = 37,
  RecordType_PadOp = 38,
  RecordType_PermuteOp = 39,
  RecordType_RandomOp = 40,
  RecordType_ReductionMax = 41,
  RecordType_ReductionMin = 42,
  RecordType_ReductionProd = 43,
  RecordType_ReductionSum = 44,
  RecordType_ReshapeOp = 45,
  RecordType_Scalar = 46,
  RecordType_ShapeOp = 47,
  RecordType_SizeOp = 48,
  RecordType_SliceOp = 49,
  RecordType_SqueezeOp = 50,
  RecordType_Start = 51,
  RecordType_Tensor = 52,
  RecordType_TensorSizes = 53,
  RecordType_VarianceOp = 54,
  RecordType_VarianceMeanOp = 55,
  RecordType_Vector = 56,
  RecordType_MIN = RecordType_Base,
  RecordType_MAX = RecordType_Vector
};

inline const RecordType (&EnumValuesRecordType())[57] {
  static const RecordType values[] = {
    RecordType_Base,
    RecordType_AtOp,
    RecordType_BatchNormOp,
    RecordType_BroadcastOp,
    RecordType_BroadcastInDim,
    RecordType_CastTv,
    RecordType_CastVal,
    RecordType_CatOp,
    RecordType_End,
    RecordType_FullOp,
    RecordType_IotaOp,
    RecordType_IndexSelectOp,
    RecordType_TorchGatherOp,
    RecordType_TakeAlongAxisOp,
    RecordType_Unary_TV,
    RecordType_Unary_VAL,
    RecordType_Binary_TV,
    RecordType_Binary_VAL,
    RecordType_Binary_TV_VAL,
    RecordType_Binary_VAL_TV,
    RecordType_Ternary_TV,
    RecordType_Ternary_VAL,
    RecordType_Ternary_TV_TV_VAL,
    RecordType_Ternary_TV_VAL_TV,
    RecordType_Ternary_VAL_TV_TV,
    RecordType_Ternary_VAL_VAL_TV,
    RecordType_Ternary_TV_VAL_VAL,
    RecordType_Ternary_VAL_TV_VAL,
    RecordType_Ternary_Alpha_TV,
    RecordType_Ternary_Alpha_VAL,
    RecordType_Ternary_Alpha_TV_TV_VAL,
    RecordType_Ternary_Alpha_TV_VAL_TV,
    RecordType_Ternary_Alpha_VAL_TV_TV,
    RecordType_Ternary_Alpha_VAL_VAL_TV,
    RecordType_Ternary_Alpha_TV_VAL_VAL,
    RecordType_Ternary_Alpha_VAL_TV_VAL,
    RecordType_OutputTv,
    RecordType_OutputVal,
    RecordType_PadOp,
    RecordType_PermuteOp,
    RecordType_RandomOp,
    RecordType_ReductionMax,
    RecordType_ReductionMin,
    RecordType_ReductionProd,
    RecordType_ReductionSum,
    RecordType_ReshapeOp,
    RecordType_Scalar,
    RecordType_ShapeOp,
    RecordType_SizeOp,
    RecordType_SliceOp,
    RecordType_SqueezeOp,
    RecordType_Start,
    RecordType_Tensor,
    RecordType_TensorSizes,
    RecordType_VarianceOp,
    RecordType_VarianceMeanOp,
    RecordType_Vector
  };
  return values;
}

inline const char * const *EnumNamesRecordType() {
  static const char * const names[58] = {
    "Base",
    "AtOp",
    "BatchNormOp",
    "BroadcastOp",
    "BroadcastInDim",
    "CastTv",
    "CastVal",
    "CatOp",
    "End",
    "FullOp",
    "IotaOp",
    "IndexSelectOp",
    "TorchGatherOp",
    "TakeAlongAxisOp",
    "Unary_TV",
    "Unary_VAL",
    "Binary_TV",
    "Binary_VAL",
    "Binary_TV_VAL",
    "Binary_VAL_TV",
    "Ternary_TV",
    "Ternary_VAL",
    "Ternary_TV_TV_VAL",
    "Ternary_TV_VAL_TV",
    "Ternary_VAL_TV_TV",
    "Ternary_VAL_VAL_TV",
    "Ternary_TV_VAL_VAL",
    "Ternary_VAL_TV_VAL",
    "Ternary_Alpha_TV",
    "Ternary_Alpha_VAL",
    "Ternary_Alpha_TV_TV_VAL",
    "Ternary_Alpha_TV_VAL_TV",
    "Ternary_Alpha_VAL_TV_TV",
    "Ternary_Alpha_VAL_VAL_TV",
    "Ternary_Alpha_TV_VAL_VAL",
    "Ternary_Alpha_VAL_TV_VAL",
    "OutputTv",
    "OutputVal",
    "PadOp",
    "PermuteOp",
    "RandomOp",
    "ReductionMax",
    "ReductionMin",
    "ReductionProd",
    "ReductionSum",
    "ReshapeOp",
    "Scalar",
    "ShapeOp",
    "SizeOp",
    "SliceOp",
    "SqueezeOp",
    "Start",
    "Tensor",
    "TensorSizes",
    "VarianceOp",
    "VarianceMeanOp",
    "Vector",
    nullptr
  };
  return names;
}

inline const char *EnumNameRecordType(RecordType e) {
  if (::flatbuffers::IsOutRange(e, RecordType_Base, RecordType_Vector)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesRecordType()[index];
}

enum RecordData : uint8_t {
  RecordData_NONE = 0,
  RecordData_At = 1,
  RecordData_BatchNorm = 2,
  RecordData_Broadcast = 3,
  RecordData_BroadcastInDim = 4,
  RecordData_Dimension = 5,
  RecordData_Dtype = 6,
  RecordData_Norm = 7,
  RecordData_Output = 8,
  RecordData_Pad = 9,
  RecordData_Permute = 10,
  RecordData_Slice = 11,
  RecordData_Squeeze = 12,
  RecordData_Reduction = 13,
  RecordData_Reshape = 14,
  RecordData_Scalar = 15,
  RecordData_Size = 16,
  RecordData_Tensor = 17,
  RecordData_TensorCreation = 18,
  RecordData_TensorCreationSymbolic = 19,
  RecordData_Vector = 20,
  RecordData_MIN = RecordData_NONE,
  RecordData_MAX = RecordData_Vector
};

inline const RecordData (&EnumValuesRecordData())[21] {
  static const RecordData values[] = {
    RecordData_NONE,
    RecordData_At,
    RecordData_BatchNorm,
    RecordData_Broadcast,
    RecordData_BroadcastInDim,
    RecordData_Dimension,
    RecordData_Dtype,
    RecordData_Norm,
    RecordData_Output,
    RecordData_Pad,
    RecordData_Permute,
    RecordData_Slice,
    RecordData_Squeeze,
    RecordData_Reduction,
    RecordData_Reshape,
    RecordData_Scalar,
    RecordData_Size,
    RecordData_Tensor,
    RecordData_TensorCreation,
    RecordData_TensorCreationSymbolic,
    RecordData_Vector
  };
  return values;
}

inline const char * const *EnumNamesRecordData() {
  static const char * const names[22] = {
    "NONE",
    "At",
    "BatchNorm",
    "Broadcast",
    "BroadcastInDim",
    "Dimension",
    "Dtype",
    "Norm",
    "Output",
    "Pad",
    "Permute",
    "Slice",
    "Squeeze",
    "Reduction",
    "Reshape",
    "Scalar",
    "Size",
    "Tensor",
    "TensorCreation",
    "TensorCreationSymbolic",
    "Vector",
    nullptr
  };
  return names;
}

inline const char *EnumNameRecordData(RecordData e) {
  if (::flatbuffers::IsOutRange(e, RecordData_NONE, RecordData_Vector)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesRecordData()[index];
}

template<typename T> struct RecordDataTraits {
  static const RecordData enum_value = RecordData_NONE;
};

template<> struct RecordDataTraits<nvfuser::serde::At> {
  static const RecordData enum_value = RecordData_At;
};

template<> struct RecordDataTraits<nvfuser::serde::BatchNorm> {
  static const RecordData enum_value = RecordData_BatchNorm;
};

template<> struct RecordDataTraits<nvfuser::serde::Broadcast> {
  static const RecordData enum_value = RecordData_Broadcast;
};

template<> struct RecordDataTraits<nvfuser::serde::BroadcastInDim> {
  static const RecordData enum_value = RecordData_BroadcastInDim;
};

template<> struct RecordDataTraits<nvfuser::serde::Dimension> {
  static const RecordData enum_value = RecordData_Dimension;
};

template<> struct RecordDataTraits<nvfuser::serde::Dtype> {
  static const RecordData enum_value = RecordData_Dtype;
};

template<> struct RecordDataTraits<nvfuser::serde::Norm> {
  static const RecordData enum_value = RecordData_Norm;
};

template<> struct RecordDataTraits<nvfuser::serde::Output> {
  static const RecordData enum_value = RecordData_Output;
};

template<> struct RecordDataTraits<nvfuser::serde::Pad> {
  static const RecordData enum_value = RecordData_Pad;
};

template<> struct RecordDataTraits<nvfuser::serde::Permute> {
  static const RecordData enum_value = RecordData_Permute;
};

template<> struct RecordDataTraits<nvfuser::serde::Slice> {
  static const RecordData enum_value = RecordData_Slice;
};

template<> struct RecordDataTraits<nvfuser::serde::Squeeze> {
  static const RecordData enum_value = RecordData_Squeeze;
};

template<> struct RecordDataTraits<nvfuser::serde::Reduction> {
  static const RecordData enum_value = RecordData_Reduction;
};

template<> struct RecordDataTraits<nvfuser::serde::Reshape> {
  static const RecordData enum_value = RecordData_Reshape;
};

template<> struct RecordDataTraits<nvfuser::serde::Scalar> {
  static const RecordData enum_value = RecordData_Scalar;
};

template<> struct RecordDataTraits<nvfuser::serde::Size> {
  static const RecordData enum_value = RecordData_Size;
};

template<> struct RecordDataTraits<nvfuser::serde::Tensor> {
  static const RecordData enum_value = RecordData_Tensor;
};

template<> struct RecordDataTraits<nvfuser::serde::TensorCreation> {
  static const RecordData enum_value = RecordData_TensorCreation;
};

template<> struct RecordDataTraits<nvfuser::serde::TensorCreationSymbolic> {
  static const RecordData enum_value = RecordData_TensorCreationSymbolic;
};

template<> struct RecordDataTraits<nvfuser::serde::Vector> {
  static const RecordData enum_value = RecordData_Vector;
};

bool VerifyRecordData(::flatbuffers::Verifier &verifier, const void *obj, RecordData type);
bool VerifyRecordDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<uint8_t> *types);

enum ArgAbstractData : uint8_t {
  ArgAbstractData_NONE = 0,
  ArgAbstractData_Scalar = 1,
  ArgAbstractData_PhiloxCudaState = 2,
  ArgAbstractData_ScalarCpu = 3,
  ArgAbstractData_TensorArg = 4,
  ArgAbstractData_MIN = ArgAbstractData_NONE,
  ArgAbstractData_MAX = ArgAbstractData_TensorArg
};

inline const ArgAbstractData (&EnumValuesArgAbstractData())[5] {
  static const ArgAbstractData values[] = {
    ArgAbstractData_NONE,
    ArgAbstractData_Scalar,
    ArgAbstractData_PhiloxCudaState,
    ArgAbstractData_ScalarCpu,
    ArgAbstractData_TensorArg
  };
  return values;
}

inline const char * const *EnumNamesArgAbstractData() {
  static const char * const names[6] = {
    "NONE",
    "Scalar",
    "PhiloxCudaState",
    "ScalarCpu",
    "TensorArg",
    nullptr
  };
  return names;
}

inline const char *EnumNameArgAbstractData(ArgAbstractData e) {
  if (::flatbuffers::IsOutRange(e, ArgAbstractData_NONE, ArgAbstractData_TensorArg)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesArgAbstractData()[index];
}

template<typename T> struct ArgAbstractDataTraits {
  static const ArgAbstractData enum_value = ArgAbstractData_NONE;
};

template<> struct ArgAbstractDataTraits<nvfuser::serde::Scalar> {
  static const ArgAbstractData enum_value = ArgAbstractData_Scalar;
};

template<> struct ArgAbstractDataTraits<nvfuser::serde::PhiloxCudaState> {
  static const ArgAbstractData enum_value = ArgAbstractData_PhiloxCudaState;
};

template<> struct ArgAbstractDataTraits<nvfuser::serde::ScalarCpu> {
  static const ArgAbstractData enum_value = ArgAbstractData_ScalarCpu;
};

template<> struct ArgAbstractDataTraits<nvfuser::serde::TensorArg> {
  static const ArgAbstractData enum_value = ArgAbstractData_TensorArg;
};

bool VerifyArgAbstractData(::flatbuffers::Verifier &verifier, const void *obj, ArgAbstractData type);
bool VerifyArgAbstractDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<uint8_t> *types);

FLATBUFFERS_MANUALLY_ALIGNED_STRUCT(4) State FLATBUFFERS_FINAL_CLASS {
 private:
  int32_t index_;
  int32_t type_;

 public:
  State()
      : index_(0),
        type_(0) {
  }
  State(int32_t _index, nvfuser::serde::StateType _type)
      : index_(::flatbuffers::EndianScalar(_index)),
        type_(::flatbuffers::EndianScalar(static_cast<int32_t>(_type))) {
  }
  int32_t index() const {
    return ::flatbuffers::EndianScalar(index_);
  }
  nvfuser::serde::StateType type() const {
    return static_cast<nvfuser::serde::StateType>(::flatbuffers::EndianScalar(type_));
  }
};
FLATBUFFERS_STRUCT_END(State, 8);

FLATBUFFERS_MANUALLY_ALIGNED_STRUCT(8) EncodingEntry FLATBUFFERS_FINAL_CLASS {
 private:
  uint64_t id_;
  uint64_t lru_iter_;

 public:
  EncodingEntry()
      : id_(0),
        lru_iter_(0) {
  }
  EncodingEntry(uint64_t _id, uint64_t _lru_iter)
      : id_(::flatbuffers::EndianScalar(_id)),
        lru_iter_(::flatbuffers::EndianScalar(_lru_iter)) {
  }
  uint64_t id() const {
    return ::flatbuffers::EndianScalar(id_);
  }
  uint64_t lru_iter() const {
    return ::flatbuffers::EndianScalar(lru_iter_);
  }
};
FLATBUFFERS_STRUCT_END(EncodingEntry, 16);

struct At FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef AtBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INDEX = 4
  };
  int64_t index() const {
    return GetField<int64_t>(VT_INDEX, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_INDEX, 8) &&
           verifier.EndTable();
  }
};

struct AtBuilder {
  typedef At Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_index(int64_t index) {
    fbb_.AddElement<int64_t>(At::VT_INDEX, index, 0);
  }
  explicit AtBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<At> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<At>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<At> CreateAt(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t index = 0) {
  AtBuilder builder_(_fbb);
  builder_.add_index(index);
  return builder_.Finish();
}

struct Scalar FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ScalarBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DTYPE = 4,
    VT_HAS_VALUE = 6,
    VT_VALUE_TYPE = 8,
    VT_BOOL_VALUE = 10,
    VT_LONG_VALUE = 12,
    VT_DOUBLE_VALUE = 14,
    VT_REAL_VALUE = 16,
    VT_IMAG_VALUE = 18
  };
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool has_value() const {
    return GetField<uint8_t>(VT_HAS_VALUE, 0) != 0;
  }
  nvfuser::serde::DataType value_type() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_VALUE_TYPE, 0));
  }
  bool bool_value() const {
    return GetField<uint8_t>(VT_BOOL_VALUE, 0) != 0;
  }
  int64_t long_value() const {
    return GetField<int64_t>(VT_LONG_VALUE, 0);
  }
  double double_value() const {
    return GetField<double>(VT_DOUBLE_VALUE, 0.0);
  }
  double real_value() const {
    return GetField<double>(VT_REAL_VALUE, 0.0);
  }
  double imag_value() const {
    return GetField<double>(VT_IMAG_VALUE, 0.0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_HAS_VALUE, 1) &&
           VerifyField<int32_t>(verifier, VT_VALUE_TYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_BOOL_VALUE, 1) &&
           VerifyField<int64_t>(verifier, VT_LONG_VALUE, 8) &&
           VerifyField<double>(verifier, VT_DOUBLE_VALUE, 8) &&
           VerifyField<double>(verifier, VT_REAL_VALUE, 8) &&
           VerifyField<double>(verifier, VT_IMAG_VALUE, 8) &&
           verifier.EndTable();
  }
};

struct ScalarBuilder {
  typedef Scalar Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(Scalar::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  void add_has_value(bool has_value) {
    fbb_.AddElement<uint8_t>(Scalar::VT_HAS_VALUE, static_cast<uint8_t>(has_value), 0);
  }
  void add_value_type(nvfuser::serde::DataType value_type) {
    fbb_.AddElement<int32_t>(Scalar::VT_VALUE_TYPE, static_cast<int32_t>(value_type), 0);
  }
  void add_bool_value(bool bool_value) {
    fbb_.AddElement<uint8_t>(Scalar::VT_BOOL_VALUE, static_cast<uint8_t>(bool_value), 0);
  }
  void add_long_value(int64_t long_value) {
    fbb_.AddElement<int64_t>(Scalar::VT_LONG_VALUE, long_value, 0);
  }
  void add_double_value(double double_value) {
    fbb_.AddElement<double>(Scalar::VT_DOUBLE_VALUE, double_value, 0.0);
  }
  void add_real_value(double real_value) {
    fbb_.AddElement<double>(Scalar::VT_REAL_VALUE, real_value, 0.0);
  }
  void add_imag_value(double imag_value) {
    fbb_.AddElement<double>(Scalar::VT_IMAG_VALUE, imag_value, 0.0);
  }
  explicit ScalarBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Scalar> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Scalar>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Scalar> CreateScalar(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double,
    bool has_value = false,
    nvfuser::serde::DataType value_type = nvfuser::serde::DataType_Double,
    bool bool_value = false,
    int64_t long_value = 0,
    double double_value = 0.0,
    double real_value = 0.0,
    double imag_value = 0.0) {
  ScalarBuilder builder_(_fbb);
  builder_.add_imag_value(imag_value);
  builder_.add_real_value(real_value);
  builder_.add_double_value(double_value);
  builder_.add_long_value(long_value);
  builder_.add_value_type(value_type);
  builder_.add_dtype(dtype);
  builder_.add_bool_value(bool_value);
  builder_.add_has_value(has_value);
  return builder_.Finish();
}

struct TensorShape FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorShapeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SHAPE = 4
  };
  const ::flatbuffers::Vector<int64_t> *shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SHAPE);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SHAPE) &&
           verifier.VerifyVector(shape()) &&
           verifier.EndTable();
  }
};

struct TensorShapeBuilder {
  typedef TensorShape Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape) {
    fbb_.AddOffset(TensorShape::VT_SHAPE, shape);
  }
  explicit TensorShapeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TensorShape> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TensorShape>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TensorShape> CreateTensorShape(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape = 0) {
  TensorShapeBuilder builder_(_fbb);
  builder_.add_shape(shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TensorShape> CreateTensorShapeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *shape = nullptr) {
  auto shape__ = shape ? _fbb.CreateVector<int64_t>(*shape) : 0;
  return nvfuser::serde::CreateTensorShape(
      _fbb,
      shape__);
}

struct Size FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SizeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIM = 4
  };
  int64_t dim() const {
    return GetField<int64_t>(VT_DIM, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DIM, 8) &&
           verifier.EndTable();
  }
};

struct SizeBuilder {
  typedef Size Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dim(int64_t dim) {
    fbb_.AddElement<int64_t>(Size::VT_DIM, dim, 0);
  }
  explicit SizeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Size> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Size>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Size> CreateSize(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t dim = 0) {
  SizeBuilder builder_(_fbb);
  builder_.add_dim(dim);
  return builder_.Finish();
}

struct PhiloxCudaState FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef PhiloxCudaStateBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SEED = 4,
    VT_OFFSET = 6
  };
  uint64_t seed() const {
    return GetField<uint64_t>(VT_SEED, 0);
  }
  uint64_t offset() const {
    return GetField<uint64_t>(VT_OFFSET, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_SEED, 8) &&
           VerifyField<uint64_t>(verifier, VT_OFFSET, 8) &&
           verifier.EndTable();
  }
};

struct PhiloxCudaStateBuilder {
  typedef PhiloxCudaState Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_seed(uint64_t seed) {
    fbb_.AddElement<uint64_t>(PhiloxCudaState::VT_SEED, seed, 0);
  }
  void add_offset(uint64_t offset) {
    fbb_.AddElement<uint64_t>(PhiloxCudaState::VT_OFFSET, offset, 0);
  }
  explicit PhiloxCudaStateBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<PhiloxCudaState> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<PhiloxCudaState>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<PhiloxCudaState> CreatePhiloxCudaState(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t seed = 0,
    uint64_t offset = 0) {
  PhiloxCudaStateBuilder builder_(_fbb);
  builder_.add_offset(offset);
  builder_.add_seed(seed);
  return builder_.Finish();
}

struct ScalarCpu FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ScalarCpuBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INSTANCE = 4,
    VT_SIZE = 6
  };
  const ::flatbuffers::Vector<int8_t> *instance() const {
    return GetPointer<const ::flatbuffers::Vector<int8_t> *>(VT_INSTANCE);
  }
  uint64_t size() const {
    return GetField<uint64_t>(VT_SIZE, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_INSTANCE) &&
           verifier.VerifyVector(instance()) &&
           VerifyField<uint64_t>(verifier, VT_SIZE, 8) &&
           verifier.EndTable();
  }
};

struct ScalarCpuBuilder {
  typedef ScalarCpu Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_instance(::flatbuffers::Offset<::flatbuffers::Vector<int8_t>> instance) {
    fbb_.AddOffset(ScalarCpu::VT_INSTANCE, instance);
  }
  void add_size(uint64_t size) {
    fbb_.AddElement<uint64_t>(ScalarCpu::VT_SIZE, size, 0);
  }
  explicit ScalarCpuBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<ScalarCpu> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<ScalarCpu>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<ScalarCpu> CreateScalarCpu(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int8_t>> instance = 0,
    uint64_t size = 0) {
  ScalarCpuBuilder builder_(_fbb);
  builder_.add_size(size);
  builder_.add_instance(instance);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<ScalarCpu> CreateScalarCpuDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int8_t> *instance = nullptr,
    uint64_t size = 0) {
  auto instance__ = instance ? _fbb.CreateVector<int8_t>(*instance) : 0;
  return nvfuser::serde::CreateScalarCpu(
      _fbb,
      instance__,
      size);
}

struct TensorArg FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorArgBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_PTR = 4,
    VT_SIZES = 6,
    VT_STRIDES = 8,
    VT_DTYPE = 10,
    VT_IS_INT_INDEX_MODE = 12,
    VT_INDEX_TYPE_RESOLVED = 14
  };
  uint64_t ptr() const {
    return GetField<uint64_t>(VT_PTR, 0);
  }
  const ::flatbuffers::Vector<int64_t> *sizes() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SIZES);
  }
  const ::flatbuffers::Vector<int64_t> *strides() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_STRIDES);
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool is_int_index_mode() const {
    return GetField<uint8_t>(VT_IS_INT_INDEX_MODE, 0) != 0;
  }
  bool index_type_resolved() const {
    return GetField<uint8_t>(VT_INDEX_TYPE_RESOLVED, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_PTR, 8) &&
           VerifyOffset(verifier, VT_SIZES) &&
           verifier.VerifyVector(sizes()) &&
           VerifyOffset(verifier, VT_STRIDES) &&
           verifier.VerifyVector(strides()) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_IS_INT_INDEX_MODE, 1) &&
           VerifyField<uint8_t>(verifier, VT_INDEX_TYPE_RESOLVED, 1) &&
           verifier.EndTable();
  }
};

struct TensorArgBuilder {
  typedef TensorArg Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_ptr(uint64_t ptr) {
    fbb_.AddElement<uint64_t>(TensorArg::VT_PTR, ptr, 0);
  }
  void add_sizes(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes) {
    fbb_.AddOffset(TensorArg::VT_SIZES, sizes);
  }
  void add_strides(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides) {
    fbb_.AddOffset(TensorArg::VT_STRIDES, strides);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(TensorArg::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  void add_is_int_index_mode(bool is_int_index_mode) {
    fbb_.AddElement<uint8_t>(TensorArg::VT_IS_INT_INDEX_MODE, static_cast<uint8_t>(is_int_index_mode), 0);
  }
  void add_index_type_resolved(bool index_type_resolved) {
    fbb_.AddElement<uint8_t>(TensorArg::VT_INDEX_TYPE_RESOLVED, static_cast<uint8_t>(index_type_resolved), 0);
  }
  explicit TensorArgBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TensorArg> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TensorArg>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TensorArg> CreateTensorArg(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t ptr = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides = 0,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double,
    bool is_int_index_mode = false,
    bool index_type_resolved = false) {
  TensorArgBuilder builder_(_fbb);
  builder_.add_ptr(ptr);
  builder_.add_dtype(dtype);
  builder_.add_strides(strides);
  builder_.add_sizes(sizes);
  builder_.add_index_type_resolved(index_type_resolved);
  builder_.add_is_int_index_mode(is_int_index_mode);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TensorArg> CreateTensorArgDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t ptr = 0,
    const std::vector<int64_t> *sizes = nullptr,
    const std::vector<int64_t> *strides = nullptr,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double,
    bool is_int_index_mode = false,
    bool index_type_resolved = false) {
  auto sizes__ = sizes ? _fbb.CreateVector<int64_t>(*sizes) : 0;
  auto strides__ = strides ? _fbb.CreateVector<int64_t>(*strides) : 0;
  return nvfuser::serde::CreateTensorArg(
      _fbb,
      ptr,
      sizes__,
      strides__,
      dtype,
      is_int_index_mode,
      index_type_resolved);
}

struct ArgAbstract FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ArgAbstractBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DATA_TYPE = 4,
    VT_DATA = 6
  };
  nvfuser::serde::ArgAbstractData data_type() const {
    return static_cast<nvfuser::serde::ArgAbstractData>(GetField<uint8_t>(VT_DATA_TYPE, 0));
  }
  const void *data() const {
    return GetPointer<const void *>(VT_DATA);
  }
  template<typename T> const T *data_as() const;
  const nvfuser::serde::Scalar *data_as_Scalar() const {
    return data_type() == nvfuser::serde::ArgAbstractData_Scalar ? static_cast<const nvfuser::serde::Scalar *>(data()) : nullptr;
  }
  const nvfuser::serde::PhiloxCudaState *data_as_PhiloxCudaState() const {
    return data_type() == nvfuser::serde::ArgAbstractData_PhiloxCudaState ? static_cast<const nvfuser::serde::PhiloxCudaState *>(data()) : nullptr;
  }
  const nvfuser::serde::ScalarCpu *data_as_ScalarCpu() const {
    return data_type() == nvfuser::serde::ArgAbstractData_ScalarCpu ? static_cast<const nvfuser::serde::ScalarCpu *>(data()) : nullptr;
  }
  const nvfuser::serde::TensorArg *data_as_TensorArg() const {
    return data_type() == nvfuser::serde::ArgAbstractData_TensorArg ? static_cast<const nvfuser::serde::TensorArg *>(data()) : nullptr;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_DATA_TYPE, 1) &&
           VerifyOffset(verifier, VT_DATA) &&
           VerifyArgAbstractData(verifier, data(), data_type()) &&
           verifier.EndTable();
  }
};

template<> inline const nvfuser::serde::Scalar *ArgAbstract::data_as<nvfuser::serde::Scalar>() const {
  return data_as_Scalar();
}

template<> inline const nvfuser::serde::PhiloxCudaState *ArgAbstract::data_as<nvfuser::serde::PhiloxCudaState>() const {
  return data_as_PhiloxCudaState();
}

template<> inline const nvfuser::serde::ScalarCpu *ArgAbstract::data_as<nvfuser::serde::ScalarCpu>() const {
  return data_as_ScalarCpu();
}

template<> inline const nvfuser::serde::TensorArg *ArgAbstract::data_as<nvfuser::serde::TensorArg>() const {
  return data_as_TensorArg();
}

struct ArgAbstractBuilder {
  typedef ArgAbstract Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_data_type(nvfuser::serde::ArgAbstractData data_type) {
    fbb_.AddElement<uint8_t>(ArgAbstract::VT_DATA_TYPE, static_cast<uint8_t>(data_type), 0);
  }
  void add_data(::flatbuffers::Offset<void> data) {
    fbb_.AddOffset(ArgAbstract::VT_DATA, data);
  }
  explicit ArgAbstractBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<ArgAbstract> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<ArgAbstract>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<ArgAbstract> CreateArgAbstract(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::ArgAbstractData data_type = nvfuser::serde::ArgAbstractData_NONE,
    ::flatbuffers::Offset<void> data = 0) {
  ArgAbstractBuilder builder_(_fbb);
  builder_.add_data(data);
  builder_.add_data_type(data_type);
  return builder_.Finish();
}

struct KernelArgumentHolder FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef KernelArgumentHolderBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ARGUMENTS = 4,
    VT_DEVICE_INDEX = 6,
    VT_CACHE_ID = 8
  };
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::ArgAbstract>> *arguments() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::ArgAbstract>> *>(VT_ARGUMENTS);
  }
  int8_t device_index() const {
    return GetField<int8_t>(VT_DEVICE_INDEX, 0);
  }
  uint64_t cache_id() const {
    return GetField<uint64_t>(VT_CACHE_ID, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ARGUMENTS) &&
           verifier.VerifyVector(arguments()) &&
           verifier.VerifyVectorOfTables(arguments()) &&
           VerifyField<int8_t>(verifier, VT_DEVICE_INDEX, 1) &&
           VerifyField<uint64_t>(verifier, VT_CACHE_ID, 8) &&
           verifier.EndTable();
  }
};

struct KernelArgumentHolderBuilder {
  typedef KernelArgumentHolder Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_arguments(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::ArgAbstract>>> arguments) {
    fbb_.AddOffset(KernelArgumentHolder::VT_ARGUMENTS, arguments);
  }
  void add_device_index(int8_t device_index) {
    fbb_.AddElement<int8_t>(KernelArgumentHolder::VT_DEVICE_INDEX, device_index, 0);
  }
  void add_cache_id(uint64_t cache_id) {
    fbb_.AddElement<uint64_t>(KernelArgumentHolder::VT_CACHE_ID, cache_id, 0);
  }
  explicit KernelArgumentHolderBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<KernelArgumentHolder> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<KernelArgumentHolder>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<KernelArgumentHolder> CreateKernelArgumentHolder(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::ArgAbstract>>> arguments = 0,
    int8_t device_index = 0,
    uint64_t cache_id = 0) {
  KernelArgumentHolderBuilder builder_(_fbb);
  builder_.add_cache_id(cache_id);
  builder_.add_arguments(arguments);
  builder_.add_device_index(device_index);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<KernelArgumentHolder> CreateKernelArgumentHolderDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::ArgAbstract>> *arguments = nullptr,
    int8_t device_index = 0,
    uint64_t cache_id = 0) {
  auto arguments__ = arguments ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::ArgAbstract>>(*arguments) : 0;
  return nvfuser::serde::CreateKernelArgumentHolder(
      _fbb,
      arguments__,
      device_index,
      cache_id);
}

struct LaunchParams FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef LaunchParamsBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_GDIMX = 4,
    VT_GDIMY = 6,
    VT_GDIMZ = 8,
    VT_BDIMX = 10,
    VT_BDIMY = 12,
    VT_BDIMZ = 14,
    VT_SMEM = 16,
    VT_OUTPUT_SIZES = 18
  };
  int64_t gdimx() const {
    return GetField<int64_t>(VT_GDIMX, 0);
  }
  int64_t gdimy() const {
    return GetField<int64_t>(VT_GDIMY, 0);
  }
  int64_t gdimz() const {
    return GetField<int64_t>(VT_GDIMZ, 0);
  }
  int64_t bdimx() const {
    return GetField<int64_t>(VT_BDIMX, 0);
  }
  int64_t bdimy() const {
    return GetField<int64_t>(VT_BDIMY, 0);
  }
  int64_t bdimz() const {
    return GetField<int64_t>(VT_BDIMZ, 0);
  }
  int64_t smem() const {
    return GetField<int64_t>(VT_SMEM, 0);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>> *output_sizes() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>> *>(VT_OUTPUT_SIZES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_GDIMX, 8) &&
           VerifyField<int64_t>(verifier, VT_GDIMY, 8) &&
           VerifyField<int64_t>(verifier, VT_GDIMZ, 8) &&
           VerifyField<int64_t>(verifier, VT_BDIMX, 8) &&
           VerifyField<int64_t>(verifier, VT_BDIMY, 8) &&
           VerifyField<int64_t>(verifier, VT_BDIMZ, 8) &&
           VerifyField<int64_t>(verifier, VT_SMEM, 8) &&
           VerifyOffset(verifier, VT_OUTPUT_SIZES) &&
           verifier.VerifyVector(output_sizes()) &&
           verifier.VerifyVectorOfTables(output_sizes()) &&
           verifier.EndTable();
  }
};

struct LaunchParamsBuilder {
  typedef LaunchParams Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_gdimx(int64_t gdimx) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_GDIMX, gdimx, 0);
  }
  void add_gdimy(int64_t gdimy) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_GDIMY, gdimy, 0);
  }
  void add_gdimz(int64_t gdimz) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_GDIMZ, gdimz, 0);
  }
  void add_bdimx(int64_t bdimx) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_BDIMX, bdimx, 0);
  }
  void add_bdimy(int64_t bdimy) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_BDIMY, bdimy, 0);
  }
  void add_bdimz(int64_t bdimz) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_BDIMZ, bdimz, 0);
  }
  void add_smem(int64_t smem) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_SMEM, smem, 0);
  }
  void add_output_sizes(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>>> output_sizes) {
    fbb_.AddOffset(LaunchParams::VT_OUTPUT_SIZES, output_sizes);
  }
  explicit LaunchParamsBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<LaunchParams> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<LaunchParams>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<LaunchParams> CreateLaunchParams(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t gdimx = 0,
    int64_t gdimy = 0,
    int64_t gdimz = 0,
    int64_t bdimx = 0,
    int64_t bdimy = 0,
    int64_t bdimz = 0,
    int64_t smem = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>>> output_sizes = 0) {
  LaunchParamsBuilder builder_(_fbb);
  builder_.add_smem(smem);
  builder_.add_bdimz(bdimz);
  builder_.add_bdimy(bdimy);
  builder_.add_bdimx(bdimx);
  builder_.add_gdimz(gdimz);
  builder_.add_gdimy(gdimy);
  builder_.add_gdimx(gdimx);
  builder_.add_output_sizes(output_sizes);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<LaunchParams> CreateLaunchParamsDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t gdimx = 0,
    int64_t gdimy = 0,
    int64_t gdimz = 0,
    int64_t bdimx = 0,
    int64_t bdimy = 0,
    int64_t bdimz = 0,
    int64_t smem = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>> *output_sizes = nullptr) {
  auto output_sizes__ = output_sizes ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::TensorShape>>(*output_sizes) : 0;
  return nvfuser::serde::CreateLaunchParams(
      _fbb,
      gdimx,
      gdimy,
      gdimz,
      bdimx,
      bdimy,
      bdimz,
      smem,
      output_sizes__);
}

struct GlobalBufferInfo FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef GlobalBufferInfoBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TV = 4,
    VT_SIZES = 6,
    VT_STRIDES = 8,
    VT_DTYPE = 10,
    VT_ZERO_INIT = 12,
    VT_IS_PROFILE_BUFFER = 14,
    VT_IS_FUSION_OUTPUT = 16
  };
  int64_t tv() const {
    return GetField<int64_t>(VT_TV, -1LL);
  }
  const ::flatbuffers::Vector<int64_t> *sizes() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SIZES);
  }
  const ::flatbuffers::Vector<int64_t> *strides() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_STRIDES);
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool zero_init() const {
    return GetField<uint8_t>(VT_ZERO_INIT, 0) != 0;
  }
  bool is_profile_buffer() const {
    return GetField<uint8_t>(VT_IS_PROFILE_BUFFER, 0) != 0;
  }
  bool is_fusion_output() const {
    return GetField<uint8_t>(VT_IS_FUSION_OUTPUT, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_TV, 8) &&
           VerifyOffset(verifier, VT_SIZES) &&
           verifier.VerifyVector(sizes()) &&
           VerifyOffset(verifier, VT_STRIDES) &&
           verifier.VerifyVector(strides()) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_ZERO_INIT, 1) &&
           VerifyField<uint8_t>(verifier, VT_IS_PROFILE_BUFFER, 1) &&
           VerifyField<uint8_t>(verifier, VT_IS_FUSION_OUTPUT, 1) &&
           verifier.EndTable();
  }
};

struct GlobalBufferInfoBuilder {
  typedef GlobalBufferInfo Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_tv(int64_t tv) {
    fbb_.AddElement<int64_t>(GlobalBufferInfo::VT_TV, tv, -1LL);
  }
  void add_sizes(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes) {
    fbb_.AddOffset(GlobalBufferInfo::VT_SIZES, sizes);
  }
  void add_strides(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides) {
    fbb_.AddOffset(GlobalBufferInfo::VT_STRIDES, strides);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(GlobalBufferInfo::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  void add_zero_init(bool zero_init) {
    fbb_.AddElement<uint8_t>(GlobalBufferInfo::VT_ZERO_INIT, static_cast<uint8_t>(zero_init), 0);
  }
  void add_is_profile_buffer(bool is_profile_buffer) {
    fbb_.AddElement<uint8_t>(GlobalBufferInfo::VT_IS_PROFILE_BUFFER, static_cast<uint8_t>(is_profile_buffer), 0);
  }
  void add_is_fusion_output(bool is_fusion_output) {
    fbb_.AddElement<uint8_t>(GlobalBufferInfo::VT_IS_FUSION_OUTPUT, static_cast<uint8_t>(is_fusion_output), 0);
  }
  explicit GlobalBufferInfoBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<GlobalBufferInfo> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<GlobalBufferInfo>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<GlobalBufferInfo> CreateGlobalBufferInfo(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t tv = -1LL,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides = 0,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double,
    bool zero_init = false,
    bool is_profile_buffer = false,
    bool is_fusion_output = false) {
  GlobalBufferInfoBuilder builder_(_fbb);
  builder_.add_tv(tv);
  builder_.add_dtype(dtype);
  builder_.add_strides(strides);
  builder_.add_sizes(sizes);
  builder_.add_is_fusion_output(is_fusion_output);
  builder_.add_is_profile_buffer(is_profile_buffer);
  builder_.add_zero_init(zero_init);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<GlobalBufferInfo> CreateGlobalBufferInfoDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t tv = -1LL,
    const std::vector<int64_t> *sizes = nullptr,
    const std::vector<int64_t> *strides = nullptr,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double,
    bool zero_init = false,
    bool is_profile_buffer = false,
    bool is_fusion_output = false) {
  auto sizes__ = sizes ? _fbb.CreateVector<int64_t>(*sizes) : 0;
  auto strides__ = strides ? _fbb.CreateVector<int64_t>(*strides) : 0;
  return nvfuser::serde::CreateGlobalBufferInfo(
      _fbb,
      tv,
      sizes__,
      strides__,
      dtype,
      zero_init,
      is_profile_buffer,
      is_fusion_output);
}

struct ExecutorEntry FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ExecutorEntryBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INIT = 4,
    VT_LAUNCH_PARAMS = 6,
    VT_OUTPUT_ALIASES = 8,
    VT_INPUT_ALIASES = 10,
    VT_OUTPUTS = 12,
    VT_INTERMEDIATES = 14,
    VT_RAND_OFFSET = 16
  };
  bool init() const {
    return GetField<uint8_t>(VT_INIT, 0) != 0;
  }
  const nvfuser::serde::LaunchParams *launch_params() const {
    return GetPointer<const nvfuser::serde::LaunchParams *>(VT_LAUNCH_PARAMS);
  }
  const ::flatbuffers::Vector<int32_t> *output_aliases() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_OUTPUT_ALIASES);
  }
  const ::flatbuffers::Vector<int32_t> *input_aliases() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_INPUT_ALIASES);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *outputs() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *>(VT_OUTPUTS);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *intermediates() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *>(VT_INTERMEDIATES);
  }
  uint64_t rand_offset() const {
    return GetField<uint64_t>(VT_RAND_OFFSET, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_INIT, 1) &&
           VerifyOffset(verifier, VT_LAUNCH_PARAMS) &&
           verifier.VerifyTable(launch_params()) &&
           VerifyOffset(verifier, VT_OUTPUT_ALIASES) &&
           verifier.VerifyVector(output_aliases()) &&
           VerifyOffset(verifier, VT_INPUT_ALIASES) &&
           verifier.VerifyVector(input_aliases()) &&
           VerifyOffset(verifier, VT_OUTPUTS) &&
           verifier.VerifyVector(outputs()) &&
           verifier.VerifyVectorOfTables(outputs()) &&
           VerifyOffset(verifier, VT_INTERMEDIATES) &&
           verifier.VerifyVector(intermediates()) &&
           verifier.VerifyVectorOfTables(intermediates()) &&
           VerifyField<uint64_t>(verifier, VT_RAND_OFFSET, 8) &&
           verifier.EndTable();
  }
};

struct ExecutorEntryBuilder {
  typedef ExecutorEntry Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_init(bool init) {
    fbb_.AddElement<uint8_t>(ExecutorEntry::VT_INIT, static_cast<uint8_t>(init), 0);
  }
  void add_launch_params(::flatbuffers::Offset<nvfuser::serde::LaunchParams> launch_params) {
    fbb_.AddOffset(ExecutorEntry::VT_LAUNCH_PARAMS, launch_params);
  }
  void add_output_aliases(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> output_aliases) {
    fbb_.AddOffset(ExecutorEntry::VT_OUTPUT_ALIASES, output_aliases);
  }
  void add_input_aliases(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> input_aliases) {
    fbb_.AddOffset(ExecutorEntry::VT_INPUT_ALIASES, input_aliases);
  }
  void add_outputs(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> outputs) {
    fbb_.AddOffset(ExecutorEntry::VT_OUTPUTS, outputs);
  }
  void add_intermediates(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> intermediates) {
    fbb_.AddOffset(ExecutorEntry::VT_INTERMEDIATES, intermediates);
  }
  void add_rand_offset(uint64_t rand_offset) {
    fbb_.AddElement<uint64_t>(ExecutorEntry::VT_RAND_OFFSET, rand_offset, 0);
  }
  explicit ExecutorEntryBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<ExecutorEntry> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<ExecutorEntry>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<ExecutorEntry> CreateExecutorEntry(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool init = false,
    ::flatbuffers::Offset<nvfuser::serde::LaunchParams> launch_params = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> output_aliases = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> input_aliases = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> outputs = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> intermediates = 0,
    uint64_t rand_offset = 0) {
  ExecutorEntryBuilder builder_(_fbb);
  builder_.add_rand_offset(rand_offset);
  builder_.add_intermediates(intermediates);
  builder_.add_outputs(outputs);
  builder_.add_input_aliases(input_aliases);
  builder_.add_output_aliases(output_aliases);
  builder_.add_launch_params(launch_params);
  builder_.add_init(init);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<ExecutorEntry> CreateExecutorEntryDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool init = false,
    ::flatbuffers::Offset<nvfuser::serde::LaunchParams> launch_params = 0,
    const std::vector<int32_t> *output_aliases = nullptr,
    const std::vector<int32_t> *input_aliases = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *outputs = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *intermediates = nullptr,
    uint64_t rand_offset = 0) {
  auto output_aliases__ = output_aliases ? _fbb.CreateVector<int32_t>(*output_aliases) : 0;
  auto input_aliases__ = input_aliases ? _fbb.CreateVector<int32_t>(*input_aliases) : 0;
  auto outputs__ = outputs ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>(*outputs) : 0;
  auto intermediates__ = intermediates ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>(*intermediates) : 0;
  return nvfuser::serde::CreateExecutorEntry(
      _fbb,
      init,
      launch_params,
      output_aliases__,
      input_aliases__,
      outputs__,
      intermediates__,
      rand_offset);
}

struct BatchNorm FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef BatchNormBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TRAINING = 4,
    VT_CHANNELS_LAST = 6
  };
  bool training() const {
    return GetField<uint8_t>(VT_TRAINING, 0) != 0;
  }
  bool channels_last() const {
    return GetField<uint8_t>(VT_CHANNELS_LAST, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_TRAINING, 1) &&
           VerifyField<uint8_t>(verifier, VT_CHANNELS_LAST, 1) &&
           verifier.EndTable();
  }
};

struct BatchNormBuilder {
  typedef BatchNorm Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_training(bool training) {
    fbb_.AddElement<uint8_t>(BatchNorm::VT_TRAINING, static_cast<uint8_t>(training), 0);
  }
  void add_channels_last(bool channels_last) {
    fbb_.AddElement<uint8_t>(BatchNorm::VT_CHANNELS_LAST, static_cast<uint8_t>(channels_last), 0);
  }
  explicit BatchNormBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<BatchNorm> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<BatchNorm>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<BatchNorm> CreateBatchNorm(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool training = false,
    bool channels_last = false) {
  BatchNormBuilder builder_(_fbb);
  builder_.add_channels_last(channels_last);
  builder_.add_training(training);
  return builder_.Finish();
}

struct Broadcast FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef BroadcastBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_BROADCAST_DIMS = 4
  };
  const ::flatbuffers::Vector<uint8_t> *broadcast_dims() const {
    return GetPointer<const ::flatbuffers::Vector<uint8_t> *>(VT_BROADCAST_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_BROADCAST_DIMS) &&
           verifier.VerifyVector(broadcast_dims()) &&
           verifier.EndTable();
  }
};

struct BroadcastBuilder {
  typedef Broadcast Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_broadcast_dims(::flatbuffers::Offset<::flatbuffers::Vector<uint8_t>> broadcast_dims) {
    fbb_.AddOffset(Broadcast::VT_BROADCAST_DIMS, broadcast_dims);
  }
  explicit BroadcastBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Broadcast> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Broadcast>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Broadcast> CreateBroadcast(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint8_t>> broadcast_dims = 0) {
  BroadcastBuilder builder_(_fbb);
  builder_.add_broadcast_dims(broadcast_dims);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Broadcast> CreateBroadcastDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<uint8_t> *broadcast_dims = nullptr) {
  auto broadcast_dims__ = broadcast_dims ? _fbb.CreateVector<uint8_t>(*broadcast_dims) : 0;
  return nvfuser::serde::CreateBroadcast(
      _fbb,
      broadcast_dims__);
}

struct BroadcastInDim FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef BroadcastInDimBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OUTPUT_SIZE = 4,
    VT_BROADCAST_DIMS = 6
  };
  uint64_t output_size() const {
    return GetField<uint64_t>(VT_OUTPUT_SIZE, 0);
  }
  const ::flatbuffers::Vector<int64_t> *broadcast_dims() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_BROADCAST_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_OUTPUT_SIZE, 8) &&
           VerifyOffset(verifier, VT_BROADCAST_DIMS) &&
           verifier.VerifyVector(broadcast_dims()) &&
           verifier.EndTable();
  }
};

struct BroadcastInDimBuilder {
  typedef BroadcastInDim Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_output_size(uint64_t output_size) {
    fbb_.AddElement<uint64_t>(BroadcastInDim::VT_OUTPUT_SIZE, output_size, 0);
  }
  void add_broadcast_dims(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> broadcast_dims) {
    fbb_.AddOffset(BroadcastInDim::VT_BROADCAST_DIMS, broadcast_dims);
  }
  explicit BroadcastInDimBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<BroadcastInDim> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<BroadcastInDim>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<BroadcastInDim> CreateBroadcastInDim(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t output_size = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> broadcast_dims = 0) {
  BroadcastInDimBuilder builder_(_fbb);
  builder_.add_output_size(output_size);
  builder_.add_broadcast_dims(broadcast_dims);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<BroadcastInDim> CreateBroadcastInDimDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t output_size = 0,
    const std::vector<int64_t> *broadcast_dims = nullptr) {
  auto broadcast_dims__ = broadcast_dims ? _fbb.CreateVector<int64_t>(*broadcast_dims) : 0;
  return nvfuser::serde::CreateBroadcastInDim(
      _fbb,
      output_size,
      broadcast_dims__);
}

struct Dtype FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef DtypeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DTYPE = 4
  };
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
};

struct DtypeBuilder {
  typedef Dtype Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(Dtype::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit DtypeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Dtype> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Dtype>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Dtype> CreateDtype(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double) {
  DtypeBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  return builder_.Finish();
}

struct Dimension FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef DimensionBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIM = 4
  };
  int64_t dim() const {
    return GetField<int64_t>(VT_DIM, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DIM, 8) &&
           verifier.EndTable();
  }
};

struct DimensionBuilder {
  typedef Dimension Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dim(int64_t dim) {
    fbb_.AddElement<int64_t>(Dimension::VT_DIM, dim, 0);
  }
  explicit DimensionBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Dimension> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Dimension>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Dimension> CreateDimension(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t dim = 0) {
  DimensionBuilder builder_(_fbb);
  builder_.add_dim(dim);
  return builder_.Finish();
}

struct Norm FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef NormBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_AXES = 4,
    VT_CORRECTION = 6,
    VT_KEEP_DIM = 8
  };
  const ::flatbuffers::Vector<int32_t> *axes() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_AXES);
  }
  int64_t correction() const {
    return GetField<int64_t>(VT_CORRECTION, 0);
  }
  bool keep_dim() const {
    return GetField<uint8_t>(VT_KEEP_DIM, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_AXES) &&
           verifier.VerifyVector(axes()) &&
           VerifyField<int64_t>(verifier, VT_CORRECTION, 8) &&
           VerifyField<uint8_t>(verifier, VT_KEEP_DIM, 1) &&
           verifier.EndTable();
  }
};

struct NormBuilder {
  typedef Norm Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_axes(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> axes) {
    fbb_.AddOffset(Norm::VT_AXES, axes);
  }
  void add_correction(int64_t correction) {
    fbb_.AddElement<int64_t>(Norm::VT_CORRECTION, correction, 0);
  }
  void add_keep_dim(bool keep_dim) {
    fbb_.AddElement<uint8_t>(Norm::VT_KEEP_DIM, static_cast<uint8_t>(keep_dim), 0);
  }
  explicit NormBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Norm> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Norm>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Norm> CreateNorm(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> axes = 0,
    int64_t correction = 0,
    bool keep_dim = false) {
  NormBuilder builder_(_fbb);
  builder_.add_correction(correction);
  builder_.add_axes(axes);
  builder_.add_keep_dim(keep_dim);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Norm> CreateNormDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int32_t> *axes = nullptr,
    int64_t correction = 0,
    bool keep_dim = false) {
  auto axes__ = axes ? _fbb.CreateVector<int32_t>(*axes) : 0;
  return nvfuser::serde::CreateNorm(
      _fbb,
      axes__,
      correction,
      keep_dim);
}

struct Output FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef OutputBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_STRIDE_ORDER = 4
  };
  const ::flatbuffers::Vector<int64_t> *stride_order() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_STRIDE_ORDER);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_STRIDE_ORDER) &&
           verifier.VerifyVector(stride_order()) &&
           verifier.EndTable();
  }
};

struct OutputBuilder {
  typedef Output Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_stride_order(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> stride_order) {
    fbb_.AddOffset(Output::VT_STRIDE_ORDER, stride_order);
  }
  explicit OutputBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Output> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Output>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Output> CreateOutput(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> stride_order = 0) {
  OutputBuilder builder_(_fbb);
  builder_.add_stride_order(stride_order);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Output> CreateOutputDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *stride_order = nullptr) {
  auto stride_order__ = stride_order ? _fbb.CreateVector<int64_t>(*stride_order) : 0;
  return nvfuser::serde::CreateOutput(
      _fbb,
      stride_order__);
}

struct Pad FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef PadBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_PAD_WIDTHS = 4
  };
  const ::flatbuffers::Vector<int64_t> *pad_widths() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_PAD_WIDTHS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_PAD_WIDTHS) &&
           verifier.VerifyVector(pad_widths()) &&
           verifier.EndTable();
  }
};

struct PadBuilder {
  typedef Pad Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_pad_widths(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> pad_widths) {
    fbb_.AddOffset(Pad::VT_PAD_WIDTHS, pad_widths);
  }
  explicit PadBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Pad> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Pad>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Pad> CreatePad(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> pad_widths = 0) {
  PadBuilder builder_(_fbb);
  builder_.add_pad_widths(pad_widths);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Pad> CreatePadDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *pad_widths = nullptr) {
  auto pad_widths__ = pad_widths ? _fbb.CreateVector<int64_t>(*pad_widths) : 0;
  return nvfuser::serde::CreatePad(
      _fbb,
      pad_widths__);
}

struct Permute FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef PermuteBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIMS = 4
  };
  const ::flatbuffers::Vector<int64_t> *dims() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_DIMS) &&
           verifier.VerifyVector(dims()) &&
           verifier.EndTable();
  }
};

struct PermuteBuilder {
  typedef Permute Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dims(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> dims) {
    fbb_.AddOffset(Permute::VT_DIMS, dims);
  }
  explicit PermuteBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Permute> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Permute>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Permute> CreatePermute(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> dims = 0) {
  PermuteBuilder builder_(_fbb);
  builder_.add_dims(dims);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Permute> CreatePermuteDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *dims = nullptr) {
  auto dims__ = dims ? _fbb.CreateVector<int64_t>(*dims) : 0;
  return nvfuser::serde::CreatePermute(
      _fbb,
      dims__);
}

struct Reduction FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ReductionBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_AXES = 4,
    VT_KEEP_DIM = 6,
    VT_DTYPE = 8
  };
  const ::flatbuffers::Vector<int32_t> *axes() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_AXES);
  }
  bool keep_dim() const {
    return GetField<uint8_t>(VT_KEEP_DIM, 0) != 0;
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_AXES) &&
           verifier.VerifyVector(axes()) &&
           VerifyField<uint8_t>(verifier, VT_KEEP_DIM, 1) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
};

struct ReductionBuilder {
  typedef Reduction Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_axes(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> axes) {
    fbb_.AddOffset(Reduction::VT_AXES, axes);
  }
  void add_keep_dim(bool keep_dim) {
    fbb_.AddElement<uint8_t>(Reduction::VT_KEEP_DIM, static_cast<uint8_t>(keep_dim), 0);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(Reduction::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit ReductionBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Reduction> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Reduction>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Reduction> CreateReduction(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> axes = 0,
    bool keep_dim = false,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double) {
  ReductionBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  builder_.add_axes(axes);
  builder_.add_keep_dim(keep_dim);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Reduction> CreateReductionDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int32_t> *axes = nullptr,
    bool keep_dim = false,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double) {
  auto axes__ = axes ? _fbb.CreateVector<int32_t>(*axes) : 0;
  return nvfuser::serde::CreateReduction(
      _fbb,
      axes__,
      keep_dim,
      dtype);
}

struct Reshape FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ReshapeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ORIGINAL_SHAPE = 4,
    VT_NEW_SHAPE = 6
  };
  const ::flatbuffers::Vector<int64_t> *original_shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_ORIGINAL_SHAPE);
  }
  const ::flatbuffers::Vector<int64_t> *new_shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_NEW_SHAPE);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ORIGINAL_SHAPE) &&
           verifier.VerifyVector(original_shape()) &&
           VerifyOffset(verifier, VT_NEW_SHAPE) &&
           verifier.VerifyVector(new_shape()) &&
           verifier.EndTable();
  }
};

struct ReshapeBuilder {
  typedef Reshape Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_original_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> original_shape) {
    fbb_.AddOffset(Reshape::VT_ORIGINAL_SHAPE, original_shape);
  }
  void add_new_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> new_shape) {
    fbb_.AddOffset(Reshape::VT_NEW_SHAPE, new_shape);
  }
  explicit ReshapeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Reshape> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Reshape>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Reshape> CreateReshape(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> original_shape = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> new_shape = 0) {
  ReshapeBuilder builder_(_fbb);
  builder_.add_new_shape(new_shape);
  builder_.add_original_shape(original_shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Reshape> CreateReshapeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *original_shape = nullptr,
    const std::vector<int64_t> *new_shape = nullptr) {
  auto original_shape__ = original_shape ? _fbb.CreateVector<int64_t>(*original_shape) : 0;
  auto new_shape__ = new_shape ? _fbb.CreateVector<int64_t>(*new_shape) : 0;
  return nvfuser::serde::CreateReshape(
      _fbb,
      original_shape__,
      new_shape__);
}

struct Slice FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SliceBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_START_INDICES = 4,
    VT_END_INDICES = 6,
    VT_STRIDES = 8
  };
  const ::flatbuffers::Vector<int64_t> *start_indices() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_START_INDICES);
  }
  const ::flatbuffers::Vector<int64_t> *end_indices() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_END_INDICES);
  }
  const ::flatbuffers::Vector<int64_t> *strides() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_STRIDES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_START_INDICES) &&
           verifier.VerifyVector(start_indices()) &&
           VerifyOffset(verifier, VT_END_INDICES) &&
           verifier.VerifyVector(end_indices()) &&
           VerifyOffset(verifier, VT_STRIDES) &&
           verifier.VerifyVector(strides()) &&
           verifier.EndTable();
  }
};

struct SliceBuilder {
  typedef Slice Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_start_indices(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> start_indices) {
    fbb_.AddOffset(Slice::VT_START_INDICES, start_indices);
  }
  void add_end_indices(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> end_indices) {
    fbb_.AddOffset(Slice::VT_END_INDICES, end_indices);
  }
  void add_strides(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides) {
    fbb_.AddOffset(Slice::VT_STRIDES, strides);
  }
  explicit SliceBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Slice> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Slice>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Slice> CreateSlice(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> start_indices = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> end_indices = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides = 0) {
  SliceBuilder builder_(_fbb);
  builder_.add_strides(strides);
  builder_.add_end_indices(end_indices);
  builder_.add_start_indices(start_indices);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Slice> CreateSliceDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *start_indices = nullptr,
    const std::vector<int64_t> *end_indices = nullptr,
    const std::vector<int64_t> *strides = nullptr) {
  auto start_indices__ = start_indices ? _fbb.CreateVector<int64_t>(*start_indices) : 0;
  auto end_indices__ = end_indices ? _fbb.CreateVector<int64_t>(*end_indices) : 0;
  auto strides__ = strides ? _fbb.CreateVector<int64_t>(*strides) : 0;
  return nvfuser::serde::CreateSlice(
      _fbb,
      start_indices__,
      end_indices__,
      strides__);
}

struct Squeeze FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SqueezeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ORIGINAL_SHAPE = 4,
    VT_SQUEEZE_DIMS = 6
  };
  const ::flatbuffers::Vector<int64_t> *original_shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_ORIGINAL_SHAPE);
  }
  const ::flatbuffers::Vector<int64_t> *squeeze_dims() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SQUEEZE_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ORIGINAL_SHAPE) &&
           verifier.VerifyVector(original_shape()) &&
           VerifyOffset(verifier, VT_SQUEEZE_DIMS) &&
           verifier.VerifyVector(squeeze_dims()) &&
           verifier.EndTable();
  }
};

struct SqueezeBuilder {
  typedef Squeeze Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_original_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> original_shape) {
    fbb_.AddOffset(Squeeze::VT_ORIGINAL_SHAPE, original_shape);
  }
  void add_squeeze_dims(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> squeeze_dims) {
    fbb_.AddOffset(Squeeze::VT_SQUEEZE_DIMS, squeeze_dims);
  }
  explicit SqueezeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Squeeze> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Squeeze>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Squeeze> CreateSqueeze(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> original_shape = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> squeeze_dims = 0) {
  SqueezeBuilder builder_(_fbb);
  builder_.add_squeeze_dims(squeeze_dims);
  builder_.add_original_shape(original_shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Squeeze> CreateSqueezeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *original_shape = nullptr,
    const std::vector<int64_t> *squeeze_dims = nullptr) {
  auto original_shape__ = original_shape ? _fbb.CreateVector<int64_t>(*original_shape) : 0;
  auto squeeze_dims__ = squeeze_dims ? _fbb.CreateVector<int64_t>(*squeeze_dims) : 0;
  return nvfuser::serde::CreateSqueeze(
      _fbb,
      original_shape__,
      squeeze_dims__);
}

struct Tensor FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SIZES = 4,
    VT_CONTIGUITY = 6,
    VT_DTYPE = 8,
    VT_IS_CPU = 10
  };
  const ::flatbuffers::Vector<int64_t> *sizes() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SIZES);
  }
  const ::flatbuffers::Vector<int32_t> *contiguity() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_CONTIGUITY);
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool is_cpu() const {
    return GetField<uint8_t>(VT_IS_CPU, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SIZES) &&
           verifier.VerifyVector(sizes()) &&
           VerifyOffset(verifier, VT_CONTIGUITY) &&
           verifier.VerifyVector(contiguity()) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_IS_CPU, 1) &&
           verifier.EndTable();
  }
};

struct TensorBuilder {
  typedef Tensor Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_sizes(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes) {
    fbb_.AddOffset(Tensor::VT_SIZES, sizes);
  }
  void add_contiguity(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> contiguity) {
    fbb_.AddOffset(Tensor::VT_CONTIGUITY, contiguity);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(Tensor::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  void add_is_cpu(bool is_cpu) {
    fbb_.AddElement<uint8_t>(Tensor::VT_IS_CPU, static_cast<uint8_t>(is_cpu), 0);
  }
  explicit TensorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Tensor> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Tensor>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Tensor> CreateTensor(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> contiguity = 0,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double,
    bool is_cpu = false) {
  TensorBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  builder_.add_contiguity(contiguity);
  builder_.add_sizes(sizes);
  builder_.add_is_cpu(is_cpu);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Tensor> CreateTensorDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *sizes = nullptr,
    const std::vector<int32_t> *contiguity = nullptr,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double,
    bool is_cpu = false) {
  auto sizes__ = sizes ? _fbb.CreateVector<int64_t>(*sizes) : 0;
  auto contiguity__ = contiguity ? _fbb.CreateVector<int32_t>(*contiguity) : 0;
  return nvfuser::serde::CreateTensor(
      _fbb,
      sizes__,
      contiguity__,
      dtype,
      is_cpu);
}

struct TensorCreation FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorCreationBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SHAPE = 4,
    VT_DTYPE = 6
  };
  const ::flatbuffers::Vector<int64_t> *shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SHAPE);
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SHAPE) &&
           verifier.VerifyVector(shape()) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
};

struct TensorCreationBuilder {
  typedef TensorCreation Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape) {
    fbb_.AddOffset(TensorCreation::VT_SHAPE, shape);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(TensorCreation::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit TensorCreationBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TensorCreation> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TensorCreation>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TensorCreation> CreateTensorCreation(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape = 0,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double) {
  TensorCreationBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  builder_.add_shape(shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TensorCreation> CreateTensorCreationDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *shape = nullptr,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double) {
  auto shape__ = shape ? _fbb.CreateVector<int64_t>(*shape) : 0;
  return nvfuser::serde::CreateTensorCreation(
      _fbb,
      shape__,
      dtype);
}

struct TensorCreationSymbolic FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorCreationSymbolicBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SHAPE = 4,
    VT_DTYPE = 6
  };
  const ::flatbuffers::Vector<const nvfuser::serde::State *> *shape() const {
    return GetPointer<const ::flatbuffers::Vector<const nvfuser::serde::State *> *>(VT_SHAPE);
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SHAPE) &&
           verifier.VerifyVector(shape()) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
};

struct TensorCreationSymbolicBuilder {
  typedef TensorCreationSymbolic Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_shape(::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> shape) {
    fbb_.AddOffset(TensorCreationSymbolic::VT_SHAPE, shape);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(TensorCreationSymbolic::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit TensorCreationSymbolicBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TensorCreationSymbolic> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TensorCreationSymbolic>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TensorCreationSymbolic> CreateTensorCreationSymbolic(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> shape = 0,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double) {
  TensorCreationSymbolicBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  builder_.add_shape(shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TensorCreationSymbolic> CreateTensorCreationSymbolicDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<nvfuser::serde::State> *shape = nullptr,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double) {
  auto shape__ = shape ? _fbb.CreateVectorOfStructs<nvfuser::serde::State>(*shape) : 0;
  return nvfuser::serde::CreateTensorCreationSymbolic(
      _fbb,
      shape__,
      dtype);
}

struct Vector FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef VectorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DTYPE = 4
  };
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
};

struct VectorBuilder {
  typedef Vector Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(Vector::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit VectorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Vector> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Vector>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Vector> CreateVector(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double) {
  VectorBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  return builder_.Finish();
}

struct FusionExecutor FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef FusionExecutorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DEVICE_SMEM_LIMIT = 4,
    VT_BLOCK_SIZE_HIGH_WATER_MARK = 6,
    VT_MAXRREGCOUNT_HIGH_WATER_MARK = 8,
    VT_WARP_SIZE = 10,
    VT_FUSION_ID = 12,
    VT_FUSION_ID_COUNTER = 14,
    VT_KERNEL_CODE = 16,
    VT_EXECUTOR_ENTRY_LOOKUP_KEYS = 18,
    VT_EXECUTOR_ENTRY_LOOKUP_VALUES = 20,
    VT_INDEX_TYPE = 22
  };
  int64_t device_smem_limit() const {
    return GetField<int64_t>(VT_DEVICE_SMEM_LIMIT, 0);
  }
  int64_t block_size_high_water_mark() const {
    return GetField<int64_t>(VT_BLOCK_SIZE_HIGH_WATER_MARK, 0);
  }
  int64_t maxrregcount_high_water_mark() const {
    return GetField<int64_t>(VT_MAXRREGCOUNT_HIGH_WATER_MARK, 0);
  }
  int64_t warp_size() const {
    return GetField<int64_t>(VT_WARP_SIZE, 0);
  }
  int64_t fusion_id() const {
    return GetField<int64_t>(VT_FUSION_ID, 0);
  }
  int64_t fusion_id_counter() const {
    return GetField<int64_t>(VT_FUSION_ID_COUNTER, 0);
  }
  const ::flatbuffers::String *kernel_code() const {
    return GetPointer<const ::flatbuffers::String *>(VT_KERNEL_CODE);
  }
  const ::flatbuffers::Vector<uint64_t> *executor_entry_lookup_keys() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_EXECUTOR_ENTRY_LOOKUP_KEYS);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>> *executor_entry_lookup_values() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>> *>(VT_EXECUTOR_ENTRY_LOOKUP_VALUES);
  }
  nvfuser::serde::DataType index_type() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_INDEX_TYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DEVICE_SMEM_LIMIT, 8) &&
           VerifyField<int64_t>(verifier, VT_BLOCK_SIZE_HIGH_WATER_MARK, 8) &&
           VerifyField<int64_t>(verifier, VT_MAXRREGCOUNT_HIGH_WATER_MARK, 8) &&
           VerifyField<int64_t>(verifier, VT_WARP_SIZE, 8) &&
           VerifyField<int64_t>(verifier, VT_FUSION_ID, 8) &&
           VerifyField<int64_t>(verifier, VT_FUSION_ID_COUNTER, 8) &&
           VerifyOffset(verifier, VT_KERNEL_CODE) &&
           verifier.VerifyString(kernel_code()) &&
           VerifyOffset(verifier, VT_EXECUTOR_ENTRY_LOOKUP_KEYS) &&
           verifier.VerifyVector(executor_entry_lookup_keys()) &&
           VerifyOffset(verifier, VT_EXECUTOR_ENTRY_LOOKUP_VALUES) &&
           verifier.VerifyVector(executor_entry_lookup_values()) &&
           verifier.VerifyVectorOfTables(executor_entry_lookup_values()) &&
           VerifyField<int32_t>(verifier, VT_INDEX_TYPE, 4) &&
           verifier.EndTable();
  }
};

struct FusionExecutorBuilder {
  typedef FusionExecutor Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_device_smem_limit(int64_t device_smem_limit) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_DEVICE_SMEM_LIMIT, device_smem_limit, 0);
  }
  void add_block_size_high_water_mark(int64_t block_size_high_water_mark) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_BLOCK_SIZE_HIGH_WATER_MARK, block_size_high_water_mark, 0);
  }
  void add_maxrregcount_high_water_mark(int64_t maxrregcount_high_water_mark) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_MAXRREGCOUNT_HIGH_WATER_MARK, maxrregcount_high_water_mark, 0);
  }
  void add_warp_size(int64_t warp_size) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_WARP_SIZE, warp_size, 0);
  }
  void add_fusion_id(int64_t fusion_id) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_FUSION_ID, fusion_id, 0);
  }
  void add_fusion_id_counter(int64_t fusion_id_counter) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_FUSION_ID_COUNTER, fusion_id_counter, 0);
  }
  void add_kernel_code(::flatbuffers::Offset<::flatbuffers::String> kernel_code) {
    fbb_.AddOffset(FusionExecutor::VT_KERNEL_CODE, kernel_code);
  }
  void add_executor_entry_lookup_keys(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> executor_entry_lookup_keys) {
    fbb_.AddOffset(FusionExecutor::VT_EXECUTOR_ENTRY_LOOKUP_KEYS, executor_entry_lookup_keys);
  }
  void add_executor_entry_lookup_values(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>>> executor_entry_lookup_values) {
    fbb_.AddOffset(FusionExecutor::VT_EXECUTOR_ENTRY_LOOKUP_VALUES, executor_entry_lookup_values);
  }
  void add_index_type(nvfuser::serde::DataType index_type) {
    fbb_.AddElement<int32_t>(FusionExecutor::VT_INDEX_TYPE, static_cast<int32_t>(index_type), 0);
  }
  explicit FusionExecutorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<FusionExecutor> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<FusionExecutor>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<FusionExecutor> CreateFusionExecutor(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t device_smem_limit = 0,
    int64_t block_size_high_water_mark = 0,
    int64_t maxrregcount_high_water_mark = 0,
    int64_t warp_size = 0,
    int64_t fusion_id = 0,
    int64_t fusion_id_counter = 0,
    ::flatbuffers::Offset<::flatbuffers::String> kernel_code = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> executor_entry_lookup_keys = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>>> executor_entry_lookup_values = 0,
    nvfuser::serde::DataType index_type = nvfuser::serde::DataType_Double) {
  FusionExecutorBuilder builder_(_fbb);
  builder_.add_fusion_id_counter(fusion_id_counter);
  builder_.add_fusion_id(fusion_id);
  builder_.add_warp_size(warp_size);
  builder_.add_maxrregcount_high_water_mark(maxrregcount_high_water_mark);
  builder_.add_block_size_high_water_mark(block_size_high_water_mark);
  builder_.add_device_smem_limit(device_smem_limit);
  builder_.add_index_type(index_type);
  builder_.add_executor_entry_lookup_values(executor_entry_lookup_values);
  builder_.add_executor_entry_lookup_keys(executor_entry_lookup_keys);
  builder_.add_kernel_code(kernel_code);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<FusionExecutor> CreateFusionExecutorDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t device_smem_limit = 0,
    int64_t block_size_high_water_mark = 0,
    int64_t maxrregcount_high_water_mark = 0,
    int64_t warp_size = 0,
    int64_t fusion_id = 0,
    int64_t fusion_id_counter = 0,
    const char *kernel_code = nullptr,
    const std::vector<uint64_t> *executor_entry_lookup_keys = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>> *executor_entry_lookup_values = nullptr,
    nvfuser::serde::DataType index_type = nvfuser::serde::DataType_Double) {
  auto kernel_code__ = kernel_code ? _fbb.CreateString(kernel_code) : 0;
  auto executor_entry_lookup_keys__ = executor_entry_lookup_keys ? _fbb.CreateVector<uint64_t>(*executor_entry_lookup_keys) : 0;
  auto executor_entry_lookup_values__ = executor_entry_lookup_values ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>>(*executor_entry_lookup_values) : 0;
  return nvfuser::serde::CreateFusionExecutor(
      _fbb,
      device_smem_limit,
      block_size_high_water_mark,
      maxrregcount_high_water_mark,
      warp_size,
      fusion_id,
      fusion_id_counter,
      kernel_code__,
      executor_entry_lookup_keys__,
      executor_entry_lookup_values__,
      index_type);
}

struct FusionKernelRuntime FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef FusionKernelRuntimeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ARGS = 4,
    VT_EXECUTORS = 6
  };
  const nvfuser::serde::KernelArgumentHolder *args() const {
    return GetPointer<const nvfuser::serde::KernelArgumentHolder *>(VT_ARGS);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>> *executors() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>> *>(VT_EXECUTORS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ARGS) &&
           verifier.VerifyTable(args()) &&
           VerifyOffset(verifier, VT_EXECUTORS) &&
           verifier.VerifyVector(executors()) &&
           verifier.VerifyVectorOfTables(executors()) &&
           verifier.EndTable();
  }
};

struct FusionKernelRuntimeBuilder {
  typedef FusionKernelRuntime Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_args(::flatbuffers::Offset<nvfuser::serde::KernelArgumentHolder> args) {
    fbb_.AddOffset(FusionKernelRuntime::VT_ARGS, args);
  }
  void add_executors(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>>> executors) {
    fbb_.AddOffset(FusionKernelRuntime::VT_EXECUTORS, executors);
  }
  explicit FusionKernelRuntimeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<FusionKernelRuntime> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<FusionKernelRuntime>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<FusionKernelRuntime> CreateFusionKernelRuntime(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::KernelArgumentHolder> args = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>>> executors = 0) {
  FusionKernelRuntimeBuilder builder_(_fbb);
  builder_.add_executors(executors);
  builder_.add_args(args);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<FusionKernelRuntime> CreateFusionKernelRuntimeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::KernelArgumentHolder> args = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>> *executors = nullptr) {
  auto executors__ = executors ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>>(*executors) : 0;
  return nvfuser::serde::CreateFusionKernelRuntime(
      _fbb,
      args,
      executors__);
}

struct InputsIdLookup FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef InputsIdLookupBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_MAX_CACHE_SIZE = 4,
    VT_CURRENT_ID = 6,
    VT_LRU_CACHE = 8,
    VT_ENCODING_LOOKUP_KEYS = 10,
    VT_ENCODING_LOOKUP_VALUES = 12
  };
  uint64_t max_cache_size() const {
    return GetField<uint64_t>(VT_MAX_CACHE_SIZE, 0);
  }
  uint64_t current_id() const {
    return GetField<uint64_t>(VT_CURRENT_ID, 0);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>> *lru_cache() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>> *>(VT_LRU_CACHE);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>> *encoding_lookup_keys() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>> *>(VT_ENCODING_LOOKUP_KEYS);
  }
  const ::flatbuffers::Vector<const nvfuser::serde::EncodingEntry *> *encoding_lookup_values() const {
    return GetPointer<const ::flatbuffers::Vector<const nvfuser::serde::EncodingEntry *> *>(VT_ENCODING_LOOKUP_VALUES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_MAX_CACHE_SIZE, 8) &&
           VerifyField<uint64_t>(verifier, VT_CURRENT_ID, 8) &&
           VerifyOffset(verifier, VT_LRU_CACHE) &&
           verifier.VerifyVector(lru_cache()) &&
           verifier.VerifyVectorOfStrings(lru_cache()) &&
           VerifyOffset(verifier, VT_ENCODING_LOOKUP_KEYS) &&
           verifier.VerifyVector(encoding_lookup_keys()) &&
           verifier.VerifyVectorOfStrings(encoding_lookup_keys()) &&
           VerifyOffset(verifier, VT_ENCODING_LOOKUP_VALUES) &&
           verifier.VerifyVector(encoding_lookup_values()) &&
           verifier.EndTable();
  }
};

struct InputsIdLookupBuilder {
  typedef InputsIdLookup Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_max_cache_size(uint64_t max_cache_size) {
    fbb_.AddElement<uint64_t>(InputsIdLookup::VT_MAX_CACHE_SIZE, max_cache_size, 0);
  }
  void add_current_id(uint64_t current_id) {
    fbb_.AddElement<uint64_t>(InputsIdLookup::VT_CURRENT_ID, current_id, 0);
  }
  void add_lru_cache(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>>> lru_cache) {
    fbb_.AddOffset(InputsIdLookup::VT_LRU_CACHE, lru_cache);
  }
  void add_encoding_lookup_keys(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>>> encoding_lookup_keys) {
    fbb_.AddOffset(InputsIdLookup::VT_ENCODING_LOOKUP_KEYS, encoding_lookup_keys);
  }
  void add_encoding_lookup_values(::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::EncodingEntry *>> encoding_lookup_values) {
    fbb_.AddOffset(InputsIdLookup::VT_ENCODING_LOOKUP_VALUES, encoding_lookup_values);
  }
  explicit InputsIdLookupBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<InputsIdLookup> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<InputsIdLookup>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<InputsIdLookup> CreateInputsIdLookup(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t max_cache_size = 0,
    uint64_t current_id = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>>> lru_cache = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>>> encoding_lookup_keys = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::EncodingEntry *>> encoding_lookup_values = 0) {
  InputsIdLookupBuilder builder_(_fbb);
  builder_.add_current_id(current_id);
  builder_.add_max_cache_size(max_cache_size);
  builder_.add_encoding_lookup_values(encoding_lookup_values);
  builder_.add_encoding_lookup_keys(encoding_lookup_keys);
  builder_.add_lru_cache(lru_cache);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<InputsIdLookup> CreateInputsIdLookupDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t max_cache_size = 0,
    uint64_t current_id = 0,
    const std::vector<::flatbuffers::Offset<::flatbuffers::String>> *lru_cache = nullptr,
    const std::vector<::flatbuffers::Offset<::flatbuffers::String>> *encoding_lookup_keys = nullptr,
    const std::vector<nvfuser::serde::EncodingEntry> *encoding_lookup_values = nullptr) {
  auto lru_cache__ = lru_cache ? _fbb.CreateVector<::flatbuffers::Offset<::flatbuffers::String>>(*lru_cache) : 0;
  auto encoding_lookup_keys__ = encoding_lookup_keys ? _fbb.CreateVector<::flatbuffers::Offset<::flatbuffers::String>>(*encoding_lookup_keys) : 0;
  auto encoding_lookup_values__ = encoding_lookup_values ? _fbb.CreateVectorOfStructs<nvfuser::serde::EncodingEntry>(*encoding_lookup_values) : 0;
  return nvfuser::serde::CreateInputsIdLookup(
      _fbb,
      max_cache_size,
      current_id,
      lru_cache__,
      encoding_lookup_keys__,
      encoding_lookup_values__);
}

struct KernelRuntimes FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef KernelRuntimesBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DEVICE_ID = 4,
    VT_HAS_DYNAMIC_TRANSFORM_INFO = 6,
    VT_RUNTIMES = 8
  };
  uint64_t device_id() const {
    return GetField<uint64_t>(VT_DEVICE_ID, 0);
  }
  bool has_dynamic_transform_info() const {
    return GetField<uint8_t>(VT_HAS_DYNAMIC_TRANSFORM_INFO, 0) != 0;
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>> *runtimes() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>> *>(VT_RUNTIMES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_DEVICE_ID, 8) &&
           VerifyField<uint8_t>(verifier, VT_HAS_DYNAMIC_TRANSFORM_INFO, 1) &&
           VerifyOffset(verifier, VT_RUNTIMES) &&
           verifier.VerifyVector(runtimes()) &&
           verifier.VerifyVectorOfTables(runtimes()) &&
           verifier.EndTable();
  }
};

struct KernelRuntimesBuilder {
  typedef KernelRuntimes Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_device_id(uint64_t device_id) {
    fbb_.AddElement<uint64_t>(KernelRuntimes::VT_DEVICE_ID, device_id, 0);
  }
  void add_has_dynamic_transform_info(bool has_dynamic_transform_info) {
    fbb_.AddElement<uint8_t>(KernelRuntimes::VT_HAS_DYNAMIC_TRANSFORM_INFO, static_cast<uint8_t>(has_dynamic_transform_info), 0);
  }
  void add_runtimes(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>>> runtimes) {
    fbb_.AddOffset(KernelRuntimes::VT_RUNTIMES, runtimes);
  }
  explicit KernelRuntimesBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<KernelRuntimes> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<KernelRuntimes>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<KernelRuntimes> CreateKernelRuntimes(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t device_id = 0,
    bool has_dynamic_transform_info = false,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>>> runtimes = 0) {
  KernelRuntimesBuilder builder_(_fbb);
  builder_.add_device_id(device_id);
  builder_.add_runtimes(runtimes);
  builder_.add_has_dynamic_transform_info(has_dynamic_transform_info);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<KernelRuntimes> CreateKernelRuntimesDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t device_id = 0,
    bool has_dynamic_transform_info = false,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>> *runtimes = nullptr) {
  auto runtimes__ = runtimes ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>>(*runtimes) : 0;
  return nvfuser::serde::CreateKernelRuntimes(
      _fbb,
      device_id,
      has_dynamic_transform_info,
      runtimes__);
}

struct FusionExecutorCache FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef FusionExecutorCacheBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INPUTS_CACHE = 4,
    VT_KERNEL_RUNTIMES = 6,
    VT_KERNEL_CACHE_KEYS = 8,
    VT_KERNEL_CACHE_VALUES = 10
  };
  const nvfuser::serde::InputsIdLookup *inputs_cache() const {
    return GetPointer<const nvfuser::serde::InputsIdLookup *>(VT_INPUTS_CACHE);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimes>> *kernel_runtimes() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimes>> *>(VT_KERNEL_RUNTIMES);
  }
  const ::flatbuffers::Vector<uint64_t> *kernel_cache_keys() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_KERNEL_CACHE_KEYS);
  }
  const ::flatbuffers::Vector<uint64_t> *kernel_cache_values() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_KERNEL_CACHE_VALUES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_INPUTS_CACHE) &&
           verifier.VerifyTable(inputs_cache()) &&
           VerifyOffset(verifier, VT_KERNEL_RUNTIMES) &&
           verifier.VerifyVector(kernel_runtimes()) &&
           verifier.VerifyVectorOfTables(kernel_runtimes()) &&
           VerifyOffset(verifier, VT_KERNEL_CACHE_KEYS) &&
           verifier.VerifyVector(kernel_cache_keys()) &&
           VerifyOffset(verifier, VT_KERNEL_CACHE_VALUES) &&
           verifier.VerifyVector(kernel_cache_values()) &&
           verifier.EndTable();
  }
};

struct FusionExecutorCacheBuilder {
  typedef FusionExecutorCache Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_inputs_cache(::flatbuffers::Offset<nvfuser::serde::InputsIdLookup> inputs_cache) {
    fbb_.AddOffset(FusionExecutorCache::VT_INPUTS_CACHE, inputs_cache);
  }
  void add_kernel_runtimes(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimes>>> kernel_runtimes) {
    fbb_.AddOffset(FusionExecutorCache::VT_KERNEL_RUNTIMES, kernel_runtimes);
  }
  void add_kernel_cache_keys(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> kernel_cache_keys) {
    fbb_.AddOffset(FusionExecutorCache::VT_KERNEL_CACHE_KEYS, kernel_cache_keys);
  }
  void add_kernel_cache_values(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> kernel_cache_values) {
    fbb_.AddOffset(FusionExecutorCache::VT_KERNEL_CACHE_VALUES, kernel_cache_values);
  }
  explicit FusionExecutorCacheBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<FusionExecutorCache> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<FusionExecutorCache>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<FusionExecutorCache> CreateFusionExecutorCache(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::InputsIdLookup> inputs_cache = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimes>>> kernel_runtimes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> kernel_cache_keys = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> kernel_cache_values = 0) {
  FusionExecutorCacheBuilder builder_(_fbb);
  builder_.add_kernel_cache_values(kernel_cache_values);
  builder_.add_kernel_cache_keys(kernel_cache_keys);
  builder_.add_kernel_runtimes(kernel_runtimes);
  builder_.add_inputs_cache(inputs_cache);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<FusionExecutorCache> CreateFusionExecutorCacheDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::InputsIdLookup> inputs_cache = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimes>> *kernel_runtimes = nullptr,
    const std::vector<uint64_t> *kernel_cache_keys = nullptr,
    const std::vector<uint64_t> *kernel_cache_values = nullptr) {
  auto kernel_runtimes__ = kernel_runtimes ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimes>>(*kernel_runtimes) : 0;
  auto kernel_cache_keys__ = kernel_cache_keys ? _fbb.CreateVector<uint64_t>(*kernel_cache_keys) : 0;
  auto kernel_cache_values__ = kernel_cache_values ? _fbb.CreateVector<uint64_t>(*kernel_cache_values) : 0;
  return nvfuser::serde::CreateFusionExecutorCache(
      _fbb,
      inputs_cache,
      kernel_runtimes__,
      kernel_cache_keys__,
      kernel_cache_values__);
}

struct RecordFunctor FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef RecordFunctorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ARGS = 4,
    VT_OUTPUTS = 6,
    VT_NAME = 8,
    VT_TYPE = 10,
    VT_DATA_TYPE = 12,
    VT_DATA = 14
  };
  const ::flatbuffers::Vector<const nvfuser::serde::State *> *args() const {
    return GetPointer<const ::flatbuffers::Vector<const nvfuser::serde::State *> *>(VT_ARGS);
  }
  const ::flatbuffers::Vector<const nvfuser::serde::State *> *outputs() const {
    return GetPointer<const ::flatbuffers::Vector<const nvfuser::serde::State *> *>(VT_OUTPUTS);
  }
  const ::flatbuffers::String *name() const {
    return GetPointer<const ::flatbuffers::String *>(VT_NAME);
  }
  nvfuser::serde::RecordType type() const {
    return static_cast<nvfuser::serde::RecordType>(GetField<int32_t>(VT_TYPE, 0));
  }
  nvfuser::serde::RecordData data_type() const {
    return static_cast<nvfuser::serde::RecordData>(GetField<uint8_t>(VT_DATA_TYPE, 0));
  }
  const void *data() const {
    return GetPointer<const void *>(VT_DATA);
  }
  template<typename T> const T *data_as() const;
  const nvfuser::serde::At *data_as_At() const {
    return data_type() == nvfuser::serde::RecordData_At ? static_cast<const nvfuser::serde::At *>(data()) : nullptr;
  }
  const nvfuser::serde::BatchNorm *data_as_BatchNorm() const {
    return data_type() == nvfuser::serde::RecordData_BatchNorm ? static_cast<const nvfuser::serde::BatchNorm *>(data()) : nullptr;
  }
  const nvfuser::serde::Broadcast *data_as_Broadcast() const {
    return data_type() == nvfuser::serde::RecordData_Broadcast ? static_cast<const nvfuser::serde::Broadcast *>(data()) : nullptr;
  }
  const nvfuser::serde::BroadcastInDim *data_as_BroadcastInDim() const {
    return data_type() == nvfuser::serde::RecordData_BroadcastInDim ? static_cast<const nvfuser::serde::BroadcastInDim *>(data()) : nullptr;
  }
  const nvfuser::serde::Dimension *data_as_Dimension() const {
    return data_type() == nvfuser::serde::RecordData_Dimension ? static_cast<const nvfuser::serde::Dimension *>(data()) : nullptr;
  }
  const nvfuser::serde::Dtype *data_as_Dtype() const {
    return data_type() == nvfuser::serde::RecordData_Dtype ? static_cast<const nvfuser::serde::Dtype *>(data()) : nullptr;
  }
  const nvfuser::serde::Norm *data_as_Norm() const {
    return data_type() == nvfuser::serde::RecordData_Norm ? static_cast<const nvfuser::serde::Norm *>(data()) : nullptr;
  }
  const nvfuser::serde::Output *data_as_Output() const {
    return data_type() == nvfuser::serde::RecordData_Output ? static_cast<const nvfuser::serde::Output *>(data()) : nullptr;
  }
  const nvfuser::serde::Pad *data_as_Pad() const {
    return data_type() == nvfuser::serde::RecordData_Pad ? static_cast<const nvfuser::serde::Pad *>(data()) : nullptr;
  }
  const nvfuser::serde::Permute *data_as_Permute() const {
    return data_type() == nvfuser::serde::RecordData_Permute ? static_cast<const nvfuser::serde::Permute *>(data()) : nullptr;
  }
  const nvfuser::serde::Slice *data_as_Slice() const {
    return data_type() == nvfuser::serde::RecordData_Slice ? static_cast<const nvfuser::serde::Slice *>(data()) : nullptr;
  }
  const nvfuser::serde::Squeeze *data_as_Squeeze() const {
    return data_type() == nvfuser::serde::RecordData_Squeeze ? static_cast<const nvfuser::serde::Squeeze *>(data()) : nullptr;
  }
  const nvfuser::serde::Reduction *data_as_Reduction() const {
    return data_type() == nvfuser::serde::RecordData_Reduction ? static_cast<const nvfuser::serde::Reduction *>(data()) : nullptr;
  }
  const nvfuser::serde::Reshape *data_as_Reshape() const {
    return data_type() == nvfuser::serde::RecordData_Reshape ? static_cast<const nvfuser::serde::Reshape *>(data()) : nullptr;
  }
  const nvfuser::serde::Scalar *data_as_Scalar() const {
    return data_type() == nvfuser::serde::RecordData_Scalar ? static_cast<const nvfuser::serde::Scalar *>(data()) : nullptr;
  }
  const nvfuser::serde::Size *data_as_Size() const {
    return data_type() == nvfuser::serde::RecordData_Size ? static_cast<const nvfuser::serde::Size *>(data()) : nullptr;
  }
  const nvfuser::serde::Tensor *data_as_Tensor() const {
    return data_type() == nvfuser::serde::RecordData_Tensor ? static_cast<const nvfuser::serde::Tensor *>(data()) : nullptr;
  }
  const nvfuser::serde::TensorCreation *data_as_TensorCreation() const {
    return data_type() == nvfuser::serde::RecordData_TensorCreation ? static_cast<const nvfuser::serde::TensorCreation *>(data()) : nullptr;
  }
  const nvfuser::serde::TensorCreationSymbolic *data_as_TensorCreationSymbolic() const {
    return data_type() == nvfuser::serde::RecordData_TensorCreationSymbolic ? static_cast<const nvfuser::serde::TensorCreationSymbolic *>(data()) : nullptr;
  }
  const nvfuser::serde::Vector *data_as_Vector() const {
    return data_type() == nvfuser::serde::RecordData_Vector ? static_cast<const nvfuser::serde::Vector *>(data()) : nullptr;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ARGS) &&
           verifier.VerifyVector(args()) &&
           VerifyOffset(verifier, VT_OUTPUTS) &&
           verifier.VerifyVector(outputs()) &&
           VerifyOffset(verifier, VT_NAME) &&
           verifier.VerifyString(name()) &&
           VerifyField<int32_t>(verifier, VT_TYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_DATA_TYPE, 1) &&
           VerifyOffset(verifier, VT_DATA) &&
           VerifyRecordData(verifier, data(), data_type()) &&
           verifier.EndTable();
  }
};

template<> inline const nvfuser::serde::At *RecordFunctor::data_as<nvfuser::serde::At>() const {
  return data_as_At();
}

template<> inline const nvfuser::serde::BatchNorm *RecordFunctor::data_as<nvfuser::serde::BatchNorm>() const {
  return data_as_BatchNorm();
}

template<> inline const nvfuser::serde::Broadcast *RecordFunctor::data_as<nvfuser::serde::Broadcast>() const {
  return data_as_Broadcast();
}

template<> inline const nvfuser::serde::BroadcastInDim *RecordFunctor::data_as<nvfuser::serde::BroadcastInDim>() const {
  return data_as_BroadcastInDim();
}

template<> inline const nvfuser::serde::Dimension *RecordFunctor::data_as<nvfuser::serde::Dimension>() const {
  return data_as_Dimension();
}

template<> inline const nvfuser::serde::Dtype *RecordFunctor::data_as<nvfuser::serde::Dtype>() const {
  return data_as_Dtype();
}

template<> inline const nvfuser::serde::Norm *RecordFunctor::data_as<nvfuser::serde::Norm>() const {
  return data_as_Norm();
}

template<> inline const nvfuser::serde::Output *RecordFunctor::data_as<nvfuser::serde::Output>() const {
  return data_as_Output();
}

template<> inline const nvfuser::serde::Pad *RecordFunctor::data_as<nvfuser::serde::Pad>() const {
  return data_as_Pad();
}

template<> inline const nvfuser::serde::Permute *RecordFunctor::data_as<nvfuser::serde::Permute>() const {
  return data_as_Permute();
}

template<> inline const nvfuser::serde::Slice *RecordFunctor::data_as<nvfuser::serde::Slice>() const {
  return data_as_Slice();
}

template<> inline const nvfuser::serde::Squeeze *RecordFunctor::data_as<nvfuser::serde::Squeeze>() const {
  return data_as_Squeeze();
}

template<> inline const nvfuser::serde::Reduction *RecordFunctor::data_as<nvfuser::serde::Reduction>() const {
  return data_as_Reduction();
}

template<> inline const nvfuser::serde::Reshape *RecordFunctor::data_as<nvfuser::serde::Reshape>() const {
  return data_as_Reshape();
}

template<> inline const nvfuser::serde::Scalar *RecordFunctor::data_as<nvfuser::serde::Scalar>() const {
  return data_as_Scalar();
}

template<> inline const nvfuser::serde::Size *RecordFunctor::data_as<nvfuser::serde::Size>() const {
  return data_as_Size();
}

template<> inline const nvfuser::serde::Tensor *RecordFunctor::data_as<nvfuser::serde::Tensor>() const {
  return data_as_Tensor();
}

template<> inline const nvfuser::serde::TensorCreation *RecordFunctor::data_as<nvfuser::serde::TensorCreation>() const {
  return data_as_TensorCreation();
}

template<> inline const nvfuser::serde::TensorCreationSymbolic *RecordFunctor::data_as<nvfuser::serde::TensorCreationSymbolic>() const {
  return data_as_TensorCreationSymbolic();
}

template<> inline const nvfuser::serde::Vector *RecordFunctor::data_as<nvfuser::serde::Vector>() const {
  return data_as_Vector();
}

struct RecordFunctorBuilder {
  typedef RecordFunctor Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_args(::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> args) {
    fbb_.AddOffset(RecordFunctor::VT_ARGS, args);
  }
  void add_outputs(::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> outputs) {
    fbb_.AddOffset(RecordFunctor::VT_OUTPUTS, outputs);
  }
  void add_name(::flatbuffers::Offset<::flatbuffers::String> name) {
    fbb_.AddOffset(RecordFunctor::VT_NAME, name);
  }
  void add_type(nvfuser::serde::RecordType type) {
    fbb_.AddElement<int32_t>(RecordFunctor::VT_TYPE, static_cast<int32_t>(type), 0);
  }
  void add_data_type(nvfuser::serde::RecordData data_type) {
    fbb_.AddElement<uint8_t>(RecordFunctor::VT_DATA_TYPE, static_cast<uint8_t>(data_type), 0);
  }
  void add_data(::flatbuffers::Offset<void> data) {
    fbb_.AddOffset(RecordFunctor::VT_DATA, data);
  }
  explicit RecordFunctorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<RecordFunctor> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<RecordFunctor>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<RecordFunctor> CreateRecordFunctor(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> args = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> outputs = 0,
    ::flatbuffers::Offset<::flatbuffers::String> name = 0,
    nvfuser::serde::RecordType type = nvfuser::serde::RecordType_Base,
    nvfuser::serde::RecordData data_type = nvfuser::serde::RecordData_NONE,
    ::flatbuffers::Offset<void> data = 0) {
  RecordFunctorBuilder builder_(_fbb);
  builder_.add_data(data);
  builder_.add_type(type);
  builder_.add_name(name);
  builder_.add_outputs(outputs);
  builder_.add_args(args);
  builder_.add_data_type(data_type);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<RecordFunctor> CreateRecordFunctorDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<nvfuser::serde::State> *args = nullptr,
    const std::vector<nvfuser::serde::State> *outputs = nullptr,
    const char *name = nullptr,
    nvfuser::serde::RecordType type = nvfuser::serde::RecordType_Base,
    nvfuser::serde::RecordData data_type = nvfuser::serde::RecordData_NONE,
    ::flatbuffers::Offset<void> data = 0) {
  auto args__ = args ? _fbb.CreateVectorOfStructs<nvfuser::serde::State>(*args) : 0;
  auto outputs__ = outputs ? _fbb.CreateVectorOfStructs<nvfuser::serde::State>(*outputs) : 0;
  auto name__ = name ? _fbb.CreateString(name) : 0;
  return nvfuser::serde::CreateRecordFunctor(
      _fbb,
      args__,
      outputs__,
      name__,
      type,
      data_type,
      data);
}

struct TrieNode FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TrieNodeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_RECORD = 4,
    VT_CHILDREN = 6,
    VT_FUSION_ID = 8,
    VT_VISITS = 10,
    VT_IS_TERMINAL = 12
  };
  const nvfuser::serde::RecordFunctor *record() const {
    return GetPointer<const nvfuser::serde::RecordFunctor *>(VT_RECORD);
  }
  const ::flatbuffers::Vector<uint64_t> *children() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_CHILDREN);
  }
  uint64_t fusion_id() const {
    return GetField<uint64_t>(VT_FUSION_ID, 0);
  }
  uint64_t visits() const {
    return GetField<uint64_t>(VT_VISITS, 0);
  }
  bool is_terminal() const {
    return GetField<uint8_t>(VT_IS_TERMINAL, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_RECORD) &&
           verifier.VerifyTable(record()) &&
           VerifyOffset(verifier, VT_CHILDREN) &&
           verifier.VerifyVector(children()) &&
           VerifyField<uint64_t>(verifier, VT_FUSION_ID, 8) &&
           VerifyField<uint64_t>(verifier, VT_VISITS, 8) &&
           VerifyField<uint8_t>(verifier, VT_IS_TERMINAL, 1) &&
           verifier.EndTable();
  }
};

struct TrieNodeBuilder {
  typedef TrieNode Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_record(::flatbuffers::Offset<nvfuser::serde::RecordFunctor> record) {
    fbb_.AddOffset(TrieNode::VT_RECORD, record);
  }
  void add_children(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> children) {
    fbb_.AddOffset(TrieNode::VT_CHILDREN, children);
  }
  void add_fusion_id(uint64_t fusion_id) {
    fbb_.AddElement<uint64_t>(TrieNode::VT_FUSION_ID, fusion_id, 0);
  }
  void add_visits(uint64_t visits) {
    fbb_.AddElement<uint64_t>(TrieNode::VT_VISITS, visits, 0);
  }
  void add_is_terminal(bool is_terminal) {
    fbb_.AddElement<uint8_t>(TrieNode::VT_IS_TERMINAL, static_cast<uint8_t>(is_terminal), 0);
  }
  explicit TrieNodeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TrieNode> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TrieNode>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TrieNode> CreateTrieNode(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::RecordFunctor> record = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> children = 0,
    uint64_t fusion_id = 0,
    uint64_t visits = 0,
    bool is_terminal = false) {
  TrieNodeBuilder builder_(_fbb);
  builder_.add_visits(visits);
  builder_.add_fusion_id(fusion_id);
  builder_.add_children(children);
  builder_.add_record(record);
  builder_.add_is_terminal(is_terminal);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TrieNode> CreateTrieNodeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::RecordFunctor> record = 0,
    const std::vector<uint64_t> *children = nullptr,
    uint64_t fusion_id = 0,
    uint64_t visits = 0,
    bool is_terminal = false) {
  auto children__ = children ? _fbb.CreateVector<uint64_t>(*children) : 0;
  return nvfuser::serde::CreateTrieNode(
      _fbb,
      record,
      children__,
      fusion_id,
      visits,
      is_terminal);
}

struct FusionCache FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef FusionCacheBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_MAX_FUSIONS = 4,
    VT_STRUCTURE = 6,
    VT_TERMINAL_NODES = 8,
    VT_AUTO_GEN_SCHEDULES = 10
  };
  uint64_t max_fusions() const {
    return GetField<uint64_t>(VT_MAX_FUSIONS, 0);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>> *structure() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>> *>(VT_STRUCTURE);
  }
  const ::flatbuffers::Vector<uint64_t> *terminal_nodes() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_TERMINAL_NODES);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>> *auto_gen_schedules() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>> *>(VT_AUTO_GEN_SCHEDULES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_MAX_FUSIONS, 8) &&
           VerifyOffset(verifier, VT_STRUCTURE) &&
           verifier.VerifyVector(structure()) &&
           verifier.VerifyVectorOfTables(structure()) &&
           VerifyOffset(verifier, VT_TERMINAL_NODES) &&
           verifier.VerifyVector(terminal_nodes()) &&
           VerifyOffset(verifier, VT_AUTO_GEN_SCHEDULES) &&
           verifier.VerifyVector(auto_gen_schedules()) &&
           verifier.VerifyVectorOfTables(auto_gen_schedules()) &&
           verifier.EndTable();
  }
};

struct FusionCacheBuilder {
  typedef FusionCache Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_max_fusions(uint64_t max_fusions) {
    fbb_.AddElement<uint64_t>(FusionCache::VT_MAX_FUSIONS, max_fusions, 0);
  }
  void add_structure(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>>> structure) {
    fbb_.AddOffset(FusionCache::VT_STRUCTURE, structure);
  }
  void add_terminal_nodes(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> terminal_nodes) {
    fbb_.AddOffset(FusionCache::VT_TERMINAL_NODES, terminal_nodes);
  }
  void add_auto_gen_schedules(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>>> auto_gen_schedules) {
    fbb_.AddOffset(FusionCache::VT_AUTO_GEN_SCHEDULES, auto_gen_schedules);
  }
  explicit FusionCacheBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<FusionCache> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<FusionCache>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<FusionCache> CreateFusionCache(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t max_fusions = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>>> structure = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> terminal_nodes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>>> auto_gen_schedules = 0) {
  FusionCacheBuilder builder_(_fbb);
  builder_.add_max_fusions(max_fusions);
  builder_.add_auto_gen_schedules(auto_gen_schedules);
  builder_.add_terminal_nodes(terminal_nodes);
  builder_.add_structure(structure);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<FusionCache> CreateFusionCacheDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t max_fusions = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>> *structure = nullptr,
    const std::vector<uint64_t> *terminal_nodes = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>> *auto_gen_schedules = nullptr) {
  auto structure__ = structure ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::TrieNode>>(*structure) : 0;
  auto terminal_nodes__ = terminal_nodes ? _fbb.CreateVector<uint64_t>(*terminal_nodes) : 0;
  auto auto_gen_schedules__ = auto_gen_schedules ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>>(*auto_gen_schedules) : 0;
  return nvfuser::serde::CreateFusionCache(
      _fbb,
      max_fusions,
      structure__,
      terminal_nodes__,
      auto_gen_schedules__);
}

inline bool VerifyRecordData(::flatbuffers::Verifier &verifier, const void *obj, RecordData type) {
  switch (type) {
    case RecordData_NONE: {
      return true;
    }
    case RecordData_At: {
      auto ptr = reinterpret_cast<const nvfuser::serde::At *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_BatchNorm: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BatchNorm *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Broadcast: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Broadcast *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_BroadcastInDim: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BroadcastInDim *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Dimension: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Dimension *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Dtype: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Dtype *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Norm: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Norm *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Output: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Output *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Pad: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Pad *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Permute: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Permute *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Slice: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Slice *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Squeeze: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Squeeze *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Reduction: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Reduction *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Reshape: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Reshape *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Scalar *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Size: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Size *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Tensor: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Tensor *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_TensorCreation: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorCreation *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_TensorCreationSymbolic: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorCreationSymbolic *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Vector: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Vector *>(obj);
      return verifier.VerifyTable(ptr);
    }
    default: return true;
  }
}

inline bool VerifyRecordDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<uint8_t> *types) {
  if (!values || !types) return !values && !types;
  if (values->size() != types->size()) return false;
  for (::flatbuffers::uoffset_t i = 0; i < values->size(); ++i) {
    if (!VerifyRecordData(
        verifier,  values->Get(i), types->GetEnum<RecordData>(i))) {
      return false;
    }
  }
  return true;
}

inline bool VerifyArgAbstractData(::flatbuffers::Verifier &verifier, const void *obj, ArgAbstractData type) {
  switch (type) {
    case ArgAbstractData_NONE: {
      return true;
    }
    case ArgAbstractData_Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Scalar *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case ArgAbstractData_PhiloxCudaState: {
      auto ptr = reinterpret_cast<const nvfuser::serde::PhiloxCudaState *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case ArgAbstractData_ScalarCpu: {
      auto ptr = reinterpret_cast<const nvfuser::serde::ScalarCpu *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case ArgAbstractData_TensorArg: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorArg *>(obj);
      return verifier.VerifyTable(ptr);
    }
    default: return true;
  }
}

inline bool VerifyArgAbstractDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<uint8_t> *types) {
  if (!values || !types) return !values && !types;
  if (values->size() != types->size()) return false;
  for (::flatbuffers::uoffset_t i = 0; i < values->size(); ++i) {
    if (!VerifyArgAbstractData(
        verifier,  values->Get(i), types->GetEnum<ArgAbstractData>(i))) {
      return false;
    }
  }
  return true;
}

inline const nvfuser::serde::FusionCache *GetFusionCache(const void *buf) {
  return ::flatbuffers::GetRoot<nvfuser::serde::FusionCache>(buf);
}

inline const nvfuser::serde::FusionCache *GetSizePrefixedFusionCache(const void *buf) {
  return ::flatbuffers::GetSizePrefixedRoot<nvfuser::serde::FusionCache>(buf);
}

inline const char *FusionCacheIdentifier() {
  return "NV00";
}

inline bool FusionCacheBufferHasIdentifier(const void *buf) {
  return ::flatbuffers::BufferHasIdentifier(
      buf, FusionCacheIdentifier());
}

inline bool SizePrefixedFusionCacheBufferHasIdentifier(const void *buf) {
  return ::flatbuffers::BufferHasIdentifier(
      buf, FusionCacheIdentifier(), true);
}

inline bool VerifyFusionCacheBuffer(
    ::flatbuffers::Verifier &verifier) {
  return verifier.VerifyBuffer<nvfuser::serde::FusionCache>(FusionCacheIdentifier());
}

inline bool VerifySizePrefixedFusionCacheBuffer(
    ::flatbuffers::Verifier &verifier) {
  return verifier.VerifySizePrefixedBuffer<nvfuser::serde::FusionCache>(FusionCacheIdentifier());
}

inline void FinishFusionCacheBuffer(
    ::flatbuffers::FlatBufferBuilder &fbb,
    ::flatbuffers::Offset<nvfuser::serde::FusionCache> root) {
  fbb.Finish(root, FusionCacheIdentifier());
}

inline void FinishSizePrefixedFusionCacheBuffer(
    ::flatbuffers::FlatBufferBuilder &fbb,
    ::flatbuffers::Offset<nvfuser::serde::FusionCache> root) {
  fbb.FinishSizePrefixed(root, FusionCacheIdentifier());
}

}  // namespace serde
}  // namespace nvfuser

#endif  // FLATBUFFERS_GENERATED_FUSIONCACHE_NVFUSER_SERDE_H_
