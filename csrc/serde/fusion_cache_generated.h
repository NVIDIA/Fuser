// automatically generated by the FlatBuffers compiler, do not modify


#ifndef FLATBUFFERS_GENERATED_FUSIONCACHE_NVFUSER_SERDE_H_
#define FLATBUFFERS_GENERATED_FUSIONCACHE_NVFUSER_SERDE_H_

#include "flatbuffers/flatbuffers.h"

// Ensure the included flatbuffers.h is the same version as when this file was
// generated, otherwise it may not be compatible.
static_assert(FLATBUFFERS_VERSION_MAJOR == 23 &&
              FLATBUFFERS_VERSION_MINOR == 3 &&
              FLATBUFFERS_VERSION_REVISION == 3,
             "Non-compatible flatbuffers version included");

namespace nvfuser {
namespace serde {

struct State;

struct Scalar;
struct ScalarBuilder;
struct ScalarT;

struct ScalarCpu;
struct ScalarCpuBuilder;
struct ScalarCpuT;

struct TensorArg;
struct TensorArgBuilder;
struct TensorArgT;

struct PolymorphicValue;
struct PolymorphicValueBuilder;
struct PolymorphicValueT;

struct KernelArgumentHolder;
struct KernelArgumentHolderBuilder;
struct KernelArgumentHolderT;

struct TensorShape;
struct TensorShapeBuilder;
struct TensorShapeT;

struct LaunchParams;
struct LaunchParamsBuilder;
struct LaunchParamsT;

struct GlobalBufferInfo;
struct GlobalBufferInfoBuilder;
struct GlobalBufferInfoT;

struct ExecutorEntry;
struct ExecutorEntryBuilder;
struct ExecutorEntryT;

struct At;
struct AtBuilder;
struct AtT;

struct BatchNorm;
struct BatchNormBuilder;
struct BatchNormT;

struct Broadcast;
struct BroadcastBuilder;
struct BroadcastT;

struct BroadcastInDim;
struct BroadcastInDimBuilder;
struct BroadcastInDimT;

struct Dtype;
struct DtypeBuilder;
struct DtypeT;

struct Dimension;
struct DimensionBuilder;
struct DimensionT;

struct Norm;
struct NormBuilder;
struct NormT;

struct Output;
struct OutputBuilder;
struct OutputT;

struct Pad;
struct PadBuilder;
struct PadT;

struct Permute;
struct PermuteBuilder;
struct PermuteT;

struct Reduction;
struct ReductionBuilder;
struct ReductionT;

struct Reshape;
struct ReshapeBuilder;
struct ReshapeT;

struct Size;
struct SizeBuilder;
struct SizeT;

struct Slice;
struct SliceBuilder;
struct SliceT;

struct Squeeze;
struct SqueezeBuilder;
struct SqueezeT;

struct Tensor;
struct TensorBuilder;
struct TensorT;

struct TensorCreation;
struct TensorCreationBuilder;
struct TensorCreationT;

struct TensorCreationSymbolic;
struct TensorCreationSymbolicBuilder;
struct TensorCreationSymbolicT;

struct Vector;
struct VectorBuilder;
struct VectorT;

struct CudaKernel;
struct CudaKernelBuilder;
struct CudaKernelT;

struct FusionExecutor;
struct FusionExecutorBuilder;
struct FusionExecutorT;

struct FusionKernelRuntime;
struct FusionKernelRuntimeBuilder;
struct FusionKernelRuntimeT;

struct EncodingEntry;

struct InputsIdLookup;
struct InputsIdLookupBuilder;
struct InputsIdLookupT;

struct KernelRuntimeState;
struct KernelRuntimeStateBuilder;
struct KernelRuntimeStateT;

struct FusionExecutorCache;
struct FusionExecutorCacheBuilder;
struct FusionExecutorCacheT;

struct RecordFunctor;
struct RecordFunctorBuilder;
struct RecordFunctorT;

struct TrieNode;
struct TrieNodeBuilder;
struct TrieNodeT;

struct FusionCache;
struct FusionCacheBuilder;
struct FusionCacheT;

enum DataType : int32_t {
  DataType_Double = 0,
  DataType_Float = 1,
  DataType_Half = 2,
  DataType_Int = 3,
  DataType_Int32 = 4,
  DataType_Bool = 5,
  DataType_BFloat16 = 6,
  DataType_ComplexFloat = 7,
  DataType_ComplexDouble = 8,
  DataType_None = 9,
  DataType_MIN = DataType_Double,
  DataType_MAX = DataType_None
};

inline const DataType (&EnumValuesDataType())[10] {
  static const DataType values[] = {
    DataType_Double,
    DataType_Float,
    DataType_Half,
    DataType_Int,
    DataType_Int32,
    DataType_Bool,
    DataType_BFloat16,
    DataType_ComplexFloat,
    DataType_ComplexDouble,
    DataType_None
  };
  return values;
}

inline const char * const *EnumNamesDataType() {
  static const char * const names[11] = {
    "Double",
    "Float",
    "Half",
    "Int",
    "Int32",
    "Bool",
    "BFloat16",
    "ComplexFloat",
    "ComplexDouble",
    "None",
    nullptr
  };
  return names;
}

inline const char *EnumNameDataType(DataType e) {
  if (::flatbuffers::IsOutRange(e, DataType_Double, DataType_None)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesDataType()[index];
}

enum StateType : int32_t {
  StateType_Tensor = 0,
  StateType_Scalar = 1,
  StateType_Vector = 2,
  StateType_None = 3,
  StateType_MIN = StateType_Tensor,
  StateType_MAX = StateType_None
};

inline const StateType (&EnumValuesStateType())[4] {
  static const StateType values[] = {
    StateType_Tensor,
    StateType_Scalar,
    StateType_Vector,
    StateType_None
  };
  return values;
}

inline const char * const *EnumNamesStateType() {
  static const char * const names[5] = {
    "Tensor",
    "Scalar",
    "Vector",
    "None",
    nullptr
  };
  return names;
}

inline const char *EnumNameStateType(StateType e) {
  if (::flatbuffers::IsOutRange(e, StateType_Tensor, StateType_None)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesStateType()[index];
}

enum Contiguity : int32_t {
  Contiguity_Strided = 0,
  Contiguity_Contiguous = 1,
  Contiguity_None = 2,
  Contiguity_MIN = Contiguity_Strided,
  Contiguity_MAX = Contiguity_None
};

inline const Contiguity (&EnumValuesContiguity())[3] {
  static const Contiguity values[] = {
    Contiguity_Strided,
    Contiguity_Contiguous,
    Contiguity_None
  };
  return values;
}

inline const char * const *EnumNamesContiguity() {
  static const char * const names[4] = {
    "Strided",
    "Contiguous",
    "None",
    nullptr
  };
  return names;
}

inline const char *EnumNameContiguity(Contiguity e) {
  if (::flatbuffers::IsOutRange(e, Contiguity_Strided, Contiguity_None)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesContiguity()[index];
}

enum RecordType : int32_t {
  RecordType_Base = 0,
  RecordType_AtOp = 1,
  RecordType_BatchNormOp = 2,
  RecordType_BroadcastOp = 3,
  RecordType_BroadcastInDim = 4,
  RecordType_CastTv = 5,
  RecordType_CastVal = 6,
  RecordType_CatOp = 7,
  RecordType_End = 8,
  RecordType_FullOp = 9,
  RecordType_IotaOp = 10,
  RecordType_IndexSelectOp = 11,
  RecordType_TorchGatherOp = 12,
  RecordType_TakeAlongAxisOp = 13,
  RecordType_Unary_TV = 14,
  RecordType_Unary_VAL = 15,
  RecordType_Binary_TV = 16,
  RecordType_Binary_VAL = 17,
  RecordType_Binary_TV_VAL = 18,
  RecordType_Binary_VAL_TV = 19,
  RecordType_Ternary_TV = 20,
  RecordType_Ternary_VAL = 21,
  RecordType_Ternary_TV_TV_VAL = 22,
  RecordType_Ternary_TV_VAL_TV = 23,
  RecordType_Ternary_VAL_TV_TV = 24,
  RecordType_Ternary_VAL_VAL_TV = 25,
  RecordType_Ternary_TV_VAL_VAL = 26,
  RecordType_Ternary_VAL_TV_VAL = 27,
  RecordType_Ternary_Alpha_TV = 28,
  RecordType_Ternary_Alpha_VAL = 29,
  RecordType_Ternary_Alpha_TV_TV_VAL = 30,
  RecordType_Ternary_Alpha_TV_VAL_TV = 31,
  RecordType_Ternary_Alpha_VAL_TV_TV = 32,
  RecordType_Ternary_Alpha_VAL_VAL_TV = 33,
  RecordType_Ternary_Alpha_TV_VAL_VAL = 34,
  RecordType_Ternary_Alpha_VAL_TV_VAL = 35,
  RecordType_OutputTv = 36,
  RecordType_OutputVal = 37,
  RecordType_PadOp = 38,
  RecordType_PermuteOp = 39,
  RecordType_RandomOp = 40,
  RecordType_ReductionMax = 41,
  RecordType_ReductionMin = 42,
  RecordType_ReductionProd = 43,
  RecordType_ReductionSum = 44,
  RecordType_ReshapeOp = 45,
  RecordType_Scalar = 46,
  RecordType_ShapeOp = 47,
  RecordType_SizeOp = 48,
  RecordType_SliceOp = 49,
  RecordType_SqueezeOp = 50,
  RecordType_Start = 51,
  RecordType_Tensor = 52,
  RecordType_TensorSizes = 53,
  RecordType_VarianceOp = 54,
  RecordType_VarianceMeanOp = 55,
  RecordType_Vector = 56,
  RecordType_MIN = RecordType_Base,
  RecordType_MAX = RecordType_Vector
};

inline const RecordType (&EnumValuesRecordType())[57] {
  static const RecordType values[] = {
    RecordType_Base,
    RecordType_AtOp,
    RecordType_BatchNormOp,
    RecordType_BroadcastOp,
    RecordType_BroadcastInDim,
    RecordType_CastTv,
    RecordType_CastVal,
    RecordType_CatOp,
    RecordType_End,
    RecordType_FullOp,
    RecordType_IotaOp,
    RecordType_IndexSelectOp,
    RecordType_TorchGatherOp,
    RecordType_TakeAlongAxisOp,
    RecordType_Unary_TV,
    RecordType_Unary_VAL,
    RecordType_Binary_TV,
    RecordType_Binary_VAL,
    RecordType_Binary_TV_VAL,
    RecordType_Binary_VAL_TV,
    RecordType_Ternary_TV,
    RecordType_Ternary_VAL,
    RecordType_Ternary_TV_TV_VAL,
    RecordType_Ternary_TV_VAL_TV,
    RecordType_Ternary_VAL_TV_TV,
    RecordType_Ternary_VAL_VAL_TV,
    RecordType_Ternary_TV_VAL_VAL,
    RecordType_Ternary_VAL_TV_VAL,
    RecordType_Ternary_Alpha_TV,
    RecordType_Ternary_Alpha_VAL,
    RecordType_Ternary_Alpha_TV_TV_VAL,
    RecordType_Ternary_Alpha_TV_VAL_TV,
    RecordType_Ternary_Alpha_VAL_TV_TV,
    RecordType_Ternary_Alpha_VAL_VAL_TV,
    RecordType_Ternary_Alpha_TV_VAL_VAL,
    RecordType_Ternary_Alpha_VAL_TV_VAL,
    RecordType_OutputTv,
    RecordType_OutputVal,
    RecordType_PadOp,
    RecordType_PermuteOp,
    RecordType_RandomOp,
    RecordType_ReductionMax,
    RecordType_ReductionMin,
    RecordType_ReductionProd,
    RecordType_ReductionSum,
    RecordType_ReshapeOp,
    RecordType_Scalar,
    RecordType_ShapeOp,
    RecordType_SizeOp,
    RecordType_SliceOp,
    RecordType_SqueezeOp,
    RecordType_Start,
    RecordType_Tensor,
    RecordType_TensorSizes,
    RecordType_VarianceOp,
    RecordType_VarianceMeanOp,
    RecordType_Vector
  };
  return values;
}

inline const char * const *EnumNamesRecordType() {
  static const char * const names[58] = {
    "Base",
    "AtOp",
    "BatchNormOp",
    "BroadcastOp",
    "BroadcastInDim",
    "CastTv",
    "CastVal",
    "CatOp",
    "End",
    "FullOp",
    "IotaOp",
    "IndexSelectOp",
    "TorchGatherOp",
    "TakeAlongAxisOp",
    "Unary_TV",
    "Unary_VAL",
    "Binary_TV",
    "Binary_VAL",
    "Binary_TV_VAL",
    "Binary_VAL_TV",
    "Ternary_TV",
    "Ternary_VAL",
    "Ternary_TV_TV_VAL",
    "Ternary_TV_VAL_TV",
    "Ternary_VAL_TV_TV",
    "Ternary_VAL_VAL_TV",
    "Ternary_TV_VAL_VAL",
    "Ternary_VAL_TV_VAL",
    "Ternary_Alpha_TV",
    "Ternary_Alpha_VAL",
    "Ternary_Alpha_TV_TV_VAL",
    "Ternary_Alpha_TV_VAL_TV",
    "Ternary_Alpha_VAL_TV_TV",
    "Ternary_Alpha_VAL_VAL_TV",
    "Ternary_Alpha_TV_VAL_VAL",
    "Ternary_Alpha_VAL_TV_VAL",
    "OutputTv",
    "OutputVal",
    "PadOp",
    "PermuteOp",
    "RandomOp",
    "ReductionMax",
    "ReductionMin",
    "ReductionProd",
    "ReductionSum",
    "ReshapeOp",
    "Scalar",
    "ShapeOp",
    "SizeOp",
    "SliceOp",
    "SqueezeOp",
    "Start",
    "Tensor",
    "TensorSizes",
    "VarianceOp",
    "VarianceMeanOp",
    "Vector",
    nullptr
  };
  return names;
}

inline const char *EnumNameRecordType(RecordType e) {
  if (::flatbuffers::IsOutRange(e, RecordType_Base, RecordType_Vector)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesRecordType()[index];
}

enum RecordData : uint8_t {
  RecordData_NONE = 0,
  RecordData_At = 1,
  RecordData_BatchNorm = 2,
  RecordData_Broadcast = 3,
  RecordData_BroadcastInDim = 4,
  RecordData_Dimension = 5,
  RecordData_Dtype = 6,
  RecordData_Norm = 7,
  RecordData_Output = 8,
  RecordData_Pad = 9,
  RecordData_Permute = 10,
  RecordData_Slice = 11,
  RecordData_Squeeze = 12,
  RecordData_Reduction = 13,
  RecordData_Reshape = 14,
  RecordData_Scalar = 15,
  RecordData_Size = 16,
  RecordData_Tensor = 17,
  RecordData_TensorCreation = 18,
  RecordData_TensorCreationSymbolic = 19,
  RecordData_Vector = 20,
  RecordData_MIN = RecordData_NONE,
  RecordData_MAX = RecordData_Vector
};

inline const RecordData (&EnumValuesRecordData())[21] {
  static const RecordData values[] = {
    RecordData_NONE,
    RecordData_At,
    RecordData_BatchNorm,
    RecordData_Broadcast,
    RecordData_BroadcastInDim,
    RecordData_Dimension,
    RecordData_Dtype,
    RecordData_Norm,
    RecordData_Output,
    RecordData_Pad,
    RecordData_Permute,
    RecordData_Slice,
    RecordData_Squeeze,
    RecordData_Reduction,
    RecordData_Reshape,
    RecordData_Scalar,
    RecordData_Size,
    RecordData_Tensor,
    RecordData_TensorCreation,
    RecordData_TensorCreationSymbolic,
    RecordData_Vector
  };
  return values;
}

inline const char * const *EnumNamesRecordData() {
  static const char * const names[22] = {
    "NONE",
    "At",
    "BatchNorm",
    "Broadcast",
    "BroadcastInDim",
    "Dimension",
    "Dtype",
    "Norm",
    "Output",
    "Pad",
    "Permute",
    "Slice",
    "Squeeze",
    "Reduction",
    "Reshape",
    "Scalar",
    "Size",
    "Tensor",
    "TensorCreation",
    "TensorCreationSymbolic",
    "Vector",
    nullptr
  };
  return names;
}

inline const char *EnumNameRecordData(RecordData e) {
  if (::flatbuffers::IsOutRange(e, RecordData_NONE, RecordData_Vector)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesRecordData()[index];
}

template<typename T> struct RecordDataTraits {
  static const RecordData enum_value = RecordData_NONE;
};

template<> struct RecordDataTraits<nvfuser::serde::At> {
  static const RecordData enum_value = RecordData_At;
};

template<> struct RecordDataTraits<nvfuser::serde::BatchNorm> {
  static const RecordData enum_value = RecordData_BatchNorm;
};

template<> struct RecordDataTraits<nvfuser::serde::Broadcast> {
  static const RecordData enum_value = RecordData_Broadcast;
};

template<> struct RecordDataTraits<nvfuser::serde::BroadcastInDim> {
  static const RecordData enum_value = RecordData_BroadcastInDim;
};

template<> struct RecordDataTraits<nvfuser::serde::Dimension> {
  static const RecordData enum_value = RecordData_Dimension;
};

template<> struct RecordDataTraits<nvfuser::serde::Dtype> {
  static const RecordData enum_value = RecordData_Dtype;
};

template<> struct RecordDataTraits<nvfuser::serde::Norm> {
  static const RecordData enum_value = RecordData_Norm;
};

template<> struct RecordDataTraits<nvfuser::serde::Output> {
  static const RecordData enum_value = RecordData_Output;
};

template<> struct RecordDataTraits<nvfuser::serde::Pad> {
  static const RecordData enum_value = RecordData_Pad;
};

template<> struct RecordDataTraits<nvfuser::serde::Permute> {
  static const RecordData enum_value = RecordData_Permute;
};

template<> struct RecordDataTraits<nvfuser::serde::Slice> {
  static const RecordData enum_value = RecordData_Slice;
};

template<> struct RecordDataTraits<nvfuser::serde::Squeeze> {
  static const RecordData enum_value = RecordData_Squeeze;
};

template<> struct RecordDataTraits<nvfuser::serde::Reduction> {
  static const RecordData enum_value = RecordData_Reduction;
};

template<> struct RecordDataTraits<nvfuser::serde::Reshape> {
  static const RecordData enum_value = RecordData_Reshape;
};

template<> struct RecordDataTraits<nvfuser::serde::Scalar> {
  static const RecordData enum_value = RecordData_Scalar;
};

template<> struct RecordDataTraits<nvfuser::serde::Size> {
  static const RecordData enum_value = RecordData_Size;
};

template<> struct RecordDataTraits<nvfuser::serde::Tensor> {
  static const RecordData enum_value = RecordData_Tensor;
};

template<> struct RecordDataTraits<nvfuser::serde::TensorCreation> {
  static const RecordData enum_value = RecordData_TensorCreation;
};

template<> struct RecordDataTraits<nvfuser::serde::TensorCreationSymbolic> {
  static const RecordData enum_value = RecordData_TensorCreationSymbolic;
};

template<> struct RecordDataTraits<nvfuser::serde::Vector> {
  static const RecordData enum_value = RecordData_Vector;
};

template<typename T> struct RecordDataUnionTraits {
  static const RecordData enum_value = RecordData_NONE;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::AtT> {
  static const RecordData enum_value = RecordData_At;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::BatchNormT> {
  static const RecordData enum_value = RecordData_BatchNorm;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::BroadcastT> {
  static const RecordData enum_value = RecordData_Broadcast;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::BroadcastInDimT> {
  static const RecordData enum_value = RecordData_BroadcastInDim;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::DimensionT> {
  static const RecordData enum_value = RecordData_Dimension;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::DtypeT> {
  static const RecordData enum_value = RecordData_Dtype;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::NormT> {
  static const RecordData enum_value = RecordData_Norm;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::OutputT> {
  static const RecordData enum_value = RecordData_Output;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::PadT> {
  static const RecordData enum_value = RecordData_Pad;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::PermuteT> {
  static const RecordData enum_value = RecordData_Permute;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::SliceT> {
  static const RecordData enum_value = RecordData_Slice;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::SqueezeT> {
  static const RecordData enum_value = RecordData_Squeeze;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::ReductionT> {
  static const RecordData enum_value = RecordData_Reduction;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::ReshapeT> {
  static const RecordData enum_value = RecordData_Reshape;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::ScalarT> {
  static const RecordData enum_value = RecordData_Scalar;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::SizeT> {
  static const RecordData enum_value = RecordData_Size;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::TensorT> {
  static const RecordData enum_value = RecordData_Tensor;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::TensorCreationT> {
  static const RecordData enum_value = RecordData_TensorCreation;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::TensorCreationSymbolicT> {
  static const RecordData enum_value = RecordData_TensorCreationSymbolic;
};

template<> struct RecordDataUnionTraits<nvfuser::serde::VectorT> {
  static const RecordData enum_value = RecordData_Vector;
};

struct RecordDataUnion {
  RecordData type;
  void *value;

  RecordDataUnion() : type(RecordData_NONE), value(nullptr) {}
  RecordDataUnion(RecordDataUnion&& u) FLATBUFFERS_NOEXCEPT :
    type(RecordData_NONE), value(nullptr)
    { std::swap(type, u.type); std::swap(value, u.value); }
  RecordDataUnion(const RecordDataUnion &);
  RecordDataUnion &operator=(const RecordDataUnion &u)
    { RecordDataUnion t(u); std::swap(type, t.type); std::swap(value, t.value); return *this; }
  RecordDataUnion &operator=(RecordDataUnion &&u) FLATBUFFERS_NOEXCEPT
    { std::swap(type, u.type); std::swap(value, u.value); return *this; }
  ~RecordDataUnion() { Reset(); }

  void Reset();

  template <typename T>
  void Set(T&& val) {
    typedef typename std::remove_reference<T>::type RT;
    Reset();
    type = RecordDataUnionTraits<RT>::enum_value;
    if (type != RecordData_NONE) {
      value = new RT(std::forward<T>(val));
    }
  }

  static void *UnPack(const void *obj, RecordData type, const ::flatbuffers::resolver_function_t *resolver);
  ::flatbuffers::Offset<void> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr) const;

  nvfuser::serde::AtT *AsAt() {
    return type == RecordData_At ?
      reinterpret_cast<nvfuser::serde::AtT *>(value) : nullptr;
  }
  const nvfuser::serde::AtT *AsAt() const {
    return type == RecordData_At ?
      reinterpret_cast<const nvfuser::serde::AtT *>(value) : nullptr;
  }
  nvfuser::serde::BatchNormT *AsBatchNorm() {
    return type == RecordData_BatchNorm ?
      reinterpret_cast<nvfuser::serde::BatchNormT *>(value) : nullptr;
  }
  const nvfuser::serde::BatchNormT *AsBatchNorm() const {
    return type == RecordData_BatchNorm ?
      reinterpret_cast<const nvfuser::serde::BatchNormT *>(value) : nullptr;
  }
  nvfuser::serde::BroadcastT *AsBroadcast() {
    return type == RecordData_Broadcast ?
      reinterpret_cast<nvfuser::serde::BroadcastT *>(value) : nullptr;
  }
  const nvfuser::serde::BroadcastT *AsBroadcast() const {
    return type == RecordData_Broadcast ?
      reinterpret_cast<const nvfuser::serde::BroadcastT *>(value) : nullptr;
  }
  nvfuser::serde::BroadcastInDimT *AsBroadcastInDim() {
    return type == RecordData_BroadcastInDim ?
      reinterpret_cast<nvfuser::serde::BroadcastInDimT *>(value) : nullptr;
  }
  const nvfuser::serde::BroadcastInDimT *AsBroadcastInDim() const {
    return type == RecordData_BroadcastInDim ?
      reinterpret_cast<const nvfuser::serde::BroadcastInDimT *>(value) : nullptr;
  }
  nvfuser::serde::DimensionT *AsDimension() {
    return type == RecordData_Dimension ?
      reinterpret_cast<nvfuser::serde::DimensionT *>(value) : nullptr;
  }
  const nvfuser::serde::DimensionT *AsDimension() const {
    return type == RecordData_Dimension ?
      reinterpret_cast<const nvfuser::serde::DimensionT *>(value) : nullptr;
  }
  nvfuser::serde::DtypeT *AsDtype() {
    return type == RecordData_Dtype ?
      reinterpret_cast<nvfuser::serde::DtypeT *>(value) : nullptr;
  }
  const nvfuser::serde::DtypeT *AsDtype() const {
    return type == RecordData_Dtype ?
      reinterpret_cast<const nvfuser::serde::DtypeT *>(value) : nullptr;
  }
  nvfuser::serde::NormT *AsNorm() {
    return type == RecordData_Norm ?
      reinterpret_cast<nvfuser::serde::NormT *>(value) : nullptr;
  }
  const nvfuser::serde::NormT *AsNorm() const {
    return type == RecordData_Norm ?
      reinterpret_cast<const nvfuser::serde::NormT *>(value) : nullptr;
  }
  nvfuser::serde::OutputT *AsOutput() {
    return type == RecordData_Output ?
      reinterpret_cast<nvfuser::serde::OutputT *>(value) : nullptr;
  }
  const nvfuser::serde::OutputT *AsOutput() const {
    return type == RecordData_Output ?
      reinterpret_cast<const nvfuser::serde::OutputT *>(value) : nullptr;
  }
  nvfuser::serde::PadT *AsPad() {
    return type == RecordData_Pad ?
      reinterpret_cast<nvfuser::serde::PadT *>(value) : nullptr;
  }
  const nvfuser::serde::PadT *AsPad() const {
    return type == RecordData_Pad ?
      reinterpret_cast<const nvfuser::serde::PadT *>(value) : nullptr;
  }
  nvfuser::serde::PermuteT *AsPermute() {
    return type == RecordData_Permute ?
      reinterpret_cast<nvfuser::serde::PermuteT *>(value) : nullptr;
  }
  const nvfuser::serde::PermuteT *AsPermute() const {
    return type == RecordData_Permute ?
      reinterpret_cast<const nvfuser::serde::PermuteT *>(value) : nullptr;
  }
  nvfuser::serde::SliceT *AsSlice() {
    return type == RecordData_Slice ?
      reinterpret_cast<nvfuser::serde::SliceT *>(value) : nullptr;
  }
  const nvfuser::serde::SliceT *AsSlice() const {
    return type == RecordData_Slice ?
      reinterpret_cast<const nvfuser::serde::SliceT *>(value) : nullptr;
  }
  nvfuser::serde::SqueezeT *AsSqueeze() {
    return type == RecordData_Squeeze ?
      reinterpret_cast<nvfuser::serde::SqueezeT *>(value) : nullptr;
  }
  const nvfuser::serde::SqueezeT *AsSqueeze() const {
    return type == RecordData_Squeeze ?
      reinterpret_cast<const nvfuser::serde::SqueezeT *>(value) : nullptr;
  }
  nvfuser::serde::ReductionT *AsReduction() {
    return type == RecordData_Reduction ?
      reinterpret_cast<nvfuser::serde::ReductionT *>(value) : nullptr;
  }
  const nvfuser::serde::ReductionT *AsReduction() const {
    return type == RecordData_Reduction ?
      reinterpret_cast<const nvfuser::serde::ReductionT *>(value) : nullptr;
  }
  nvfuser::serde::ReshapeT *AsReshape() {
    return type == RecordData_Reshape ?
      reinterpret_cast<nvfuser::serde::ReshapeT *>(value) : nullptr;
  }
  const nvfuser::serde::ReshapeT *AsReshape() const {
    return type == RecordData_Reshape ?
      reinterpret_cast<const nvfuser::serde::ReshapeT *>(value) : nullptr;
  }
  nvfuser::serde::ScalarT *AsScalar() {
    return type == RecordData_Scalar ?
      reinterpret_cast<nvfuser::serde::ScalarT *>(value) : nullptr;
  }
  const nvfuser::serde::ScalarT *AsScalar() const {
    return type == RecordData_Scalar ?
      reinterpret_cast<const nvfuser::serde::ScalarT *>(value) : nullptr;
  }
  nvfuser::serde::SizeT *AsSize() {
    return type == RecordData_Size ?
      reinterpret_cast<nvfuser::serde::SizeT *>(value) : nullptr;
  }
  const nvfuser::serde::SizeT *AsSize() const {
    return type == RecordData_Size ?
      reinterpret_cast<const nvfuser::serde::SizeT *>(value) : nullptr;
  }
  nvfuser::serde::TensorT *AsTensor() {
    return type == RecordData_Tensor ?
      reinterpret_cast<nvfuser::serde::TensorT *>(value) : nullptr;
  }
  const nvfuser::serde::TensorT *AsTensor() const {
    return type == RecordData_Tensor ?
      reinterpret_cast<const nvfuser::serde::TensorT *>(value) : nullptr;
  }
  nvfuser::serde::TensorCreationT *AsTensorCreation() {
    return type == RecordData_TensorCreation ?
      reinterpret_cast<nvfuser::serde::TensorCreationT *>(value) : nullptr;
  }
  const nvfuser::serde::TensorCreationT *AsTensorCreation() const {
    return type == RecordData_TensorCreation ?
      reinterpret_cast<const nvfuser::serde::TensorCreationT *>(value) : nullptr;
  }
  nvfuser::serde::TensorCreationSymbolicT *AsTensorCreationSymbolic() {
    return type == RecordData_TensorCreationSymbolic ?
      reinterpret_cast<nvfuser::serde::TensorCreationSymbolicT *>(value) : nullptr;
  }
  const nvfuser::serde::TensorCreationSymbolicT *AsTensorCreationSymbolic() const {
    return type == RecordData_TensorCreationSymbolic ?
      reinterpret_cast<const nvfuser::serde::TensorCreationSymbolicT *>(value) : nullptr;
  }
  nvfuser::serde::VectorT *AsVector() {
    return type == RecordData_Vector ?
      reinterpret_cast<nvfuser::serde::VectorT *>(value) : nullptr;
  }
  const nvfuser::serde::VectorT *AsVector() const {
    return type == RecordData_Vector ?
      reinterpret_cast<const nvfuser::serde::VectorT *>(value) : nullptr;
  }
};

bool VerifyRecordData(::flatbuffers::Verifier &verifier, const void *obj, RecordData type);
bool VerifyRecordDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<uint8_t> *types);

enum PolymorphicValueData : uint8_t {
  PolymorphicValueData_NONE = 0,
  PolymorphicValueData_Scalar = 1,
  PolymorphicValueData_ScalarCpu = 2,
  PolymorphicValueData_TensorArg = 3,
  PolymorphicValueData_MIN = PolymorphicValueData_NONE,
  PolymorphicValueData_MAX = PolymorphicValueData_TensorArg
};

inline const PolymorphicValueData (&EnumValuesPolymorphicValueData())[4] {
  static const PolymorphicValueData values[] = {
    PolymorphicValueData_NONE,
    PolymorphicValueData_Scalar,
    PolymorphicValueData_ScalarCpu,
    PolymorphicValueData_TensorArg
  };
  return values;
}

inline const char * const *EnumNamesPolymorphicValueData() {
  static const char * const names[5] = {
    "NONE",
    "Scalar",
    "ScalarCpu",
    "TensorArg",
    nullptr
  };
  return names;
}

inline const char *EnumNamePolymorphicValueData(PolymorphicValueData e) {
  if (::flatbuffers::IsOutRange(e, PolymorphicValueData_NONE, PolymorphicValueData_TensorArg)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesPolymorphicValueData()[index];
}

template<typename T> struct PolymorphicValueDataTraits {
  static const PolymorphicValueData enum_value = PolymorphicValueData_NONE;
};

template<> struct PolymorphicValueDataTraits<nvfuser::serde::Scalar> {
  static const PolymorphicValueData enum_value = PolymorphicValueData_Scalar;
};

template<> struct PolymorphicValueDataTraits<nvfuser::serde::ScalarCpu> {
  static const PolymorphicValueData enum_value = PolymorphicValueData_ScalarCpu;
};

template<> struct PolymorphicValueDataTraits<nvfuser::serde::TensorArg> {
  static const PolymorphicValueData enum_value = PolymorphicValueData_TensorArg;
};

template<typename T> struct PolymorphicValueDataUnionTraits {
  static const PolymorphicValueData enum_value = PolymorphicValueData_NONE;
};

template<> struct PolymorphicValueDataUnionTraits<nvfuser::serde::ScalarT> {
  static const PolymorphicValueData enum_value = PolymorphicValueData_Scalar;
};

template<> struct PolymorphicValueDataUnionTraits<nvfuser::serde::ScalarCpuT> {
  static const PolymorphicValueData enum_value = PolymorphicValueData_ScalarCpu;
};

template<> struct PolymorphicValueDataUnionTraits<nvfuser::serde::TensorArgT> {
  static const PolymorphicValueData enum_value = PolymorphicValueData_TensorArg;
};

struct PolymorphicValueDataUnion {
  PolymorphicValueData type;
  void *value;

  PolymorphicValueDataUnion() : type(PolymorphicValueData_NONE), value(nullptr) {}
  PolymorphicValueDataUnion(PolymorphicValueDataUnion&& u) FLATBUFFERS_NOEXCEPT :
    type(PolymorphicValueData_NONE), value(nullptr)
    { std::swap(type, u.type); std::swap(value, u.value); }
  PolymorphicValueDataUnion(const PolymorphicValueDataUnion &);
  PolymorphicValueDataUnion &operator=(const PolymorphicValueDataUnion &u)
    { PolymorphicValueDataUnion t(u); std::swap(type, t.type); std::swap(value, t.value); return *this; }
  PolymorphicValueDataUnion &operator=(PolymorphicValueDataUnion &&u) FLATBUFFERS_NOEXCEPT
    { std::swap(type, u.type); std::swap(value, u.value); return *this; }
  ~PolymorphicValueDataUnion() { Reset(); }

  void Reset();

  template <typename T>
  void Set(T&& val) {
    typedef typename std::remove_reference<T>::type RT;
    Reset();
    type = PolymorphicValueDataUnionTraits<RT>::enum_value;
    if (type != PolymorphicValueData_NONE) {
      value = new RT(std::forward<T>(val));
    }
  }

  static void *UnPack(const void *obj, PolymorphicValueData type, const ::flatbuffers::resolver_function_t *resolver);
  ::flatbuffers::Offset<void> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr) const;

  nvfuser::serde::ScalarT *AsScalar() {
    return type == PolymorphicValueData_Scalar ?
      reinterpret_cast<nvfuser::serde::ScalarT *>(value) : nullptr;
  }
  const nvfuser::serde::ScalarT *AsScalar() const {
    return type == PolymorphicValueData_Scalar ?
      reinterpret_cast<const nvfuser::serde::ScalarT *>(value) : nullptr;
  }
  nvfuser::serde::ScalarCpuT *AsScalarCpu() {
    return type == PolymorphicValueData_ScalarCpu ?
      reinterpret_cast<nvfuser::serde::ScalarCpuT *>(value) : nullptr;
  }
  const nvfuser::serde::ScalarCpuT *AsScalarCpu() const {
    return type == PolymorphicValueData_ScalarCpu ?
      reinterpret_cast<const nvfuser::serde::ScalarCpuT *>(value) : nullptr;
  }
  nvfuser::serde::TensorArgT *AsTensorArg() {
    return type == PolymorphicValueData_TensorArg ?
      reinterpret_cast<nvfuser::serde::TensorArgT *>(value) : nullptr;
  }
  const nvfuser::serde::TensorArgT *AsTensorArg() const {
    return type == PolymorphicValueData_TensorArg ?
      reinterpret_cast<const nvfuser::serde::TensorArgT *>(value) : nullptr;
  }
};

bool VerifyPolymorphicValueData(::flatbuffers::Verifier &verifier, const void *obj, PolymorphicValueData type);
bool VerifyPolymorphicValueDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<uint8_t> *types);

FLATBUFFERS_MANUALLY_ALIGNED_STRUCT(4) State FLATBUFFERS_FINAL_CLASS {
 private:
  int32_t index_;
  int32_t type_;

 public:
  State()
      : index_(0),
        type_(0) {
  }
  State(int32_t _index, nvfuser::serde::StateType _type)
      : index_(::flatbuffers::EndianScalar(_index)),
        type_(::flatbuffers::EndianScalar(static_cast<int32_t>(_type))) {
  }
  int32_t index() const {
    return ::flatbuffers::EndianScalar(index_);
  }
  nvfuser::serde::StateType type() const {
    return static_cast<nvfuser::serde::StateType>(::flatbuffers::EndianScalar(type_));
  }
};
FLATBUFFERS_STRUCT_END(State, 8);

FLATBUFFERS_MANUALLY_ALIGNED_STRUCT(8) EncodingEntry FLATBUFFERS_FINAL_CLASS {
 private:
  uint64_t id_;
  uint64_t lru_iter_;

 public:
  EncodingEntry()
      : id_(0),
        lru_iter_(0) {
  }
  EncodingEntry(uint64_t _id, uint64_t _lru_iter)
      : id_(::flatbuffers::EndianScalar(_id)),
        lru_iter_(::flatbuffers::EndianScalar(_lru_iter)) {
  }
  uint64_t id() const {
    return ::flatbuffers::EndianScalar(id_);
  }
  uint64_t lru_iter() const {
    return ::flatbuffers::EndianScalar(lru_iter_);
  }
};
FLATBUFFERS_STRUCT_END(EncodingEntry, 16);

struct ScalarT : public ::flatbuffers::NativeTable {
  typedef Scalar TableType;
  nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double;
  bool has_value = false;
  nvfuser::serde::DataType value_type = nvfuser::serde::DataType_Double;
  bool bool_value = false;
  int64_t long_value = 0;
  double double_value = 0.0;
  double real_value = 0.0;
  double imag_value = 0.0;
};

struct Scalar FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ScalarT NativeTableType;
  typedef ScalarBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DTYPE = 4,
    VT_HAS_VALUE = 6,
    VT_VALUE_TYPE = 8,
    VT_BOOL_VALUE = 10,
    VT_LONG_VALUE = 12,
    VT_DOUBLE_VALUE = 14,
    VT_REAL_VALUE = 16,
    VT_IMAG_VALUE = 18
  };
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool has_value() const {
    return GetField<uint8_t>(VT_HAS_VALUE, 0) != 0;
  }
  nvfuser::serde::DataType value_type() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_VALUE_TYPE, 0));
  }
  bool bool_value() const {
    return GetField<uint8_t>(VT_BOOL_VALUE, 0) != 0;
  }
  int64_t long_value() const {
    return GetField<int64_t>(VT_LONG_VALUE, 0);
  }
  double double_value() const {
    return GetField<double>(VT_DOUBLE_VALUE, 0.0);
  }
  double real_value() const {
    return GetField<double>(VT_REAL_VALUE, 0.0);
  }
  double imag_value() const {
    return GetField<double>(VT_IMAG_VALUE, 0.0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_HAS_VALUE, 1) &&
           VerifyField<int32_t>(verifier, VT_VALUE_TYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_BOOL_VALUE, 1) &&
           VerifyField<int64_t>(verifier, VT_LONG_VALUE, 8) &&
           VerifyField<double>(verifier, VT_DOUBLE_VALUE, 8) &&
           VerifyField<double>(verifier, VT_REAL_VALUE, 8) &&
           VerifyField<double>(verifier, VT_IMAG_VALUE, 8) &&
           verifier.EndTable();
  }
  ScalarT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(ScalarT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Scalar> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ScalarT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct ScalarBuilder {
  typedef Scalar Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(Scalar::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  void add_has_value(bool has_value) {
    fbb_.AddElement<uint8_t>(Scalar::VT_HAS_VALUE, static_cast<uint8_t>(has_value), 0);
  }
  void add_value_type(nvfuser::serde::DataType value_type) {
    fbb_.AddElement<int32_t>(Scalar::VT_VALUE_TYPE, static_cast<int32_t>(value_type), 0);
  }
  void add_bool_value(bool bool_value) {
    fbb_.AddElement<uint8_t>(Scalar::VT_BOOL_VALUE, static_cast<uint8_t>(bool_value), 0);
  }
  void add_long_value(int64_t long_value) {
    fbb_.AddElement<int64_t>(Scalar::VT_LONG_VALUE, long_value, 0);
  }
  void add_double_value(double double_value) {
    fbb_.AddElement<double>(Scalar::VT_DOUBLE_VALUE, double_value, 0.0);
  }
  void add_real_value(double real_value) {
    fbb_.AddElement<double>(Scalar::VT_REAL_VALUE, real_value, 0.0);
  }
  void add_imag_value(double imag_value) {
    fbb_.AddElement<double>(Scalar::VT_IMAG_VALUE, imag_value, 0.0);
  }
  explicit ScalarBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Scalar> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Scalar>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Scalar> CreateScalar(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double,
    bool has_value = false,
    nvfuser::serde::DataType value_type = nvfuser::serde::DataType_Double,
    bool bool_value = false,
    int64_t long_value = 0,
    double double_value = 0.0,
    double real_value = 0.0,
    double imag_value = 0.0) {
  ScalarBuilder builder_(_fbb);
  builder_.add_imag_value(imag_value);
  builder_.add_real_value(real_value);
  builder_.add_double_value(double_value);
  builder_.add_long_value(long_value);
  builder_.add_value_type(value_type);
  builder_.add_dtype(dtype);
  builder_.add_bool_value(bool_value);
  builder_.add_has_value(has_value);
  return builder_.Finish();
}

::flatbuffers::Offset<Scalar> CreateScalar(::flatbuffers::FlatBufferBuilder &_fbb, const ScalarT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct ScalarCpuT : public ::flatbuffers::NativeTable {
  typedef ScalarCpu TableType;
  std::unique_ptr<nvfuser::serde::ScalarT> scalar_value{};
  ScalarCpuT() = default;
  ScalarCpuT(const ScalarCpuT &o);
  ScalarCpuT(ScalarCpuT&&) FLATBUFFERS_NOEXCEPT = default;
  ScalarCpuT &operator=(ScalarCpuT o) FLATBUFFERS_NOEXCEPT;
};

struct ScalarCpu FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ScalarCpuT NativeTableType;
  typedef ScalarCpuBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SCALAR_VALUE = 4
  };
  const nvfuser::serde::Scalar *scalar_value() const {
    return GetPointer<const nvfuser::serde::Scalar *>(VT_SCALAR_VALUE);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SCALAR_VALUE) &&
           verifier.VerifyTable(scalar_value()) &&
           verifier.EndTable();
  }
  ScalarCpuT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(ScalarCpuT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<ScalarCpu> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ScalarCpuT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct ScalarCpuBuilder {
  typedef ScalarCpu Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_scalar_value(::flatbuffers::Offset<nvfuser::serde::Scalar> scalar_value) {
    fbb_.AddOffset(ScalarCpu::VT_SCALAR_VALUE, scalar_value);
  }
  explicit ScalarCpuBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<ScalarCpu> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<ScalarCpu>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<ScalarCpu> CreateScalarCpu(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::Scalar> scalar_value = 0) {
  ScalarCpuBuilder builder_(_fbb);
  builder_.add_scalar_value(scalar_value);
  return builder_.Finish();
}

::flatbuffers::Offset<ScalarCpu> CreateScalarCpu(::flatbuffers::FlatBufferBuilder &_fbb, const ScalarCpuT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct TensorArgT : public ::flatbuffers::NativeTable {
  typedef TensorArg TableType;
  uint64_t ptr = 0;
  std::vector<int64_t> sizes{};
  std::vector<int64_t> strides{};
  nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double;
};

struct TensorArg FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorArgT NativeTableType;
  typedef TensorArgBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_PTR = 4,
    VT_SIZES = 6,
    VT_STRIDES = 8,
    VT_DTYPE = 10
  };
  uint64_t ptr() const {
    return GetField<uint64_t>(VT_PTR, 0);
  }
  const ::flatbuffers::Vector<int64_t> *sizes() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SIZES);
  }
  const ::flatbuffers::Vector<int64_t> *strides() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_STRIDES);
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_PTR, 8) &&
           VerifyOffset(verifier, VT_SIZES) &&
           verifier.VerifyVector(sizes()) &&
           VerifyOffset(verifier, VT_STRIDES) &&
           verifier.VerifyVector(strides()) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
  TensorArgT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(TensorArgT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<TensorArg> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TensorArgT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct TensorArgBuilder {
  typedef TensorArg Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_ptr(uint64_t ptr) {
    fbb_.AddElement<uint64_t>(TensorArg::VT_PTR, ptr, 0);
  }
  void add_sizes(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes) {
    fbb_.AddOffset(TensorArg::VT_SIZES, sizes);
  }
  void add_strides(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides) {
    fbb_.AddOffset(TensorArg::VT_STRIDES, strides);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(TensorArg::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit TensorArgBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TensorArg> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TensorArg>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TensorArg> CreateTensorArg(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t ptr = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides = 0,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double) {
  TensorArgBuilder builder_(_fbb);
  builder_.add_ptr(ptr);
  builder_.add_dtype(dtype);
  builder_.add_strides(strides);
  builder_.add_sizes(sizes);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TensorArg> CreateTensorArgDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t ptr = 0,
    const std::vector<int64_t> *sizes = nullptr,
    const std::vector<int64_t> *strides = nullptr,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double) {
  auto sizes__ = sizes ? _fbb.CreateVector<int64_t>(*sizes) : 0;
  auto strides__ = strides ? _fbb.CreateVector<int64_t>(*strides) : 0;
  return nvfuser::serde::CreateTensorArg(
      _fbb,
      ptr,
      sizes__,
      strides__,
      dtype);
}

::flatbuffers::Offset<TensorArg> CreateTensorArg(::flatbuffers::FlatBufferBuilder &_fbb, const TensorArgT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct PolymorphicValueT : public ::flatbuffers::NativeTable {
  typedef PolymorphicValue TableType;
  nvfuser::serde::PolymorphicValueDataUnion data{};
};

struct PolymorphicValue FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef PolymorphicValueT NativeTableType;
  typedef PolymorphicValueBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DATA_TYPE = 4,
    VT_DATA = 6
  };
  nvfuser::serde::PolymorphicValueData data_type() const {
    return static_cast<nvfuser::serde::PolymorphicValueData>(GetField<uint8_t>(VT_DATA_TYPE, 0));
  }
  const void *data() const {
    return GetPointer<const void *>(VT_DATA);
  }
  template<typename T> const T *data_as() const;
  const nvfuser::serde::Scalar *data_as_Scalar() const {
    return data_type() == nvfuser::serde::PolymorphicValueData_Scalar ? static_cast<const nvfuser::serde::Scalar *>(data()) : nullptr;
  }
  const nvfuser::serde::ScalarCpu *data_as_ScalarCpu() const {
    return data_type() == nvfuser::serde::PolymorphicValueData_ScalarCpu ? static_cast<const nvfuser::serde::ScalarCpu *>(data()) : nullptr;
  }
  const nvfuser::serde::TensorArg *data_as_TensorArg() const {
    return data_type() == nvfuser::serde::PolymorphicValueData_TensorArg ? static_cast<const nvfuser::serde::TensorArg *>(data()) : nullptr;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_DATA_TYPE, 1) &&
           VerifyOffset(verifier, VT_DATA) &&
           VerifyPolymorphicValueData(verifier, data(), data_type()) &&
           verifier.EndTable();
  }
  PolymorphicValueT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(PolymorphicValueT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<PolymorphicValue> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const PolymorphicValueT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

template<> inline const nvfuser::serde::Scalar *PolymorphicValue::data_as<nvfuser::serde::Scalar>() const {
  return data_as_Scalar();
}

template<> inline const nvfuser::serde::ScalarCpu *PolymorphicValue::data_as<nvfuser::serde::ScalarCpu>() const {
  return data_as_ScalarCpu();
}

template<> inline const nvfuser::serde::TensorArg *PolymorphicValue::data_as<nvfuser::serde::TensorArg>() const {
  return data_as_TensorArg();
}

struct PolymorphicValueBuilder {
  typedef PolymorphicValue Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_data_type(nvfuser::serde::PolymorphicValueData data_type) {
    fbb_.AddElement<uint8_t>(PolymorphicValue::VT_DATA_TYPE, static_cast<uint8_t>(data_type), 0);
  }
  void add_data(::flatbuffers::Offset<void> data) {
    fbb_.AddOffset(PolymorphicValue::VT_DATA, data);
  }
  explicit PolymorphicValueBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<PolymorphicValue> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<PolymorphicValue>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<PolymorphicValue> CreatePolymorphicValue(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::PolymorphicValueData data_type = nvfuser::serde::PolymorphicValueData_NONE,
    ::flatbuffers::Offset<void> data = 0) {
  PolymorphicValueBuilder builder_(_fbb);
  builder_.add_data(data);
  builder_.add_data_type(data_type);
  return builder_.Finish();
}

::flatbuffers::Offset<PolymorphicValue> CreatePolymorphicValue(::flatbuffers::FlatBufferBuilder &_fbb, const PolymorphicValueT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct KernelArgumentHolderT : public ::flatbuffers::NativeTable {
  typedef KernelArgumentHolder TableType;
  std::vector<std::unique_ptr<nvfuser::serde::PolymorphicValueT>> arguments{};
  int8_t device_index = 0;
  uint64_t cache_id = 0;
  KernelArgumentHolderT() = default;
  KernelArgumentHolderT(const KernelArgumentHolderT &o);
  KernelArgumentHolderT(KernelArgumentHolderT&&) FLATBUFFERS_NOEXCEPT = default;
  KernelArgumentHolderT &operator=(KernelArgumentHolderT o) FLATBUFFERS_NOEXCEPT;
};

struct KernelArgumentHolder FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef KernelArgumentHolderT NativeTableType;
  typedef KernelArgumentHolderBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ARGUMENTS = 4,
    VT_DEVICE_INDEX = 6,
    VT_CACHE_ID = 8
  };
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>> *arguments() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>> *>(VT_ARGUMENTS);
  }
  int8_t device_index() const {
    return GetField<int8_t>(VT_DEVICE_INDEX, 0);
  }
  uint64_t cache_id() const {
    return GetField<uint64_t>(VT_CACHE_ID, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ARGUMENTS) &&
           verifier.VerifyVector(arguments()) &&
           verifier.VerifyVectorOfTables(arguments()) &&
           VerifyField<int8_t>(verifier, VT_DEVICE_INDEX, 1) &&
           VerifyField<uint64_t>(verifier, VT_CACHE_ID, 8) &&
           verifier.EndTable();
  }
  KernelArgumentHolderT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(KernelArgumentHolderT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<KernelArgumentHolder> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const KernelArgumentHolderT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct KernelArgumentHolderBuilder {
  typedef KernelArgumentHolder Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_arguments(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>>> arguments) {
    fbb_.AddOffset(KernelArgumentHolder::VT_ARGUMENTS, arguments);
  }
  void add_device_index(int8_t device_index) {
    fbb_.AddElement<int8_t>(KernelArgumentHolder::VT_DEVICE_INDEX, device_index, 0);
  }
  void add_cache_id(uint64_t cache_id) {
    fbb_.AddElement<uint64_t>(KernelArgumentHolder::VT_CACHE_ID, cache_id, 0);
  }
  explicit KernelArgumentHolderBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<KernelArgumentHolder> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<KernelArgumentHolder>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<KernelArgumentHolder> CreateKernelArgumentHolder(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>>> arguments = 0,
    int8_t device_index = 0,
    uint64_t cache_id = 0) {
  KernelArgumentHolderBuilder builder_(_fbb);
  builder_.add_cache_id(cache_id);
  builder_.add_arguments(arguments);
  builder_.add_device_index(device_index);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<KernelArgumentHolder> CreateKernelArgumentHolderDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>> *arguments = nullptr,
    int8_t device_index = 0,
    uint64_t cache_id = 0) {
  auto arguments__ = arguments ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>>(*arguments) : 0;
  return nvfuser::serde::CreateKernelArgumentHolder(
      _fbb,
      arguments__,
      device_index,
      cache_id);
}

::flatbuffers::Offset<KernelArgumentHolder> CreateKernelArgumentHolder(::flatbuffers::FlatBufferBuilder &_fbb, const KernelArgumentHolderT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct TensorShapeT : public ::flatbuffers::NativeTable {
  typedef TensorShape TableType;
  std::vector<int64_t> shape{};
};

struct TensorShape FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorShapeT NativeTableType;
  typedef TensorShapeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SHAPE = 4
  };
  const ::flatbuffers::Vector<int64_t> *shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SHAPE);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SHAPE) &&
           verifier.VerifyVector(shape()) &&
           verifier.EndTable();
  }
  TensorShapeT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(TensorShapeT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<TensorShape> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TensorShapeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct TensorShapeBuilder {
  typedef TensorShape Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape) {
    fbb_.AddOffset(TensorShape::VT_SHAPE, shape);
  }
  explicit TensorShapeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TensorShape> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TensorShape>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TensorShape> CreateTensorShape(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape = 0) {
  TensorShapeBuilder builder_(_fbb);
  builder_.add_shape(shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TensorShape> CreateTensorShapeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *shape = nullptr) {
  auto shape__ = shape ? _fbb.CreateVector<int64_t>(*shape) : 0;
  return nvfuser::serde::CreateTensorShape(
      _fbb,
      shape__);
}

::flatbuffers::Offset<TensorShape> CreateTensorShape(::flatbuffers::FlatBufferBuilder &_fbb, const TensorShapeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct LaunchParamsT : public ::flatbuffers::NativeTable {
  typedef LaunchParams TableType;
  int64_t gdimx = 0;
  int64_t gdimy = 0;
  int64_t gdimz = 0;
  int64_t bdimx = 0;
  int64_t bdimy = 0;
  int64_t bdimz = 0;
  int64_t smem = 0;
  std::vector<std::unique_ptr<nvfuser::serde::TensorShapeT>> output_sizes{};
  LaunchParamsT() = default;
  LaunchParamsT(const LaunchParamsT &o);
  LaunchParamsT(LaunchParamsT&&) FLATBUFFERS_NOEXCEPT = default;
  LaunchParamsT &operator=(LaunchParamsT o) FLATBUFFERS_NOEXCEPT;
};

struct LaunchParams FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef LaunchParamsT NativeTableType;
  typedef LaunchParamsBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_GDIMX = 4,
    VT_GDIMY = 6,
    VT_GDIMZ = 8,
    VT_BDIMX = 10,
    VT_BDIMY = 12,
    VT_BDIMZ = 14,
    VT_SMEM = 16,
    VT_OUTPUT_SIZES = 18
  };
  int64_t gdimx() const {
    return GetField<int64_t>(VT_GDIMX, 0);
  }
  int64_t gdimy() const {
    return GetField<int64_t>(VT_GDIMY, 0);
  }
  int64_t gdimz() const {
    return GetField<int64_t>(VT_GDIMZ, 0);
  }
  int64_t bdimx() const {
    return GetField<int64_t>(VT_BDIMX, 0);
  }
  int64_t bdimy() const {
    return GetField<int64_t>(VT_BDIMY, 0);
  }
  int64_t bdimz() const {
    return GetField<int64_t>(VT_BDIMZ, 0);
  }
  int64_t smem() const {
    return GetField<int64_t>(VT_SMEM, 0);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>> *output_sizes() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>> *>(VT_OUTPUT_SIZES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_GDIMX, 8) &&
           VerifyField<int64_t>(verifier, VT_GDIMY, 8) &&
           VerifyField<int64_t>(verifier, VT_GDIMZ, 8) &&
           VerifyField<int64_t>(verifier, VT_BDIMX, 8) &&
           VerifyField<int64_t>(verifier, VT_BDIMY, 8) &&
           VerifyField<int64_t>(verifier, VT_BDIMZ, 8) &&
           VerifyField<int64_t>(verifier, VT_SMEM, 8) &&
           VerifyOffset(verifier, VT_OUTPUT_SIZES) &&
           verifier.VerifyVector(output_sizes()) &&
           verifier.VerifyVectorOfTables(output_sizes()) &&
           verifier.EndTable();
  }
  LaunchParamsT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(LaunchParamsT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<LaunchParams> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const LaunchParamsT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct LaunchParamsBuilder {
  typedef LaunchParams Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_gdimx(int64_t gdimx) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_GDIMX, gdimx, 0);
  }
  void add_gdimy(int64_t gdimy) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_GDIMY, gdimy, 0);
  }
  void add_gdimz(int64_t gdimz) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_GDIMZ, gdimz, 0);
  }
  void add_bdimx(int64_t bdimx) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_BDIMX, bdimx, 0);
  }
  void add_bdimy(int64_t bdimy) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_BDIMY, bdimy, 0);
  }
  void add_bdimz(int64_t bdimz) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_BDIMZ, bdimz, 0);
  }
  void add_smem(int64_t smem) {
    fbb_.AddElement<int64_t>(LaunchParams::VT_SMEM, smem, 0);
  }
  void add_output_sizes(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>>> output_sizes) {
    fbb_.AddOffset(LaunchParams::VT_OUTPUT_SIZES, output_sizes);
  }
  explicit LaunchParamsBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<LaunchParams> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<LaunchParams>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<LaunchParams> CreateLaunchParams(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t gdimx = 0,
    int64_t gdimy = 0,
    int64_t gdimz = 0,
    int64_t bdimx = 0,
    int64_t bdimy = 0,
    int64_t bdimz = 0,
    int64_t smem = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>>> output_sizes = 0) {
  LaunchParamsBuilder builder_(_fbb);
  builder_.add_smem(smem);
  builder_.add_bdimz(bdimz);
  builder_.add_bdimy(bdimy);
  builder_.add_bdimx(bdimx);
  builder_.add_gdimz(gdimz);
  builder_.add_gdimy(gdimy);
  builder_.add_gdimx(gdimx);
  builder_.add_output_sizes(output_sizes);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<LaunchParams> CreateLaunchParamsDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t gdimx = 0,
    int64_t gdimy = 0,
    int64_t gdimz = 0,
    int64_t bdimx = 0,
    int64_t bdimy = 0,
    int64_t bdimz = 0,
    int64_t smem = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::TensorShape>> *output_sizes = nullptr) {
  auto output_sizes__ = output_sizes ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::TensorShape>>(*output_sizes) : 0;
  return nvfuser::serde::CreateLaunchParams(
      _fbb,
      gdimx,
      gdimy,
      gdimz,
      bdimx,
      bdimy,
      bdimz,
      smem,
      output_sizes__);
}

::flatbuffers::Offset<LaunchParams> CreateLaunchParams(::flatbuffers::FlatBufferBuilder &_fbb, const LaunchParamsT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct GlobalBufferInfoT : public ::flatbuffers::NativeTable {
  typedef GlobalBufferInfo TableType;
  int64_t tv = -1LL;
  std::vector<int64_t> sizes{};
  std::vector<int64_t> strides{};
  nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double;
  bool zero_init = false;
  bool is_profile_buffer = false;
  bool is_fusion_output = false;
};

struct GlobalBufferInfo FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef GlobalBufferInfoT NativeTableType;
  typedef GlobalBufferInfoBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TV = 4,
    VT_SIZES = 6,
    VT_STRIDES = 8,
    VT_DTYPE = 10,
    VT_ZERO_INIT = 12,
    VT_IS_PROFILE_BUFFER = 14,
    VT_IS_FUSION_OUTPUT = 16
  };
  int64_t tv() const {
    return GetField<int64_t>(VT_TV, -1LL);
  }
  const ::flatbuffers::Vector<int64_t> *sizes() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SIZES);
  }
  const ::flatbuffers::Vector<int64_t> *strides() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_STRIDES);
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool zero_init() const {
    return GetField<uint8_t>(VT_ZERO_INIT, 0) != 0;
  }
  bool is_profile_buffer() const {
    return GetField<uint8_t>(VT_IS_PROFILE_BUFFER, 0) != 0;
  }
  bool is_fusion_output() const {
    return GetField<uint8_t>(VT_IS_FUSION_OUTPUT, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_TV, 8) &&
           VerifyOffset(verifier, VT_SIZES) &&
           verifier.VerifyVector(sizes()) &&
           VerifyOffset(verifier, VT_STRIDES) &&
           verifier.VerifyVector(strides()) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_ZERO_INIT, 1) &&
           VerifyField<uint8_t>(verifier, VT_IS_PROFILE_BUFFER, 1) &&
           VerifyField<uint8_t>(verifier, VT_IS_FUSION_OUTPUT, 1) &&
           verifier.EndTable();
  }
  GlobalBufferInfoT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(GlobalBufferInfoT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<GlobalBufferInfo> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const GlobalBufferInfoT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct GlobalBufferInfoBuilder {
  typedef GlobalBufferInfo Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_tv(int64_t tv) {
    fbb_.AddElement<int64_t>(GlobalBufferInfo::VT_TV, tv, -1LL);
  }
  void add_sizes(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes) {
    fbb_.AddOffset(GlobalBufferInfo::VT_SIZES, sizes);
  }
  void add_strides(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides) {
    fbb_.AddOffset(GlobalBufferInfo::VT_STRIDES, strides);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(GlobalBufferInfo::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  void add_zero_init(bool zero_init) {
    fbb_.AddElement<uint8_t>(GlobalBufferInfo::VT_ZERO_INIT, static_cast<uint8_t>(zero_init), 0);
  }
  void add_is_profile_buffer(bool is_profile_buffer) {
    fbb_.AddElement<uint8_t>(GlobalBufferInfo::VT_IS_PROFILE_BUFFER, static_cast<uint8_t>(is_profile_buffer), 0);
  }
  void add_is_fusion_output(bool is_fusion_output) {
    fbb_.AddElement<uint8_t>(GlobalBufferInfo::VT_IS_FUSION_OUTPUT, static_cast<uint8_t>(is_fusion_output), 0);
  }
  explicit GlobalBufferInfoBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<GlobalBufferInfo> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<GlobalBufferInfo>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<GlobalBufferInfo> CreateGlobalBufferInfo(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t tv = -1LL,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides = 0,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double,
    bool zero_init = false,
    bool is_profile_buffer = false,
    bool is_fusion_output = false) {
  GlobalBufferInfoBuilder builder_(_fbb);
  builder_.add_tv(tv);
  builder_.add_dtype(dtype);
  builder_.add_strides(strides);
  builder_.add_sizes(sizes);
  builder_.add_is_fusion_output(is_fusion_output);
  builder_.add_is_profile_buffer(is_profile_buffer);
  builder_.add_zero_init(zero_init);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<GlobalBufferInfo> CreateGlobalBufferInfoDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t tv = -1LL,
    const std::vector<int64_t> *sizes = nullptr,
    const std::vector<int64_t> *strides = nullptr,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double,
    bool zero_init = false,
    bool is_profile_buffer = false,
    bool is_fusion_output = false) {
  auto sizes__ = sizes ? _fbb.CreateVector<int64_t>(*sizes) : 0;
  auto strides__ = strides ? _fbb.CreateVector<int64_t>(*strides) : 0;
  return nvfuser::serde::CreateGlobalBufferInfo(
      _fbb,
      tv,
      sizes__,
      strides__,
      dtype,
      zero_init,
      is_profile_buffer,
      is_fusion_output);
}

::flatbuffers::Offset<GlobalBufferInfo> CreateGlobalBufferInfo(::flatbuffers::FlatBufferBuilder &_fbb, const GlobalBufferInfoT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct ExecutorEntryT : public ::flatbuffers::NativeTable {
  typedef ExecutorEntry TableType;
  bool init = false;
  std::unique_ptr<nvfuser::serde::LaunchParamsT> launch_params{};
  std::vector<int32_t> output_aliases{};
  std::vector<int32_t> input_aliases{};
  std::vector<std::unique_ptr<nvfuser::serde::GlobalBufferInfoT>> outputs{};
  std::vector<std::unique_ptr<nvfuser::serde::GlobalBufferInfoT>> intermediates{};
  ExecutorEntryT() = default;
  ExecutorEntryT(const ExecutorEntryT &o);
  ExecutorEntryT(ExecutorEntryT&&) FLATBUFFERS_NOEXCEPT = default;
  ExecutorEntryT &operator=(ExecutorEntryT o) FLATBUFFERS_NOEXCEPT;
};

struct ExecutorEntry FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ExecutorEntryT NativeTableType;
  typedef ExecutorEntryBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INIT = 4,
    VT_LAUNCH_PARAMS = 6,
    VT_OUTPUT_ALIASES = 8,
    VT_INPUT_ALIASES = 10,
    VT_OUTPUTS = 12,
    VT_INTERMEDIATES = 14
  };
  bool init() const {
    return GetField<uint8_t>(VT_INIT, 0) != 0;
  }
  const nvfuser::serde::LaunchParams *launch_params() const {
    return GetPointer<const nvfuser::serde::LaunchParams *>(VT_LAUNCH_PARAMS);
  }
  const ::flatbuffers::Vector<int32_t> *output_aliases() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_OUTPUT_ALIASES);
  }
  const ::flatbuffers::Vector<int32_t> *input_aliases() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_INPUT_ALIASES);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *outputs() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *>(VT_OUTPUTS);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *intermediates() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *>(VT_INTERMEDIATES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_INIT, 1) &&
           VerifyOffset(verifier, VT_LAUNCH_PARAMS) &&
           verifier.VerifyTable(launch_params()) &&
           VerifyOffset(verifier, VT_OUTPUT_ALIASES) &&
           verifier.VerifyVector(output_aliases()) &&
           VerifyOffset(verifier, VT_INPUT_ALIASES) &&
           verifier.VerifyVector(input_aliases()) &&
           VerifyOffset(verifier, VT_OUTPUTS) &&
           verifier.VerifyVector(outputs()) &&
           verifier.VerifyVectorOfTables(outputs()) &&
           VerifyOffset(verifier, VT_INTERMEDIATES) &&
           verifier.VerifyVector(intermediates()) &&
           verifier.VerifyVectorOfTables(intermediates()) &&
           verifier.EndTable();
  }
  ExecutorEntryT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(ExecutorEntryT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<ExecutorEntry> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ExecutorEntryT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct ExecutorEntryBuilder {
  typedef ExecutorEntry Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_init(bool init) {
    fbb_.AddElement<uint8_t>(ExecutorEntry::VT_INIT, static_cast<uint8_t>(init), 0);
  }
  void add_launch_params(::flatbuffers::Offset<nvfuser::serde::LaunchParams> launch_params) {
    fbb_.AddOffset(ExecutorEntry::VT_LAUNCH_PARAMS, launch_params);
  }
  void add_output_aliases(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> output_aliases) {
    fbb_.AddOffset(ExecutorEntry::VT_OUTPUT_ALIASES, output_aliases);
  }
  void add_input_aliases(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> input_aliases) {
    fbb_.AddOffset(ExecutorEntry::VT_INPUT_ALIASES, input_aliases);
  }
  void add_outputs(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> outputs) {
    fbb_.AddOffset(ExecutorEntry::VT_OUTPUTS, outputs);
  }
  void add_intermediates(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> intermediates) {
    fbb_.AddOffset(ExecutorEntry::VT_INTERMEDIATES, intermediates);
  }
  explicit ExecutorEntryBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<ExecutorEntry> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<ExecutorEntry>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<ExecutorEntry> CreateExecutorEntry(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool init = false,
    ::flatbuffers::Offset<nvfuser::serde::LaunchParams> launch_params = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> output_aliases = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> input_aliases = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> outputs = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>> intermediates = 0) {
  ExecutorEntryBuilder builder_(_fbb);
  builder_.add_intermediates(intermediates);
  builder_.add_outputs(outputs);
  builder_.add_input_aliases(input_aliases);
  builder_.add_output_aliases(output_aliases);
  builder_.add_launch_params(launch_params);
  builder_.add_init(init);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<ExecutorEntry> CreateExecutorEntryDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool init = false,
    ::flatbuffers::Offset<nvfuser::serde::LaunchParams> launch_params = 0,
    const std::vector<int32_t> *output_aliases = nullptr,
    const std::vector<int32_t> *input_aliases = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *outputs = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> *intermediates = nullptr) {
  auto output_aliases__ = output_aliases ? _fbb.CreateVector<int32_t>(*output_aliases) : 0;
  auto input_aliases__ = input_aliases ? _fbb.CreateVector<int32_t>(*input_aliases) : 0;
  auto outputs__ = outputs ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>(*outputs) : 0;
  auto intermediates__ = intermediates ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>>(*intermediates) : 0;
  return nvfuser::serde::CreateExecutorEntry(
      _fbb,
      init,
      launch_params,
      output_aliases__,
      input_aliases__,
      outputs__,
      intermediates__);
}

::flatbuffers::Offset<ExecutorEntry> CreateExecutorEntry(::flatbuffers::FlatBufferBuilder &_fbb, const ExecutorEntryT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct AtT : public ::flatbuffers::NativeTable {
  typedef At TableType;
  int64_t index = 0;
};

struct At FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef AtT NativeTableType;
  typedef AtBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INDEX = 4
  };
  int64_t index() const {
    return GetField<int64_t>(VT_INDEX, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_INDEX, 8) &&
           verifier.EndTable();
  }
  AtT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(AtT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<At> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const AtT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct AtBuilder {
  typedef At Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_index(int64_t index) {
    fbb_.AddElement<int64_t>(At::VT_INDEX, index, 0);
  }
  explicit AtBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<At> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<At>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<At> CreateAt(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t index = 0) {
  AtBuilder builder_(_fbb);
  builder_.add_index(index);
  return builder_.Finish();
}

::flatbuffers::Offset<At> CreateAt(::flatbuffers::FlatBufferBuilder &_fbb, const AtT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct BatchNormT : public ::flatbuffers::NativeTable {
  typedef BatchNorm TableType;
  bool training = false;
  bool channels_last = false;
};

struct BatchNorm FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef BatchNormT NativeTableType;
  typedef BatchNormBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TRAINING = 4,
    VT_CHANNELS_LAST = 6
  };
  bool training() const {
    return GetField<uint8_t>(VT_TRAINING, 0) != 0;
  }
  bool channels_last() const {
    return GetField<uint8_t>(VT_CHANNELS_LAST, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint8_t>(verifier, VT_TRAINING, 1) &&
           VerifyField<uint8_t>(verifier, VT_CHANNELS_LAST, 1) &&
           verifier.EndTable();
  }
  BatchNormT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(BatchNormT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<BatchNorm> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const BatchNormT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct BatchNormBuilder {
  typedef BatchNorm Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_training(bool training) {
    fbb_.AddElement<uint8_t>(BatchNorm::VT_TRAINING, static_cast<uint8_t>(training), 0);
  }
  void add_channels_last(bool channels_last) {
    fbb_.AddElement<uint8_t>(BatchNorm::VT_CHANNELS_LAST, static_cast<uint8_t>(channels_last), 0);
  }
  explicit BatchNormBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<BatchNorm> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<BatchNorm>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<BatchNorm> CreateBatchNorm(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    bool training = false,
    bool channels_last = false) {
  BatchNormBuilder builder_(_fbb);
  builder_.add_channels_last(channels_last);
  builder_.add_training(training);
  return builder_.Finish();
}

::flatbuffers::Offset<BatchNorm> CreateBatchNorm(::flatbuffers::FlatBufferBuilder &_fbb, const BatchNormT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct BroadcastT : public ::flatbuffers::NativeTable {
  typedef Broadcast TableType;
  std::vector<bool> broadcast_dims{};
};

struct Broadcast FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef BroadcastT NativeTableType;
  typedef BroadcastBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_BROADCAST_DIMS = 4
  };
  const ::flatbuffers::Vector<uint8_t> *broadcast_dims() const {
    return GetPointer<const ::flatbuffers::Vector<uint8_t> *>(VT_BROADCAST_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_BROADCAST_DIMS) &&
           verifier.VerifyVector(broadcast_dims()) &&
           verifier.EndTable();
  }
  BroadcastT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(BroadcastT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Broadcast> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const BroadcastT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct BroadcastBuilder {
  typedef Broadcast Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_broadcast_dims(::flatbuffers::Offset<::flatbuffers::Vector<uint8_t>> broadcast_dims) {
    fbb_.AddOffset(Broadcast::VT_BROADCAST_DIMS, broadcast_dims);
  }
  explicit BroadcastBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Broadcast> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Broadcast>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Broadcast> CreateBroadcast(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint8_t>> broadcast_dims = 0) {
  BroadcastBuilder builder_(_fbb);
  builder_.add_broadcast_dims(broadcast_dims);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Broadcast> CreateBroadcastDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<uint8_t> *broadcast_dims = nullptr) {
  auto broadcast_dims__ = broadcast_dims ? _fbb.CreateVector<uint8_t>(*broadcast_dims) : 0;
  return nvfuser::serde::CreateBroadcast(
      _fbb,
      broadcast_dims__);
}

::flatbuffers::Offset<Broadcast> CreateBroadcast(::flatbuffers::FlatBufferBuilder &_fbb, const BroadcastT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct BroadcastInDimT : public ::flatbuffers::NativeTable {
  typedef BroadcastInDim TableType;
  uint64_t output_size = 0;
  std::vector<int64_t> broadcast_dims{};
};

struct BroadcastInDim FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef BroadcastInDimT NativeTableType;
  typedef BroadcastInDimBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OUTPUT_SIZE = 4,
    VT_BROADCAST_DIMS = 6
  };
  uint64_t output_size() const {
    return GetField<uint64_t>(VT_OUTPUT_SIZE, 0);
  }
  const ::flatbuffers::Vector<int64_t> *broadcast_dims() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_BROADCAST_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_OUTPUT_SIZE, 8) &&
           VerifyOffset(verifier, VT_BROADCAST_DIMS) &&
           verifier.VerifyVector(broadcast_dims()) &&
           verifier.EndTable();
  }
  BroadcastInDimT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(BroadcastInDimT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<BroadcastInDim> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const BroadcastInDimT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct BroadcastInDimBuilder {
  typedef BroadcastInDim Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_output_size(uint64_t output_size) {
    fbb_.AddElement<uint64_t>(BroadcastInDim::VT_OUTPUT_SIZE, output_size, 0);
  }
  void add_broadcast_dims(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> broadcast_dims) {
    fbb_.AddOffset(BroadcastInDim::VT_BROADCAST_DIMS, broadcast_dims);
  }
  explicit BroadcastInDimBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<BroadcastInDim> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<BroadcastInDim>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<BroadcastInDim> CreateBroadcastInDim(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t output_size = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> broadcast_dims = 0) {
  BroadcastInDimBuilder builder_(_fbb);
  builder_.add_output_size(output_size);
  builder_.add_broadcast_dims(broadcast_dims);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<BroadcastInDim> CreateBroadcastInDimDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t output_size = 0,
    const std::vector<int64_t> *broadcast_dims = nullptr) {
  auto broadcast_dims__ = broadcast_dims ? _fbb.CreateVector<int64_t>(*broadcast_dims) : 0;
  return nvfuser::serde::CreateBroadcastInDim(
      _fbb,
      output_size,
      broadcast_dims__);
}

::flatbuffers::Offset<BroadcastInDim> CreateBroadcastInDim(::flatbuffers::FlatBufferBuilder &_fbb, const BroadcastInDimT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct DtypeT : public ::flatbuffers::NativeTable {
  typedef Dtype TableType;
  nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double;
};

struct Dtype FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef DtypeT NativeTableType;
  typedef DtypeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DTYPE = 4
  };
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
  DtypeT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(DtypeT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Dtype> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const DtypeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct DtypeBuilder {
  typedef Dtype Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(Dtype::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit DtypeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Dtype> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Dtype>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Dtype> CreateDtype(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double) {
  DtypeBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  return builder_.Finish();
}

::flatbuffers::Offset<Dtype> CreateDtype(::flatbuffers::FlatBufferBuilder &_fbb, const DtypeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct DimensionT : public ::flatbuffers::NativeTable {
  typedef Dimension TableType;
  int64_t dim = 0;
};

struct Dimension FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef DimensionT NativeTableType;
  typedef DimensionBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIM = 4
  };
  int64_t dim() const {
    return GetField<int64_t>(VT_DIM, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DIM, 8) &&
           verifier.EndTable();
  }
  DimensionT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(DimensionT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Dimension> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const DimensionT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct DimensionBuilder {
  typedef Dimension Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dim(int64_t dim) {
    fbb_.AddElement<int64_t>(Dimension::VT_DIM, dim, 0);
  }
  explicit DimensionBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Dimension> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Dimension>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Dimension> CreateDimension(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t dim = 0) {
  DimensionBuilder builder_(_fbb);
  builder_.add_dim(dim);
  return builder_.Finish();
}

::flatbuffers::Offset<Dimension> CreateDimension(::flatbuffers::FlatBufferBuilder &_fbb, const DimensionT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct NormT : public ::flatbuffers::NativeTable {
  typedef Norm TableType;
  std::vector<int32_t> axes{};
  int64_t correction = 0;
  bool keep_dim = false;
};

struct Norm FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef NormT NativeTableType;
  typedef NormBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_AXES = 4,
    VT_CORRECTION = 6,
    VT_KEEP_DIM = 8
  };
  const ::flatbuffers::Vector<int32_t> *axes() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_AXES);
  }
  int64_t correction() const {
    return GetField<int64_t>(VT_CORRECTION, 0);
  }
  bool keep_dim() const {
    return GetField<uint8_t>(VT_KEEP_DIM, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_AXES) &&
           verifier.VerifyVector(axes()) &&
           VerifyField<int64_t>(verifier, VT_CORRECTION, 8) &&
           VerifyField<uint8_t>(verifier, VT_KEEP_DIM, 1) &&
           verifier.EndTable();
  }
  NormT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(NormT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Norm> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const NormT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct NormBuilder {
  typedef Norm Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_axes(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> axes) {
    fbb_.AddOffset(Norm::VT_AXES, axes);
  }
  void add_correction(int64_t correction) {
    fbb_.AddElement<int64_t>(Norm::VT_CORRECTION, correction, 0);
  }
  void add_keep_dim(bool keep_dim) {
    fbb_.AddElement<uint8_t>(Norm::VT_KEEP_DIM, static_cast<uint8_t>(keep_dim), 0);
  }
  explicit NormBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Norm> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Norm>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Norm> CreateNorm(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> axes = 0,
    int64_t correction = 0,
    bool keep_dim = false) {
  NormBuilder builder_(_fbb);
  builder_.add_correction(correction);
  builder_.add_axes(axes);
  builder_.add_keep_dim(keep_dim);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Norm> CreateNormDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int32_t> *axes = nullptr,
    int64_t correction = 0,
    bool keep_dim = false) {
  auto axes__ = axes ? _fbb.CreateVector<int32_t>(*axes) : 0;
  return nvfuser::serde::CreateNorm(
      _fbb,
      axes__,
      correction,
      keep_dim);
}

::flatbuffers::Offset<Norm> CreateNorm(::flatbuffers::FlatBufferBuilder &_fbb, const NormT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct OutputT : public ::flatbuffers::NativeTable {
  typedef Output TableType;
  std::vector<int64_t> stride_order{};
};

struct Output FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef OutputT NativeTableType;
  typedef OutputBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_STRIDE_ORDER = 4
  };
  const ::flatbuffers::Vector<int64_t> *stride_order() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_STRIDE_ORDER);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_STRIDE_ORDER) &&
           verifier.VerifyVector(stride_order()) &&
           verifier.EndTable();
  }
  OutputT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(OutputT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Output> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const OutputT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct OutputBuilder {
  typedef Output Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_stride_order(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> stride_order) {
    fbb_.AddOffset(Output::VT_STRIDE_ORDER, stride_order);
  }
  explicit OutputBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Output> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Output>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Output> CreateOutput(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> stride_order = 0) {
  OutputBuilder builder_(_fbb);
  builder_.add_stride_order(stride_order);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Output> CreateOutputDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *stride_order = nullptr) {
  auto stride_order__ = stride_order ? _fbb.CreateVector<int64_t>(*stride_order) : 0;
  return nvfuser::serde::CreateOutput(
      _fbb,
      stride_order__);
}

::flatbuffers::Offset<Output> CreateOutput(::flatbuffers::FlatBufferBuilder &_fbb, const OutputT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct PadT : public ::flatbuffers::NativeTable {
  typedef Pad TableType;
  std::vector<int64_t> pad_widths{};
};

struct Pad FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef PadT NativeTableType;
  typedef PadBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_PAD_WIDTHS = 4
  };
  const ::flatbuffers::Vector<int64_t> *pad_widths() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_PAD_WIDTHS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_PAD_WIDTHS) &&
           verifier.VerifyVector(pad_widths()) &&
           verifier.EndTable();
  }
  PadT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(PadT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Pad> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const PadT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct PadBuilder {
  typedef Pad Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_pad_widths(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> pad_widths) {
    fbb_.AddOffset(Pad::VT_PAD_WIDTHS, pad_widths);
  }
  explicit PadBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Pad> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Pad>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Pad> CreatePad(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> pad_widths = 0) {
  PadBuilder builder_(_fbb);
  builder_.add_pad_widths(pad_widths);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Pad> CreatePadDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *pad_widths = nullptr) {
  auto pad_widths__ = pad_widths ? _fbb.CreateVector<int64_t>(*pad_widths) : 0;
  return nvfuser::serde::CreatePad(
      _fbb,
      pad_widths__);
}

::flatbuffers::Offset<Pad> CreatePad(::flatbuffers::FlatBufferBuilder &_fbb, const PadT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct PermuteT : public ::flatbuffers::NativeTable {
  typedef Permute TableType;
  std::vector<int64_t> dims{};
};

struct Permute FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef PermuteT NativeTableType;
  typedef PermuteBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIMS = 4
  };
  const ::flatbuffers::Vector<int64_t> *dims() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_DIMS) &&
           verifier.VerifyVector(dims()) &&
           verifier.EndTable();
  }
  PermuteT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(PermuteT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Permute> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const PermuteT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct PermuteBuilder {
  typedef Permute Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dims(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> dims) {
    fbb_.AddOffset(Permute::VT_DIMS, dims);
  }
  explicit PermuteBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Permute> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Permute>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Permute> CreatePermute(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> dims = 0) {
  PermuteBuilder builder_(_fbb);
  builder_.add_dims(dims);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Permute> CreatePermuteDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *dims = nullptr) {
  auto dims__ = dims ? _fbb.CreateVector<int64_t>(*dims) : 0;
  return nvfuser::serde::CreatePermute(
      _fbb,
      dims__);
}

::flatbuffers::Offset<Permute> CreatePermute(::flatbuffers::FlatBufferBuilder &_fbb, const PermuteT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct ReductionT : public ::flatbuffers::NativeTable {
  typedef Reduction TableType;
  std::vector<int32_t> axes{};
  bool keep_dim = false;
  nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double;
};

struct Reduction FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ReductionT NativeTableType;
  typedef ReductionBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_AXES = 4,
    VT_KEEP_DIM = 6,
    VT_DTYPE = 8
  };
  const ::flatbuffers::Vector<int32_t> *axes() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_AXES);
  }
  bool keep_dim() const {
    return GetField<uint8_t>(VT_KEEP_DIM, 0) != 0;
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_AXES) &&
           verifier.VerifyVector(axes()) &&
           VerifyField<uint8_t>(verifier, VT_KEEP_DIM, 1) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
  ReductionT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(ReductionT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Reduction> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ReductionT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct ReductionBuilder {
  typedef Reduction Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_axes(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> axes) {
    fbb_.AddOffset(Reduction::VT_AXES, axes);
  }
  void add_keep_dim(bool keep_dim) {
    fbb_.AddElement<uint8_t>(Reduction::VT_KEEP_DIM, static_cast<uint8_t>(keep_dim), 0);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(Reduction::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit ReductionBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Reduction> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Reduction>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Reduction> CreateReduction(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> axes = 0,
    bool keep_dim = false,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double) {
  ReductionBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  builder_.add_axes(axes);
  builder_.add_keep_dim(keep_dim);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Reduction> CreateReductionDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int32_t> *axes = nullptr,
    bool keep_dim = false,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double) {
  auto axes__ = axes ? _fbb.CreateVector<int32_t>(*axes) : 0;
  return nvfuser::serde::CreateReduction(
      _fbb,
      axes__,
      keep_dim,
      dtype);
}

::flatbuffers::Offset<Reduction> CreateReduction(::flatbuffers::FlatBufferBuilder &_fbb, const ReductionT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct ReshapeT : public ::flatbuffers::NativeTable {
  typedef Reshape TableType;
  std::vector<int64_t> original_shape{};
  std::vector<int64_t> new_shape{};
};

struct Reshape FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef ReshapeT NativeTableType;
  typedef ReshapeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ORIGINAL_SHAPE = 4,
    VT_NEW_SHAPE = 6
  };
  const ::flatbuffers::Vector<int64_t> *original_shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_ORIGINAL_SHAPE);
  }
  const ::flatbuffers::Vector<int64_t> *new_shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_NEW_SHAPE);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ORIGINAL_SHAPE) &&
           verifier.VerifyVector(original_shape()) &&
           VerifyOffset(verifier, VT_NEW_SHAPE) &&
           verifier.VerifyVector(new_shape()) &&
           verifier.EndTable();
  }
  ReshapeT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(ReshapeT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Reshape> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ReshapeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct ReshapeBuilder {
  typedef Reshape Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_original_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> original_shape) {
    fbb_.AddOffset(Reshape::VT_ORIGINAL_SHAPE, original_shape);
  }
  void add_new_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> new_shape) {
    fbb_.AddOffset(Reshape::VT_NEW_SHAPE, new_shape);
  }
  explicit ReshapeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Reshape> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Reshape>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Reshape> CreateReshape(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> original_shape = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> new_shape = 0) {
  ReshapeBuilder builder_(_fbb);
  builder_.add_new_shape(new_shape);
  builder_.add_original_shape(original_shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Reshape> CreateReshapeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *original_shape = nullptr,
    const std::vector<int64_t> *new_shape = nullptr) {
  auto original_shape__ = original_shape ? _fbb.CreateVector<int64_t>(*original_shape) : 0;
  auto new_shape__ = new_shape ? _fbb.CreateVector<int64_t>(*new_shape) : 0;
  return nvfuser::serde::CreateReshape(
      _fbb,
      original_shape__,
      new_shape__);
}

::flatbuffers::Offset<Reshape> CreateReshape(::flatbuffers::FlatBufferBuilder &_fbb, const ReshapeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct SizeT : public ::flatbuffers::NativeTable {
  typedef Size TableType;
  int64_t dim = 0;
};

struct Size FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SizeT NativeTableType;
  typedef SizeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DIM = 4
  };
  int64_t dim() const {
    return GetField<int64_t>(VT_DIM, 0);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DIM, 8) &&
           verifier.EndTable();
  }
  SizeT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(SizeT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Size> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const SizeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct SizeBuilder {
  typedef Size Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dim(int64_t dim) {
    fbb_.AddElement<int64_t>(Size::VT_DIM, dim, 0);
  }
  explicit SizeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Size> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Size>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Size> CreateSize(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t dim = 0) {
  SizeBuilder builder_(_fbb);
  builder_.add_dim(dim);
  return builder_.Finish();
}

::flatbuffers::Offset<Size> CreateSize(::flatbuffers::FlatBufferBuilder &_fbb, const SizeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct SliceT : public ::flatbuffers::NativeTable {
  typedef Slice TableType;
  std::vector<int64_t> start_indices{};
  std::vector<int64_t> end_indices{};
  std::vector<int64_t> strides{};
};

struct Slice FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SliceT NativeTableType;
  typedef SliceBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_START_INDICES = 4,
    VT_END_INDICES = 6,
    VT_STRIDES = 8
  };
  const ::flatbuffers::Vector<int64_t> *start_indices() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_START_INDICES);
  }
  const ::flatbuffers::Vector<int64_t> *end_indices() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_END_INDICES);
  }
  const ::flatbuffers::Vector<int64_t> *strides() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_STRIDES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_START_INDICES) &&
           verifier.VerifyVector(start_indices()) &&
           VerifyOffset(verifier, VT_END_INDICES) &&
           verifier.VerifyVector(end_indices()) &&
           VerifyOffset(verifier, VT_STRIDES) &&
           verifier.VerifyVector(strides()) &&
           verifier.EndTable();
  }
  SliceT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(SliceT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Slice> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const SliceT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct SliceBuilder {
  typedef Slice Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_start_indices(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> start_indices) {
    fbb_.AddOffset(Slice::VT_START_INDICES, start_indices);
  }
  void add_end_indices(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> end_indices) {
    fbb_.AddOffset(Slice::VT_END_INDICES, end_indices);
  }
  void add_strides(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides) {
    fbb_.AddOffset(Slice::VT_STRIDES, strides);
  }
  explicit SliceBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Slice> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Slice>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Slice> CreateSlice(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> start_indices = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> end_indices = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> strides = 0) {
  SliceBuilder builder_(_fbb);
  builder_.add_strides(strides);
  builder_.add_end_indices(end_indices);
  builder_.add_start_indices(start_indices);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Slice> CreateSliceDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *start_indices = nullptr,
    const std::vector<int64_t> *end_indices = nullptr,
    const std::vector<int64_t> *strides = nullptr) {
  auto start_indices__ = start_indices ? _fbb.CreateVector<int64_t>(*start_indices) : 0;
  auto end_indices__ = end_indices ? _fbb.CreateVector<int64_t>(*end_indices) : 0;
  auto strides__ = strides ? _fbb.CreateVector<int64_t>(*strides) : 0;
  return nvfuser::serde::CreateSlice(
      _fbb,
      start_indices__,
      end_indices__,
      strides__);
}

::flatbuffers::Offset<Slice> CreateSlice(::flatbuffers::FlatBufferBuilder &_fbb, const SliceT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct SqueezeT : public ::flatbuffers::NativeTable {
  typedef Squeeze TableType;
  std::vector<int64_t> original_shape{};
  std::vector<int64_t> squeeze_dims{};
};

struct Squeeze FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef SqueezeT NativeTableType;
  typedef SqueezeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ORIGINAL_SHAPE = 4,
    VT_SQUEEZE_DIMS = 6
  };
  const ::flatbuffers::Vector<int64_t> *original_shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_ORIGINAL_SHAPE);
  }
  const ::flatbuffers::Vector<int64_t> *squeeze_dims() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SQUEEZE_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ORIGINAL_SHAPE) &&
           verifier.VerifyVector(original_shape()) &&
           VerifyOffset(verifier, VT_SQUEEZE_DIMS) &&
           verifier.VerifyVector(squeeze_dims()) &&
           verifier.EndTable();
  }
  SqueezeT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(SqueezeT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Squeeze> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const SqueezeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct SqueezeBuilder {
  typedef Squeeze Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_original_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> original_shape) {
    fbb_.AddOffset(Squeeze::VT_ORIGINAL_SHAPE, original_shape);
  }
  void add_squeeze_dims(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> squeeze_dims) {
    fbb_.AddOffset(Squeeze::VT_SQUEEZE_DIMS, squeeze_dims);
  }
  explicit SqueezeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Squeeze> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Squeeze>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Squeeze> CreateSqueeze(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> original_shape = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> squeeze_dims = 0) {
  SqueezeBuilder builder_(_fbb);
  builder_.add_squeeze_dims(squeeze_dims);
  builder_.add_original_shape(original_shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Squeeze> CreateSqueezeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *original_shape = nullptr,
    const std::vector<int64_t> *squeeze_dims = nullptr) {
  auto original_shape__ = original_shape ? _fbb.CreateVector<int64_t>(*original_shape) : 0;
  auto squeeze_dims__ = squeeze_dims ? _fbb.CreateVector<int64_t>(*squeeze_dims) : 0;
  return nvfuser::serde::CreateSqueeze(
      _fbb,
      original_shape__,
      squeeze_dims__);
}

::flatbuffers::Offset<Squeeze> CreateSqueeze(::flatbuffers::FlatBufferBuilder &_fbb, const SqueezeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct TensorT : public ::flatbuffers::NativeTable {
  typedef Tensor TableType;
  std::vector<int64_t> sizes{};
  std::vector<nvfuser::serde::Contiguity> contiguity{};
  nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double;
  bool is_cpu = false;
};

struct Tensor FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorT NativeTableType;
  typedef TensorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SIZES = 4,
    VT_CONTIGUITY = 6,
    VT_DTYPE = 8,
    VT_IS_CPU = 10
  };
  const ::flatbuffers::Vector<int64_t> *sizes() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SIZES);
  }
  const ::flatbuffers::Vector<int32_t> *contiguity() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_CONTIGUITY);
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool is_cpu() const {
    return GetField<uint8_t>(VT_IS_CPU, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SIZES) &&
           verifier.VerifyVector(sizes()) &&
           VerifyOffset(verifier, VT_CONTIGUITY) &&
           verifier.VerifyVector(contiguity()) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_IS_CPU, 1) &&
           verifier.EndTable();
  }
  TensorT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(TensorT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Tensor> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TensorT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct TensorBuilder {
  typedef Tensor Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_sizes(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes) {
    fbb_.AddOffset(Tensor::VT_SIZES, sizes);
  }
  void add_contiguity(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> contiguity) {
    fbb_.AddOffset(Tensor::VT_CONTIGUITY, contiguity);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(Tensor::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  void add_is_cpu(bool is_cpu) {
    fbb_.AddElement<uint8_t>(Tensor::VT_IS_CPU, static_cast<uint8_t>(is_cpu), 0);
  }
  explicit TensorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Tensor> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Tensor>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Tensor> CreateTensor(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> sizes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> contiguity = 0,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double,
    bool is_cpu = false) {
  TensorBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  builder_.add_contiguity(contiguity);
  builder_.add_sizes(sizes);
  builder_.add_is_cpu(is_cpu);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Tensor> CreateTensorDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *sizes = nullptr,
    const std::vector<int32_t> *contiguity = nullptr,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double,
    bool is_cpu = false) {
  auto sizes__ = sizes ? _fbb.CreateVector<int64_t>(*sizes) : 0;
  auto contiguity__ = contiguity ? _fbb.CreateVector<int32_t>(*contiguity) : 0;
  return nvfuser::serde::CreateTensor(
      _fbb,
      sizes__,
      contiguity__,
      dtype,
      is_cpu);
}

::flatbuffers::Offset<Tensor> CreateTensor(::flatbuffers::FlatBufferBuilder &_fbb, const TensorT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct TensorCreationT : public ::flatbuffers::NativeTable {
  typedef TensorCreation TableType;
  std::vector<int64_t> shape{};
  nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double;
};

struct TensorCreation FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorCreationT NativeTableType;
  typedef TensorCreationBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SHAPE = 4,
    VT_DTYPE = 6
  };
  const ::flatbuffers::Vector<int64_t> *shape() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_SHAPE);
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SHAPE) &&
           verifier.VerifyVector(shape()) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
  TensorCreationT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(TensorCreationT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<TensorCreation> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TensorCreationT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct TensorCreationBuilder {
  typedef TensorCreation Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_shape(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape) {
    fbb_.AddOffset(TensorCreation::VT_SHAPE, shape);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(TensorCreation::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit TensorCreationBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TensorCreation> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TensorCreation>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TensorCreation> CreateTensorCreation(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> shape = 0,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double) {
  TensorCreationBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  builder_.add_shape(shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TensorCreation> CreateTensorCreationDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<int64_t> *shape = nullptr,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double) {
  auto shape__ = shape ? _fbb.CreateVector<int64_t>(*shape) : 0;
  return nvfuser::serde::CreateTensorCreation(
      _fbb,
      shape__,
      dtype);
}

::flatbuffers::Offset<TensorCreation> CreateTensorCreation(::flatbuffers::FlatBufferBuilder &_fbb, const TensorCreationT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct TensorCreationSymbolicT : public ::flatbuffers::NativeTable {
  typedef TensorCreationSymbolic TableType;
  std::vector<nvfuser::serde::State> shape{};
  nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double;
};

struct TensorCreationSymbolic FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorCreationSymbolicT NativeTableType;
  typedef TensorCreationSymbolicBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SHAPE = 4,
    VT_DTYPE = 6
  };
  const ::flatbuffers::Vector<const nvfuser::serde::State *> *shape() const {
    return GetPointer<const ::flatbuffers::Vector<const nvfuser::serde::State *> *>(VT_SHAPE);
  }
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SHAPE) &&
           verifier.VerifyVector(shape()) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
  TensorCreationSymbolicT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(TensorCreationSymbolicT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<TensorCreationSymbolic> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TensorCreationSymbolicT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct TensorCreationSymbolicBuilder {
  typedef TensorCreationSymbolic Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_shape(::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> shape) {
    fbb_.AddOffset(TensorCreationSymbolic::VT_SHAPE, shape);
  }
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(TensorCreationSymbolic::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit TensorCreationSymbolicBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TensorCreationSymbolic> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TensorCreationSymbolic>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TensorCreationSymbolic> CreateTensorCreationSymbolic(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> shape = 0,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double) {
  TensorCreationSymbolicBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  builder_.add_shape(shape);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TensorCreationSymbolic> CreateTensorCreationSymbolicDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<nvfuser::serde::State> *shape = nullptr,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double) {
  auto shape__ = shape ? _fbb.CreateVectorOfStructs<nvfuser::serde::State>(*shape) : 0;
  return nvfuser::serde::CreateTensorCreationSymbolic(
      _fbb,
      shape__,
      dtype);
}

::flatbuffers::Offset<TensorCreationSymbolic> CreateTensorCreationSymbolic(::flatbuffers::FlatBufferBuilder &_fbb, const TensorCreationSymbolicT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct VectorT : public ::flatbuffers::NativeTable {
  typedef Vector TableType;
  nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double;
};

struct Vector FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef VectorT NativeTableType;
  typedef VectorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DTYPE = 4
  };
  nvfuser::serde::DataType dtype() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_DTYPE, 0));
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_DTYPE, 4) &&
           verifier.EndTable();
  }
  VectorT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(VectorT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<Vector> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const VectorT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct VectorBuilder {
  typedef Vector Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_dtype(nvfuser::serde::DataType dtype) {
    fbb_.AddElement<int32_t>(Vector::VT_DTYPE, static_cast<int32_t>(dtype), 0);
  }
  explicit VectorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Vector> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Vector>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Vector> CreateVector(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    nvfuser::serde::DataType dtype = nvfuser::serde::DataType_Double) {
  VectorBuilder builder_(_fbb);
  builder_.add_dtype(dtype);
  return builder_.Finish();
}

::flatbuffers::Offset<Vector> CreateVector(::flatbuffers::FlatBufferBuilder &_fbb, const VectorT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct CudaKernelT : public ::flatbuffers::NativeTable {
  typedef CudaKernel TableType;
  std::string kernel_name{};
  std::vector<int8_t> object_code{};
};

struct CudaKernel FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef CudaKernelT NativeTableType;
  typedef CudaKernelBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_KERNEL_NAME = 4,
    VT_OBJECT_CODE = 6
  };
  const ::flatbuffers::String *kernel_name() const {
    return GetPointer<const ::flatbuffers::String *>(VT_KERNEL_NAME);
  }
  const ::flatbuffers::Vector<int8_t> *object_code() const {
    return GetPointer<const ::flatbuffers::Vector<int8_t> *>(VT_OBJECT_CODE);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_KERNEL_NAME) &&
           verifier.VerifyString(kernel_name()) &&
           VerifyOffset(verifier, VT_OBJECT_CODE) &&
           verifier.VerifyVector(object_code()) &&
           verifier.EndTable();
  }
  CudaKernelT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(CudaKernelT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<CudaKernel> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const CudaKernelT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct CudaKernelBuilder {
  typedef CudaKernel Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_kernel_name(::flatbuffers::Offset<::flatbuffers::String> kernel_name) {
    fbb_.AddOffset(CudaKernel::VT_KERNEL_NAME, kernel_name);
  }
  void add_object_code(::flatbuffers::Offset<::flatbuffers::Vector<int8_t>> object_code) {
    fbb_.AddOffset(CudaKernel::VT_OBJECT_CODE, object_code);
  }
  explicit CudaKernelBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<CudaKernel> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<CudaKernel>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<CudaKernel> CreateCudaKernel(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::String> kernel_name = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<int8_t>> object_code = 0) {
  CudaKernelBuilder builder_(_fbb);
  builder_.add_object_code(object_code);
  builder_.add_kernel_name(kernel_name);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<CudaKernel> CreateCudaKernelDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const char *kernel_name = nullptr,
    const std::vector<int8_t> *object_code = nullptr) {
  auto kernel_name__ = kernel_name ? _fbb.CreateString(kernel_name) : 0;
  auto object_code__ = object_code ? _fbb.CreateVector<int8_t>(*object_code) : 0;
  return nvfuser::serde::CreateCudaKernel(
      _fbb,
      kernel_name__,
      object_code__);
}

::flatbuffers::Offset<CudaKernel> CreateCudaKernel(::flatbuffers::FlatBufferBuilder &_fbb, const CudaKernelT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct FusionExecutorT : public ::flatbuffers::NativeTable {
  typedef FusionExecutor TableType;
  int64_t device_smem_limit = 0;
  int64_t block_size_high_water_mark = 0;
  int64_t maxrregcount_high_water_mark = 0;
  int64_t warp_size = 0;
  int64_t fusion_id = 0;
  int64_t fusion_id_counter = 0;
  std::string kernel_code{};
  std::vector<uint64_t> executor_entry_lookup_keys{};
  std::vector<std::unique_ptr<nvfuser::serde::ExecutorEntryT>> executor_entry_lookup_values{};
  nvfuser::serde::DataType index_type = nvfuser::serde::DataType_Double;
  std::unique_ptr<nvfuser::serde::CudaKernelT> compiled_kernel{};
  FusionExecutorT() = default;
  FusionExecutorT(const FusionExecutorT &o);
  FusionExecutorT(FusionExecutorT&&) FLATBUFFERS_NOEXCEPT = default;
  FusionExecutorT &operator=(FusionExecutorT o) FLATBUFFERS_NOEXCEPT;
};

struct FusionExecutor FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef FusionExecutorT NativeTableType;
  typedef FusionExecutorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DEVICE_SMEM_LIMIT = 4,
    VT_BLOCK_SIZE_HIGH_WATER_MARK = 6,
    VT_MAXRREGCOUNT_HIGH_WATER_MARK = 8,
    VT_WARP_SIZE = 10,
    VT_FUSION_ID = 12,
    VT_FUSION_ID_COUNTER = 14,
    VT_KERNEL_CODE = 16,
    VT_EXECUTOR_ENTRY_LOOKUP_KEYS = 18,
    VT_EXECUTOR_ENTRY_LOOKUP_VALUES = 20,
    VT_INDEX_TYPE = 22,
    VT_COMPILED_KERNEL = 24
  };
  int64_t device_smem_limit() const {
    return GetField<int64_t>(VT_DEVICE_SMEM_LIMIT, 0);
  }
  int64_t block_size_high_water_mark() const {
    return GetField<int64_t>(VT_BLOCK_SIZE_HIGH_WATER_MARK, 0);
  }
  int64_t maxrregcount_high_water_mark() const {
    return GetField<int64_t>(VT_MAXRREGCOUNT_HIGH_WATER_MARK, 0);
  }
  int64_t warp_size() const {
    return GetField<int64_t>(VT_WARP_SIZE, 0);
  }
  int64_t fusion_id() const {
    return GetField<int64_t>(VT_FUSION_ID, 0);
  }
  int64_t fusion_id_counter() const {
    return GetField<int64_t>(VT_FUSION_ID_COUNTER, 0);
  }
  const ::flatbuffers::String *kernel_code() const {
    return GetPointer<const ::flatbuffers::String *>(VT_KERNEL_CODE);
  }
  const ::flatbuffers::Vector<uint64_t> *executor_entry_lookup_keys() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_EXECUTOR_ENTRY_LOOKUP_KEYS);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>> *executor_entry_lookup_values() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>> *>(VT_EXECUTOR_ENTRY_LOOKUP_VALUES);
  }
  nvfuser::serde::DataType index_type() const {
    return static_cast<nvfuser::serde::DataType>(GetField<int32_t>(VT_INDEX_TYPE, 0));
  }
  const nvfuser::serde::CudaKernel *compiled_kernel() const {
    return GetPointer<const nvfuser::serde::CudaKernel *>(VT_COMPILED_KERNEL);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int64_t>(verifier, VT_DEVICE_SMEM_LIMIT, 8) &&
           VerifyField<int64_t>(verifier, VT_BLOCK_SIZE_HIGH_WATER_MARK, 8) &&
           VerifyField<int64_t>(verifier, VT_MAXRREGCOUNT_HIGH_WATER_MARK, 8) &&
           VerifyField<int64_t>(verifier, VT_WARP_SIZE, 8) &&
           VerifyField<int64_t>(verifier, VT_FUSION_ID, 8) &&
           VerifyField<int64_t>(verifier, VT_FUSION_ID_COUNTER, 8) &&
           VerifyOffset(verifier, VT_KERNEL_CODE) &&
           verifier.VerifyString(kernel_code()) &&
           VerifyOffset(verifier, VT_EXECUTOR_ENTRY_LOOKUP_KEYS) &&
           verifier.VerifyVector(executor_entry_lookup_keys()) &&
           VerifyOffset(verifier, VT_EXECUTOR_ENTRY_LOOKUP_VALUES) &&
           verifier.VerifyVector(executor_entry_lookup_values()) &&
           verifier.VerifyVectorOfTables(executor_entry_lookup_values()) &&
           VerifyField<int32_t>(verifier, VT_INDEX_TYPE, 4) &&
           VerifyOffset(verifier, VT_COMPILED_KERNEL) &&
           verifier.VerifyTable(compiled_kernel()) &&
           verifier.EndTable();
  }
  FusionExecutorT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(FusionExecutorT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<FusionExecutor> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const FusionExecutorT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct FusionExecutorBuilder {
  typedef FusionExecutor Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_device_smem_limit(int64_t device_smem_limit) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_DEVICE_SMEM_LIMIT, device_smem_limit, 0);
  }
  void add_block_size_high_water_mark(int64_t block_size_high_water_mark) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_BLOCK_SIZE_HIGH_WATER_MARK, block_size_high_water_mark, 0);
  }
  void add_maxrregcount_high_water_mark(int64_t maxrregcount_high_water_mark) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_MAXRREGCOUNT_HIGH_WATER_MARK, maxrregcount_high_water_mark, 0);
  }
  void add_warp_size(int64_t warp_size) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_WARP_SIZE, warp_size, 0);
  }
  void add_fusion_id(int64_t fusion_id) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_FUSION_ID, fusion_id, 0);
  }
  void add_fusion_id_counter(int64_t fusion_id_counter) {
    fbb_.AddElement<int64_t>(FusionExecutor::VT_FUSION_ID_COUNTER, fusion_id_counter, 0);
  }
  void add_kernel_code(::flatbuffers::Offset<::flatbuffers::String> kernel_code) {
    fbb_.AddOffset(FusionExecutor::VT_KERNEL_CODE, kernel_code);
  }
  void add_executor_entry_lookup_keys(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> executor_entry_lookup_keys) {
    fbb_.AddOffset(FusionExecutor::VT_EXECUTOR_ENTRY_LOOKUP_KEYS, executor_entry_lookup_keys);
  }
  void add_executor_entry_lookup_values(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>>> executor_entry_lookup_values) {
    fbb_.AddOffset(FusionExecutor::VT_EXECUTOR_ENTRY_LOOKUP_VALUES, executor_entry_lookup_values);
  }
  void add_index_type(nvfuser::serde::DataType index_type) {
    fbb_.AddElement<int32_t>(FusionExecutor::VT_INDEX_TYPE, static_cast<int32_t>(index_type), 0);
  }
  void add_compiled_kernel(::flatbuffers::Offset<nvfuser::serde::CudaKernel> compiled_kernel) {
    fbb_.AddOffset(FusionExecutor::VT_COMPILED_KERNEL, compiled_kernel);
  }
  explicit FusionExecutorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<FusionExecutor> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<FusionExecutor>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<FusionExecutor> CreateFusionExecutor(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t device_smem_limit = 0,
    int64_t block_size_high_water_mark = 0,
    int64_t maxrregcount_high_water_mark = 0,
    int64_t warp_size = 0,
    int64_t fusion_id = 0,
    int64_t fusion_id_counter = 0,
    ::flatbuffers::Offset<::flatbuffers::String> kernel_code = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> executor_entry_lookup_keys = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>>> executor_entry_lookup_values = 0,
    nvfuser::serde::DataType index_type = nvfuser::serde::DataType_Double,
    ::flatbuffers::Offset<nvfuser::serde::CudaKernel> compiled_kernel = 0) {
  FusionExecutorBuilder builder_(_fbb);
  builder_.add_fusion_id_counter(fusion_id_counter);
  builder_.add_fusion_id(fusion_id);
  builder_.add_warp_size(warp_size);
  builder_.add_maxrregcount_high_water_mark(maxrregcount_high_water_mark);
  builder_.add_block_size_high_water_mark(block_size_high_water_mark);
  builder_.add_device_smem_limit(device_smem_limit);
  builder_.add_compiled_kernel(compiled_kernel);
  builder_.add_index_type(index_type);
  builder_.add_executor_entry_lookup_values(executor_entry_lookup_values);
  builder_.add_executor_entry_lookup_keys(executor_entry_lookup_keys);
  builder_.add_kernel_code(kernel_code);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<FusionExecutor> CreateFusionExecutorDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int64_t device_smem_limit = 0,
    int64_t block_size_high_water_mark = 0,
    int64_t maxrregcount_high_water_mark = 0,
    int64_t warp_size = 0,
    int64_t fusion_id = 0,
    int64_t fusion_id_counter = 0,
    const char *kernel_code = nullptr,
    const std::vector<uint64_t> *executor_entry_lookup_keys = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>> *executor_entry_lookup_values = nullptr,
    nvfuser::serde::DataType index_type = nvfuser::serde::DataType_Double,
    ::flatbuffers::Offset<nvfuser::serde::CudaKernel> compiled_kernel = 0) {
  auto kernel_code__ = kernel_code ? _fbb.CreateString(kernel_code) : 0;
  auto executor_entry_lookup_keys__ = executor_entry_lookup_keys ? _fbb.CreateVector<uint64_t>(*executor_entry_lookup_keys) : 0;
  auto executor_entry_lookup_values__ = executor_entry_lookup_values ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>>(*executor_entry_lookup_values) : 0;
  return nvfuser::serde::CreateFusionExecutor(
      _fbb,
      device_smem_limit,
      block_size_high_water_mark,
      maxrregcount_high_water_mark,
      warp_size,
      fusion_id,
      fusion_id_counter,
      kernel_code__,
      executor_entry_lookup_keys__,
      executor_entry_lookup_values__,
      index_type,
      compiled_kernel);
}

::flatbuffers::Offset<FusionExecutor> CreateFusionExecutor(::flatbuffers::FlatBufferBuilder &_fbb, const FusionExecutorT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct FusionKernelRuntimeT : public ::flatbuffers::NativeTable {
  typedef FusionKernelRuntime TableType;
  std::unique_ptr<nvfuser::serde::KernelArgumentHolderT> args{};
  std::vector<std::unique_ptr<nvfuser::serde::FusionExecutorT>> executors{};
  FusionKernelRuntimeT() = default;
  FusionKernelRuntimeT(const FusionKernelRuntimeT &o);
  FusionKernelRuntimeT(FusionKernelRuntimeT&&) FLATBUFFERS_NOEXCEPT = default;
  FusionKernelRuntimeT &operator=(FusionKernelRuntimeT o) FLATBUFFERS_NOEXCEPT;
};

struct FusionKernelRuntime FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef FusionKernelRuntimeT NativeTableType;
  typedef FusionKernelRuntimeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ARGS = 4,
    VT_EXECUTORS = 6
  };
  const nvfuser::serde::KernelArgumentHolder *args() const {
    return GetPointer<const nvfuser::serde::KernelArgumentHolder *>(VT_ARGS);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>> *executors() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>> *>(VT_EXECUTORS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ARGS) &&
           verifier.VerifyTable(args()) &&
           VerifyOffset(verifier, VT_EXECUTORS) &&
           verifier.VerifyVector(executors()) &&
           verifier.VerifyVectorOfTables(executors()) &&
           verifier.EndTable();
  }
  FusionKernelRuntimeT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(FusionKernelRuntimeT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<FusionKernelRuntime> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const FusionKernelRuntimeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct FusionKernelRuntimeBuilder {
  typedef FusionKernelRuntime Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_args(::flatbuffers::Offset<nvfuser::serde::KernelArgumentHolder> args) {
    fbb_.AddOffset(FusionKernelRuntime::VT_ARGS, args);
  }
  void add_executors(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>>> executors) {
    fbb_.AddOffset(FusionKernelRuntime::VT_EXECUTORS, executors);
  }
  explicit FusionKernelRuntimeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<FusionKernelRuntime> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<FusionKernelRuntime>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<FusionKernelRuntime> CreateFusionKernelRuntime(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::KernelArgumentHolder> args = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>>> executors = 0) {
  FusionKernelRuntimeBuilder builder_(_fbb);
  builder_.add_executors(executors);
  builder_.add_args(args);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<FusionKernelRuntime> CreateFusionKernelRuntimeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::KernelArgumentHolder> args = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>> *executors = nullptr) {
  auto executors__ = executors ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>>(*executors) : 0;
  return nvfuser::serde::CreateFusionKernelRuntime(
      _fbb,
      args,
      executors__);
}

::flatbuffers::Offset<FusionKernelRuntime> CreateFusionKernelRuntime(::flatbuffers::FlatBufferBuilder &_fbb, const FusionKernelRuntimeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct InputsIdLookupT : public ::flatbuffers::NativeTable {
  typedef InputsIdLookup TableType;
  uint64_t max_cache_size = 0;
  uint64_t current_id = 0;
  std::vector<std::string> lru_cache{};
  std::vector<std::string> encoding_lookup_keys{};
  std::vector<nvfuser::serde::EncodingEntry> encoding_lookup_values{};
};

struct InputsIdLookup FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef InputsIdLookupT NativeTableType;
  typedef InputsIdLookupBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_MAX_CACHE_SIZE = 4,
    VT_CURRENT_ID = 6,
    VT_LRU_CACHE = 8,
    VT_ENCODING_LOOKUP_KEYS = 10,
    VT_ENCODING_LOOKUP_VALUES = 12
  };
  uint64_t max_cache_size() const {
    return GetField<uint64_t>(VT_MAX_CACHE_SIZE, 0);
  }
  uint64_t current_id() const {
    return GetField<uint64_t>(VT_CURRENT_ID, 0);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>> *lru_cache() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>> *>(VT_LRU_CACHE);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>> *encoding_lookup_keys() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>> *>(VT_ENCODING_LOOKUP_KEYS);
  }
  const ::flatbuffers::Vector<const nvfuser::serde::EncodingEntry *> *encoding_lookup_values() const {
    return GetPointer<const ::flatbuffers::Vector<const nvfuser::serde::EncodingEntry *> *>(VT_ENCODING_LOOKUP_VALUES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_MAX_CACHE_SIZE, 8) &&
           VerifyField<uint64_t>(verifier, VT_CURRENT_ID, 8) &&
           VerifyOffset(verifier, VT_LRU_CACHE) &&
           verifier.VerifyVector(lru_cache()) &&
           verifier.VerifyVectorOfStrings(lru_cache()) &&
           VerifyOffset(verifier, VT_ENCODING_LOOKUP_KEYS) &&
           verifier.VerifyVector(encoding_lookup_keys()) &&
           verifier.VerifyVectorOfStrings(encoding_lookup_keys()) &&
           VerifyOffset(verifier, VT_ENCODING_LOOKUP_VALUES) &&
           verifier.VerifyVector(encoding_lookup_values()) &&
           verifier.EndTable();
  }
  InputsIdLookupT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(InputsIdLookupT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<InputsIdLookup> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const InputsIdLookupT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct InputsIdLookupBuilder {
  typedef InputsIdLookup Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_max_cache_size(uint64_t max_cache_size) {
    fbb_.AddElement<uint64_t>(InputsIdLookup::VT_MAX_CACHE_SIZE, max_cache_size, 0);
  }
  void add_current_id(uint64_t current_id) {
    fbb_.AddElement<uint64_t>(InputsIdLookup::VT_CURRENT_ID, current_id, 0);
  }
  void add_lru_cache(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>>> lru_cache) {
    fbb_.AddOffset(InputsIdLookup::VT_LRU_CACHE, lru_cache);
  }
  void add_encoding_lookup_keys(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>>> encoding_lookup_keys) {
    fbb_.AddOffset(InputsIdLookup::VT_ENCODING_LOOKUP_KEYS, encoding_lookup_keys);
  }
  void add_encoding_lookup_values(::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::EncodingEntry *>> encoding_lookup_values) {
    fbb_.AddOffset(InputsIdLookup::VT_ENCODING_LOOKUP_VALUES, encoding_lookup_values);
  }
  explicit InputsIdLookupBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<InputsIdLookup> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<InputsIdLookup>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<InputsIdLookup> CreateInputsIdLookup(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t max_cache_size = 0,
    uint64_t current_id = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>>> lru_cache = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<::flatbuffers::String>>> encoding_lookup_keys = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::EncodingEntry *>> encoding_lookup_values = 0) {
  InputsIdLookupBuilder builder_(_fbb);
  builder_.add_current_id(current_id);
  builder_.add_max_cache_size(max_cache_size);
  builder_.add_encoding_lookup_values(encoding_lookup_values);
  builder_.add_encoding_lookup_keys(encoding_lookup_keys);
  builder_.add_lru_cache(lru_cache);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<InputsIdLookup> CreateInputsIdLookupDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t max_cache_size = 0,
    uint64_t current_id = 0,
    const std::vector<::flatbuffers::Offset<::flatbuffers::String>> *lru_cache = nullptr,
    const std::vector<::flatbuffers::Offset<::flatbuffers::String>> *encoding_lookup_keys = nullptr,
    const std::vector<nvfuser::serde::EncodingEntry> *encoding_lookup_values = nullptr) {
  auto lru_cache__ = lru_cache ? _fbb.CreateVector<::flatbuffers::Offset<::flatbuffers::String>>(*lru_cache) : 0;
  auto encoding_lookup_keys__ = encoding_lookup_keys ? _fbb.CreateVector<::flatbuffers::Offset<::flatbuffers::String>>(*encoding_lookup_keys) : 0;
  auto encoding_lookup_values__ = encoding_lookup_values ? _fbb.CreateVectorOfStructs<nvfuser::serde::EncodingEntry>(*encoding_lookup_values) : 0;
  return nvfuser::serde::CreateInputsIdLookup(
      _fbb,
      max_cache_size,
      current_id,
      lru_cache__,
      encoding_lookup_keys__,
      encoding_lookup_values__);
}

::flatbuffers::Offset<InputsIdLookup> CreateInputsIdLookup(::flatbuffers::FlatBufferBuilder &_fbb, const InputsIdLookupT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct KernelRuntimeStateT : public ::flatbuffers::NativeTable {
  typedef KernelRuntimeState TableType;
  uint64_t device_id = 0;
  bool has_dynamic_transform_info = false;
  std::vector<std::unique_ptr<nvfuser::serde::FusionKernelRuntimeT>> runtimes{};
  KernelRuntimeStateT() = default;
  KernelRuntimeStateT(const KernelRuntimeStateT &o);
  KernelRuntimeStateT(KernelRuntimeStateT&&) FLATBUFFERS_NOEXCEPT = default;
  KernelRuntimeStateT &operator=(KernelRuntimeStateT o) FLATBUFFERS_NOEXCEPT;
};

struct KernelRuntimeState FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef KernelRuntimeStateT NativeTableType;
  typedef KernelRuntimeStateBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DEVICE_ID = 4,
    VT_HAS_DYNAMIC_TRANSFORM_INFO = 6,
    VT_RUNTIMES = 8
  };
  uint64_t device_id() const {
    return GetField<uint64_t>(VT_DEVICE_ID, 0);
  }
  bool has_dynamic_transform_info() const {
    return GetField<uint8_t>(VT_HAS_DYNAMIC_TRANSFORM_INFO, 0) != 0;
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>> *runtimes() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>> *>(VT_RUNTIMES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_DEVICE_ID, 8) &&
           VerifyField<uint8_t>(verifier, VT_HAS_DYNAMIC_TRANSFORM_INFO, 1) &&
           VerifyOffset(verifier, VT_RUNTIMES) &&
           verifier.VerifyVector(runtimes()) &&
           verifier.VerifyVectorOfTables(runtimes()) &&
           verifier.EndTable();
  }
  KernelRuntimeStateT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(KernelRuntimeStateT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<KernelRuntimeState> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const KernelRuntimeStateT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct KernelRuntimeStateBuilder {
  typedef KernelRuntimeState Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_device_id(uint64_t device_id) {
    fbb_.AddElement<uint64_t>(KernelRuntimeState::VT_DEVICE_ID, device_id, 0);
  }
  void add_has_dynamic_transform_info(bool has_dynamic_transform_info) {
    fbb_.AddElement<uint8_t>(KernelRuntimeState::VT_HAS_DYNAMIC_TRANSFORM_INFO, static_cast<uint8_t>(has_dynamic_transform_info), 0);
  }
  void add_runtimes(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>>> runtimes) {
    fbb_.AddOffset(KernelRuntimeState::VT_RUNTIMES, runtimes);
  }
  explicit KernelRuntimeStateBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<KernelRuntimeState> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<KernelRuntimeState>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<KernelRuntimeState> CreateKernelRuntimeState(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t device_id = 0,
    bool has_dynamic_transform_info = false,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>>> runtimes = 0) {
  KernelRuntimeStateBuilder builder_(_fbb);
  builder_.add_device_id(device_id);
  builder_.add_runtimes(runtimes);
  builder_.add_has_dynamic_transform_info(has_dynamic_transform_info);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<KernelRuntimeState> CreateKernelRuntimeStateDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t device_id = 0,
    bool has_dynamic_transform_info = false,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>> *runtimes = nullptr) {
  auto runtimes__ = runtimes ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>>(*runtimes) : 0;
  return nvfuser::serde::CreateKernelRuntimeState(
      _fbb,
      device_id,
      has_dynamic_transform_info,
      runtimes__);
}

::flatbuffers::Offset<KernelRuntimeState> CreateKernelRuntimeState(::flatbuffers::FlatBufferBuilder &_fbb, const KernelRuntimeStateT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct FusionExecutorCacheT : public ::flatbuffers::NativeTable {
  typedef FusionExecutorCache TableType;
  std::unique_ptr<nvfuser::serde::InputsIdLookupT> inputs_cache{};
  std::vector<std::unique_ptr<nvfuser::serde::KernelRuntimeStateT>> kernel_runtimes_map{};
  std::vector<uint64_t> kernel_cache_keys{};
  std::vector<uint64_t> kernel_cache_values{};
  FusionExecutorCacheT() = default;
  FusionExecutorCacheT(const FusionExecutorCacheT &o);
  FusionExecutorCacheT(FusionExecutorCacheT&&) FLATBUFFERS_NOEXCEPT = default;
  FusionExecutorCacheT &operator=(FusionExecutorCacheT o) FLATBUFFERS_NOEXCEPT;
};

struct FusionExecutorCache FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef FusionExecutorCacheT NativeTableType;
  typedef FusionExecutorCacheBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INPUTS_CACHE = 4,
    VT_KERNEL_RUNTIMES_MAP = 6,
    VT_KERNEL_CACHE_KEYS = 8,
    VT_KERNEL_CACHE_VALUES = 10
  };
  const nvfuser::serde::InputsIdLookup *inputs_cache() const {
    return GetPointer<const nvfuser::serde::InputsIdLookup *>(VT_INPUTS_CACHE);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>> *kernel_runtimes_map() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>> *>(VT_KERNEL_RUNTIMES_MAP);
  }
  const ::flatbuffers::Vector<uint64_t> *kernel_cache_keys() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_KERNEL_CACHE_KEYS);
  }
  const ::flatbuffers::Vector<uint64_t> *kernel_cache_values() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_KERNEL_CACHE_VALUES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_INPUTS_CACHE) &&
           verifier.VerifyTable(inputs_cache()) &&
           VerifyOffset(verifier, VT_KERNEL_RUNTIMES_MAP) &&
           verifier.VerifyVector(kernel_runtimes_map()) &&
           verifier.VerifyVectorOfTables(kernel_runtimes_map()) &&
           VerifyOffset(verifier, VT_KERNEL_CACHE_KEYS) &&
           verifier.VerifyVector(kernel_cache_keys()) &&
           VerifyOffset(verifier, VT_KERNEL_CACHE_VALUES) &&
           verifier.VerifyVector(kernel_cache_values()) &&
           verifier.EndTable();
  }
  FusionExecutorCacheT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(FusionExecutorCacheT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<FusionExecutorCache> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const FusionExecutorCacheT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct FusionExecutorCacheBuilder {
  typedef FusionExecutorCache Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_inputs_cache(::flatbuffers::Offset<nvfuser::serde::InputsIdLookup> inputs_cache) {
    fbb_.AddOffset(FusionExecutorCache::VT_INPUTS_CACHE, inputs_cache);
  }
  void add_kernel_runtimes_map(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>>> kernel_runtimes_map) {
    fbb_.AddOffset(FusionExecutorCache::VT_KERNEL_RUNTIMES_MAP, kernel_runtimes_map);
  }
  void add_kernel_cache_keys(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> kernel_cache_keys) {
    fbb_.AddOffset(FusionExecutorCache::VT_KERNEL_CACHE_KEYS, kernel_cache_keys);
  }
  void add_kernel_cache_values(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> kernel_cache_values) {
    fbb_.AddOffset(FusionExecutorCache::VT_KERNEL_CACHE_VALUES, kernel_cache_values);
  }
  explicit FusionExecutorCacheBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<FusionExecutorCache> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<FusionExecutorCache>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<FusionExecutorCache> CreateFusionExecutorCache(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::InputsIdLookup> inputs_cache = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>>> kernel_runtimes_map = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> kernel_cache_keys = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> kernel_cache_values = 0) {
  FusionExecutorCacheBuilder builder_(_fbb);
  builder_.add_kernel_cache_values(kernel_cache_values);
  builder_.add_kernel_cache_keys(kernel_cache_keys);
  builder_.add_kernel_runtimes_map(kernel_runtimes_map);
  builder_.add_inputs_cache(inputs_cache);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<FusionExecutorCache> CreateFusionExecutorCacheDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::InputsIdLookup> inputs_cache = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>> *kernel_runtimes_map = nullptr,
    const std::vector<uint64_t> *kernel_cache_keys = nullptr,
    const std::vector<uint64_t> *kernel_cache_values = nullptr) {
  auto kernel_runtimes_map__ = kernel_runtimes_map ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>>(*kernel_runtimes_map) : 0;
  auto kernel_cache_keys__ = kernel_cache_keys ? _fbb.CreateVector<uint64_t>(*kernel_cache_keys) : 0;
  auto kernel_cache_values__ = kernel_cache_values ? _fbb.CreateVector<uint64_t>(*kernel_cache_values) : 0;
  return nvfuser::serde::CreateFusionExecutorCache(
      _fbb,
      inputs_cache,
      kernel_runtimes_map__,
      kernel_cache_keys__,
      kernel_cache_values__);
}

::flatbuffers::Offset<FusionExecutorCache> CreateFusionExecutorCache(::flatbuffers::FlatBufferBuilder &_fbb, const FusionExecutorCacheT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct RecordFunctorT : public ::flatbuffers::NativeTable {
  typedef RecordFunctor TableType;
  std::vector<nvfuser::serde::State> args{};
  std::vector<nvfuser::serde::State> outputs{};
  std::string name{};
  nvfuser::serde::RecordType type = nvfuser::serde::RecordType_Base;
  nvfuser::serde::RecordDataUnion data{};
};

struct RecordFunctor FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef RecordFunctorT NativeTableType;
  typedef RecordFunctorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ARGS = 4,
    VT_OUTPUTS = 6,
    VT_NAME = 8,
    VT_TYPE = 10,
    VT_DATA_TYPE = 12,
    VT_DATA = 14
  };
  const ::flatbuffers::Vector<const nvfuser::serde::State *> *args() const {
    return GetPointer<const ::flatbuffers::Vector<const nvfuser::serde::State *> *>(VT_ARGS);
  }
  const ::flatbuffers::Vector<const nvfuser::serde::State *> *outputs() const {
    return GetPointer<const ::flatbuffers::Vector<const nvfuser::serde::State *> *>(VT_OUTPUTS);
  }
  const ::flatbuffers::String *name() const {
    return GetPointer<const ::flatbuffers::String *>(VT_NAME);
  }
  nvfuser::serde::RecordType type() const {
    return static_cast<nvfuser::serde::RecordType>(GetField<int32_t>(VT_TYPE, 0));
  }
  nvfuser::serde::RecordData data_type() const {
    return static_cast<nvfuser::serde::RecordData>(GetField<uint8_t>(VT_DATA_TYPE, 0));
  }
  const void *data() const {
    return GetPointer<const void *>(VT_DATA);
  }
  template<typename T> const T *data_as() const;
  const nvfuser::serde::At *data_as_At() const {
    return data_type() == nvfuser::serde::RecordData_At ? static_cast<const nvfuser::serde::At *>(data()) : nullptr;
  }
  const nvfuser::serde::BatchNorm *data_as_BatchNorm() const {
    return data_type() == nvfuser::serde::RecordData_BatchNorm ? static_cast<const nvfuser::serde::BatchNorm *>(data()) : nullptr;
  }
  const nvfuser::serde::Broadcast *data_as_Broadcast() const {
    return data_type() == nvfuser::serde::RecordData_Broadcast ? static_cast<const nvfuser::serde::Broadcast *>(data()) : nullptr;
  }
  const nvfuser::serde::BroadcastInDim *data_as_BroadcastInDim() const {
    return data_type() == nvfuser::serde::RecordData_BroadcastInDim ? static_cast<const nvfuser::serde::BroadcastInDim *>(data()) : nullptr;
  }
  const nvfuser::serde::Dimension *data_as_Dimension() const {
    return data_type() == nvfuser::serde::RecordData_Dimension ? static_cast<const nvfuser::serde::Dimension *>(data()) : nullptr;
  }
  const nvfuser::serde::Dtype *data_as_Dtype() const {
    return data_type() == nvfuser::serde::RecordData_Dtype ? static_cast<const nvfuser::serde::Dtype *>(data()) : nullptr;
  }
  const nvfuser::serde::Norm *data_as_Norm() const {
    return data_type() == nvfuser::serde::RecordData_Norm ? static_cast<const nvfuser::serde::Norm *>(data()) : nullptr;
  }
  const nvfuser::serde::Output *data_as_Output() const {
    return data_type() == nvfuser::serde::RecordData_Output ? static_cast<const nvfuser::serde::Output *>(data()) : nullptr;
  }
  const nvfuser::serde::Pad *data_as_Pad() const {
    return data_type() == nvfuser::serde::RecordData_Pad ? static_cast<const nvfuser::serde::Pad *>(data()) : nullptr;
  }
  const nvfuser::serde::Permute *data_as_Permute() const {
    return data_type() == nvfuser::serde::RecordData_Permute ? static_cast<const nvfuser::serde::Permute *>(data()) : nullptr;
  }
  const nvfuser::serde::Slice *data_as_Slice() const {
    return data_type() == nvfuser::serde::RecordData_Slice ? static_cast<const nvfuser::serde::Slice *>(data()) : nullptr;
  }
  const nvfuser::serde::Squeeze *data_as_Squeeze() const {
    return data_type() == nvfuser::serde::RecordData_Squeeze ? static_cast<const nvfuser::serde::Squeeze *>(data()) : nullptr;
  }
  const nvfuser::serde::Reduction *data_as_Reduction() const {
    return data_type() == nvfuser::serde::RecordData_Reduction ? static_cast<const nvfuser::serde::Reduction *>(data()) : nullptr;
  }
  const nvfuser::serde::Reshape *data_as_Reshape() const {
    return data_type() == nvfuser::serde::RecordData_Reshape ? static_cast<const nvfuser::serde::Reshape *>(data()) : nullptr;
  }
  const nvfuser::serde::Scalar *data_as_Scalar() const {
    return data_type() == nvfuser::serde::RecordData_Scalar ? static_cast<const nvfuser::serde::Scalar *>(data()) : nullptr;
  }
  const nvfuser::serde::Size *data_as_Size() const {
    return data_type() == nvfuser::serde::RecordData_Size ? static_cast<const nvfuser::serde::Size *>(data()) : nullptr;
  }
  const nvfuser::serde::Tensor *data_as_Tensor() const {
    return data_type() == nvfuser::serde::RecordData_Tensor ? static_cast<const nvfuser::serde::Tensor *>(data()) : nullptr;
  }
  const nvfuser::serde::TensorCreation *data_as_TensorCreation() const {
    return data_type() == nvfuser::serde::RecordData_TensorCreation ? static_cast<const nvfuser::serde::TensorCreation *>(data()) : nullptr;
  }
  const nvfuser::serde::TensorCreationSymbolic *data_as_TensorCreationSymbolic() const {
    return data_type() == nvfuser::serde::RecordData_TensorCreationSymbolic ? static_cast<const nvfuser::serde::TensorCreationSymbolic *>(data()) : nullptr;
  }
  const nvfuser::serde::Vector *data_as_Vector() const {
    return data_type() == nvfuser::serde::RecordData_Vector ? static_cast<const nvfuser::serde::Vector *>(data()) : nullptr;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ARGS) &&
           verifier.VerifyVector(args()) &&
           VerifyOffset(verifier, VT_OUTPUTS) &&
           verifier.VerifyVector(outputs()) &&
           VerifyOffset(verifier, VT_NAME) &&
           verifier.VerifyString(name()) &&
           VerifyField<int32_t>(verifier, VT_TYPE, 4) &&
           VerifyField<uint8_t>(verifier, VT_DATA_TYPE, 1) &&
           VerifyOffset(verifier, VT_DATA) &&
           VerifyRecordData(verifier, data(), data_type()) &&
           verifier.EndTable();
  }
  RecordFunctorT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(RecordFunctorT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<RecordFunctor> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const RecordFunctorT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

template<> inline const nvfuser::serde::At *RecordFunctor::data_as<nvfuser::serde::At>() const {
  return data_as_At();
}

template<> inline const nvfuser::serde::BatchNorm *RecordFunctor::data_as<nvfuser::serde::BatchNorm>() const {
  return data_as_BatchNorm();
}

template<> inline const nvfuser::serde::Broadcast *RecordFunctor::data_as<nvfuser::serde::Broadcast>() const {
  return data_as_Broadcast();
}

template<> inline const nvfuser::serde::BroadcastInDim *RecordFunctor::data_as<nvfuser::serde::BroadcastInDim>() const {
  return data_as_BroadcastInDim();
}

template<> inline const nvfuser::serde::Dimension *RecordFunctor::data_as<nvfuser::serde::Dimension>() const {
  return data_as_Dimension();
}

template<> inline const nvfuser::serde::Dtype *RecordFunctor::data_as<nvfuser::serde::Dtype>() const {
  return data_as_Dtype();
}

template<> inline const nvfuser::serde::Norm *RecordFunctor::data_as<nvfuser::serde::Norm>() const {
  return data_as_Norm();
}

template<> inline const nvfuser::serde::Output *RecordFunctor::data_as<nvfuser::serde::Output>() const {
  return data_as_Output();
}

template<> inline const nvfuser::serde::Pad *RecordFunctor::data_as<nvfuser::serde::Pad>() const {
  return data_as_Pad();
}

template<> inline const nvfuser::serde::Permute *RecordFunctor::data_as<nvfuser::serde::Permute>() const {
  return data_as_Permute();
}

template<> inline const nvfuser::serde::Slice *RecordFunctor::data_as<nvfuser::serde::Slice>() const {
  return data_as_Slice();
}

template<> inline const nvfuser::serde::Squeeze *RecordFunctor::data_as<nvfuser::serde::Squeeze>() const {
  return data_as_Squeeze();
}

template<> inline const nvfuser::serde::Reduction *RecordFunctor::data_as<nvfuser::serde::Reduction>() const {
  return data_as_Reduction();
}

template<> inline const nvfuser::serde::Reshape *RecordFunctor::data_as<nvfuser::serde::Reshape>() const {
  return data_as_Reshape();
}

template<> inline const nvfuser::serde::Scalar *RecordFunctor::data_as<nvfuser::serde::Scalar>() const {
  return data_as_Scalar();
}

template<> inline const nvfuser::serde::Size *RecordFunctor::data_as<nvfuser::serde::Size>() const {
  return data_as_Size();
}

template<> inline const nvfuser::serde::Tensor *RecordFunctor::data_as<nvfuser::serde::Tensor>() const {
  return data_as_Tensor();
}

template<> inline const nvfuser::serde::TensorCreation *RecordFunctor::data_as<nvfuser::serde::TensorCreation>() const {
  return data_as_TensorCreation();
}

template<> inline const nvfuser::serde::TensorCreationSymbolic *RecordFunctor::data_as<nvfuser::serde::TensorCreationSymbolic>() const {
  return data_as_TensorCreationSymbolic();
}

template<> inline const nvfuser::serde::Vector *RecordFunctor::data_as<nvfuser::serde::Vector>() const {
  return data_as_Vector();
}

struct RecordFunctorBuilder {
  typedef RecordFunctor Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_args(::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> args) {
    fbb_.AddOffset(RecordFunctor::VT_ARGS, args);
  }
  void add_outputs(::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> outputs) {
    fbb_.AddOffset(RecordFunctor::VT_OUTPUTS, outputs);
  }
  void add_name(::flatbuffers::Offset<::flatbuffers::String> name) {
    fbb_.AddOffset(RecordFunctor::VT_NAME, name);
  }
  void add_type(nvfuser::serde::RecordType type) {
    fbb_.AddElement<int32_t>(RecordFunctor::VT_TYPE, static_cast<int32_t>(type), 0);
  }
  void add_data_type(nvfuser::serde::RecordData data_type) {
    fbb_.AddElement<uint8_t>(RecordFunctor::VT_DATA_TYPE, static_cast<uint8_t>(data_type), 0);
  }
  void add_data(::flatbuffers::Offset<void> data) {
    fbb_.AddOffset(RecordFunctor::VT_DATA, data);
  }
  explicit RecordFunctorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<RecordFunctor> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<RecordFunctor>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<RecordFunctor> CreateRecordFunctor(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> args = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<const nvfuser::serde::State *>> outputs = 0,
    ::flatbuffers::Offset<::flatbuffers::String> name = 0,
    nvfuser::serde::RecordType type = nvfuser::serde::RecordType_Base,
    nvfuser::serde::RecordData data_type = nvfuser::serde::RecordData_NONE,
    ::flatbuffers::Offset<void> data = 0) {
  RecordFunctorBuilder builder_(_fbb);
  builder_.add_data(data);
  builder_.add_type(type);
  builder_.add_name(name);
  builder_.add_outputs(outputs);
  builder_.add_args(args);
  builder_.add_data_type(data_type);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<RecordFunctor> CreateRecordFunctorDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<nvfuser::serde::State> *args = nullptr,
    const std::vector<nvfuser::serde::State> *outputs = nullptr,
    const char *name = nullptr,
    nvfuser::serde::RecordType type = nvfuser::serde::RecordType_Base,
    nvfuser::serde::RecordData data_type = nvfuser::serde::RecordData_NONE,
    ::flatbuffers::Offset<void> data = 0) {
  auto args__ = args ? _fbb.CreateVectorOfStructs<nvfuser::serde::State>(*args) : 0;
  auto outputs__ = outputs ? _fbb.CreateVectorOfStructs<nvfuser::serde::State>(*outputs) : 0;
  auto name__ = name ? _fbb.CreateString(name) : 0;
  return nvfuser::serde::CreateRecordFunctor(
      _fbb,
      args__,
      outputs__,
      name__,
      type,
      data_type,
      data);
}

::flatbuffers::Offset<RecordFunctor> CreateRecordFunctor(::flatbuffers::FlatBufferBuilder &_fbb, const RecordFunctorT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct TrieNodeT : public ::flatbuffers::NativeTable {
  typedef TrieNode TableType;
  std::unique_ptr<nvfuser::serde::RecordFunctorT> record{};
  std::vector<uint64_t> children{};
  uint64_t fusion_id = 0;
  uint64_t visits = 0;
  bool is_terminal = false;
  TrieNodeT() = default;
  TrieNodeT(const TrieNodeT &o);
  TrieNodeT(TrieNodeT&&) FLATBUFFERS_NOEXCEPT = default;
  TrieNodeT &operator=(TrieNodeT o) FLATBUFFERS_NOEXCEPT;
};

struct TrieNode FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TrieNodeT NativeTableType;
  typedef TrieNodeBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_RECORD = 4,
    VT_CHILDREN = 6,
    VT_FUSION_ID = 8,
    VT_VISITS = 10,
    VT_IS_TERMINAL = 12
  };
  const nvfuser::serde::RecordFunctor *record() const {
    return GetPointer<const nvfuser::serde::RecordFunctor *>(VT_RECORD);
  }
  const ::flatbuffers::Vector<uint64_t> *children() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_CHILDREN);
  }
  uint64_t fusion_id() const {
    return GetField<uint64_t>(VT_FUSION_ID, 0);
  }
  uint64_t visits() const {
    return GetField<uint64_t>(VT_VISITS, 0);
  }
  bool is_terminal() const {
    return GetField<uint8_t>(VT_IS_TERMINAL, 0) != 0;
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_RECORD) &&
           verifier.VerifyTable(record()) &&
           VerifyOffset(verifier, VT_CHILDREN) &&
           verifier.VerifyVector(children()) &&
           VerifyField<uint64_t>(verifier, VT_FUSION_ID, 8) &&
           VerifyField<uint64_t>(verifier, VT_VISITS, 8) &&
           VerifyField<uint8_t>(verifier, VT_IS_TERMINAL, 1) &&
           verifier.EndTable();
  }
  TrieNodeT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(TrieNodeT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<TrieNode> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TrieNodeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct TrieNodeBuilder {
  typedef TrieNode Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_record(::flatbuffers::Offset<nvfuser::serde::RecordFunctor> record) {
    fbb_.AddOffset(TrieNode::VT_RECORD, record);
  }
  void add_children(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> children) {
    fbb_.AddOffset(TrieNode::VT_CHILDREN, children);
  }
  void add_fusion_id(uint64_t fusion_id) {
    fbb_.AddElement<uint64_t>(TrieNode::VT_FUSION_ID, fusion_id, 0);
  }
  void add_visits(uint64_t visits) {
    fbb_.AddElement<uint64_t>(TrieNode::VT_VISITS, visits, 0);
  }
  void add_is_terminal(bool is_terminal) {
    fbb_.AddElement<uint8_t>(TrieNode::VT_IS_TERMINAL, static_cast<uint8_t>(is_terminal), 0);
  }
  explicit TrieNodeBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<TrieNode> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<TrieNode>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<TrieNode> CreateTrieNode(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::RecordFunctor> record = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> children = 0,
    uint64_t fusion_id = 0,
    uint64_t visits = 0,
    bool is_terminal = false) {
  TrieNodeBuilder builder_(_fbb);
  builder_.add_visits(visits);
  builder_.add_fusion_id(fusion_id);
  builder_.add_children(children);
  builder_.add_record(record);
  builder_.add_is_terminal(is_terminal);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<TrieNode> CreateTrieNodeDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<nvfuser::serde::RecordFunctor> record = 0,
    const std::vector<uint64_t> *children = nullptr,
    uint64_t fusion_id = 0,
    uint64_t visits = 0,
    bool is_terminal = false) {
  auto children__ = children ? _fbb.CreateVector<uint64_t>(*children) : 0;
  return nvfuser::serde::CreateTrieNode(
      _fbb,
      record,
      children__,
      fusion_id,
      visits,
      is_terminal);
}

::flatbuffers::Offset<TrieNode> CreateTrieNode(::flatbuffers::FlatBufferBuilder &_fbb, const TrieNodeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct FusionCacheT : public ::flatbuffers::NativeTable {
  typedef FusionCache TableType;
  uint64_t max_fusions = 0;
  std::vector<std::unique_ptr<nvfuser::serde::TrieNodeT>> structure{};
  std::vector<uint64_t> terminal_nodes{};
  std::vector<std::unique_ptr<nvfuser::serde::FusionExecutorCacheT>> auto_gen_schedules{};
  FusionCacheT() = default;
  FusionCacheT(const FusionCacheT &o);
  FusionCacheT(FusionCacheT&&) FLATBUFFERS_NOEXCEPT = default;
  FusionCacheT &operator=(FusionCacheT o) FLATBUFFERS_NOEXCEPT;
};

struct FusionCache FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef FusionCacheT NativeTableType;
  typedef FusionCacheBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_MAX_FUSIONS = 4,
    VT_STRUCTURE = 6,
    VT_TERMINAL_NODES = 8,
    VT_AUTO_GEN_SCHEDULES = 10
  };
  uint64_t max_fusions() const {
    return GetField<uint64_t>(VT_MAX_FUSIONS, 0);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>> *structure() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>> *>(VT_STRUCTURE);
  }
  const ::flatbuffers::Vector<uint64_t> *terminal_nodes() const {
    return GetPointer<const ::flatbuffers::Vector<uint64_t> *>(VT_TERMINAL_NODES);
  }
  const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>> *auto_gen_schedules() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>> *>(VT_AUTO_GEN_SCHEDULES);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<uint64_t>(verifier, VT_MAX_FUSIONS, 8) &&
           VerifyOffset(verifier, VT_STRUCTURE) &&
           verifier.VerifyVector(structure()) &&
           verifier.VerifyVectorOfTables(structure()) &&
           VerifyOffset(verifier, VT_TERMINAL_NODES) &&
           verifier.VerifyVector(terminal_nodes()) &&
           VerifyOffset(verifier, VT_AUTO_GEN_SCHEDULES) &&
           verifier.VerifyVector(auto_gen_schedules()) &&
           verifier.VerifyVectorOfTables(auto_gen_schedules()) &&
           verifier.EndTable();
  }
  FusionCacheT *UnPack(const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(FusionCacheT *_o, const ::flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static ::flatbuffers::Offset<FusionCache> Pack(::flatbuffers::FlatBufferBuilder &_fbb, const FusionCacheT* _o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct FusionCacheBuilder {
  typedef FusionCache Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_max_fusions(uint64_t max_fusions) {
    fbb_.AddElement<uint64_t>(FusionCache::VT_MAX_FUSIONS, max_fusions, 0);
  }
  void add_structure(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>>> structure) {
    fbb_.AddOffset(FusionCache::VT_STRUCTURE, structure);
  }
  void add_terminal_nodes(::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> terminal_nodes) {
    fbb_.AddOffset(FusionCache::VT_TERMINAL_NODES, terminal_nodes);
  }
  void add_auto_gen_schedules(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>>> auto_gen_schedules) {
    fbb_.AddOffset(FusionCache::VT_AUTO_GEN_SCHEDULES, auto_gen_schedules);
  }
  explicit FusionCacheBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<FusionCache> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<FusionCache>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<FusionCache> CreateFusionCache(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t max_fusions = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>>> structure = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint64_t>> terminal_nodes = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>>> auto_gen_schedules = 0) {
  FusionCacheBuilder builder_(_fbb);
  builder_.add_max_fusions(max_fusions);
  builder_.add_auto_gen_schedules(auto_gen_schedules);
  builder_.add_terminal_nodes(terminal_nodes);
  builder_.add_structure(structure);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<FusionCache> CreateFusionCacheDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    uint64_t max_fusions = 0,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::TrieNode>> *structure = nullptr,
    const std::vector<uint64_t> *terminal_nodes = nullptr,
    const std::vector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>> *auto_gen_schedules = nullptr) {
  auto structure__ = structure ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::TrieNode>>(*structure) : 0;
  auto terminal_nodes__ = terminal_nodes ? _fbb.CreateVector<uint64_t>(*terminal_nodes) : 0;
  auto auto_gen_schedules__ = auto_gen_schedules ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>>(*auto_gen_schedules) : 0;
  return nvfuser::serde::CreateFusionCache(
      _fbb,
      max_fusions,
      structure__,
      terminal_nodes__,
      auto_gen_schedules__);
}

::flatbuffers::Offset<FusionCache> CreateFusionCache(::flatbuffers::FlatBufferBuilder &_fbb, const FusionCacheT *_o, const ::flatbuffers::rehasher_function_t *_rehasher = nullptr);

inline ScalarT *Scalar::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<ScalarT>(new ScalarT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Scalar::UnPackTo(ScalarT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = dtype(); _o->dtype = _e; }
  { auto _e = has_value(); _o->has_value = _e; }
  { auto _e = value_type(); _o->value_type = _e; }
  { auto _e = bool_value(); _o->bool_value = _e; }
  { auto _e = long_value(); _o->long_value = _e; }
  { auto _e = double_value(); _o->double_value = _e; }
  { auto _e = real_value(); _o->real_value = _e; }
  { auto _e = imag_value(); _o->imag_value = _e; }
}

inline ::flatbuffers::Offset<Scalar> Scalar::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ScalarT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateScalar(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Scalar> CreateScalar(::flatbuffers::FlatBufferBuilder &_fbb, const ScalarT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const ScalarT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _dtype = _o->dtype;
  auto _has_value = _o->has_value;
  auto _value_type = _o->value_type;
  auto _bool_value = _o->bool_value;
  auto _long_value = _o->long_value;
  auto _double_value = _o->double_value;
  auto _real_value = _o->real_value;
  auto _imag_value = _o->imag_value;
  return nvfuser::serde::CreateScalar(
      _fbb,
      _dtype,
      _has_value,
      _value_type,
      _bool_value,
      _long_value,
      _double_value,
      _real_value,
      _imag_value);
}

inline ScalarCpuT::ScalarCpuT(const ScalarCpuT &o)
      : scalar_value((o.scalar_value) ? new nvfuser::serde::ScalarT(*o.scalar_value) : nullptr) {
}

inline ScalarCpuT &ScalarCpuT::operator=(ScalarCpuT o) FLATBUFFERS_NOEXCEPT {
  std::swap(scalar_value, o.scalar_value);
  return *this;
}

inline ScalarCpuT *ScalarCpu::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<ScalarCpuT>(new ScalarCpuT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void ScalarCpu::UnPackTo(ScalarCpuT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = scalar_value(); if (_e) { if(_o->scalar_value) { _e->UnPackTo(_o->scalar_value.get(), _resolver); } else { _o->scalar_value = std::unique_ptr<nvfuser::serde::ScalarT>(_e->UnPack(_resolver)); } } else if (_o->scalar_value) { _o->scalar_value.reset(); } }
}

inline ::flatbuffers::Offset<ScalarCpu> ScalarCpu::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ScalarCpuT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateScalarCpu(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<ScalarCpu> CreateScalarCpu(::flatbuffers::FlatBufferBuilder &_fbb, const ScalarCpuT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const ScalarCpuT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _scalar_value = _o->scalar_value ? CreateScalar(_fbb, _o->scalar_value.get(), _rehasher) : 0;
  return nvfuser::serde::CreateScalarCpu(
      _fbb,
      _scalar_value);
}

inline TensorArgT *TensorArg::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<TensorArgT>(new TensorArgT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void TensorArg::UnPackTo(TensorArgT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = ptr(); _o->ptr = _e; }
  { auto _e = sizes(); if (_e) { _o->sizes.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->sizes[_i] = _e->Get(_i); } } else { _o->sizes.resize(0); } }
  { auto _e = strides(); if (_e) { _o->strides.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->strides[_i] = _e->Get(_i); } } else { _o->strides.resize(0); } }
  { auto _e = dtype(); _o->dtype = _e; }
}

inline ::flatbuffers::Offset<TensorArg> TensorArg::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TensorArgT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateTensorArg(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<TensorArg> CreateTensorArg(::flatbuffers::FlatBufferBuilder &_fbb, const TensorArgT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const TensorArgT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _ptr = _o->ptr;
  auto _sizes = _o->sizes.size() ? _fbb.CreateVector(_o->sizes) : 0;
  auto _strides = _o->strides.size() ? _fbb.CreateVector(_o->strides) : 0;
  auto _dtype = _o->dtype;
  return nvfuser::serde::CreateTensorArg(
      _fbb,
      _ptr,
      _sizes,
      _strides,
      _dtype);
}

inline PolymorphicValueT *PolymorphicValue::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<PolymorphicValueT>(new PolymorphicValueT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void PolymorphicValue::UnPackTo(PolymorphicValueT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = data_type(); _o->data.type = _e; }
  { auto _e = data(); if (_e) _o->data.value = nvfuser::serde::PolymorphicValueDataUnion::UnPack(_e, data_type(), _resolver); }
}

inline ::flatbuffers::Offset<PolymorphicValue> PolymorphicValue::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const PolymorphicValueT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreatePolymorphicValue(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<PolymorphicValue> CreatePolymorphicValue(::flatbuffers::FlatBufferBuilder &_fbb, const PolymorphicValueT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const PolymorphicValueT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _data_type = _o->data.type;
  auto _data = _o->data.Pack(_fbb);
  return nvfuser::serde::CreatePolymorphicValue(
      _fbb,
      _data_type,
      _data);
}

inline KernelArgumentHolderT::KernelArgumentHolderT(const KernelArgumentHolderT &o)
      : device_index(o.device_index),
        cache_id(o.cache_id) {
  arguments.reserve(o.arguments.size());
  for (const auto &arguments_ : o.arguments) { arguments.emplace_back((arguments_) ? new nvfuser::serde::PolymorphicValueT(*arguments_) : nullptr); }
}

inline KernelArgumentHolderT &KernelArgumentHolderT::operator=(KernelArgumentHolderT o) FLATBUFFERS_NOEXCEPT {
  std::swap(arguments, o.arguments);
  std::swap(device_index, o.device_index);
  std::swap(cache_id, o.cache_id);
  return *this;
}

inline KernelArgumentHolderT *KernelArgumentHolder::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<KernelArgumentHolderT>(new KernelArgumentHolderT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void KernelArgumentHolder::UnPackTo(KernelArgumentHolderT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = arguments(); if (_e) { _o->arguments.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->arguments[_i]) { _e->Get(_i)->UnPackTo(_o->arguments[_i].get(), _resolver); } else { _o->arguments[_i] = std::unique_ptr<nvfuser::serde::PolymorphicValueT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->arguments.resize(0); } }
  { auto _e = device_index(); _o->device_index = _e; }
  { auto _e = cache_id(); _o->cache_id = _e; }
}

inline ::flatbuffers::Offset<KernelArgumentHolder> KernelArgumentHolder::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const KernelArgumentHolderT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateKernelArgumentHolder(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<KernelArgumentHolder> CreateKernelArgumentHolder(::flatbuffers::FlatBufferBuilder &_fbb, const KernelArgumentHolderT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const KernelArgumentHolderT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _arguments = _o->arguments.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::PolymorphicValue>> (_o->arguments.size(), [](size_t i, _VectorArgs *__va) { return CreatePolymorphicValue(*__va->__fbb, __va->__o->arguments[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _device_index = _o->device_index;
  auto _cache_id = _o->cache_id;
  return nvfuser::serde::CreateKernelArgumentHolder(
      _fbb,
      _arguments,
      _device_index,
      _cache_id);
}

inline TensorShapeT *TensorShape::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<TensorShapeT>(new TensorShapeT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void TensorShape::UnPackTo(TensorShapeT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = shape(); if (_e) { _o->shape.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->shape[_i] = _e->Get(_i); } } else { _o->shape.resize(0); } }
}

inline ::flatbuffers::Offset<TensorShape> TensorShape::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TensorShapeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateTensorShape(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<TensorShape> CreateTensorShape(::flatbuffers::FlatBufferBuilder &_fbb, const TensorShapeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const TensorShapeT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _shape = _o->shape.size() ? _fbb.CreateVector(_o->shape) : 0;
  return nvfuser::serde::CreateTensorShape(
      _fbb,
      _shape);
}

inline LaunchParamsT::LaunchParamsT(const LaunchParamsT &o)
      : gdimx(o.gdimx),
        gdimy(o.gdimy),
        gdimz(o.gdimz),
        bdimx(o.bdimx),
        bdimy(o.bdimy),
        bdimz(o.bdimz),
        smem(o.smem) {
  output_sizes.reserve(o.output_sizes.size());
  for (const auto &output_sizes_ : o.output_sizes) { output_sizes.emplace_back((output_sizes_) ? new nvfuser::serde::TensorShapeT(*output_sizes_) : nullptr); }
}

inline LaunchParamsT &LaunchParamsT::operator=(LaunchParamsT o) FLATBUFFERS_NOEXCEPT {
  std::swap(gdimx, o.gdimx);
  std::swap(gdimy, o.gdimy);
  std::swap(gdimz, o.gdimz);
  std::swap(bdimx, o.bdimx);
  std::swap(bdimy, o.bdimy);
  std::swap(bdimz, o.bdimz);
  std::swap(smem, o.smem);
  std::swap(output_sizes, o.output_sizes);
  return *this;
}

inline LaunchParamsT *LaunchParams::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<LaunchParamsT>(new LaunchParamsT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void LaunchParams::UnPackTo(LaunchParamsT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = gdimx(); _o->gdimx = _e; }
  { auto _e = gdimy(); _o->gdimy = _e; }
  { auto _e = gdimz(); _o->gdimz = _e; }
  { auto _e = bdimx(); _o->bdimx = _e; }
  { auto _e = bdimy(); _o->bdimy = _e; }
  { auto _e = bdimz(); _o->bdimz = _e; }
  { auto _e = smem(); _o->smem = _e; }
  { auto _e = output_sizes(); if (_e) { _o->output_sizes.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->output_sizes[_i]) { _e->Get(_i)->UnPackTo(_o->output_sizes[_i].get(), _resolver); } else { _o->output_sizes[_i] = std::unique_ptr<nvfuser::serde::TensorShapeT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->output_sizes.resize(0); } }
}

inline ::flatbuffers::Offset<LaunchParams> LaunchParams::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const LaunchParamsT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateLaunchParams(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<LaunchParams> CreateLaunchParams(::flatbuffers::FlatBufferBuilder &_fbb, const LaunchParamsT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const LaunchParamsT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _gdimx = _o->gdimx;
  auto _gdimy = _o->gdimy;
  auto _gdimz = _o->gdimz;
  auto _bdimx = _o->bdimx;
  auto _bdimy = _o->bdimy;
  auto _bdimz = _o->bdimz;
  auto _smem = _o->smem;
  auto _output_sizes = _o->output_sizes.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::TensorShape>> (_o->output_sizes.size(), [](size_t i, _VectorArgs *__va) { return CreateTensorShape(*__va->__fbb, __va->__o->output_sizes[i].get(), __va->__rehasher); }, &_va ) : 0;
  return nvfuser::serde::CreateLaunchParams(
      _fbb,
      _gdimx,
      _gdimy,
      _gdimz,
      _bdimx,
      _bdimy,
      _bdimz,
      _smem,
      _output_sizes);
}

inline GlobalBufferInfoT *GlobalBufferInfo::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<GlobalBufferInfoT>(new GlobalBufferInfoT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void GlobalBufferInfo::UnPackTo(GlobalBufferInfoT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = tv(); _o->tv = _e; }
  { auto _e = sizes(); if (_e) { _o->sizes.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->sizes[_i] = _e->Get(_i); } } else { _o->sizes.resize(0); } }
  { auto _e = strides(); if (_e) { _o->strides.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->strides[_i] = _e->Get(_i); } } else { _o->strides.resize(0); } }
  { auto _e = dtype(); _o->dtype = _e; }
  { auto _e = zero_init(); _o->zero_init = _e; }
  { auto _e = is_profile_buffer(); _o->is_profile_buffer = _e; }
  { auto _e = is_fusion_output(); _o->is_fusion_output = _e; }
}

inline ::flatbuffers::Offset<GlobalBufferInfo> GlobalBufferInfo::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const GlobalBufferInfoT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateGlobalBufferInfo(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<GlobalBufferInfo> CreateGlobalBufferInfo(::flatbuffers::FlatBufferBuilder &_fbb, const GlobalBufferInfoT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const GlobalBufferInfoT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _tv = _o->tv;
  auto _sizes = _o->sizes.size() ? _fbb.CreateVector(_o->sizes) : 0;
  auto _strides = _o->strides.size() ? _fbb.CreateVector(_o->strides) : 0;
  auto _dtype = _o->dtype;
  auto _zero_init = _o->zero_init;
  auto _is_profile_buffer = _o->is_profile_buffer;
  auto _is_fusion_output = _o->is_fusion_output;
  return nvfuser::serde::CreateGlobalBufferInfo(
      _fbb,
      _tv,
      _sizes,
      _strides,
      _dtype,
      _zero_init,
      _is_profile_buffer,
      _is_fusion_output);
}

inline ExecutorEntryT::ExecutorEntryT(const ExecutorEntryT &o)
      : init(o.init),
        launch_params((o.launch_params) ? new nvfuser::serde::LaunchParamsT(*o.launch_params) : nullptr),
        output_aliases(o.output_aliases),
        input_aliases(o.input_aliases) {
  outputs.reserve(o.outputs.size());
  for (const auto &outputs_ : o.outputs) { outputs.emplace_back((outputs_) ? new nvfuser::serde::GlobalBufferInfoT(*outputs_) : nullptr); }
  intermediates.reserve(o.intermediates.size());
  for (const auto &intermediates_ : o.intermediates) { intermediates.emplace_back((intermediates_) ? new nvfuser::serde::GlobalBufferInfoT(*intermediates_) : nullptr); }
}

inline ExecutorEntryT &ExecutorEntryT::operator=(ExecutorEntryT o) FLATBUFFERS_NOEXCEPT {
  std::swap(init, o.init);
  std::swap(launch_params, o.launch_params);
  std::swap(output_aliases, o.output_aliases);
  std::swap(input_aliases, o.input_aliases);
  std::swap(outputs, o.outputs);
  std::swap(intermediates, o.intermediates);
  return *this;
}

inline ExecutorEntryT *ExecutorEntry::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<ExecutorEntryT>(new ExecutorEntryT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void ExecutorEntry::UnPackTo(ExecutorEntryT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = init(); _o->init = _e; }
  { auto _e = launch_params(); if (_e) { if(_o->launch_params) { _e->UnPackTo(_o->launch_params.get(), _resolver); } else { _o->launch_params = std::unique_ptr<nvfuser::serde::LaunchParamsT>(_e->UnPack(_resolver)); } } else if (_o->launch_params) { _o->launch_params.reset(); } }
  { auto _e = output_aliases(); if (_e) { _o->output_aliases.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->output_aliases[_i] = _e->Get(_i); } } else { _o->output_aliases.resize(0); } }
  { auto _e = input_aliases(); if (_e) { _o->input_aliases.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->input_aliases[_i] = _e->Get(_i); } } else { _o->input_aliases.resize(0); } }
  { auto _e = outputs(); if (_e) { _o->outputs.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->outputs[_i]) { _e->Get(_i)->UnPackTo(_o->outputs[_i].get(), _resolver); } else { _o->outputs[_i] = std::unique_ptr<nvfuser::serde::GlobalBufferInfoT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->outputs.resize(0); } }
  { auto _e = intermediates(); if (_e) { _o->intermediates.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->intermediates[_i]) { _e->Get(_i)->UnPackTo(_o->intermediates[_i].get(), _resolver); } else { _o->intermediates[_i] = std::unique_ptr<nvfuser::serde::GlobalBufferInfoT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->intermediates.resize(0); } }
}

inline ::flatbuffers::Offset<ExecutorEntry> ExecutorEntry::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ExecutorEntryT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateExecutorEntry(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<ExecutorEntry> CreateExecutorEntry(::flatbuffers::FlatBufferBuilder &_fbb, const ExecutorEntryT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const ExecutorEntryT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _init = _o->init;
  auto _launch_params = _o->launch_params ? CreateLaunchParams(_fbb, _o->launch_params.get(), _rehasher) : 0;
  auto _output_aliases = _o->output_aliases.size() ? _fbb.CreateVector(_o->output_aliases) : 0;
  auto _input_aliases = _o->input_aliases.size() ? _fbb.CreateVector(_o->input_aliases) : 0;
  auto _outputs = _o->outputs.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> (_o->outputs.size(), [](size_t i, _VectorArgs *__va) { return CreateGlobalBufferInfo(*__va->__fbb, __va->__o->outputs[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _intermediates = _o->intermediates.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::GlobalBufferInfo>> (_o->intermediates.size(), [](size_t i, _VectorArgs *__va) { return CreateGlobalBufferInfo(*__va->__fbb, __va->__o->intermediates[i].get(), __va->__rehasher); }, &_va ) : 0;
  return nvfuser::serde::CreateExecutorEntry(
      _fbb,
      _init,
      _launch_params,
      _output_aliases,
      _input_aliases,
      _outputs,
      _intermediates);
}

inline AtT *At::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<AtT>(new AtT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void At::UnPackTo(AtT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = index(); _o->index = _e; }
}

inline ::flatbuffers::Offset<At> At::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const AtT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateAt(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<At> CreateAt(::flatbuffers::FlatBufferBuilder &_fbb, const AtT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const AtT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _index = _o->index;
  return nvfuser::serde::CreateAt(
      _fbb,
      _index);
}

inline BatchNormT *BatchNorm::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<BatchNormT>(new BatchNormT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void BatchNorm::UnPackTo(BatchNormT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = training(); _o->training = _e; }
  { auto _e = channels_last(); _o->channels_last = _e; }
}

inline ::flatbuffers::Offset<BatchNorm> BatchNorm::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const BatchNormT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateBatchNorm(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<BatchNorm> CreateBatchNorm(::flatbuffers::FlatBufferBuilder &_fbb, const BatchNormT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const BatchNormT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _training = _o->training;
  auto _channels_last = _o->channels_last;
  return nvfuser::serde::CreateBatchNorm(
      _fbb,
      _training,
      _channels_last);
}

inline BroadcastT *Broadcast::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<BroadcastT>(new BroadcastT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Broadcast::UnPackTo(BroadcastT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = broadcast_dims(); if (_e) { _o->broadcast_dims.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->broadcast_dims[_i] = _e->Get(_i) != 0; } } else { _o->broadcast_dims.resize(0); } }
}

inline ::flatbuffers::Offset<Broadcast> Broadcast::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const BroadcastT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateBroadcast(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Broadcast> CreateBroadcast(::flatbuffers::FlatBufferBuilder &_fbb, const BroadcastT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const BroadcastT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _broadcast_dims = _o->broadcast_dims.size() ? _fbb.CreateVector(_o->broadcast_dims) : 0;
  return nvfuser::serde::CreateBroadcast(
      _fbb,
      _broadcast_dims);
}

inline BroadcastInDimT *BroadcastInDim::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<BroadcastInDimT>(new BroadcastInDimT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void BroadcastInDim::UnPackTo(BroadcastInDimT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = output_size(); _o->output_size = _e; }
  { auto _e = broadcast_dims(); if (_e) { _o->broadcast_dims.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->broadcast_dims[_i] = _e->Get(_i); } } else { _o->broadcast_dims.resize(0); } }
}

inline ::flatbuffers::Offset<BroadcastInDim> BroadcastInDim::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const BroadcastInDimT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateBroadcastInDim(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<BroadcastInDim> CreateBroadcastInDim(::flatbuffers::FlatBufferBuilder &_fbb, const BroadcastInDimT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const BroadcastInDimT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _output_size = _o->output_size;
  auto _broadcast_dims = _o->broadcast_dims.size() ? _fbb.CreateVector(_o->broadcast_dims) : 0;
  return nvfuser::serde::CreateBroadcastInDim(
      _fbb,
      _output_size,
      _broadcast_dims);
}

inline DtypeT *Dtype::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<DtypeT>(new DtypeT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Dtype::UnPackTo(DtypeT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = dtype(); _o->dtype = _e; }
}

inline ::flatbuffers::Offset<Dtype> Dtype::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const DtypeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateDtype(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Dtype> CreateDtype(::flatbuffers::FlatBufferBuilder &_fbb, const DtypeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const DtypeT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _dtype = _o->dtype;
  return nvfuser::serde::CreateDtype(
      _fbb,
      _dtype);
}

inline DimensionT *Dimension::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<DimensionT>(new DimensionT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Dimension::UnPackTo(DimensionT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = dim(); _o->dim = _e; }
}

inline ::flatbuffers::Offset<Dimension> Dimension::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const DimensionT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateDimension(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Dimension> CreateDimension(::flatbuffers::FlatBufferBuilder &_fbb, const DimensionT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const DimensionT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _dim = _o->dim;
  return nvfuser::serde::CreateDimension(
      _fbb,
      _dim);
}

inline NormT *Norm::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<NormT>(new NormT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Norm::UnPackTo(NormT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = axes(); if (_e) { _o->axes.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->axes[_i] = _e->Get(_i); } } else { _o->axes.resize(0); } }
  { auto _e = correction(); _o->correction = _e; }
  { auto _e = keep_dim(); _o->keep_dim = _e; }
}

inline ::flatbuffers::Offset<Norm> Norm::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const NormT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateNorm(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Norm> CreateNorm(::flatbuffers::FlatBufferBuilder &_fbb, const NormT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const NormT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _axes = _o->axes.size() ? _fbb.CreateVector(_o->axes) : 0;
  auto _correction = _o->correction;
  auto _keep_dim = _o->keep_dim;
  return nvfuser::serde::CreateNorm(
      _fbb,
      _axes,
      _correction,
      _keep_dim);
}

inline OutputT *Output::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<OutputT>(new OutputT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Output::UnPackTo(OutputT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = stride_order(); if (_e) { _o->stride_order.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->stride_order[_i] = _e->Get(_i); } } else { _o->stride_order.resize(0); } }
}

inline ::flatbuffers::Offset<Output> Output::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const OutputT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateOutput(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Output> CreateOutput(::flatbuffers::FlatBufferBuilder &_fbb, const OutputT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const OutputT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _stride_order = _o->stride_order.size() ? _fbb.CreateVector(_o->stride_order) : 0;
  return nvfuser::serde::CreateOutput(
      _fbb,
      _stride_order);
}

inline PadT *Pad::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<PadT>(new PadT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Pad::UnPackTo(PadT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = pad_widths(); if (_e) { _o->pad_widths.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->pad_widths[_i] = _e->Get(_i); } } else { _o->pad_widths.resize(0); } }
}

inline ::flatbuffers::Offset<Pad> Pad::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const PadT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreatePad(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Pad> CreatePad(::flatbuffers::FlatBufferBuilder &_fbb, const PadT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const PadT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _pad_widths = _o->pad_widths.size() ? _fbb.CreateVector(_o->pad_widths) : 0;
  return nvfuser::serde::CreatePad(
      _fbb,
      _pad_widths);
}

inline PermuteT *Permute::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<PermuteT>(new PermuteT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Permute::UnPackTo(PermuteT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = dims(); if (_e) { _o->dims.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->dims[_i] = _e->Get(_i); } } else { _o->dims.resize(0); } }
}

inline ::flatbuffers::Offset<Permute> Permute::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const PermuteT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreatePermute(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Permute> CreatePermute(::flatbuffers::FlatBufferBuilder &_fbb, const PermuteT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const PermuteT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _dims = _o->dims.size() ? _fbb.CreateVector(_o->dims) : 0;
  return nvfuser::serde::CreatePermute(
      _fbb,
      _dims);
}

inline ReductionT *Reduction::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<ReductionT>(new ReductionT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Reduction::UnPackTo(ReductionT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = axes(); if (_e) { _o->axes.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->axes[_i] = _e->Get(_i); } } else { _o->axes.resize(0); } }
  { auto _e = keep_dim(); _o->keep_dim = _e; }
  { auto _e = dtype(); _o->dtype = _e; }
}

inline ::flatbuffers::Offset<Reduction> Reduction::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ReductionT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateReduction(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Reduction> CreateReduction(::flatbuffers::FlatBufferBuilder &_fbb, const ReductionT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const ReductionT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _axes = _o->axes.size() ? _fbb.CreateVector(_o->axes) : 0;
  auto _keep_dim = _o->keep_dim;
  auto _dtype = _o->dtype;
  return nvfuser::serde::CreateReduction(
      _fbb,
      _axes,
      _keep_dim,
      _dtype);
}

inline ReshapeT *Reshape::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<ReshapeT>(new ReshapeT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Reshape::UnPackTo(ReshapeT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = original_shape(); if (_e) { _o->original_shape.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->original_shape[_i] = _e->Get(_i); } } else { _o->original_shape.resize(0); } }
  { auto _e = new_shape(); if (_e) { _o->new_shape.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->new_shape[_i] = _e->Get(_i); } } else { _o->new_shape.resize(0); } }
}

inline ::flatbuffers::Offset<Reshape> Reshape::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ReshapeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateReshape(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Reshape> CreateReshape(::flatbuffers::FlatBufferBuilder &_fbb, const ReshapeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const ReshapeT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _original_shape = _o->original_shape.size() ? _fbb.CreateVector(_o->original_shape) : 0;
  auto _new_shape = _o->new_shape.size() ? _fbb.CreateVector(_o->new_shape) : 0;
  return nvfuser::serde::CreateReshape(
      _fbb,
      _original_shape,
      _new_shape);
}

inline SizeT *Size::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<SizeT>(new SizeT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Size::UnPackTo(SizeT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = dim(); _o->dim = _e; }
}

inline ::flatbuffers::Offset<Size> Size::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const SizeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateSize(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Size> CreateSize(::flatbuffers::FlatBufferBuilder &_fbb, const SizeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const SizeT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _dim = _o->dim;
  return nvfuser::serde::CreateSize(
      _fbb,
      _dim);
}

inline SliceT *Slice::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<SliceT>(new SliceT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Slice::UnPackTo(SliceT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = start_indices(); if (_e) { _o->start_indices.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->start_indices[_i] = _e->Get(_i); } } else { _o->start_indices.resize(0); } }
  { auto _e = end_indices(); if (_e) { _o->end_indices.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->end_indices[_i] = _e->Get(_i); } } else { _o->end_indices.resize(0); } }
  { auto _e = strides(); if (_e) { _o->strides.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->strides[_i] = _e->Get(_i); } } else { _o->strides.resize(0); } }
}

inline ::flatbuffers::Offset<Slice> Slice::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const SliceT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateSlice(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Slice> CreateSlice(::flatbuffers::FlatBufferBuilder &_fbb, const SliceT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const SliceT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _start_indices = _o->start_indices.size() ? _fbb.CreateVector(_o->start_indices) : 0;
  auto _end_indices = _o->end_indices.size() ? _fbb.CreateVector(_o->end_indices) : 0;
  auto _strides = _o->strides.size() ? _fbb.CreateVector(_o->strides) : 0;
  return nvfuser::serde::CreateSlice(
      _fbb,
      _start_indices,
      _end_indices,
      _strides);
}

inline SqueezeT *Squeeze::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<SqueezeT>(new SqueezeT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Squeeze::UnPackTo(SqueezeT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = original_shape(); if (_e) { _o->original_shape.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->original_shape[_i] = _e->Get(_i); } } else { _o->original_shape.resize(0); } }
  { auto _e = squeeze_dims(); if (_e) { _o->squeeze_dims.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->squeeze_dims[_i] = _e->Get(_i); } } else { _o->squeeze_dims.resize(0); } }
}

inline ::flatbuffers::Offset<Squeeze> Squeeze::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const SqueezeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateSqueeze(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Squeeze> CreateSqueeze(::flatbuffers::FlatBufferBuilder &_fbb, const SqueezeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const SqueezeT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _original_shape = _o->original_shape.size() ? _fbb.CreateVector(_o->original_shape) : 0;
  auto _squeeze_dims = _o->squeeze_dims.size() ? _fbb.CreateVector(_o->squeeze_dims) : 0;
  return nvfuser::serde::CreateSqueeze(
      _fbb,
      _original_shape,
      _squeeze_dims);
}

inline TensorT *Tensor::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<TensorT>(new TensorT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Tensor::UnPackTo(TensorT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = sizes(); if (_e) { _o->sizes.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->sizes[_i] = _e->Get(_i); } } else { _o->sizes.resize(0); } }
  { auto _e = contiguity(); if (_e) { _o->contiguity.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->contiguity[_i] = static_cast<nvfuser::serde::Contiguity>(_e->Get(_i)); } } else { _o->contiguity.resize(0); } }
  { auto _e = dtype(); _o->dtype = _e; }
  { auto _e = is_cpu(); _o->is_cpu = _e; }
}

inline ::flatbuffers::Offset<Tensor> Tensor::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TensorT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateTensor(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Tensor> CreateTensor(::flatbuffers::FlatBufferBuilder &_fbb, const TensorT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const TensorT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _sizes = _o->sizes.size() ? _fbb.CreateVector(_o->sizes) : 0;
  auto _contiguity = _o->contiguity.size() ? _fbb.CreateVectorScalarCast<int32_t>(::flatbuffers::data(_o->contiguity), _o->contiguity.size()) : 0;
  auto _dtype = _o->dtype;
  auto _is_cpu = _o->is_cpu;
  return nvfuser::serde::CreateTensor(
      _fbb,
      _sizes,
      _contiguity,
      _dtype,
      _is_cpu);
}

inline TensorCreationT *TensorCreation::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<TensorCreationT>(new TensorCreationT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void TensorCreation::UnPackTo(TensorCreationT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = shape(); if (_e) { _o->shape.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->shape[_i] = _e->Get(_i); } } else { _o->shape.resize(0); } }
  { auto _e = dtype(); _o->dtype = _e; }
}

inline ::flatbuffers::Offset<TensorCreation> TensorCreation::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TensorCreationT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateTensorCreation(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<TensorCreation> CreateTensorCreation(::flatbuffers::FlatBufferBuilder &_fbb, const TensorCreationT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const TensorCreationT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _shape = _o->shape.size() ? _fbb.CreateVector(_o->shape) : 0;
  auto _dtype = _o->dtype;
  return nvfuser::serde::CreateTensorCreation(
      _fbb,
      _shape,
      _dtype);
}

inline TensorCreationSymbolicT *TensorCreationSymbolic::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<TensorCreationSymbolicT>(new TensorCreationSymbolicT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void TensorCreationSymbolic::UnPackTo(TensorCreationSymbolicT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = shape(); if (_e) { _o->shape.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->shape[_i] = *_e->Get(_i); } } else { _o->shape.resize(0); } }
  { auto _e = dtype(); _o->dtype = _e; }
}

inline ::flatbuffers::Offset<TensorCreationSymbolic> TensorCreationSymbolic::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TensorCreationSymbolicT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateTensorCreationSymbolic(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<TensorCreationSymbolic> CreateTensorCreationSymbolic(::flatbuffers::FlatBufferBuilder &_fbb, const TensorCreationSymbolicT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const TensorCreationSymbolicT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _shape = _o->shape.size() ? _fbb.CreateVectorOfStructs(_o->shape) : 0;
  auto _dtype = _o->dtype;
  return nvfuser::serde::CreateTensorCreationSymbolic(
      _fbb,
      _shape,
      _dtype);
}

inline VectorT *Vector::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<VectorT>(new VectorT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void Vector::UnPackTo(VectorT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = dtype(); _o->dtype = _e; }
}

inline ::flatbuffers::Offset<Vector> Vector::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const VectorT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateVector(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<Vector> CreateVector(::flatbuffers::FlatBufferBuilder &_fbb, const VectorT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const VectorT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _dtype = _o->dtype;
  return nvfuser::serde::CreateVector(
      _fbb,
      _dtype);
}

inline CudaKernelT *CudaKernel::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<CudaKernelT>(new CudaKernelT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void CudaKernel::UnPackTo(CudaKernelT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = kernel_name(); if (_e) _o->kernel_name = _e->str(); }
  { auto _e = object_code(); if (_e) { _o->object_code.resize(_e->size()); std::copy(_e->begin(), _e->end(), _o->object_code.begin()); } }
}

inline ::flatbuffers::Offset<CudaKernel> CudaKernel::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const CudaKernelT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateCudaKernel(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<CudaKernel> CreateCudaKernel(::flatbuffers::FlatBufferBuilder &_fbb, const CudaKernelT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const CudaKernelT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _kernel_name = _o->kernel_name.empty() ? 0 : _fbb.CreateString(_o->kernel_name);
  auto _object_code = _o->object_code.size() ? _fbb.CreateVector(_o->object_code) : 0;
  return nvfuser::serde::CreateCudaKernel(
      _fbb,
      _kernel_name,
      _object_code);
}

inline FusionExecutorT::FusionExecutorT(const FusionExecutorT &o)
      : device_smem_limit(o.device_smem_limit),
        block_size_high_water_mark(o.block_size_high_water_mark),
        maxrregcount_high_water_mark(o.maxrregcount_high_water_mark),
        warp_size(o.warp_size),
        fusion_id(o.fusion_id),
        fusion_id_counter(o.fusion_id_counter),
        kernel_code(o.kernel_code),
        executor_entry_lookup_keys(o.executor_entry_lookup_keys),
        index_type(o.index_type),
        compiled_kernel((o.compiled_kernel) ? new nvfuser::serde::CudaKernelT(*o.compiled_kernel) : nullptr) {
  executor_entry_lookup_values.reserve(o.executor_entry_lookup_values.size());
  for (const auto &executor_entry_lookup_values_ : o.executor_entry_lookup_values) { executor_entry_lookup_values.emplace_back((executor_entry_lookup_values_) ? new nvfuser::serde::ExecutorEntryT(*executor_entry_lookup_values_) : nullptr); }
}

inline FusionExecutorT &FusionExecutorT::operator=(FusionExecutorT o) FLATBUFFERS_NOEXCEPT {
  std::swap(device_smem_limit, o.device_smem_limit);
  std::swap(block_size_high_water_mark, o.block_size_high_water_mark);
  std::swap(maxrregcount_high_water_mark, o.maxrregcount_high_water_mark);
  std::swap(warp_size, o.warp_size);
  std::swap(fusion_id, o.fusion_id);
  std::swap(fusion_id_counter, o.fusion_id_counter);
  std::swap(kernel_code, o.kernel_code);
  std::swap(executor_entry_lookup_keys, o.executor_entry_lookup_keys);
  std::swap(executor_entry_lookup_values, o.executor_entry_lookup_values);
  std::swap(index_type, o.index_type);
  std::swap(compiled_kernel, o.compiled_kernel);
  return *this;
}

inline FusionExecutorT *FusionExecutor::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<FusionExecutorT>(new FusionExecutorT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void FusionExecutor::UnPackTo(FusionExecutorT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = device_smem_limit(); _o->device_smem_limit = _e; }
  { auto _e = block_size_high_water_mark(); _o->block_size_high_water_mark = _e; }
  { auto _e = maxrregcount_high_water_mark(); _o->maxrregcount_high_water_mark = _e; }
  { auto _e = warp_size(); _o->warp_size = _e; }
  { auto _e = fusion_id(); _o->fusion_id = _e; }
  { auto _e = fusion_id_counter(); _o->fusion_id_counter = _e; }
  { auto _e = kernel_code(); if (_e) _o->kernel_code = _e->str(); }
  { auto _e = executor_entry_lookup_keys(); if (_e) { _o->executor_entry_lookup_keys.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->executor_entry_lookup_keys[_i] = _e->Get(_i); } } else { _o->executor_entry_lookup_keys.resize(0); } }
  { auto _e = executor_entry_lookup_values(); if (_e) { _o->executor_entry_lookup_values.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->executor_entry_lookup_values[_i]) { _e->Get(_i)->UnPackTo(_o->executor_entry_lookup_values[_i].get(), _resolver); } else { _o->executor_entry_lookup_values[_i] = std::unique_ptr<nvfuser::serde::ExecutorEntryT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->executor_entry_lookup_values.resize(0); } }
  { auto _e = index_type(); _o->index_type = _e; }
  { auto _e = compiled_kernel(); if (_e) { if(_o->compiled_kernel) { _e->UnPackTo(_o->compiled_kernel.get(), _resolver); } else { _o->compiled_kernel = std::unique_ptr<nvfuser::serde::CudaKernelT>(_e->UnPack(_resolver)); } } else if (_o->compiled_kernel) { _o->compiled_kernel.reset(); } }
}

inline ::flatbuffers::Offset<FusionExecutor> FusionExecutor::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const FusionExecutorT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateFusionExecutor(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<FusionExecutor> CreateFusionExecutor(::flatbuffers::FlatBufferBuilder &_fbb, const FusionExecutorT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const FusionExecutorT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _device_smem_limit = _o->device_smem_limit;
  auto _block_size_high_water_mark = _o->block_size_high_water_mark;
  auto _maxrregcount_high_water_mark = _o->maxrregcount_high_water_mark;
  auto _warp_size = _o->warp_size;
  auto _fusion_id = _o->fusion_id;
  auto _fusion_id_counter = _o->fusion_id_counter;
  auto _kernel_code = _o->kernel_code.empty() ? 0 : _fbb.CreateString(_o->kernel_code);
  auto _executor_entry_lookup_keys = _o->executor_entry_lookup_keys.size() ? _fbb.CreateVector(_o->executor_entry_lookup_keys) : 0;
  auto _executor_entry_lookup_values = _o->executor_entry_lookup_values.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::ExecutorEntry>> (_o->executor_entry_lookup_values.size(), [](size_t i, _VectorArgs *__va) { return CreateExecutorEntry(*__va->__fbb, __va->__o->executor_entry_lookup_values[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _index_type = _o->index_type;
  auto _compiled_kernel = _o->compiled_kernel ? CreateCudaKernel(_fbb, _o->compiled_kernel.get(), _rehasher) : 0;
  return nvfuser::serde::CreateFusionExecutor(
      _fbb,
      _device_smem_limit,
      _block_size_high_water_mark,
      _maxrregcount_high_water_mark,
      _warp_size,
      _fusion_id,
      _fusion_id_counter,
      _kernel_code,
      _executor_entry_lookup_keys,
      _executor_entry_lookup_values,
      _index_type,
      _compiled_kernel);
}

inline FusionKernelRuntimeT::FusionKernelRuntimeT(const FusionKernelRuntimeT &o)
      : args((o.args) ? new nvfuser::serde::KernelArgumentHolderT(*o.args) : nullptr) {
  executors.reserve(o.executors.size());
  for (const auto &executors_ : o.executors) { executors.emplace_back((executors_) ? new nvfuser::serde::FusionExecutorT(*executors_) : nullptr); }
}

inline FusionKernelRuntimeT &FusionKernelRuntimeT::operator=(FusionKernelRuntimeT o) FLATBUFFERS_NOEXCEPT {
  std::swap(args, o.args);
  std::swap(executors, o.executors);
  return *this;
}

inline FusionKernelRuntimeT *FusionKernelRuntime::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<FusionKernelRuntimeT>(new FusionKernelRuntimeT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void FusionKernelRuntime::UnPackTo(FusionKernelRuntimeT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = args(); if (_e) { if(_o->args) { _e->UnPackTo(_o->args.get(), _resolver); } else { _o->args = std::unique_ptr<nvfuser::serde::KernelArgumentHolderT>(_e->UnPack(_resolver)); } } else if (_o->args) { _o->args.reset(); } }
  { auto _e = executors(); if (_e) { _o->executors.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->executors[_i]) { _e->Get(_i)->UnPackTo(_o->executors[_i].get(), _resolver); } else { _o->executors[_i] = std::unique_ptr<nvfuser::serde::FusionExecutorT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->executors.resize(0); } }
}

inline ::flatbuffers::Offset<FusionKernelRuntime> FusionKernelRuntime::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const FusionKernelRuntimeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateFusionKernelRuntime(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<FusionKernelRuntime> CreateFusionKernelRuntime(::flatbuffers::FlatBufferBuilder &_fbb, const FusionKernelRuntimeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const FusionKernelRuntimeT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _args = _o->args ? CreateKernelArgumentHolder(_fbb, _o->args.get(), _rehasher) : 0;
  auto _executors = _o->executors.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::FusionExecutor>> (_o->executors.size(), [](size_t i, _VectorArgs *__va) { return CreateFusionExecutor(*__va->__fbb, __va->__o->executors[i].get(), __va->__rehasher); }, &_va ) : 0;
  return nvfuser::serde::CreateFusionKernelRuntime(
      _fbb,
      _args,
      _executors);
}

inline InputsIdLookupT *InputsIdLookup::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<InputsIdLookupT>(new InputsIdLookupT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void InputsIdLookup::UnPackTo(InputsIdLookupT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = max_cache_size(); _o->max_cache_size = _e; }
  { auto _e = current_id(); _o->current_id = _e; }
  { auto _e = lru_cache(); if (_e) { _o->lru_cache.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->lru_cache[_i] = _e->Get(_i)->str(); } } else { _o->lru_cache.resize(0); } }
  { auto _e = encoding_lookup_keys(); if (_e) { _o->encoding_lookup_keys.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->encoding_lookup_keys[_i] = _e->Get(_i)->str(); } } else { _o->encoding_lookup_keys.resize(0); } }
  { auto _e = encoding_lookup_values(); if (_e) { _o->encoding_lookup_values.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->encoding_lookup_values[_i] = *_e->Get(_i); } } else { _o->encoding_lookup_values.resize(0); } }
}

inline ::flatbuffers::Offset<InputsIdLookup> InputsIdLookup::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const InputsIdLookupT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateInputsIdLookup(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<InputsIdLookup> CreateInputsIdLookup(::flatbuffers::FlatBufferBuilder &_fbb, const InputsIdLookupT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const InputsIdLookupT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _max_cache_size = _o->max_cache_size;
  auto _current_id = _o->current_id;
  auto _lru_cache = _o->lru_cache.size() ? _fbb.CreateVectorOfStrings(_o->lru_cache) : 0;
  auto _encoding_lookup_keys = _o->encoding_lookup_keys.size() ? _fbb.CreateVectorOfStrings(_o->encoding_lookup_keys) : 0;
  auto _encoding_lookup_values = _o->encoding_lookup_values.size() ? _fbb.CreateVectorOfStructs(_o->encoding_lookup_values) : 0;
  return nvfuser::serde::CreateInputsIdLookup(
      _fbb,
      _max_cache_size,
      _current_id,
      _lru_cache,
      _encoding_lookup_keys,
      _encoding_lookup_values);
}

inline KernelRuntimeStateT::KernelRuntimeStateT(const KernelRuntimeStateT &o)
      : device_id(o.device_id),
        has_dynamic_transform_info(o.has_dynamic_transform_info) {
  runtimes.reserve(o.runtimes.size());
  for (const auto &runtimes_ : o.runtimes) { runtimes.emplace_back((runtimes_) ? new nvfuser::serde::FusionKernelRuntimeT(*runtimes_) : nullptr); }
}

inline KernelRuntimeStateT &KernelRuntimeStateT::operator=(KernelRuntimeStateT o) FLATBUFFERS_NOEXCEPT {
  std::swap(device_id, o.device_id);
  std::swap(has_dynamic_transform_info, o.has_dynamic_transform_info);
  std::swap(runtimes, o.runtimes);
  return *this;
}

inline KernelRuntimeStateT *KernelRuntimeState::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<KernelRuntimeStateT>(new KernelRuntimeStateT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void KernelRuntimeState::UnPackTo(KernelRuntimeStateT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = device_id(); _o->device_id = _e; }
  { auto _e = has_dynamic_transform_info(); _o->has_dynamic_transform_info = _e; }
  { auto _e = runtimes(); if (_e) { _o->runtimes.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->runtimes[_i]) { _e->Get(_i)->UnPackTo(_o->runtimes[_i].get(), _resolver); } else { _o->runtimes[_i] = std::unique_ptr<nvfuser::serde::FusionKernelRuntimeT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->runtimes.resize(0); } }
}

inline ::flatbuffers::Offset<KernelRuntimeState> KernelRuntimeState::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const KernelRuntimeStateT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateKernelRuntimeState(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<KernelRuntimeState> CreateKernelRuntimeState(::flatbuffers::FlatBufferBuilder &_fbb, const KernelRuntimeStateT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const KernelRuntimeStateT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _device_id = _o->device_id;
  auto _has_dynamic_transform_info = _o->has_dynamic_transform_info;
  auto _runtimes = _o->runtimes.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::FusionKernelRuntime>> (_o->runtimes.size(), [](size_t i, _VectorArgs *__va) { return CreateFusionKernelRuntime(*__va->__fbb, __va->__o->runtimes[i].get(), __va->__rehasher); }, &_va ) : 0;
  return nvfuser::serde::CreateKernelRuntimeState(
      _fbb,
      _device_id,
      _has_dynamic_transform_info,
      _runtimes);
}

inline FusionExecutorCacheT::FusionExecutorCacheT(const FusionExecutorCacheT &o)
      : inputs_cache((o.inputs_cache) ? new nvfuser::serde::InputsIdLookupT(*o.inputs_cache) : nullptr),
        kernel_cache_keys(o.kernel_cache_keys),
        kernel_cache_values(o.kernel_cache_values) {
  kernel_runtimes_map.reserve(o.kernel_runtimes_map.size());
  for (const auto &kernel_runtimes_map_ : o.kernel_runtimes_map) { kernel_runtimes_map.emplace_back((kernel_runtimes_map_) ? new nvfuser::serde::KernelRuntimeStateT(*kernel_runtimes_map_) : nullptr); }
}

inline FusionExecutorCacheT &FusionExecutorCacheT::operator=(FusionExecutorCacheT o) FLATBUFFERS_NOEXCEPT {
  std::swap(inputs_cache, o.inputs_cache);
  std::swap(kernel_runtimes_map, o.kernel_runtimes_map);
  std::swap(kernel_cache_keys, o.kernel_cache_keys);
  std::swap(kernel_cache_values, o.kernel_cache_values);
  return *this;
}

inline FusionExecutorCacheT *FusionExecutorCache::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<FusionExecutorCacheT>(new FusionExecutorCacheT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void FusionExecutorCache::UnPackTo(FusionExecutorCacheT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = inputs_cache(); if (_e) { if(_o->inputs_cache) { _e->UnPackTo(_o->inputs_cache.get(), _resolver); } else { _o->inputs_cache = std::unique_ptr<nvfuser::serde::InputsIdLookupT>(_e->UnPack(_resolver)); } } else if (_o->inputs_cache) { _o->inputs_cache.reset(); } }
  { auto _e = kernel_runtimes_map(); if (_e) { _o->kernel_runtimes_map.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->kernel_runtimes_map[_i]) { _e->Get(_i)->UnPackTo(_o->kernel_runtimes_map[_i].get(), _resolver); } else { _o->kernel_runtimes_map[_i] = std::unique_ptr<nvfuser::serde::KernelRuntimeStateT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->kernel_runtimes_map.resize(0); } }
  { auto _e = kernel_cache_keys(); if (_e) { _o->kernel_cache_keys.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->kernel_cache_keys[_i] = _e->Get(_i); } } else { _o->kernel_cache_keys.resize(0); } }
  { auto _e = kernel_cache_values(); if (_e) { _o->kernel_cache_values.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->kernel_cache_values[_i] = _e->Get(_i); } } else { _o->kernel_cache_values.resize(0); } }
}

inline ::flatbuffers::Offset<FusionExecutorCache> FusionExecutorCache::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const FusionExecutorCacheT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateFusionExecutorCache(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<FusionExecutorCache> CreateFusionExecutorCache(::flatbuffers::FlatBufferBuilder &_fbb, const FusionExecutorCacheT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const FusionExecutorCacheT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _inputs_cache = _o->inputs_cache ? CreateInputsIdLookup(_fbb, _o->inputs_cache.get(), _rehasher) : 0;
  auto _kernel_runtimes_map = _o->kernel_runtimes_map.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::KernelRuntimeState>> (_o->kernel_runtimes_map.size(), [](size_t i, _VectorArgs *__va) { return CreateKernelRuntimeState(*__va->__fbb, __va->__o->kernel_runtimes_map[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _kernel_cache_keys = _o->kernel_cache_keys.size() ? _fbb.CreateVector(_o->kernel_cache_keys) : 0;
  auto _kernel_cache_values = _o->kernel_cache_values.size() ? _fbb.CreateVector(_o->kernel_cache_values) : 0;
  return nvfuser::serde::CreateFusionExecutorCache(
      _fbb,
      _inputs_cache,
      _kernel_runtimes_map,
      _kernel_cache_keys,
      _kernel_cache_values);
}

inline RecordFunctorT *RecordFunctor::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<RecordFunctorT>(new RecordFunctorT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void RecordFunctor::UnPackTo(RecordFunctorT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = args(); if (_e) { _o->args.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->args[_i] = *_e->Get(_i); } } else { _o->args.resize(0); } }
  { auto _e = outputs(); if (_e) { _o->outputs.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->outputs[_i] = *_e->Get(_i); } } else { _o->outputs.resize(0); } }
  { auto _e = name(); if (_e) _o->name = _e->str(); }
  { auto _e = type(); _o->type = _e; }
  { auto _e = data_type(); _o->data.type = _e; }
  { auto _e = data(); if (_e) _o->data.value = nvfuser::serde::RecordDataUnion::UnPack(_e, data_type(), _resolver); }
}

inline ::flatbuffers::Offset<RecordFunctor> RecordFunctor::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const RecordFunctorT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateRecordFunctor(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<RecordFunctor> CreateRecordFunctor(::flatbuffers::FlatBufferBuilder &_fbb, const RecordFunctorT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const RecordFunctorT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _args = _o->args.size() ? _fbb.CreateVectorOfStructs(_o->args) : 0;
  auto _outputs = _o->outputs.size() ? _fbb.CreateVectorOfStructs(_o->outputs) : 0;
  auto _name = _o->name.empty() ? 0 : _fbb.CreateString(_o->name);
  auto _type = _o->type;
  auto _data_type = _o->data.type;
  auto _data = _o->data.Pack(_fbb);
  return nvfuser::serde::CreateRecordFunctor(
      _fbb,
      _args,
      _outputs,
      _name,
      _type,
      _data_type,
      _data);
}

inline TrieNodeT::TrieNodeT(const TrieNodeT &o)
      : record((o.record) ? new nvfuser::serde::RecordFunctorT(*o.record) : nullptr),
        children(o.children),
        fusion_id(o.fusion_id),
        visits(o.visits),
        is_terminal(o.is_terminal) {
}

inline TrieNodeT &TrieNodeT::operator=(TrieNodeT o) FLATBUFFERS_NOEXCEPT {
  std::swap(record, o.record);
  std::swap(children, o.children);
  std::swap(fusion_id, o.fusion_id);
  std::swap(visits, o.visits);
  std::swap(is_terminal, o.is_terminal);
  return *this;
}

inline TrieNodeT *TrieNode::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<TrieNodeT>(new TrieNodeT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void TrieNode::UnPackTo(TrieNodeT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = record(); if (_e) { if(_o->record) { _e->UnPackTo(_o->record.get(), _resolver); } else { _o->record = std::unique_ptr<nvfuser::serde::RecordFunctorT>(_e->UnPack(_resolver)); } } else if (_o->record) { _o->record.reset(); } }
  { auto _e = children(); if (_e) { _o->children.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->children[_i] = _e->Get(_i); } } else { _o->children.resize(0); } }
  { auto _e = fusion_id(); _o->fusion_id = _e; }
  { auto _e = visits(); _o->visits = _e; }
  { auto _e = is_terminal(); _o->is_terminal = _e; }
}

inline ::flatbuffers::Offset<TrieNode> TrieNode::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const TrieNodeT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateTrieNode(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<TrieNode> CreateTrieNode(::flatbuffers::FlatBufferBuilder &_fbb, const TrieNodeT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const TrieNodeT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _record = _o->record ? CreateRecordFunctor(_fbb, _o->record.get(), _rehasher) : 0;
  auto _children = _o->children.size() ? _fbb.CreateVector(_o->children) : 0;
  auto _fusion_id = _o->fusion_id;
  auto _visits = _o->visits;
  auto _is_terminal = _o->is_terminal;
  return nvfuser::serde::CreateTrieNode(
      _fbb,
      _record,
      _children,
      _fusion_id,
      _visits,
      _is_terminal);
}

inline FusionCacheT::FusionCacheT(const FusionCacheT &o)
      : max_fusions(o.max_fusions),
        terminal_nodes(o.terminal_nodes) {
  structure.reserve(o.structure.size());
  for (const auto &structure_ : o.structure) { structure.emplace_back((structure_) ? new nvfuser::serde::TrieNodeT(*structure_) : nullptr); }
  auto_gen_schedules.reserve(o.auto_gen_schedules.size());
  for (const auto &auto_gen_schedules_ : o.auto_gen_schedules) { auto_gen_schedules.emplace_back((auto_gen_schedules_) ? new nvfuser::serde::FusionExecutorCacheT(*auto_gen_schedules_) : nullptr); }
}

inline FusionCacheT &FusionCacheT::operator=(FusionCacheT o) FLATBUFFERS_NOEXCEPT {
  std::swap(max_fusions, o.max_fusions);
  std::swap(structure, o.structure);
  std::swap(terminal_nodes, o.terminal_nodes);
  std::swap(auto_gen_schedules, o.auto_gen_schedules);
  return *this;
}

inline FusionCacheT *FusionCache::UnPack(const ::flatbuffers::resolver_function_t *_resolver) const {
  auto _o = std::unique_ptr<FusionCacheT>(new FusionCacheT());
  UnPackTo(_o.get(), _resolver);
  return _o.release();
}

inline void FusionCache::UnPackTo(FusionCacheT *_o, const ::flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = max_fusions(); _o->max_fusions = _e; }
  { auto _e = structure(); if (_e) { _o->structure.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->structure[_i]) { _e->Get(_i)->UnPackTo(_o->structure[_i].get(), _resolver); } else { _o->structure[_i] = std::unique_ptr<nvfuser::serde::TrieNodeT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->structure.resize(0); } }
  { auto _e = terminal_nodes(); if (_e) { _o->terminal_nodes.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->terminal_nodes[_i] = _e->Get(_i); } } else { _o->terminal_nodes.resize(0); } }
  { auto _e = auto_gen_schedules(); if (_e) { _o->auto_gen_schedules.resize(_e->size()); for (::flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { if(_o->auto_gen_schedules[_i]) { _e->Get(_i)->UnPackTo(_o->auto_gen_schedules[_i].get(), _resolver); } else { _o->auto_gen_schedules[_i] = std::unique_ptr<nvfuser::serde::FusionExecutorCacheT>(_e->Get(_i)->UnPack(_resolver)); }; } } else { _o->auto_gen_schedules.resize(0); } }
}

inline ::flatbuffers::Offset<FusionCache> FusionCache::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const FusionCacheT* _o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  return CreateFusionCache(_fbb, _o, _rehasher);
}

inline ::flatbuffers::Offset<FusionCache> CreateFusionCache(::flatbuffers::FlatBufferBuilder &_fbb, const FusionCacheT *_o, const ::flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { ::flatbuffers::FlatBufferBuilder *__fbb; const FusionCacheT* __o; const ::flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _max_fusions = _o->max_fusions;
  auto _structure = _o->structure.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::TrieNode>> (_o->structure.size(), [](size_t i, _VectorArgs *__va) { return CreateTrieNode(*__va->__fbb, __va->__o->structure[i].get(), __va->__rehasher); }, &_va ) : 0;
  auto _terminal_nodes = _o->terminal_nodes.size() ? _fbb.CreateVector(_o->terminal_nodes) : 0;
  auto _auto_gen_schedules = _o->auto_gen_schedules.size() ? _fbb.CreateVector<::flatbuffers::Offset<nvfuser::serde::FusionExecutorCache>> (_o->auto_gen_schedules.size(), [](size_t i, _VectorArgs *__va) { return CreateFusionExecutorCache(*__va->__fbb, __va->__o->auto_gen_schedules[i].get(), __va->__rehasher); }, &_va ) : 0;
  return nvfuser::serde::CreateFusionCache(
      _fbb,
      _max_fusions,
      _structure,
      _terminal_nodes,
      _auto_gen_schedules);
}

inline bool VerifyRecordData(::flatbuffers::Verifier &verifier, const void *obj, RecordData type) {
  switch (type) {
    case RecordData_NONE: {
      return true;
    }
    case RecordData_At: {
      auto ptr = reinterpret_cast<const nvfuser::serde::At *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_BatchNorm: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BatchNorm *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Broadcast: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Broadcast *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_BroadcastInDim: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BroadcastInDim *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Dimension: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Dimension *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Dtype: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Dtype *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Norm: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Norm *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Output: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Output *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Pad: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Pad *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Permute: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Permute *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Slice: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Slice *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Squeeze: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Squeeze *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Reduction: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Reduction *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Reshape: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Reshape *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Scalar *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Size: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Size *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Tensor: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Tensor *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_TensorCreation: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorCreation *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_TensorCreationSymbolic: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorCreationSymbolic *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case RecordData_Vector: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Vector *>(obj);
      return verifier.VerifyTable(ptr);
    }
    default: return true;
  }
}

inline bool VerifyRecordDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<uint8_t> *types) {
  if (!values || !types) return !values && !types;
  if (values->size() != types->size()) return false;
  for (::flatbuffers::uoffset_t i = 0; i < values->size(); ++i) {
    if (!VerifyRecordData(
        verifier,  values->Get(i), types->GetEnum<RecordData>(i))) {
      return false;
    }
  }
  return true;
}

inline void *RecordDataUnion::UnPack(const void *obj, RecordData type, const ::flatbuffers::resolver_function_t *resolver) {
  (void)resolver;
  switch (type) {
    case RecordData_At: {
      auto ptr = reinterpret_cast<const nvfuser::serde::At *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_BatchNorm: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BatchNorm *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Broadcast: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Broadcast *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_BroadcastInDim: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BroadcastInDim *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Dimension: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Dimension *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Dtype: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Dtype *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Norm: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Norm *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Output: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Output *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Pad: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Pad *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Permute: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Permute *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Slice: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Slice *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Squeeze: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Squeeze *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Reduction: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Reduction *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Reshape: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Reshape *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Scalar *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Size: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Size *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Tensor: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Tensor *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_TensorCreation: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorCreation *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_TensorCreationSymbolic: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorCreationSymbolic *>(obj);
      return ptr->UnPack(resolver);
    }
    case RecordData_Vector: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Vector *>(obj);
      return ptr->UnPack(resolver);
    }
    default: return nullptr;
  }
}

inline ::flatbuffers::Offset<void> RecordDataUnion::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ::flatbuffers::rehasher_function_t *_rehasher) const {
  (void)_rehasher;
  switch (type) {
    case RecordData_At: {
      auto ptr = reinterpret_cast<const nvfuser::serde::AtT *>(value);
      return CreateAt(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_BatchNorm: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BatchNormT *>(value);
      return CreateBatchNorm(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Broadcast: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BroadcastT *>(value);
      return CreateBroadcast(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_BroadcastInDim: {
      auto ptr = reinterpret_cast<const nvfuser::serde::BroadcastInDimT *>(value);
      return CreateBroadcastInDim(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Dimension: {
      auto ptr = reinterpret_cast<const nvfuser::serde::DimensionT *>(value);
      return CreateDimension(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Dtype: {
      auto ptr = reinterpret_cast<const nvfuser::serde::DtypeT *>(value);
      return CreateDtype(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Norm: {
      auto ptr = reinterpret_cast<const nvfuser::serde::NormT *>(value);
      return CreateNorm(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Output: {
      auto ptr = reinterpret_cast<const nvfuser::serde::OutputT *>(value);
      return CreateOutput(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Pad: {
      auto ptr = reinterpret_cast<const nvfuser::serde::PadT *>(value);
      return CreatePad(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Permute: {
      auto ptr = reinterpret_cast<const nvfuser::serde::PermuteT *>(value);
      return CreatePermute(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Slice: {
      auto ptr = reinterpret_cast<const nvfuser::serde::SliceT *>(value);
      return CreateSlice(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Squeeze: {
      auto ptr = reinterpret_cast<const nvfuser::serde::SqueezeT *>(value);
      return CreateSqueeze(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Reduction: {
      auto ptr = reinterpret_cast<const nvfuser::serde::ReductionT *>(value);
      return CreateReduction(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Reshape: {
      auto ptr = reinterpret_cast<const nvfuser::serde::ReshapeT *>(value);
      return CreateReshape(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::ScalarT *>(value);
      return CreateScalar(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Size: {
      auto ptr = reinterpret_cast<const nvfuser::serde::SizeT *>(value);
      return CreateSize(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Tensor: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorT *>(value);
      return CreateTensor(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_TensorCreation: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorCreationT *>(value);
      return CreateTensorCreation(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_TensorCreationSymbolic: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorCreationSymbolicT *>(value);
      return CreateTensorCreationSymbolic(_fbb, ptr, _rehasher).Union();
    }
    case RecordData_Vector: {
      auto ptr = reinterpret_cast<const nvfuser::serde::VectorT *>(value);
      return CreateVector(_fbb, ptr, _rehasher).Union();
    }
    default: return 0;
  }
}

inline RecordDataUnion::RecordDataUnion(const RecordDataUnion &u) : type(u.type), value(nullptr) {
  switch (type) {
    case RecordData_At: {
      value = new nvfuser::serde::AtT(*reinterpret_cast<nvfuser::serde::AtT *>(u.value));
      break;
    }
    case RecordData_BatchNorm: {
      value = new nvfuser::serde::BatchNormT(*reinterpret_cast<nvfuser::serde::BatchNormT *>(u.value));
      break;
    }
    case RecordData_Broadcast: {
      value = new nvfuser::serde::BroadcastT(*reinterpret_cast<nvfuser::serde::BroadcastT *>(u.value));
      break;
    }
    case RecordData_BroadcastInDim: {
      value = new nvfuser::serde::BroadcastInDimT(*reinterpret_cast<nvfuser::serde::BroadcastInDimT *>(u.value));
      break;
    }
    case RecordData_Dimension: {
      value = new nvfuser::serde::DimensionT(*reinterpret_cast<nvfuser::serde::DimensionT *>(u.value));
      break;
    }
    case RecordData_Dtype: {
      value = new nvfuser::serde::DtypeT(*reinterpret_cast<nvfuser::serde::DtypeT *>(u.value));
      break;
    }
    case RecordData_Norm: {
      value = new nvfuser::serde::NormT(*reinterpret_cast<nvfuser::serde::NormT *>(u.value));
      break;
    }
    case RecordData_Output: {
      value = new nvfuser::serde::OutputT(*reinterpret_cast<nvfuser::serde::OutputT *>(u.value));
      break;
    }
    case RecordData_Pad: {
      value = new nvfuser::serde::PadT(*reinterpret_cast<nvfuser::serde::PadT *>(u.value));
      break;
    }
    case RecordData_Permute: {
      value = new nvfuser::serde::PermuteT(*reinterpret_cast<nvfuser::serde::PermuteT *>(u.value));
      break;
    }
    case RecordData_Slice: {
      value = new nvfuser::serde::SliceT(*reinterpret_cast<nvfuser::serde::SliceT *>(u.value));
      break;
    }
    case RecordData_Squeeze: {
      value = new nvfuser::serde::SqueezeT(*reinterpret_cast<nvfuser::serde::SqueezeT *>(u.value));
      break;
    }
    case RecordData_Reduction: {
      value = new nvfuser::serde::ReductionT(*reinterpret_cast<nvfuser::serde::ReductionT *>(u.value));
      break;
    }
    case RecordData_Reshape: {
      value = new nvfuser::serde::ReshapeT(*reinterpret_cast<nvfuser::serde::ReshapeT *>(u.value));
      break;
    }
    case RecordData_Scalar: {
      value = new nvfuser::serde::ScalarT(*reinterpret_cast<nvfuser::serde::ScalarT *>(u.value));
      break;
    }
    case RecordData_Size: {
      value = new nvfuser::serde::SizeT(*reinterpret_cast<nvfuser::serde::SizeT *>(u.value));
      break;
    }
    case RecordData_Tensor: {
      value = new nvfuser::serde::TensorT(*reinterpret_cast<nvfuser::serde::TensorT *>(u.value));
      break;
    }
    case RecordData_TensorCreation: {
      value = new nvfuser::serde::TensorCreationT(*reinterpret_cast<nvfuser::serde::TensorCreationT *>(u.value));
      break;
    }
    case RecordData_TensorCreationSymbolic: {
      value = new nvfuser::serde::TensorCreationSymbolicT(*reinterpret_cast<nvfuser::serde::TensorCreationSymbolicT *>(u.value));
      break;
    }
    case RecordData_Vector: {
      value = new nvfuser::serde::VectorT(*reinterpret_cast<nvfuser::serde::VectorT *>(u.value));
      break;
    }
    default:
      break;
  }
}

inline void RecordDataUnion::Reset() {
  switch (type) {
    case RecordData_At: {
      auto ptr = reinterpret_cast<nvfuser::serde::AtT *>(value);
      delete ptr;
      break;
    }
    case RecordData_BatchNorm: {
      auto ptr = reinterpret_cast<nvfuser::serde::BatchNormT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Broadcast: {
      auto ptr = reinterpret_cast<nvfuser::serde::BroadcastT *>(value);
      delete ptr;
      break;
    }
    case RecordData_BroadcastInDim: {
      auto ptr = reinterpret_cast<nvfuser::serde::BroadcastInDimT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Dimension: {
      auto ptr = reinterpret_cast<nvfuser::serde::DimensionT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Dtype: {
      auto ptr = reinterpret_cast<nvfuser::serde::DtypeT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Norm: {
      auto ptr = reinterpret_cast<nvfuser::serde::NormT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Output: {
      auto ptr = reinterpret_cast<nvfuser::serde::OutputT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Pad: {
      auto ptr = reinterpret_cast<nvfuser::serde::PadT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Permute: {
      auto ptr = reinterpret_cast<nvfuser::serde::PermuteT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Slice: {
      auto ptr = reinterpret_cast<nvfuser::serde::SliceT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Squeeze: {
      auto ptr = reinterpret_cast<nvfuser::serde::SqueezeT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Reduction: {
      auto ptr = reinterpret_cast<nvfuser::serde::ReductionT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Reshape: {
      auto ptr = reinterpret_cast<nvfuser::serde::ReshapeT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Scalar: {
      auto ptr = reinterpret_cast<nvfuser::serde::ScalarT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Size: {
      auto ptr = reinterpret_cast<nvfuser::serde::SizeT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Tensor: {
      auto ptr = reinterpret_cast<nvfuser::serde::TensorT *>(value);
      delete ptr;
      break;
    }
    case RecordData_TensorCreation: {
      auto ptr = reinterpret_cast<nvfuser::serde::TensorCreationT *>(value);
      delete ptr;
      break;
    }
    case RecordData_TensorCreationSymbolic: {
      auto ptr = reinterpret_cast<nvfuser::serde::TensorCreationSymbolicT *>(value);
      delete ptr;
      break;
    }
    case RecordData_Vector: {
      auto ptr = reinterpret_cast<nvfuser::serde::VectorT *>(value);
      delete ptr;
      break;
    }
    default: break;
  }
  value = nullptr;
  type = RecordData_NONE;
}

inline bool VerifyPolymorphicValueData(::flatbuffers::Verifier &verifier, const void *obj, PolymorphicValueData type) {
  switch (type) {
    case PolymorphicValueData_NONE: {
      return true;
    }
    case PolymorphicValueData_Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Scalar *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case PolymorphicValueData_ScalarCpu: {
      auto ptr = reinterpret_cast<const nvfuser::serde::ScalarCpu *>(obj);
      return verifier.VerifyTable(ptr);
    }
    case PolymorphicValueData_TensorArg: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorArg *>(obj);
      return verifier.VerifyTable(ptr);
    }
    default: return true;
  }
}

inline bool VerifyPolymorphicValueDataVector(::flatbuffers::Verifier &verifier, const ::flatbuffers::Vector<::flatbuffers::Offset<void>> *values, const ::flatbuffers::Vector<uint8_t> *types) {
  if (!values || !types) return !values && !types;
  if (values->size() != types->size()) return false;
  for (::flatbuffers::uoffset_t i = 0; i < values->size(); ++i) {
    if (!VerifyPolymorphicValueData(
        verifier,  values->Get(i), types->GetEnum<PolymorphicValueData>(i))) {
      return false;
    }
  }
  return true;
}

inline void *PolymorphicValueDataUnion::UnPack(const void *obj, PolymorphicValueData type, const ::flatbuffers::resolver_function_t *resolver) {
  (void)resolver;
  switch (type) {
    case PolymorphicValueData_Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::Scalar *>(obj);
      return ptr->UnPack(resolver);
    }
    case PolymorphicValueData_ScalarCpu: {
      auto ptr = reinterpret_cast<const nvfuser::serde::ScalarCpu *>(obj);
      return ptr->UnPack(resolver);
    }
    case PolymorphicValueData_TensorArg: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorArg *>(obj);
      return ptr->UnPack(resolver);
    }
    default: return nullptr;
  }
}

inline ::flatbuffers::Offset<void> PolymorphicValueDataUnion::Pack(::flatbuffers::FlatBufferBuilder &_fbb, const ::flatbuffers::rehasher_function_t *_rehasher) const {
  (void)_rehasher;
  switch (type) {
    case PolymorphicValueData_Scalar: {
      auto ptr = reinterpret_cast<const nvfuser::serde::ScalarT *>(value);
      return CreateScalar(_fbb, ptr, _rehasher).Union();
    }
    case PolymorphicValueData_ScalarCpu: {
      auto ptr = reinterpret_cast<const nvfuser::serde::ScalarCpuT *>(value);
      return CreateScalarCpu(_fbb, ptr, _rehasher).Union();
    }
    case PolymorphicValueData_TensorArg: {
      auto ptr = reinterpret_cast<const nvfuser::serde::TensorArgT *>(value);
      return CreateTensorArg(_fbb, ptr, _rehasher).Union();
    }
    default: return 0;
  }
}

inline PolymorphicValueDataUnion::PolymorphicValueDataUnion(const PolymorphicValueDataUnion &u) : type(u.type), value(nullptr) {
  switch (type) {
    case PolymorphicValueData_Scalar: {
      value = new nvfuser::serde::ScalarT(*reinterpret_cast<nvfuser::serde::ScalarT *>(u.value));
      break;
    }
    case PolymorphicValueData_ScalarCpu: {
      value = new nvfuser::serde::ScalarCpuT(*reinterpret_cast<nvfuser::serde::ScalarCpuT *>(u.value));
      break;
    }
    case PolymorphicValueData_TensorArg: {
      value = new nvfuser::serde::TensorArgT(*reinterpret_cast<nvfuser::serde::TensorArgT *>(u.value));
      break;
    }
    default:
      break;
  }
}

inline void PolymorphicValueDataUnion::Reset() {
  switch (type) {
    case PolymorphicValueData_Scalar: {
      auto ptr = reinterpret_cast<nvfuser::serde::ScalarT *>(value);
      delete ptr;
      break;
    }
    case PolymorphicValueData_ScalarCpu: {
      auto ptr = reinterpret_cast<nvfuser::serde::ScalarCpuT *>(value);
      delete ptr;
      break;
    }
    case PolymorphicValueData_TensorArg: {
      auto ptr = reinterpret_cast<nvfuser::serde::TensorArgT *>(value);
      delete ptr;
      break;
    }
    default: break;
  }
  value = nullptr;
  type = PolymorphicValueData_NONE;
}

inline const nvfuser::serde::FusionCache *GetFusionCache(const void *buf) {
  return ::flatbuffers::GetRoot<nvfuser::serde::FusionCache>(buf);
}

inline const nvfuser::serde::FusionCache *GetSizePrefixedFusionCache(const void *buf) {
  return ::flatbuffers::GetSizePrefixedRoot<nvfuser::serde::FusionCache>(buf);
}

inline const char *FusionCacheIdentifier() {
  return "NV00";
}

inline bool FusionCacheBufferHasIdentifier(const void *buf) {
  return ::flatbuffers::BufferHasIdentifier(
      buf, FusionCacheIdentifier());
}

inline bool SizePrefixedFusionCacheBufferHasIdentifier(const void *buf) {
  return ::flatbuffers::BufferHasIdentifier(
      buf, FusionCacheIdentifier(), true);
}

inline bool VerifyFusionCacheBuffer(
    ::flatbuffers::Verifier &verifier) {
  return verifier.VerifyBuffer<nvfuser::serde::FusionCache>(FusionCacheIdentifier());
}

inline bool VerifySizePrefixedFusionCacheBuffer(
    ::flatbuffers::Verifier &verifier) {
  return verifier.VerifySizePrefixedBuffer<nvfuser::serde::FusionCache>(FusionCacheIdentifier());
}

inline void FinishFusionCacheBuffer(
    ::flatbuffers::FlatBufferBuilder &fbb,
    ::flatbuffers::Offset<nvfuser::serde::FusionCache> root) {
  fbb.Finish(root, FusionCacheIdentifier());
}

inline void FinishSizePrefixedFusionCacheBuffer(
    ::flatbuffers::FlatBufferBuilder &fbb,
    ::flatbuffers::Offset<nvfuser::serde::FusionCache> root) {
  fbb.FinishSizePrefixed(root, FusionCacheIdentifier());
}

inline std::unique_ptr<nvfuser::serde::FusionCacheT> UnPackFusionCache(
    const void *buf,
    const ::flatbuffers::resolver_function_t *res = nullptr) {
  return std::unique_ptr<nvfuser::serde::FusionCacheT>(GetFusionCache(buf)->UnPack(res));
}

inline std::unique_ptr<nvfuser::serde::FusionCacheT> UnPackSizePrefixedFusionCache(
    const void *buf,
    const ::flatbuffers::resolver_function_t *res = nullptr) {
  return std::unique_ptr<nvfuser::serde::FusionCacheT>(GetSizePrefixedFusionCache(buf)->UnPack(res));
}

}  // namespace serde
}  // namespace nvfuser

#endif  // FLATBUFFERS_GENERATED_FUSIONCACHE_NVFUSER_SERDE_H_
