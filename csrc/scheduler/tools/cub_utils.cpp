// clang-format off
/*
 * SPDX-FileCopyrightText: Copyright (c) 2025-present NVIDIA CORPORATION & AFFILIATES.
 * All rights reserved.
 * SPDX-License-Identifier: BSD-3-Clause
 */
// clang-format on

#include <scheduler/tools/cub_utils.h>
#include <utils.h>

namespace nvfuser {
namespace scheduler_tools {

namespace {

constexpr int64_t alignUp(int64_t value, int64_t alignment) {
  return (value + (alignment - 1)) & ~(alignment - 1);
}

constexpr bool isPowerOfTwo(int64_t x) {
  return x != 0 && (x & (x - 1)) == 0;
}

constexpr int64_t ceilLog2(int64_t x) {
  // returns ceil(log2(x)) for x >= 1
  int64_t n = 0;
  int64_t v = 1;
  while (v < x) {
    v <<= 1;
    ++n;
  }
  return n;
}

/*
 CUB BlockRadixSort shared memory usage. The comment and the code are
 generated by Cursor with GPT-5.

 Assumptions:
 - RADIX_BITS = 4 (RADIX_DIGITS = 16)
 - SMEM bank size = 4 bytes
 - INNER_SCAN_ALGORITHM = BLOCK_SCAN_WARP_SCANS
 - align32(x) rounds x up to the next multiple of 32 bytes: ((x + 31) & ~31)

 Composition:
 - BlockRadixSort::TempStorage is a union of:
   AscendingBlockRadixRank::TempStorage,
   DescendingBlockRadixRank::TempStorage,
   BlockExchange<KeyT>::TempStorage,
   BlockExchange<ValueT>::TempStorage.
 - Total bytes: S(BlockRadixSort) = max(S_rank, S_exch(KeyT), S_exch(ValueT)).

 Rank storage (independent of KeyT/ValueT):
 - Let B = BLOCK_DIM_X x BLOCK_DIM_Y x BLOCK_DIM_Z, W = ceil(B / 32).
 - PackedCounter = unsigned int (4 bytes) for 4-byte SMEM banks.
 - Aliasable part (digit counters or raking grid): S_rank_aliasable = 36 * B
bytes  // RADIX_BITS=4 => PADDED_COUNTER_LANES=9
 - BlockScan contribution:
    * If (B % 32 == 0)  // warp-scans path
        S_blockscan ~= align32(5 * W + 4) bytes
      (W warp aggregates @4B, plus W bytes from array of empty per-warp
TempStorage, plus 4B block prefix)
     * Else               // raking fallback
         S_blockscan is larger (~1 KB near B~200). Exact size depends on
BlockRakingLayout and alignment.
 - Total: S_rank = S_rank_aliasable + S_blockscan.
 - Final TempStorage size = align16(max(S_rank, S_exch(KeyT), S_exch(ValueT)))
   because the union inherits 16-byte alignment from
BlockExchange::_TempStorage.

Exchange storage (depends on KeyT/ValueT):
 - TILE_ITEMS = B * ITEMS_PER_THREAD
 - PADDING_ITEMS = (ITEMS_PER_THREAD > 4 && power_of_two(ITEMS_PER_THREAD)) ?
(TILE_ITEMS >> 5) : 0  // 32 banks
- S_exch(T) = align16(sizeof(T) * (TILE_ITEMS + PADDING_ITEMS))  //
BlockExchange::_TempStorage is alignas(16)

 Worked examples (KeyT = int64_t, ValueT = int64_t, BLOCK_DIM_Y = BLOCK_DIM_Z =
1, RADIX_BITS = 4, SMEM banks = 4B):

ITEMS_PER_THREAD = 1
- B = 128 (W = 4):   S_rank = 36*128 + align32(5*4 + 4) = 4608 + 32 = 4640
                       S_exch(key)=8*128=1024, values=1024 -> S = 4640
- B = 224 (W = 7):   S_rank = 36*224 + align32(5*7 + 4) = 8064 + 64 = 8128
                       S = 8096
- B = 256 (W = 8):   S_rank = 36*256 + align32(5*8 + 4) = 9216 + 64 = 9280
                       S = 9280
 - B = 200 (W = 7, raking): S_rank = 36*200 + 1024 = 8224 (exact)
                             S = 8224

 ITEMS_PER_THREAD = 4  (no padding)
 - B = 128: S = 4640
- B = 224: S = 8128
 - B = 256: S = 9280
 - B = 200: S = 8224

 ITEMS_PER_THREAD = 8  (padding applies; PADDING_ITEMS = (B*8)/32 = B/4)
 - B = 128: S_rank = 4640; S_exch(T) = 8*(128*8 + 32) = 8448 -> S = 8448
 - B = 224: S_rank = 8128; S_exch(T) = 8*(224*8 + 56) = 14784 -> S = 14784
 - B = 256: S_rank = 9280; S_exch(T) = 8*(256*8 + 64) = 16896 -> S = 16896
 - B = 200: S_rank = 8224; S_exch(T) = 8*(200*8 + 50) = 13200 -> S = 13200

 Rule of thumb:
 - Rank aliasable scales as 36 * B; BlockScan adds a small aligned term if
B%32==0, else ~1 KB (varies with B).
 - Exchange per type scales as sizeof(T) * B * ITEMS_PER_THREAD (padding kicks
in when ITEMS_PER_THREAD > 4 and pow2).
 - For 64-bit key + 64-bit value, exchange overtakes rank around
ITEMS_PER_THREAD >= 5 (ignoring small/aligned term).

 sizeof() sanity check snippet:
   using BRS = cub::BlockRadixSort<long long, BLOCK_DIM_X, ITEMS_PER_THREAD,
long long, 4>; printf("%zu\n", sizeof(typename BRS::TempStorage));
*/
constexpr int64_t computeBlockRadixSortTempStorageBytes(
    int64_t block_threads,
    int64_t items_per_thread,
    int64_t key_size_bytes,
    int64_t value_size_bytes) {
  // Rank aliasable part (RADIX_BITS=4, 4B banks → PADDED_COUNTER_LANES=9 → 36
  // bytes per thread)
  const int64_t rank_aliasable_bytes = int64_t{36} * block_threads;

  // BlockScan contribution inside BlockRadixRank
  const bool multiples_of_32 = (block_threads % int64_t{32}) == 0;
  const int64_t warps = ceilDiv(block_threads, int64_t{32});

  int64_t blockscan_bytes = 0;
  if (multiples_of_32) {
    // Warp-scans path: W warp aggregates (4B each) + W bytes for array of empty
    // per-warp TempStorage + one block prefix (4B), aligned to 32 bytes
    const int64_t raw = int64_t{5} * warps + int64_t{4};
    blockscan_bytes = alignUp(raw, int64_t{32});
  } else {
    // Raking path: compute exact size using BlockRakingLayout and WarpScanSmem
    // for PackedCounter (4B)
    const int64_t max_raking_threads =
        block_threads < int64_t{32} ? block_threads : int64_t{32};
    const int64_t segment_length = ceilDiv(block_threads, max_raking_threads);
    const bool use_segment_padding =
        ((segment_length & int64_t{1}) == 0) && (segment_length > int64_t{2});
    const int64_t raking_threads = ceilDiv(block_threads, segment_length);

    // WarpScan storage: shfl variant for power-of-two raking_threads (empty),
    // smem variant otherwise (1.5 * raking_threads elements of 4B)
    int64_t warp_scan_size = 0;
    if (isPowerOfTwo(raking_threads)) {
      // Empty TempStorage still contributes 1 byte as a member
      warp_scan_size = 1;
    } else {
      const int64_t steps = ceilLog2(raking_threads);
      const int64_t half_warp_threads =
          (steps == 0) ? int64_t{0} : (int64_t{1} << (steps - 1));
      const int64_t warp_smem_elements = raking_threads + half_warp_threads;
      warp_scan_size =
          int64_t{4} * warp_smem_elements; // sizeof(PackedCounter)=4
    }

    // BlockRakingLayout grid
    const int64_t grid_elements = raking_threads *
        (segment_length + (use_segment_padding ? int64_t{1} : int64_t{0}));
    const int64_t raking_grid_bytes =
        int64_t{4} * grid_elements; // sizeof(PackedCounter)=4

    // Layout with alignments: warp_scan (4B aligned) -> pad to 16B ->
    // raking_grid (align 16) -> block_aggregate (4B)
    int64_t total = 0;
    total += warp_scan_size;
    total = alignUp(total, int64_t{16});
    total += alignUp(raking_grid_bytes, int64_t{16});
    total += int64_t{4}; // block_aggregate of PackedCounter
    total = alignUp(total, int64_t{16}); // struct alignment

    blockscan_bytes = total;
  }

  const int64_t rank_bytes = rank_aliasable_bytes + blockscan_bytes;

  // Exchange storage for key/value types
  const int64_t tile_items = block_threads * items_per_thread;
  const bool needs_padding =
      (items_per_thread > int64_t{4}) && isPowerOfTwo(items_per_thread);
  const int64_t padding_items = needs_padding
      ? (tile_items >> int64_t{5})
      : int64_t{0}; // 32 banks → LOG_SMEM_BANKS=5
  const int64_t exchange_keys = key_size_bytes * (tile_items + padding_items);
  const int64_t exchange_values =
      value_size_bytes * (tile_items + padding_items);
  int64_t exchange_bytes =
      exchange_keys > exchange_values ? exchange_keys : exchange_values;
  // BlockExchange::_TempStorage is alignas(16), so size is rounded up to
  // 16-byte multiple
  exchange_bytes = alignUp(exchange_bytes, int64_t{16});

  // Exchange buffers are in a union; contribution is the larger of the two. For
  // keys-only, set value_size_bytes=0.
  int64_t result = rank_bytes > exchange_bytes ? rank_bytes : exchange_bytes;
  // The union holding rank/exchange storage inherits max alignment (16 via
  // BlockExchange), so the final size rounds up to 16 bytes.
  result = alignUp(result, int64_t{16});
  return result;
}

/*
 CUB BlockScan shared memory usage. The comment and the code are
 generated by Cursor with GPT-5.

 Assumptions:
 - SMEM banks = 4 bytes; align32/align16 apply as below
 - SAFE_ALGORITHM: if warp-scans is requested but B%32 != 0, fallback to raking
 - B = BLOCK_DIM_X * BLOCK_DIM_Y * BLOCK_DIM_Z, W = ceil(B/32)

 Warp-scans (B % 32 == 0):
 - TempStorage layout (alignas(32)):
     T warp_aggregates[W];
     WarpScan<T,32>::TempStorage warp_scan[W]; // empty type -> contributes W
 bytes total T block_prefix;
 - Size: S = align32(W * (sizeof(T) + 1) + sizeof(T))

 Raking (fallback or forced):
 - Raking layout:
     max_raking_threads = min(B, 32)
     segment_length     = ceil(B / max_raking_threads)
     use_segment_padding = ((segment_length & 1) == 0) && (segment_length > 2)
     raking_threads     = ceil(B / segment_length)
 - WarpScan<T, raking_threads>::TempStorage:
     * if raking_threads is power-of-two => SHFL path, empty -> contributes 1
 byte
     * else SMEM path => elements = raking_threads +
 2^(ceil_log2(raking_threads)-1) bytes   = elements * sizeof(T)
 - BlockRakingLayout grid bytes:
     GRID = raking_threads * (segment_length + (use_segment_padding ? 1 : 0))
     bytes = GRID * sizeof(T)
 - TempStorage size (alignas(16)):
     S = align16( warp_scan_bytes )
       + align16( grid_bytes )
       + sizeof(T)
       => align16(total)

 Notes:
 - For B=32 (warp-scans), W=1 and S = align32((sizeof(T)+1) + sizeof(T)) = 32
 for common T sizes (e.g., 4B, 8B).
 - For B=16, SAFE fallback uses raking; raking_threads=16 (power-of-two) so
 SHFL-only WarpScan contributes 1 byte. Typical sizes: T=4 => 96 bytes; T=8 =>
 160 bytes.

 sizeof() sanity check snippet:
   using BS = cub::BlockScan<int64_t, 128, cub::BLOCK_SCAN_WARP_SCANS>;
   printf("%zu\n", sizeof(typename BS::TempStorage));
*/
constexpr int64_t computeBlockScanTempStorageBytes(
    int64_t block_threads,
    int64_t type_size_bytes,
    bool use_warp_scans) {
  const int64_t warps = ceilDiv(block_threads, int64_t{32});

  // Warp-scans path
  if (use_warp_scans && (block_threads % int64_t{32} == 0)) {
    // alignas(32) struct with W warp aggregates (T), W empty TempStorages (1B
    // each), and block_prefix (T)
    const int64_t raw =
        warps * (type_size_bytes + int64_t{1}) + type_size_bytes;
    return alignUp(raw, int64_t{32});
  }

  // Raking path (BlockScanRaking)
  const int64_t max_raking_threads =
      block_threads < int64_t{32} ? block_threads : int64_t{32};
  const int64_t segment_length = ceilDiv(block_threads, max_raking_threads);
  const bool use_segment_padding =
      ((segment_length & int64_t{1}) == 0) && (segment_length > int64_t{2});
  const int64_t raking_threads = ceilDiv(block_threads, segment_length);

  // WarpScan storage for logical warp = raking_threads
  int64_t warp_scan_size = 0;
  if (isPowerOfTwo(raking_threads)) {
    // Empty TempStorage still contributes 1 byte as a member
    warp_scan_size = 1;
  } else {
    const int64_t steps = ceilLog2(raking_threads);
    const int64_t half_warp_threads =
        (steps == 0) ? int64_t{0} : (int64_t{1} << (steps - 1));
    const int64_t warp_smem_elements = raking_threads + half_warp_threads;
    warp_scan_size = type_size_bytes * warp_smem_elements;
  }

  // BlockRakingLayout grid
  const int64_t grid_elements = raking_threads *
      (segment_length + (use_segment_padding ? int64_t{1} : int64_t{0}));
  const int64_t raking_grid_bytes = type_size_bytes * grid_elements;

  // Layout: warp_scan (4B aligned) -> pad to 16 -> raking_grid (align16) ->
  // block_aggregate (T) -> pad to 16
  int64_t total = 0;
  total += warp_scan_size;
  total = alignUp(total, int64_t{16});
  total += alignUp(raking_grid_bytes, int64_t{16});
  total += type_size_bytes;
  total = alignUp(total, int64_t{16});

  return total;
}

} // namespace

void CubSharedMemoryBuffer::registerArgsort(
    int64_t bdimx,
    int64_t items_per_thread,
    DataType dtype) {
  max_bdimx_ = std::max(max_bdimx_, bdimx);
  argsort_calls_.emplace(items_per_thread, dtype);
}

void CubSharedMemoryBuffer::registerScan(
    int64_t bdimx,
    int64_t items_per_thread,
    DataType dtype) {
  max_bdimx_ = std::max(max_bdimx_, bdimx);
  scan_calls_.emplace_back(dtype);
}

void CubSharedMemoryBuffer::registerTopK(
    int64_t bdimx,
    int64_t items_per_thread,
    DataType dtype) {
  max_bdimx_ = std::max(max_bdimx_, bdimx);
  topk_calls_.emplace(items_per_thread, dtype);
}

int64_t CubSharedMemoryBuffer::getArgsortTotalSizeInBytes() const {
  int64_t total_size = 0;
  for (const auto& template_instance : argsort_calls_) {
    total_size += computeBlockRadixSortTempStorageBytes(
        max_bdimx_,
        template_instance.items_per_thread,
        dataTypeSizeByte(template_instance.dtype),
        sizeof(int64_t));
  }

  return total_size;
}

int64_t CubSharedMemoryBuffer::getTopKTotalSizeInBytes() const {
  int64_t total_size = 0;
  for (const auto& template_instance : topk_calls_) {
    total_size += computeBlockRadixSortTempStorageBytes(
        max_bdimx_,
        template_instance.items_per_thread,
        dataTypeSizeByte(template_instance.dtype),
        sizeof(int64_t));
  }

  return total_size;
}

int64_t CubSharedMemoryBuffer::getScanTotalSizeInBytes() const {
  int64_t total_size = 0;
  for (const auto& template_instance : scan_calls_) {
    total_size += computeBlockScanTempStorageBytes(
        max_bdimx_,
        dataTypeSizeByte(template_instance.dtype),
        /*use_warp_scans=*/true);
  }

  return total_size;
}

int64_t CubSharedMemoryBuffer::getTotalSizeInBytes() const {
  return getArgsortTotalSizeInBytes() + getScanTotalSizeInBytes() +
      getTopKTotalSizeInBytes();
}

} // namespace scheduler_tools
} // namespace nvfuser
