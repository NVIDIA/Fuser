

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Host IR JIT Overview &mdash; nvFuser 0.2.34 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/css/nvidia_font.css?v=e009355c" />
      <link rel="stylesheet" type="text/css" href="../_static/css/nvidia_footer.css?v=84031d34" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=b85e2031"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="LdMatrix and StMatrix Support in NVFuser" href="ldmatrix_stmatrix.html" />
    <link rel="prev" title="Symbol Visibility" href="visibility.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" > 

          
          
          <a href="../index.html" class="icon icon-home">
            nvFuser
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-menu > p > span.caption-text {
      color: #76b900;
    }

    .wy-menu-vertical p {
      height: 32px;
      line-height: 32px;
      padding: 0 1.618em;
      margin: 12px 0 0;
      display: block;
      font-weight: 700;
      text-transform: uppercase;
      font-size: 85%;
      white-space: nowrap;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }

    .wy-nav-content {
      max-width: 1000px;
    }

    /* override table width restrictions */
    .wy-table-responsive table td, .wy-table-responsive table th {
        /* !important prevents the common CSS stylesheets from
          overriding this as on RTD they are loaded after this stylesheet */
        white-space: normal !important;
    }

    .wy-table-responsive {
        overflow: visible !important;
    }

  </style>
  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  html.writer-html5 .rst-content dl[class]:not(.option-list):not(.field-list):not(.footnote):not(.glossary):not(.simple)>dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }

  html.writer-html4 .rst-content dl:not(.docutils) .property, html.writer-html5 .rst-content dl[class]:not(.option-list):not(.field-list):not(.footnote):not(.glossary):not(.simple) .property {
    text-transform: capitalize;
    display: inline-block;
    padding-right: 8px;
  }
  </style>

  
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Table of Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#installation">Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../installation.html#nightly-nvfuser-pip-wheel">Nightly nvfuser pip wheel</a></li>
<li class="toctree-l3"><a class="reference internal" href="../installation.html#nvfuser-pip-wheel-against-pytorch-stable-release">Nvfuser pip wheel against pytorch stable release</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#developer">Developer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../installation.html#install-from-source">Install From Source:</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/general.html">General</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/general.html#statement">Statement</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/general.html#val">Val</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/general.html#expr">Expr</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/general.html#iterdomain">IterDomain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/general.html#tensordomain">TensorDomain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/general.html#tensorview">TensorView</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/general.html#fusionexecutorcache">FusionExecutorCache</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/general.html#kernelexecutor">KernelExecutor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/general.html#fusion-definition">Fusion Definition</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api/ops.html">Operations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/ops.html#ops">Ops</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api/multidevice.html">Multidevice</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/multidevice.html#communicator">Communicator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/multidevice.html#devicemesh">DeviceMesh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/multidevice.html#sharding">Sharding</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api/enum.html">Enums</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/enum.html#communicatorbackend-types">CommunicatorBackend Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/enum.html#data-types">Data Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/enum.html#parallel-types">Parallel Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/enum.html#scheduler-types">Scheduler Types</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api/pod_class.html">Data classes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/pod_class.html#launchparams">LaunchParams</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/pod_class.html#compileparams">CompileParams</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer References</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../reading/divisibility-of-split.html">Divisibility of Split</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../reading/divisibility-of-split.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reading/divisibility-of-split.html#predication">Predication</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reading/divisibility-of-split.html#allocation-and-correctness-model">Allocation and correctness model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reading/divisibility-of-split.html#properties-of-split">Properties of split</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../reading/divisibility-of-split.html#merge-then-split-vs-split-then-merge">Merge-then-split vs split-then-merge</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reading/divisibility-of-split.html#merging-discontiguous-iterdomains">Merging discontiguous IterDomains</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../reading/divisibility-of-split.html#question">Question</a></li>
<li class="toctree-l4"><a class="reference internal" href="../reading/divisibility-of-split.html#answer">Answer</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../reading/iterdomain.html">The Mathematical Theory of IterDomain</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../reading/iterdomain.html#iterdomain-transformations">1. IterDomain Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reading/iterdomain.html#properties-of-iterdomain-transformations">2. Properties of IterDomain Transformations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../reading/multigpu.html">Multi-GPU Support in nvFuser</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../reading/multigpu.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reading/multigpu.html#user-api">User API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reading/multigpu.html#parallelisms">Parallelisms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../reading/multigpu.html#tensor-parallelism-tp">Tensor Parallelism (TP)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../reading/multigpu.html#sharding-propagation">Sharding Propagation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../reading/multigpu.html#communication-computation-decomposition">Communication-computation Decomposition</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../reading/multigpu.html#sequence-parallelism-sp">Sequence Parallelism (SP)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../reading/multigpu.html#id1">Sharding Propagation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../reading/multigpu.html#id2">Communication-computation Decomposition</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../reading/multigpu.html#overlap-communication-with-gemm-via-decomposition">Overlap Communication with GEMM via Decomposition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reading/multigpu.html#distributed-data-parallelism-ddp">Distributed Data Parallelism (DDP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reading/multigpu.html#fully-sharded-data-parallelism-fsdp">Fully Sharded Data Parallelism (FSDP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reading/multigpu.html#pipeline-parallelism-pp">Pipeline Parallelism (PP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reading/multigpu.html#context-parallelism-cp">Context Parallelism (CP)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../reading/tma-modeling-in-depth.html">TMA Modeling In Depth</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../reading/tma-modeling-in-depth.html#what-is-tma">What is TMA?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reading/tma-modeling-in-depth.html#correctness-model">Correctness model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reading/tma-modeling-in-depth.html#the-unachievability-of-strong-correctness-for-indivisible-element-stride">The unachievability of strong correctness for indivisible element stride</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reading/tma-modeling-in-depth.html#the-lowering-strategy">The lowering strategy</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="debug.html">Debugging</a><ul>
<li class="toctree-l2"><a class="reference internal" href="debug.html#debug-a-failing-nvfuser-script">Debug a failing nvFuser script</a><ul>
<li class="toctree-l3"><a class="reference internal" href="debug.html#nvfuser-dump">NVFUSER_DUMP</a></li>
<li class="toctree-l3"><a class="reference internal" href="debug.html#gdb">gdb</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="debug.html#debug-memory-corruption-using-asan">Debug memory corruption using <code class="docutils literal notranslate"><span class="pre">asan</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="debug.html#if-built-with-clang">If built with clang</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="debug.html#debug-memory-leaks-or-excessive-memory-usage">Debug memory leaks or excessive memory usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="debug.html#debug-slow-kernels">Debug slow kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="debug.html#debug-slow-cpu-execution">Debug slow CPU execution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="visibility.html">Symbol Visibility</a><ul>
<li class="toctree-l2"><a class="reference internal" href="visibility.html#faq">FAQ</a><ul>
<li class="toctree-l3"><a class="reference internal" href="visibility.html#should-i-mark-a-method-visible-or-the-whole-class">Should I mark a method visible or the whole class?</a></li>
<li class="toctree-l3"><a class="reference internal" href="visibility.html#i-see-an-undefined-reference-to-typeinfo-for-class-how-do-i-fix-this">I see an undefined reference to <code class="docutils literal notranslate"><span class="pre">typeinfo</span> <span class="pre">for</span> <span class="pre">&lt;class&gt;</span></code>. How do I fix this?</a></li>
<li class="toctree-l3"><a class="reference internal" href="visibility.html#i-see-an-undefined-reference-to-vtable-for-class-how-do-i-fix-this">I see an undefined reference to <code class="docutils literal notranslate"><span class="pre">vtable</span> <span class="pre">for</span> <span class="pre">&lt;class&gt;</span></code>. How do I fix this?</a></li>
<li class="toctree-l3"><a class="reference internal" href="visibility.html#i-see-that-foo-is-visible-but-i-do-not-think-it-needs-to-be">I see that <code class="docutils literal notranslate"><span class="pre">Foo</span></code> is visible but I do not think it needs to be.</a></li>
<li class="toctree-l3"><a class="reference internal" href="visibility.html#should-i-mark-my-new-method-or-class-as-nvf-api">Should I mark my new method or class as <code class="docutils literal notranslate"><span class="pre">NVF_API</span></code>?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="visibility.html#symbol-visibility-checking">Symbol Visibility Checking</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Host IR JIT Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#jit-compilation-process">JIT Compilation Process</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#llvm-integration">1. LLVM Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compilation-pipeline">2. Compilation Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="#external-function-integration">3. External Function Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ir-translation">3. IR Translation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#runtime-execution">Runtime Execution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#function-interface">1. Function Interface</a></li>
<li class="toctree-l3"><a class="reference internal" href="#execution-flow">2. Execution Flow</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#configuration-and-build-options">Configuration and Build Options</a></li>
<li class="toctree-l2"><a class="reference internal" href="#future-integration-plan">Future Integration plan</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ldmatrix_stmatrix.html">LdMatrix and StMatrix Support in NVFuser</a><ul>
<li class="toctree-l2"><a class="reference internal" href="ldmatrix_stmatrix.html#what-is-ldmatrix">What is LdMatrix?</a></li>
<li class="toctree-l2"><a class="reference internal" href="ldmatrix_stmatrix.html#what-is-stmatrix">What is StMatrix?</a></li>
<li class="toctree-l2"><a class="reference internal" href="ldmatrix_stmatrix.html#general-details">General Details</a><ul>
<li class="toctree-l3"><a class="reference internal" href="ldmatrix_stmatrix.html#indices-shared-memory-tensor">Indices shared memory tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="ldmatrix_stmatrix.html#indices-for-register-tensor">Indices for register tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="ldmatrix_stmatrix.html#register-layout-for-one-8x8-matrix-with-16-bit-elements">Register layout for one 8x8 Matrix with 16-bit elements</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ldmatrix_stmatrix.html#example-1-a-copy-kernel-using-tma-ldmatrix-and-stmatrix">Example 1: A copy kernel using TMA, LdMatrix, and StMatrix.</a><ul>
<li class="toctree-l2"><a class="reference internal" href="ldmatrix_stmatrix.html#how-to-compute-the-index-into-register-tensorview">How to compute the index into register TensorView?</a></li>
<li class="toctree-l2"><a class="reference internal" href="ldmatrix_stmatrix.html#how-to-compute-the-index-into-shared-memory-tensorview">How to compute the index into shared memory TensorView?</a><ul>
<li class="toctree-l3"><a class="reference internal" href="ldmatrix_stmatrix.html#figure-1-loop-domain-for-ldmatrix-and-stmatrix">Figure 1: Loop domain for LdMatrix and StMatrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="ldmatrix_stmatrix.html#figure-2-tma-shared-memory-allocation-domain">Figure 2: TMA shared memory allocation domain</a></li>
<li class="toctree-l3"><a class="reference internal" href="ldmatrix_stmatrix.html#figure-3-map-from-ldmatrix-stmatrix-loop-domain-to-tma-shared-memory-allocation-domain">Figure 3: Map from LdMatrix / StMatrix loop domain to TMA shared memory allocation domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ldmatrix_stmatrix.html#derivation-of-figure-3">Derivation of Figure 3</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="ldmatrix_stmatrix.html#code-walkthrough">Code Walkthrough</a><ul>
<li class="toctree-l3"><a class="reference internal" href="ldmatrix_stmatrix.html#scheduleldstmatrix-function">scheduleLdStMatrix function</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tma.html">Introduction to TMA Support in NVFuser</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tma.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="tma.html#schedule">Schedule</a><ul>
<li class="toctree-l3"><a class="reference internal" href="tma.html#step-1-define-tma-domain">Step 1: define TMA domain</a></li>
<li class="toctree-l3"><a class="reference internal" href="tma.html#step-2-define-box">Step 2: define box</a><ul>
<li class="toctree-l4"><a class="reference internal" href="tma.html#the-canonical-way-to-define-box">The canonical way to define box</a></li>
<li class="toctree-l4"><a class="reference internal" href="tma.html#define-box-by-mathematical-equivalence">Define box by mathematical equivalence</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="tma.html#step-3-define-tile">Step 3: define tile</a></li>
<li class="toctree-l3"><a class="reference internal" href="tma.html#step-4-schedule-the-shared-memory-tensor">Step 4: schedule the shared memory tensor</a><ul>
<li class="toctree-l4"><a class="reference internal" href="tma.html#data-swizzle">Data swizzle</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="tma.html#step-5-schedule-the-consumer-tensor">Step 5: schedule the consumer tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="tma.html#code-walk-through">Code walk-through</a></li>
<li class="toctree-l3"><a class="reference internal" href="tma.html#examples">Examples</a><ul>
<li class="toctree-l4"><a class="reference internal" href="tma.html#example-1-tma-load-inputs-and-vectorize-store-output-pointwise-kernel">Example 1: tma-load inputs and vectorize-store output pointwise kernel</a></li>
<li class="toctree-l4"><a class="reference internal" href="tma.html#example-2-broadcast-kernel-with-discontiguous-input">Example 2: broadcast kernel with discontiguous input</a></li>
<li class="toctree-l4"><a class="reference internal" href="tma.html#example-3-bank-conflict-free-transpose-of-32bit-data">Example 3: bank-conflict-free transpose of 32bit data</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tmem.html">Tensor Memory Support in NVFuser</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tmem.html#review-of-inlining-and-parallelization">Review of inlining and parallelization</a></li>
<li class="toctree-l2"><a class="reference internal" href="tmem.html#tensor-memory">Tensor memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="tmem.html#the-loop-domain-of-tmem-load-and-store">The loop domain of TMem load and store</a></li>
<li class="toctree-l2"><a class="reference internal" href="tmem.html#vectorization-of-tmem-load-and-store">Vectorization of TMem load and store</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">nvFuser</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Host IR JIT Overview</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/dev/host_ir_jit.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <!--
 * SPDX-FileCopyrightText: Copyright (c) 2025-present NVIDIA CORPORATION & AFFILIATES.
 * All rights reserved.
 * SPDX-License-Identifier: BSD-3-Clause
-->
<section class="tex2jax_ignore mathjax_ignore" id="host-ir-jit-overview">
<h1>Host IR JIT Overview<a class="headerlink" href="#host-ir-jit-overview" title="Link to this heading"></a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>Host IR JIT (Just-In-Time) is a new runtime targeting to reduce host side latency.
It:</p>
<ol class="arabic simple">
<li><p>captures graph dependencies at compile time</p></li>
<li><p>has register aligned access in runtime comparing with hash table lookup</p></li>
</ol>
</section>
<section id="jit-compilation-process">
<h2>JIT Compilation Process<a class="headerlink" href="#jit-compilation-process" title="Link to this heading"></a></h2>
<section id="llvm-integration">
<h3>1. LLVM Integration<a class="headerlink" href="#llvm-integration" title="Link to this heading"></a></h3>
<p>Host IR JIT uses LLVM’s ORC (On-Request Compilation) JIT framework:
LLVM IR is translated from Host IR and saved as an executable in LLVM ORC JIT
at compile time. During runtime, LLVM ORC JIT calls the saved executable with
given inputs and derives results.</p>
</section>
<section id="compilation-pipeline">
<h3>2. Compilation Pipeline<a class="headerlink" href="#compilation-pipeline" title="Link to this heading"></a></h3>
<p><strong>Below code snippets are a pseudo code for Host IR jit architecture.</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">HostIrJitImpl::compile</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// 1. Create LLVM context and module</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">llvm</span><span class="o">::</span><span class="n">LLVMContext</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="k">module</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">llvm</span><span class="o">::</span><span class="n">Module</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;host_ir_jit_module&quot;</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">context</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// 2. Compile function declarations</span>
<span class="w">  </span><span class="n">compileFunctionDeclarations</span><span class="p">(</span><span class="k">module</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span><span class="w"> </span><span class="o">*</span><span class="n">context</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// 3. Generate LLVM IR for inputs</span>
<span class="w">  </span><span class="n">unpackInputs</span><span class="p">(</span><span class="n">container_</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span><span class="w"> </span><span class="n">builder</span><span class="p">,</span><span class="w"> </span><span class="n">val_to_value</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// 4. Compile all top-level expressions</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="o">*</span><span class="w"> </span><span class="n">expr</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">container_</span><span class="o">-&gt;</span><span class="n">topLevelExprs</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">dispatcher</span><span class="p">.</span><span class="n">dispatch</span><span class="p">(</span><span class="n">expr</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// 5. Generate LLVM IR for outputs</span>
<span class="w">  </span><span class="n">packOutputs</span><span class="p">(</span><span class="n">container_</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span><span class="w"> </span><span class="n">builder</span><span class="p">,</span><span class="w"> </span><span class="n">val_to_value</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// 6. Add module to JIT and lookup main function</span>
<span class="w">  </span><span class="n">jit_</span><span class="o">-&gt;</span><span class="n">addIRModule</span><span class="p">(</span><span class="n">ThreadSafeModule</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="k">module</span><span class="p">),</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">context</span><span class="p">)));</span>
<span class="w">  </span><span class="n">main_func_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">jit_</span><span class="o">-&gt;</span><span class="n">lookup</span><span class="p">(</span><span class="n">kMainFuncName</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p><em>Detailed Implementation:</em> https://github.com/NVIDIA/Fuser/blob/3ac1a4697b6b5c31e4dbb9763b3b6db2f0e0164b/csrc/host_ir/jit.cpp#L1125-L1176</p>
</section>
<section id="external-function-integration">
<h3>3. External Function Integration<a class="headerlink" href="#external-function-integration" title="Link to this heading"></a></h3>
<p>The LLVM IR we generate contain external C++ calls that wrap ATen fallbacks and other things
that are hard to implement in LLVM IR. Currently Host IR JIT supports wrapper functions with:</p>
<ul class="simple">
<li><p><strong>Aten Fallbacks</strong>: <code class="docutils literal notranslate"><span class="pre">matmul</span></code>, <code class="docutils literal notranslate"><span class="pre">linear</span></code>, <code class="docutils literal notranslate"><span class="pre">permute</span></code>, <code class="docutils literal notranslate"><span class="pre">reshape</span></code>, <code class="docutils literal notranslate"><span class="pre">at_empty_strided_cuda</span></code></p></li>
<li><p><strong>Memory Management</strong>: <code class="docutils literal notranslate"><span class="pre">new_tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">delete_tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">set_tensor</span></code></p></li>
<li><p><strong>nvFuser Interals</strong> <code class="docutils literal notranslate"><span class="pre">launchKernel</span></code></p></li>
<li><p><strong>Profiling</strong>: NVTX range push/pop for performance analysis</p></li>
</ul>
<p><em>Detailed Implementation:</em> https://github.com/NVIDIA/Fuser/blob/3ac1a4697b6b5c31e4dbb9763b3b6db2f0e0164b/csrc/host_ir/jit.cpp#L1195-L1396</p>
</section>
<section id="ir-translation">
<h3>3. IR Translation<a class="headerlink" href="#ir-translation" title="Link to this heading"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">HostIrCompileDispatcher</span></code> translates Host IR expression nodes to LLVM IR:
Currently, Host IR JIT supports these expressions:
<code class="docutils literal notranslate"><span class="pre">ViewOp</span></code>, <code class="docutils literal notranslate"><span class="pre">LoadStoreOp</span></code>, <code class="docutils literal notranslate"><span class="pre">MatmulOp</span></code>, <code class="docutils literal notranslate"><span class="pre">LinearOp</span></code>, <code class="docutils literal notranslate"><span class="pre">LaunchKernel</span></code>, <code class="docutils literal notranslate"><span class="pre">Allocate</span></code>, <code class="docutils literal notranslate"><span class="pre">Deallocate</span></code></p>
<p><em>Detailed Implementation:</em> https://github.com/NVIDIA/Fuser/blob/3ac1a4697b6b5c31e4dbb9763b3b6db2f0e0164b/csrc/host_ir/jit.cpp#L783-L1123</p>
</section>
</section>
<section id="runtime-execution">
<h2>Runtime Execution<a class="headerlink" href="#runtime-execution" title="Link to this heading"></a></h2>
<p><strong>Below code snippets are a pseudo code for Host IR jit architecture.</strong></p>
<section id="function-interface">
<h3>1. Function Interface<a class="headerlink" href="#function-interface" title="Link to this heading"></a></h3>
<p>The compiled JIT function follows this signature:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">main_func_t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="p">)(</span><span class="kt">int64_t</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">void</span><span class="o">**</span><span class="p">,</span><span class="w"> </span><span class="kt">void</span><span class="o">**</span><span class="p">);</span>
<span class="c1">// Parameters: cache_id, input_tensors, output_tensors</span>
</pre></div>
</div>
<p><em>Detailed Implementation:</em> https://github.com/NVIDIA/Fuser/blob/3ac1a4697b6b5c31e4dbb9763b3b6db2f0e0164b/csrc/host_ir/jit.cpp#L46</p>
</section>
<section id="execution-flow">
<h3>2. Execution Flow<a class="headerlink" href="#execution-flow" title="Link to this heading"></a></h3>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">KernelArgumentHolder</span><span class="w"> </span><span class="nf">HostIrJitImpl::runWithInputs</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">KernelArgumentHolder</span><span class="o">&amp;</span><span class="w"> </span><span class="n">args</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// 1. Validate inputs and prepare tensor arrays</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="k">const</span><span class="w"> </span><span class="kt">void</span><span class="o">*&gt;</span><span class="w"> </span><span class="n">input_aten_tensors</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">*&gt;</span><span class="w"> </span><span class="n">output_aten_tensors</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// 2. Call compiled JIT function</span>
<span class="w">  </span><span class="n">main_func_</span><span class="p">(</span><span class="n">cache_id</span><span class="p">,</span><span class="w"> </span><span class="n">input_aten_tensors</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span><span class="w"> </span><span class="n">output_aten_tensors</span><span class="p">.</span><span class="n">data</span><span class="p">());</span>

<span class="w">  </span><span class="c1">// 3. Collect and return outputs</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">outputs</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p><em>Detailed Implementation:</em> https://github.com/NVIDIA/Fuser/blob/3ac1a4697b6b5c31e4dbb9763b3b6db2f0e0164b/csrc/host_ir/jit.cpp#L1399-L1453</p>
</section>
</section>
<section id="configuration-and-build-options">
<h2>Configuration and Build Options<a class="headerlink" href="#configuration-and-build-options" title="Link to this heading"></a></h2>
<p>Building nvFuser project with <code class="docutils literal notranslate"><span class="pre">NVFUSER_BUILD_HOST_IR_JIT=1</span></code> will enables Host IR JIT as default runtime in Host IR execution path.
Otherwise the default runtime is Host IR Evaluator. In the future, when llvm is fully supported in all build machines, we are able
to get rid of this opt-in flag and rather use <code class="docutils literal notranslate"><span class="pre">enableOption</span></code> to control backend switching after build is done.</p>
<p>Sample build</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">NVFUSER_BUILD_HOST_IR_JIT</span><span class="o">=</span><span class="mi">1</span> <span class="n">pip</span> <span class="n">install</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">build</span><span class="o">-</span><span class="n">isolation</span> <span class="o">-</span><span class="n">e</span> <span class="n">python</span> <span class="o">-</span><span class="n">v</span>
</pre></div>
</div>
<p>or</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">NVFUSER_BUILD_HOST_IR_JIT</span><span class="o">=</span><span class="mi">1</span> <span class="n">_bn</span>
</pre></div>
</div>
</section>
<section id="future-integration-plan">
<h2>Future Integration plan<a class="headerlink" href="#future-integration-plan" title="Link to this heading"></a></h2>
<p>We plan to turn on host IR JIT by default after its function and performance are on par.
Known missing supports and bugs are:</p>
<p><strong>Ops need to be supported:</strong></p>
<ul class="simple">
<li><p>Stream operations</p></li>
<li><p>Communication operations</p></li>
<li><p>remaining Aten fallbacks supported by nvFuser (Alternate way is to compile entire libtorch)</p></li>
</ul>
<p><strong>Other functionalities need to be supported:</strong></p>
<ul class="simple">
<li><p>Alias analysis</p></li>
<li><p>Dynamic shape support in launchKernel</p></li>
<li><p>Correct shape and stride handling for multi device tensor</p></li>
</ul>
<p>A key challenge to turn on Host IR JIT by default is that Host IR JIT and host IR lowering, another opt-in feature, are currently inter-dependent. Host IR JIT requires FusionKernelRuntime to turn on Host IR lowering to generate Host IR. Host IR lowering also needs Host IR JIT to keep latency low.
We plan to follow the steps below to turn on both:</p>
<ul class="simple">
<li><p>Enable currently Host IR path (without JIT) by default for multi-gpu, with loose requirement of latency</p></li>
<li><p>Enable Host IR JIT for multi-gpu with small set of ops coverage.</p></li>
<li><p>Enable Host IR JIT for single-gpu with small set of ops coverage, ensuring no latency regression</p></li>
<li><p>Enable Host IR JIT for single gpu with full set of ops coverage</p></li>
</ul>
<p><strong>Link to intern presentation slides</strong>
http://nv/eS2</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="visibility.html" class="btn btn-neutral float-left" title="Symbol Visibility" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="ldmatrix_stmatrix.html" class="btn btn-neutral float-right" title="LdMatrix and StMatrix Support in NVFuser" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
<img src="../_static/NVIDIA-LogoBlack.svg"/>
<p class="notices">
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_blank">Privacy Policy</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/" target="_blank">Manage My Privacy</a>
|
<a href="https://www.nvidia.com/en-us/preferences/start/" target="_blank">Do Not Sell or Share My Data</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" target="_blank">Terms of Service</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" target="_blank">Accessibility</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" target="_blank">Corporate Policies</a>
|
<a href="https://www.nvidia.com/en-us/product-security/" target="_blank">Product Security</a>
|
<a href="https://www.nvidia.com/en-us/contact/" target="_blank">Contact</a>
</p>

    <p>&#169; Copyright 2023-2025, NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved..</p>

  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
        
        Version: 0.2.34
        
    </span>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>