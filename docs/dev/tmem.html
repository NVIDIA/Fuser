

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tensor Memory Support in NVFuser &mdash; nvFuser 0.2.34 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/css/nvidia_font.css?v=e009355c" />
      <link rel="stylesheet" type="text/css" href="../_static/css/nvidia_footer.css?v=84031d34" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=b85e2031"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Introduction to TMA Support in NVFuser" href="tma.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" > 

          
          
          <a href="../index.html" class="icon icon-home">
            nvFuser
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-menu > p > span.caption-text {
      color: #76b900;
    }

    .wy-menu-vertical p {
      height: 32px;
      line-height: 32px;
      padding: 0 1.618em;
      margin: 12px 0 0;
      display: block;
      font-weight: 700;
      text-transform: uppercase;
      font-size: 85%;
      white-space: nowrap;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }

    .wy-nav-content {
      max-width: 1000px;
    }

    /* override table width restrictions */
    .wy-table-responsive table td, .wy-table-responsive table th {
        /* !important prevents the common CSS stylesheets from
          overriding this as on RTD they are loaded after this stylesheet */
        white-space: normal !important;
    }

    .wy-table-responsive {
        overflow: visible !important;
    }

  </style>
  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  html.writer-html5 .rst-content dl[class]:not(.option-list):not(.field-list):not(.footnote):not(.glossary):not(.simple)>dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }

  html.writer-html4 .rst-content dl:not(.docutils) .property, html.writer-html5 .rst-content dl[class]:not(.option-list):not(.field-list):not(.footnote):not(.glossary):not(.simple) .property {
    text-transform: capitalize;
    display: inline-block;
    padding-right: 8px;
  }
  </style>

  
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Table of Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#installation">Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../installation.html#nightly-nvfuser-pip-wheel">Nightly nvfuser pip wheel</a></li>
<li class="toctree-l3"><a class="reference internal" href="../installation.html#nvfuser-pip-wheel-against-pytorch-stable-release">Nvfuser pip wheel against pytorch stable release</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../installation.html#developer">Developer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../installation.html#install-from-source">Install From Source:</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/general.html">General</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/general.html#statement">Statement</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/general.html#nvfuser_direct.Statement"><code class="docutils literal notranslate"><span class="pre">Statement</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/general.html#val">Val</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/general.html#nvfuser_direct.Val"><code class="docutils literal notranslate"><span class="pre">Val</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.Val.definition"><code class="docutils literal notranslate"><span class="pre">Val.definition()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.Val.is_symbolic"><code class="docutils literal notranslate"><span class="pre">Val.is_symbolic()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.Val.is_tensor"><code class="docutils literal notranslate"><span class="pre">Val.is_tensor()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.Val.uses"><code class="docutils literal notranslate"><span class="pre">Val.uses()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/general.html#expr">Expr</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/general.html#nvfuser_direct.Expr"><code class="docutils literal notranslate"><span class="pre">Expr</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.Expr.input"><code class="docutils literal notranslate"><span class="pre">Expr.input()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.Expr.output"><code class="docutils literal notranslate"><span class="pre">Expr.output()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/general.html#iterdomain">IterDomain</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/general.html#nvfuser_direct.IterDomain"><code class="docutils literal notranslate"><span class="pre">IterDomain</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.IterDomain.extent"><code class="docutils literal notranslate"><span class="pre">IterDomain.extent()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.IterDomain.parallelize"><code class="docutils literal notranslate"><span class="pre">IterDomain.parallelize()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/general.html#tensordomain">TensorDomain</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/general.html#nvfuser_direct.TensorDomain"><code class="docutils literal notranslate"><span class="pre">TensorDomain</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/general.html#tensorview">TensorView</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/general.html#nvfuser_direct.TensorView"><code class="docutils literal notranslate"><span class="pre">TensorView</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.TensorView.axis"><code class="docutils literal notranslate"><span class="pre">TensorView.axis()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.TensorView.cache_after"><code class="docutils literal notranslate"><span class="pre">TensorView.cache_after()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.TensorView.cache_before"><code class="docutils literal notranslate"><span class="pre">TensorView.cache_before()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.TensorView.domain"><code class="docutils literal notranslate"><span class="pre">TensorView.domain()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.TensorView.get_logical_domain"><code class="docutils literal notranslate"><span class="pre">TensorView.get_logical_domain()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.TensorView.get_loop_domain"><code class="docutils literal notranslate"><span class="pre">TensorView.get_loop_domain()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.TensorView.get_root_domain"><code class="docutils literal notranslate"><span class="pre">TensorView.get_root_domain()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.TensorView.has_root"><code class="docutils literal notranslate"><span class="pre">TensorView.has_root()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.TensorView.merge"><code class="docutils literal notranslate"><span class="pre">TensorView.merge()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.TensorView.ndim"><code class="docutils literal notranslate"><span class="pre">TensorView.ndim</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.TensorView.reorder"><code class="docutils literal notranslate"><span class="pre">TensorView.reorder()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.TensorView.rfactor"><code class="docutils literal notranslate"><span class="pre">TensorView.rfactor()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.TensorView.set_allocation_domain"><code class="docutils literal notranslate"><span class="pre">TensorView.set_allocation_domain()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.TensorView.set_device_mesh"><code class="docutils literal notranslate"><span class="pre">TensorView.set_device_mesh()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.TensorView.set_memory_type"><code class="docutils literal notranslate"><span class="pre">TensorView.set_memory_type()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.TensorView.shape"><code class="docutils literal notranslate"><span class="pre">TensorView.shape()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.TensorView.size"><code class="docutils literal notranslate"><span class="pre">TensorView.size()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.TensorView.split"><code class="docutils literal notranslate"><span class="pre">TensorView.split()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/general.html#fusionexecutorcache">FusionExecutorCache</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionExecutorCache"><code class="docutils literal notranslate"><span class="pre">FusionExecutorCache</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionExecutorCache.execute"><code class="docutils literal notranslate"><span class="pre">FusionExecutorCache.execute()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionExecutorCache.fusion"><code class="docutils literal notranslate"><span class="pre">FusionExecutorCache.fusion()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionExecutorCache.get_cuda_kernel"><code class="docutils literal notranslate"><span class="pre">FusionExecutorCache.get_cuda_kernel()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionExecutorCache.get_most_recent_scheduled_ir"><code class="docutils literal notranslate"><span class="pre">FusionExecutorCache.get_most_recent_scheduled_ir()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionExecutorCache.get_output_shardings"><code class="docutils literal notranslate"><span class="pre">FusionExecutorCache.get_output_shardings()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionExecutorCache.get_scheduled_ir"><code class="docutils literal notranslate"><span class="pre">FusionExecutorCache.get_scheduled_ir()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionExecutorCache.is_compiled"><code class="docutils literal notranslate"><span class="pre">FusionExecutorCache.is_compiled()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/general.html#kernelexecutor">KernelExecutor</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/general.html#nvfuser_direct.KernelExecutor"><code class="docutils literal notranslate"><span class="pre">KernelExecutor</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.KernelExecutor.compile"><code class="docutils literal notranslate"><span class="pre">KernelExecutor.compile()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.KernelExecutor.is_compiled"><code class="docutils literal notranslate"><span class="pre">KernelExecutor.is_compiled()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.KernelExecutor.run"><code class="docutils literal notranslate"><span class="pre">KernelExecutor.run()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/general.html#fusion-definition">Fusion Definition</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionDefinition"><code class="docutils literal notranslate"><span class="pre">FusionDefinition</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionDefinition.add_output"><code class="docutils literal notranslate"><span class="pre">FusionDefinition.add_output()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionDefinition.define_scalar"><code class="docutils literal notranslate"><span class="pre">FusionDefinition.define_scalar()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionDefinition.define_tensor"><code class="docutils literal notranslate"><span class="pre">FusionDefinition.define_tensor()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionDefinition.define_vector"><code class="docutils literal notranslate"><span class="pre">FusionDefinition.define_vector()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionDefinition.execute"><code class="docutils literal notranslate"><span class="pre">FusionDefinition.execute()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionDefinition.from_pytorch"><code class="docutils literal notranslate"><span class="pre">FusionDefinition.from_pytorch()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionDefinition.fusion"><code class="docutils literal notranslate"><span class="pre">FusionDefinition.fusion</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionDefinition.last_repro_script"><code class="docutils literal notranslate"><span class="pre">FusionDefinition.last_repro_script()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionDefinition.manual_execute"><code class="docutils literal notranslate"><span class="pre">FusionDefinition.manual_execute()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionDefinition.manual_validate"><code class="docutils literal notranslate"><span class="pre">FusionDefinition.manual_validate()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionDefinition.repro_script_for"><code class="docutils literal notranslate"><span class="pre">FusionDefinition.repro_script_for()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionDefinition.validate"><code class="docutils literal notranslate"><span class="pre">FusionDefinition.validate()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/general.html#nvfuser_direct.FusionDefinition.validate_definition"><code class="docutils literal notranslate"><span class="pre">FusionDefinition.validate_definition()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api/ops.html">Operations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/ops.html#module-nvfuser_direct.ops">Ops</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.abs"><code class="docutils literal notranslate"><span class="pre">abs()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.acos"><code class="docutils literal notranslate"><span class="pre">acos()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.acosh"><code class="docutils literal notranslate"><span class="pre">acosh()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.add"><code class="docutils literal notranslate"><span class="pre">add()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.addcmul"><code class="docutils literal notranslate"><span class="pre">addcmul()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.argsort"><code class="docutils literal notranslate"><span class="pre">argsort()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.asin"><code class="docutils literal notranslate"><span class="pre">asin()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.asinh"><code class="docutils literal notranslate"><span class="pre">asinh()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.atan"><code class="docutils literal notranslate"><span class="pre">atan()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.atan2"><code class="docutils literal notranslate"><span class="pre">atan2()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.atanh"><code class="docutils literal notranslate"><span class="pre">atanh()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.bitwise_and"><code class="docutils literal notranslate"><span class="pre">bitwise_and()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.bitwise_left_shift"><code class="docutils literal notranslate"><span class="pre">bitwise_left_shift()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.bitwise_not"><code class="docutils literal notranslate"><span class="pre">bitwise_not()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.bitwise_or"><code class="docutils literal notranslate"><span class="pre">bitwise_or()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.bitwise_right_shift"><code class="docutils literal notranslate"><span class="pre">bitwise_right_shift()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.bitwise_xor"><code class="docutils literal notranslate"><span class="pre">bitwise_xor()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.broadcast"><code class="docutils literal notranslate"><span class="pre">broadcast()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.broadcast_in_dim"><code class="docutils literal notranslate"><span class="pre">broadcast_in_dim()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.cast"><code class="docutils literal notranslate"><span class="pre">cast()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.ceil"><code class="docutils literal notranslate"><span class="pre">ceil()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.clamp"><code class="docutils literal notranslate"><span class="pre">clamp()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.complex"><code class="docutils literal notranslate"><span class="pre">complex()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.cos"><code class="docutils literal notranslate"><span class="pre">cos()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.cosh"><code class="docutils literal notranslate"><span class="pre">cosh()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.cummax"><code class="docutils literal notranslate"><span class="pre">cummax()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.cummin"><code class="docutils literal notranslate"><span class="pre">cummin()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.cumprod"><code class="docutils literal notranslate"><span class="pre">cumprod()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.cumsum"><code class="docutils literal notranslate"><span class="pre">cumsum()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.cutlass_nvfp4_grouped_mm"><code class="docutils literal notranslate"><span class="pre">cutlass_nvfp4_grouped_mm()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.div"><code class="docutils literal notranslate"><span class="pre">div()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.embedding_fwd"><code class="docutils literal notranslate"><span class="pre">embedding_fwd()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.eq"><code class="docutils literal notranslate"><span class="pre">eq()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.erf"><code class="docutils literal notranslate"><span class="pre">erf()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.erfc"><code class="docutils literal notranslate"><span class="pre">erfc()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.erfcinv"><code class="docutils literal notranslate"><span class="pre">erfcinv()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.erfinv"><code class="docutils literal notranslate"><span class="pre">erfinv()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.exp"><code class="docutils literal notranslate"><span class="pre">exp()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.exp2"><code class="docutils literal notranslate"><span class="pre">exp2()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.expand"><code class="docutils literal notranslate"><span class="pre">expand()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.expm1"><code class="docutils literal notranslate"><span class="pre">expm1()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.floor"><code class="docutils literal notranslate"><span class="pre">floor()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.fmod"><code class="docutils literal notranslate"><span class="pre">fmod()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.frac"><code class="docutils literal notranslate"><span class="pre">frac()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.full"><code class="docutils literal notranslate"><span class="pre">full()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.gather"><code class="docutils literal notranslate"><span class="pre">gather()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.gcd"><code class="docutils literal notranslate"><span class="pre">gcd()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.ge"><code class="docutils literal notranslate"><span class="pre">ge()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.grouped_mm"><code class="docutils literal notranslate"><span class="pre">grouped_mm()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.gt"><code class="docutils literal notranslate"><span class="pre">gt()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.imag"><code class="docutils literal notranslate"><span class="pre">imag()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.index_select"><code class="docutils literal notranslate"><span class="pre">index_select()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.iota"><code class="docutils literal notranslate"><span class="pre">iota()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.isfinite"><code class="docutils literal notranslate"><span class="pre">isfinite()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.isinf"><code class="docutils literal notranslate"><span class="pre">isinf()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.isnan"><code class="docutils literal notranslate"><span class="pre">isnan()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.isneginf"><code class="docutils literal notranslate"><span class="pre">isneginf()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.isposinf"><code class="docutils literal notranslate"><span class="pre">isposinf()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.isreal"><code class="docutils literal notranslate"><span class="pre">isreal()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.le"><code class="docutils literal notranslate"><span class="pre">le()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.lerp"><code class="docutils literal notranslate"><span class="pre">lerp()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.lgamma"><code class="docutils literal notranslate"><span class="pre">lgamma()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.linear"><code class="docutils literal notranslate"><span class="pre">linear()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.log"><code class="docutils literal notranslate"><span class="pre">log()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.log10"><code class="docutils literal notranslate"><span class="pre">log10()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.log1p"><code class="docutils literal notranslate"><span class="pre">log1p()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.log2"><code class="docutils literal notranslate"><span class="pre">log2()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.logical_and"><code class="docutils literal notranslate"><span class="pre">logical_and()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.logical_not"><code class="docutils literal notranslate"><span class="pre">logical_not()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.logical_or"><code class="docutils literal notranslate"><span class="pre">logical_or()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.logical_right_shift"><code class="docutils literal notranslate"><span class="pre">logical_right_shift()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.lt"><code class="docutils literal notranslate"><span class="pre">lt()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.matmul"><code class="docutils literal notranslate"><span class="pre">matmul()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.max"><code class="docutils literal notranslate"><span class="pre">max()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.maximum"><code class="docutils literal notranslate"><span class="pre">maximum()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.min"><code class="docutils literal notranslate"><span class="pre">min()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.minimum"><code class="docutils literal notranslate"><span class="pre">minimum()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.mod"><code class="docutils literal notranslate"><span class="pre">mod()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.mul"><code class="docutils literal notranslate"><span class="pre">mul()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.ne"><code class="docutils literal notranslate"><span class="pre">ne()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.neg"><code class="docutils literal notranslate"><span class="pre">neg()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.nextafter"><code class="docutils literal notranslate"><span class="pre">nextafter()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.normal"><code class="docutils literal notranslate"><span class="pre">normal()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.pad"><code class="docutils literal notranslate"><span class="pre">pad()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.permute"><code class="docutils literal notranslate"><span class="pre">permute()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.pow"><code class="docutils literal notranslate"><span class="pre">pow()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.prod"><code class="docutils literal notranslate"><span class="pre">prod()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.rand_like"><code class="docutils literal notranslate"><span class="pre">rand_like()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.randn_like"><code class="docutils literal notranslate"><span class="pre">randn_like()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.real"><code class="docutils literal notranslate"><span class="pre">real()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.reciprocal"><code class="docutils literal notranslate"><span class="pre">reciprocal()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.relu"><code class="docutils literal notranslate"><span class="pre">relu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.remainder"><code class="docutils literal notranslate"><span class="pre">remainder()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.reshape"><code class="docutils literal notranslate"><span class="pre">reshape()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.round"><code class="docutils literal notranslate"><span class="pre">round()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.rsqrt"><code class="docutils literal notranslate"><span class="pre">rsqrt()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.scaled_mm"><code class="docutils literal notranslate"><span class="pre">scaled_mm()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.scatter"><code class="docutils literal notranslate"><span class="pre">scatter()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.sdpfa_bwd"><code class="docutils literal notranslate"><span class="pre">sdpfa_bwd()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.sdpfa_fwd"><code class="docutils literal notranslate"><span class="pre">sdpfa_fwd()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.segment_set"><code class="docutils literal notranslate"><span class="pre">segment_set()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.select"><code class="docutils literal notranslate"><span class="pre">select()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.set"><code class="docutils literal notranslate"><span class="pre">set()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.shape"><code class="docutils literal notranslate"><span class="pre">shape()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.sigmoid"><code class="docutils literal notranslate"><span class="pre">sigmoid()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.sign"><code class="docutils literal notranslate"><span class="pre">sign()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.signbit"><code class="docutils literal notranslate"><span class="pre">signbit()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.silu"><code class="docutils literal notranslate"><span class="pre">silu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.sin"><code class="docutils literal notranslate"><span class="pre">sin()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.sinh"><code class="docutils literal notranslate"><span class="pre">sinh()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.size"><code class="docutils literal notranslate"><span class="pre">size()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.slice"><code class="docutils literal notranslate"><span class="pre">slice()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.sqrt"><code class="docutils literal notranslate"><span class="pre">sqrt()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.squeeze"><code class="docutils literal notranslate"><span class="pre">squeeze()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.stride_order"><code class="docutils literal notranslate"><span class="pre">stride_order()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.sub"><code class="docutils literal notranslate"><span class="pre">sub()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.sum"><code class="docutils literal notranslate"><span class="pre">sum()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.take_along_axis"><code class="docutils literal notranslate"><span class="pre">take_along_axis()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.tan"><code class="docutils literal notranslate"><span class="pre">tan()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.tanh"><code class="docutils literal notranslate"><span class="pre">tanh()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.threshold"><code class="docutils literal notranslate"><span class="pre">threshold()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.topk"><code class="docutils literal notranslate"><span class="pre">topk()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.triu"><code class="docutils literal notranslate"><span class="pre">triu()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.truediv"><code class="docutils literal notranslate"><span class="pre">truediv()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.trunc"><code class="docutils literal notranslate"><span class="pre">trunc()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.uniform"><code class="docutils literal notranslate"><span class="pre">uniform()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.var"><code class="docutils literal notranslate"><span class="pre">var()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.var_mean"><code class="docutils literal notranslate"><span class="pre">var_mean()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.welford"><code class="docutils literal notranslate"><span class="pre">welford()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../api/ops.html#nvfuser_direct.ops.where"><code class="docutils literal notranslate"><span class="pre">where()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api/multidevice.html">Multidevice</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/multidevice.html#communicator">Communicator</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/multidevice.html#nvfuser_direct.multidevice.Communicator"><code class="docutils literal notranslate"><span class="pre">Communicator</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/multidevice.html#nvfuser_direct.multidevice.Communicator.barrier"><code class="docutils literal notranslate"><span class="pre">Communicator.barrier()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/multidevice.html#nvfuser_direct.multidevice.Communicator.instance"><code class="docutils literal notranslate"><span class="pre">Communicator.instance()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/multidevice.html#nvfuser_direct.multidevice.Communicator.local_rank"><code class="docutils literal notranslate"><span class="pre">Communicator.local_rank()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/multidevice.html#nvfuser_direct.multidevice.Communicator.local_size"><code class="docutils literal notranslate"><span class="pre">Communicator.local_size()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/multidevice.html#nvfuser_direct.multidevice.Communicator.rank"><code class="docutils literal notranslate"><span class="pre">Communicator.rank()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/multidevice.html#nvfuser_direct.multidevice.Communicator.size"><code class="docutils literal notranslate"><span class="pre">Communicator.size()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/multidevice.html#devicemesh">DeviceMesh</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/multidevice.html#nvfuser_direct.multidevice.DeviceMesh"><code class="docutils literal notranslate"><span class="pre">DeviceMesh</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/multidevice.html#nvfuser_direct.multidevice.DeviceMesh.shape"><code class="docutils literal notranslate"><span class="pre">DeviceMesh.shape</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/multidevice.html#nvfuser_direct.multidevice.DeviceMesh.shard_tensor"><code class="docutils literal notranslate"><span class="pre">DeviceMesh.shard_tensor()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/multidevice.html#nvfuser_direct.multidevice.DeviceMesh.size"><code class="docutils literal notranslate"><span class="pre">DeviceMesh.size</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/multidevice.html#sharding">Sharding</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/multidevice.html#nvfuser_direct.multidevice.Sharding"><code class="docutils literal notranslate"><span class="pre">Sharding</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/multidevice.html#nvfuser_direct.multidevice.Sharding.axis_sharded_on"><code class="docutils literal notranslate"><span class="pre">Sharding.axis_sharded_on()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/multidevice.html#nvfuser_direct.multidevice.Sharding.mesh"><code class="docutils literal notranslate"><span class="pre">Sharding.mesh</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api/enum.html">Enums</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/enum.html#communicatorbackend-types">CommunicatorBackend Types</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/enum.html#nvfuser_direct.CommunicatorBackend"><code class="docutils literal notranslate"><span class="pre">CommunicatorBackend</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/enum.html#nvfuser_direct.CommunicatorBackend.name"><code class="docutils literal notranslate"><span class="pre">CommunicatorBackend.name</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/enum.html#data-types">Data Types</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/enum.html#nvfuser_direct.DataType"><code class="docutils literal notranslate"><span class="pre">DataType</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/enum.html#nvfuser_direct.DataType.name"><code class="docutils literal notranslate"><span class="pre">DataType.name</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/enum.html#parallel-types">Parallel Types</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/enum.html#nvfuser_direct.ParallelType"><code class="docutils literal notranslate"><span class="pre">ParallelType</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/enum.html#nvfuser_direct.ParallelType.name"><code class="docutils literal notranslate"><span class="pre">ParallelType.name</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/enum.html#scheduler-types">Scheduler Types</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/enum.html#nvfuser_direct.SchedulerType"><code class="docutils literal notranslate"><span class="pre">SchedulerType</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/enum.html#nvfuser_direct.SchedulerType.name"><code class="docutils literal notranslate"><span class="pre">SchedulerType.name</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api/pod_class.html">Data classes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/pod_class.html#launchparams">LaunchParams</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/pod_class.html#nvfuser_direct.LaunchParams"><code class="docutils literal notranslate"><span class="pre">LaunchParams</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/pod_class.html#nvfuser_direct.LaunchParams.bdimx"><code class="docutils literal notranslate"><span class="pre">LaunchParams.bdimx</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/pod_class.html#nvfuser_direct.LaunchParams.bdimy"><code class="docutils literal notranslate"><span class="pre">LaunchParams.bdimy</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/pod_class.html#nvfuser_direct.LaunchParams.bdimz"><code class="docutils literal notranslate"><span class="pre">LaunchParams.bdimz</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/pod_class.html#nvfuser_direct.LaunchParams.gdimx"><code class="docutils literal notranslate"><span class="pre">LaunchParams.gdimx</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/pod_class.html#nvfuser_direct.LaunchParams.gdimy"><code class="docutils literal notranslate"><span class="pre">LaunchParams.gdimy</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/pod_class.html#nvfuser_direct.LaunchParams.gdimz"><code class="docutils literal notranslate"><span class="pre">LaunchParams.gdimz</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/pod_class.html#compileparams">CompileParams</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../api/pod_class.html#nvfuser_direct.CompileParams"><code class="docutils literal notranslate"><span class="pre">CompileParams</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../api/pod_class.html#nvfuser_direct.CompileParams.device"><code class="docutils literal notranslate"><span class="pre">CompileParams.device</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/pod_class.html#nvfuser_direct.CompileParams.enable_magic_zero"><code class="docutils literal notranslate"><span class="pre">CompileParams.enable_magic_zero</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/pod_class.html#nvfuser_direct.CompileParams.enable_ptxas_verbose"><code class="docutils literal notranslate"><span class="pre">CompileParams.enable_ptxas_verbose</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/pod_class.html#nvfuser_direct.CompileParams.include_paths"><code class="docutils literal notranslate"><span class="pre">CompileParams.include_paths</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/pod_class.html#nvfuser_direct.CompileParams.index_type"><code class="docutils literal notranslate"><span class="pre">CompileParams.index_type</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../api/pod_class.html#nvfuser_direct.CompileParams.maxrregcount"><code class="docutils literal notranslate"><span class="pre">CompileParams.maxrregcount</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer References</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../reading/divisibility-of-split.html">Divisibility of Split</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../reading/divisibility-of-split.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reading/divisibility-of-split.html#predication">Predication</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reading/divisibility-of-split.html#allocation-and-correctness-model">Allocation and correctness model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reading/divisibility-of-split.html#properties-of-split">Properties of split</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../reading/divisibility-of-split.html#merge-then-split-vs-split-then-merge">Merge-then-split vs split-then-merge</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reading/divisibility-of-split.html#merging-discontiguous-iterdomains">Merging discontiguous IterDomains</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../reading/divisibility-of-split.html#question">Question</a></li>
<li class="toctree-l4"><a class="reference internal" href="../reading/divisibility-of-split.html#answer">Answer</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../reading/iterdomain.html">The Mathematical Theory of IterDomain</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../reading/iterdomain.html#iterdomain-transformations">1. IterDomain Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reading/iterdomain.html#properties-of-iterdomain-transformations">2. Properties of IterDomain Transformations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../reading/multigpu.html">Multi-GPU Support in nvFuser</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../reading/multigpu.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reading/multigpu.html#user-api">User API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reading/multigpu.html#parallelisms">Parallelisms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../reading/multigpu.html#tensor-parallelism-tp">Tensor Parallelism (TP)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../reading/multigpu.html#sharding-propagation">Sharding Propagation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../reading/multigpu.html#communication-computation-decomposition">Communication-computation Decomposition</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../reading/multigpu.html#sequence-parallelism-sp">Sequence Parallelism (SP)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../reading/multigpu.html#id1">Sharding Propagation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../reading/multigpu.html#id2">Communication-computation Decomposition</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../reading/multigpu.html#overlap-communication-with-gemm-via-decomposition">Overlap Communication with GEMM via Decomposition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reading/multigpu.html#distributed-data-parallelism-ddp">Distributed Data Parallelism (DDP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reading/multigpu.html#fully-sharded-data-parallelism-fsdp">Fully Sharded Data Parallelism (FSDP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reading/multigpu.html#pipeline-parallelism-pp">Pipeline Parallelism (PP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../reading/multigpu.html#context-parallelism-cp">Context Parallelism (CP)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../reading/tma-modeling-in-depth.html">TMA Modeling In Depth</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../reading/tma-modeling-in-depth.html#what-is-tma">What is TMA?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reading/tma-modeling-in-depth.html#correctness-model">Correctness model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reading/tma-modeling-in-depth.html#the-unachievability-of-strong-correctness-for-indivisible-element-stride">The unachievability of strong correctness for indivisible element stride</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reading/tma-modeling-in-depth.html#the-lowering-strategy">The lowering strategy</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="debug.html">Debugging</a><ul>
<li class="toctree-l2"><a class="reference internal" href="debug.html#debug-a-failing-nvfuser-script">Debug a failing nvFuser script</a><ul>
<li class="toctree-l3"><a class="reference internal" href="debug.html#nvfuser-dump">NVFUSER_DUMP</a></li>
<li class="toctree-l3"><a class="reference internal" href="debug.html#gdb">gdb</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="debug.html#debug-memory-corruption-using-asan">Debug memory corruption using <code class="docutils literal notranslate"><span class="pre">asan</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="debug.html#if-built-with-clang">If built with clang</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="debug.html#debug-memory-leaks-or-excessive-memory-usage">Debug memory leaks or excessive memory usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="debug.html#debug-slow-kernels">Debug slow kernels</a></li>
<li class="toctree-l2"><a class="reference internal" href="debug.html#debug-slow-cpu-execution">Debug slow CPU execution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="visibility.html">Symbol Visibility</a><ul>
<li class="toctree-l2"><a class="reference internal" href="visibility.html#faq">FAQ</a><ul>
<li class="toctree-l3"><a class="reference internal" href="visibility.html#should-i-mark-a-method-visible-or-the-whole-class">Should I mark a method visible or the whole class?</a></li>
<li class="toctree-l3"><a class="reference internal" href="visibility.html#i-see-an-undefined-reference-to-typeinfo-for-class-how-do-i-fix-this">I see an undefined reference to <code class="docutils literal notranslate"><span class="pre">typeinfo</span> <span class="pre">for</span> <span class="pre">&lt;class&gt;</span></code>. How do I fix this?</a></li>
<li class="toctree-l3"><a class="reference internal" href="visibility.html#i-see-an-undefined-reference-to-vtable-for-class-how-do-i-fix-this">I see an undefined reference to <code class="docutils literal notranslate"><span class="pre">vtable</span> <span class="pre">for</span> <span class="pre">&lt;class&gt;</span></code>. How do I fix this?</a></li>
<li class="toctree-l3"><a class="reference internal" href="visibility.html#i-see-that-foo-is-visible-but-i-do-not-think-it-needs-to-be">I see that <code class="docutils literal notranslate"><span class="pre">Foo</span></code> is visible but I do not think it needs to be.</a></li>
<li class="toctree-l3"><a class="reference internal" href="visibility.html#should-i-mark-my-new-method-or-class-as-nvf-api">Should I mark my new method or class as <code class="docutils literal notranslate"><span class="pre">NVF_API</span></code>?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="visibility.html#symbol-visibility-checking">Symbol Visibility Checking</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="host_ir_jit.html">Host IR JIT Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="host_ir_jit.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="host_ir_jit.html#jit-compilation-process">JIT Compilation Process</a><ul>
<li class="toctree-l3"><a class="reference internal" href="host_ir_jit.html#llvm-integration">1. LLVM Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="host_ir_jit.html#compilation-pipeline">2. Compilation Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="host_ir_jit.html#external-function-integration">3. External Function Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="host_ir_jit.html#ir-translation">3. IR Translation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="host_ir_jit.html#runtime-execution">Runtime Execution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="host_ir_jit.html#function-interface">1. Function Interface</a></li>
<li class="toctree-l3"><a class="reference internal" href="host_ir_jit.html#execution-flow">2. Execution Flow</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="host_ir_jit.html#configuration-and-build-options">Configuration and Build Options</a></li>
<li class="toctree-l2"><a class="reference internal" href="host_ir_jit.html#future-integration-plan">Future Integration plan</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ldmatrix_stmatrix.html">LdMatrix and StMatrix Support in NVFuser</a><ul>
<li class="toctree-l2"><a class="reference internal" href="ldmatrix_stmatrix.html#what-is-ldmatrix">What is LdMatrix?</a></li>
<li class="toctree-l2"><a class="reference internal" href="ldmatrix_stmatrix.html#what-is-stmatrix">What is StMatrix?</a></li>
<li class="toctree-l2"><a class="reference internal" href="ldmatrix_stmatrix.html#general-details">General Details</a><ul>
<li class="toctree-l3"><a class="reference internal" href="ldmatrix_stmatrix.html#indices-shared-memory-tensor">Indices shared memory tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="ldmatrix_stmatrix.html#indices-for-register-tensor">Indices for register tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="ldmatrix_stmatrix.html#register-layout-for-one-8x8-matrix-with-16-bit-elements">Register layout for one 8x8 Matrix with 16-bit elements</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ldmatrix_stmatrix.html#example-1-a-copy-kernel-using-tma-ldmatrix-and-stmatrix">Example 1: A copy kernel using TMA, LdMatrix, and StMatrix.</a><ul>
<li class="toctree-l2"><a class="reference internal" href="ldmatrix_stmatrix.html#how-to-compute-the-index-into-register-tensorview">How to compute the index into register TensorView?</a></li>
<li class="toctree-l2"><a class="reference internal" href="ldmatrix_stmatrix.html#how-to-compute-the-index-into-shared-memory-tensorview">How to compute the index into shared memory TensorView?</a><ul>
<li class="toctree-l3"><a class="reference internal" href="ldmatrix_stmatrix.html#figure-1-loop-domain-for-ldmatrix-and-stmatrix">Figure 1: Loop domain for LdMatrix and StMatrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="ldmatrix_stmatrix.html#figure-2-tma-shared-memory-allocation-domain">Figure 2: TMA shared memory allocation domain</a></li>
<li class="toctree-l3"><a class="reference internal" href="ldmatrix_stmatrix.html#figure-3-map-from-ldmatrix-stmatrix-loop-domain-to-tma-shared-memory-allocation-domain">Figure 3: Map from LdMatrix / StMatrix loop domain to TMA shared memory allocation domain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="ldmatrix_stmatrix.html#derivation-of-figure-3">Derivation of Figure 3</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="ldmatrix_stmatrix.html#code-walkthrough">Code Walkthrough</a><ul>
<li class="toctree-l3"><a class="reference internal" href="ldmatrix_stmatrix.html#scheduleldstmatrix-function">scheduleLdStMatrix function</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tma.html">Introduction to TMA Support in NVFuser</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tma.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="tma.html#schedule">Schedule</a><ul>
<li class="toctree-l3"><a class="reference internal" href="tma.html#step-1-define-tma-domain">Step 1: define TMA domain</a></li>
<li class="toctree-l3"><a class="reference internal" href="tma.html#step-2-define-box">Step 2: define box</a><ul>
<li class="toctree-l4"><a class="reference internal" href="tma.html#the-canonical-way-to-define-box">The canonical way to define box</a></li>
<li class="toctree-l4"><a class="reference internal" href="tma.html#define-box-by-mathematical-equivalence">Define box by mathematical equivalence</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="tma.html#step-3-define-tile">Step 3: define tile</a></li>
<li class="toctree-l3"><a class="reference internal" href="tma.html#step-4-schedule-the-shared-memory-tensor">Step 4: schedule the shared memory tensor</a><ul>
<li class="toctree-l4"><a class="reference internal" href="tma.html#data-swizzle">Data swizzle</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="tma.html#step-5-schedule-the-consumer-tensor">Step 5: schedule the consumer tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="tma.html#code-walk-through">Code walk-through</a></li>
<li class="toctree-l3"><a class="reference internal" href="tma.html#examples">Examples</a><ul>
<li class="toctree-l4"><a class="reference internal" href="tma.html#example-1-tma-load-inputs-and-vectorize-store-output-pointwise-kernel">Example 1: tma-load inputs and vectorize-store output pointwise kernel</a></li>
<li class="toctree-l4"><a class="reference internal" href="tma.html#example-2-broadcast-kernel-with-discontiguous-input">Example 2: broadcast kernel with discontiguous input</a></li>
<li class="toctree-l4"><a class="reference internal" href="tma.html#example-3-bank-conflict-free-transpose-of-32bit-data">Example 3: bank-conflict-free transpose of 32bit data</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tensor Memory Support in NVFuser</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#review-of-inlining-and-parallelization">Review of inlining and parallelization</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tensor-memory">Tensor memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="#the-loop-domain-of-tmem-load-and-store">The loop domain of TMem load and store</a></li>
<li class="toctree-l2"><a class="reference internal" href="#vectorization-of-tmem-load-and-store">Vectorization of TMem load and store</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">nvFuser</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Tensor Memory Support in NVFuser</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/dev/tmem.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p>/*</p>
<blockquote>
<div><p>[!NOTE]
This file is both a <a class="reference internal" href="#../../tests/cpp/tutorial_tmem.cpp"><span class="xref myst">cpp</span></a> and a Markdown.
You may see some strange symbols in the rendered Markdown.
It is difficult to avoid them. But they should not affect reading.
All the unit tests displayed here are executable from the <code class="docutils literal notranslate"><span class="pre">test_tutorial</span></code>
binary</p>
</div></blockquote>
<!--*/
#pragma GCC diagnostic ignored "-Wcomment"
// clang-format off
/*
 * SPDX-FileCopyrightText: Copyright (c) 2023-present NVIDIA CORPORATION & AFFILIATES.
 * All rights reserved.
 * SPDX-License-Identifier: BSD-3-Clause
 */
// clang-format on
/*-->
<p>To see prints in the test, change below to <code class="docutils literal notranslate"><span class="pre">true</span></code>:<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">constexpr</span><span class="w"> </span><span class="k">static</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">verbose</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="tensor-memory-support-in-nvfuser">
<h1>Tensor Memory Support in NVFuser<a class="headerlink" href="#tensor-memory-support-in-nvfuser" title="Link to this heading"></a></h1>
<!--*/
#include <sstream>
#include <string>

#include <gmock/gmock-matchers.h>
#include <gtest/gtest.h>

#include <tests/cpp/utils.h>
#include <tests/cpp/validator.h>

#include <codegen.h>
#include <ops/alias.h>
#include <scheduler/tools/inlining.h>
#include <scheduler/utils.h>

namespace nvfuser {

using ReviewInliningParallelization = NVFuserTest;
using TMemTutorialC = NVFuserTest;
using TMemTutorialR = BlackwellBase;

/* -->
<p>Tensor memory is new a memory type added in the Blackwell architecture.
Similar to shared memory, it is a memory in the SM that is accessible by threads
in the CTA. Although there are many differences between tensor memory and shared
memory, the fact that they are shared by threads and distributed across
different CTAs makes the behavior of tensor memory similar to shared memory when
talking about allocation and how it is affected by inlining and parallelization.</p>
<p>Before diving deep into tensor memory, let’s first do a quick review of inlining
and parallelization, and how they impact allocation and indexing. This review
will give us a rough idea of how tensor memory should behave.</p>
<section id="review-of-inlining-and-parallelization">
<h2>Review of inlining and parallelization<a class="headerlink" href="#review-of-inlining-and-parallelization" title="Link to this heading"></a></h2>
<p>Let’s consider a simple gmem-&gt;smem-&gt;gmem copy kernel. Let’s look at the kernels
with different inlining and parallelization strategies:<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TEST_F</span><span class="p">(</span><span class="n">ReviewInliningParallelization</span><span class="p">,</span><span class="w"> </span><span class="n">GSGCopy</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">t0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">rand</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">},</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Naive copy kernel, no inlining, no parallelization</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">    </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">    </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">});</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>

<span class="w">    </span><span class="n">tv1</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Shared</span><span class="p">);</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">fusion</span><span class="p">.</span><span class="n">printKernel</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">KernelExecutor</span><span class="w"> </span><span class="n">ke</span><span class="p">;</span>
<span class="w">    </span><span class="n">ke</span><span class="p">.</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ke</span><span class="p">.</span><span class="n">run</span><span class="p">({</span><span class="n">t0</span><span class="p">});</span>
<span class="w">    </span><span class="n">EXPECT_TRUE</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">equal</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">as</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(),</span><span class="w"> </span><span class="n">t0</span><span class="p">));</span>
<span class="w">    </span><span class="c1">// T1 is allocated in full size</span>
<span class="w">    </span><span class="n">EXPECT_EQ</span><span class="p">(</span><span class="n">ke</span><span class="p">.</span><span class="n">lastLaunchParams</span><span class="p">().</span><span class="n">smem</span><span class="p">(),</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// No inlining, has BIDx parallelization</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">    </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">    </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">});</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>

<span class="w">    </span><span class="n">tv1</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Shared</span><span class="p">);</span>
<span class="w">    </span><span class="n">tv1</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">BIDx</span><span class="p">);</span>
<span class="w">    </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">BIDx</span><span class="p">);</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">fusion</span><span class="p">.</span><span class="n">printKernel</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">KernelExecutor</span><span class="w"> </span><span class="n">ke</span><span class="p">;</span>
<span class="w">    </span><span class="n">ke</span><span class="p">.</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ke</span><span class="p">.</span><span class="n">run</span><span class="p">({</span><span class="n">t0</span><span class="p">});</span>
<span class="w">    </span><span class="n">EXPECT_TRUE</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">equal</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">as</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(),</span><span class="w"> </span><span class="n">t0</span><span class="p">));</span>
<span class="w">    </span><span class="c1">// Because smem is distributed across different CTAs, only the first</span>
<span class="w">    </span><span class="c1">// dimension of T1 is allocated.</span>
<span class="w">    </span><span class="n">EXPECT_EQ</span><span class="p">(</span><span class="n">ke</span><span class="p">.</span><span class="n">lastLaunchParams</span><span class="p">().</span><span class="n">smem</span><span class="p">(),</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Inline at 1, no parallelization</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">    </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">    </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">});</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>

<span class="w">    </span><span class="n">tv1</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Shared</span><span class="p">);</span>
<span class="w">    </span><span class="n">tv1</span><span class="o">-&gt;</span><span class="n">inlineAt</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">fusion</span><span class="p">.</span><span class="n">printKernel</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">KernelExecutor</span><span class="w"> </span><span class="n">ke</span><span class="p">;</span>
<span class="w">    </span><span class="n">ke</span><span class="p">.</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ke</span><span class="p">.</span><span class="n">run</span><span class="p">({</span><span class="n">t0</span><span class="p">});</span>
<span class="w">    </span><span class="n">EXPECT_TRUE</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">equal</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">as</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(),</span><span class="w"> </span><span class="n">t0</span><span class="p">));</span>
<span class="w">    </span><span class="c1">// Because the first dimension of T1 is inlined, inside the outer loop, T1</span>
<span class="w">    </span><span class="c1">// is consumed right after it is produced. So the first dimension of T1 is</span>
<span class="w">    </span><span class="c1">// not allocated.</span>
<span class="w">    </span><span class="n">EXPECT_EQ</span><span class="p">(</span><span class="n">ke</span><span class="p">.</span><span class="n">lastLaunchParams</span><span class="p">().</span><span class="n">smem</span><span class="p">(),</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Inline at 1, with BIDx parallelization</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">    </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">    </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">});</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>

<span class="w">    </span><span class="n">tv1</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Shared</span><span class="p">);</span>
<span class="w">    </span><span class="n">tv1</span><span class="o">-&gt;</span><span class="n">inlineAt</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="w">    </span><span class="n">tv1</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">BIDx</span><span class="p">);</span>
<span class="w">    </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">BIDx</span><span class="p">);</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">fusion</span><span class="p">.</span><span class="n">printKernel</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">KernelExecutor</span><span class="w"> </span><span class="n">ke</span><span class="p">;</span>
<span class="w">    </span><span class="n">ke</span><span class="p">.</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ke</span><span class="p">.</span><span class="n">run</span><span class="p">({</span><span class="n">t0</span><span class="p">});</span>
<span class="w">    </span><span class="n">EXPECT_TRUE</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">equal</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">as</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(),</span><span class="w"> </span><span class="n">t0</span><span class="p">));</span>
<span class="w">    </span><span class="c1">// Due to inlining, the first dimension of T1 is not allocated. Due to</span>
<span class="w">    </span><span class="c1">// BIDx parallelization, the second dimension of T1 is not allocated. So T1</span>
<span class="w">    </span><span class="c1">// is only allocated in size 1.</span>
<span class="w">    </span><span class="n">EXPECT_EQ</span><span class="p">(</span><span class="n">ke</span><span class="p">.</span><span class="n">lastLaunchParams</span><span class="p">().</span><span class="n">smem</span><span class="p">(),</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Inline at 1, with TIDx parallelization</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">    </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">    </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">});</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>

<span class="w">    </span><span class="n">tv1</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Shared</span><span class="p">);</span>
<span class="w">    </span><span class="n">tv1</span><span class="o">-&gt;</span><span class="n">inlineAt</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="w">    </span><span class="n">tv1</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>
<span class="w">    </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">fusion</span><span class="p">.</span><span class="n">printKernel</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">KernelExecutor</span><span class="w"> </span><span class="n">ke</span><span class="p">;</span>
<span class="w">    </span><span class="n">ke</span><span class="p">.</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ke</span><span class="p">.</span><span class="n">run</span><span class="p">({</span><span class="n">t0</span><span class="p">});</span>
<span class="w">    </span><span class="n">EXPECT_TRUE</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">equal</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">as</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(),</span><span class="w"> </span><span class="n">t0</span><span class="p">));</span>
<span class="w">    </span><span class="c1">// Although the first dimension of T1 is inlined, because shared memory is</span>
<span class="w">    </span><span class="c1">// shared by threads, the TIDx parallelization will override the inlining,</span>
<span class="w">    </span><span class="c1">// and make the first dimension of T1 allocated. The second dimension of T1</span>
<span class="w">    </span><span class="c1">// is allocated normally because it is on the right of the compute-at</span>
<span class="w">    </span><span class="c1">// position. So T1 is allocated in full size.</span>
<span class="w">    </span><span class="n">EXPECT_EQ</span><span class="p">(</span><span class="n">ke</span><span class="p">.</span><span class="n">lastLaunchParams</span><span class="p">().</span><span class="n">smem</span><span class="p">(),</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Inline at 1, with TIDx and BIDx parallelization</span>
<span class="w">  </span><span class="p">{</span>
<span class="w">    </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">    </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">});</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>

<span class="w">    </span><span class="n">tv1</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Shared</span><span class="p">);</span>
<span class="w">    </span><span class="n">tv1</span><span class="o">-&gt;</span><span class="n">inlineAt</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="w">    </span><span class="n">tv1</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>
<span class="w">    </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>
<span class="w">    </span><span class="n">tv1</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">BIDx</span><span class="p">);</span>
<span class="w">    </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">BIDx</span><span class="p">);</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">fusion</span><span class="p">.</span><span class="n">printKernel</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">KernelExecutor</span><span class="w"> </span><span class="n">ke</span><span class="p">;</span>
<span class="w">    </span><span class="n">ke</span><span class="p">.</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ke</span><span class="p">.</span><span class="n">run</span><span class="p">({</span><span class="n">t0</span><span class="p">});</span>
<span class="w">    </span><span class="n">EXPECT_TRUE</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">equal</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">as</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(),</span><span class="w"> </span><span class="n">t0</span><span class="p">));</span>
<span class="w">    </span><span class="c1">// Although the first dimension of T1 is inlined, because shared memory is</span>
<span class="w">    </span><span class="c1">// shared by threads, the TIDx parallelization will override the inlining,</span>
<span class="w">    </span><span class="c1">// and make the first dimension of T1 allocated. The second dimension of T1</span>
<span class="w">    </span><span class="c1">// is not allocated due to BIDx parallelization. So T1 is allocated in</span>
<span class="w">    </span><span class="c1">// size 2.</span>
<span class="w">    </span><span class="n">EXPECT_EQ</span><span class="p">(</span><span class="n">ke</span><span class="p">.</span><span class="n">lastLaunchParams</span><span class="p">().</span><span class="n">smem</span><span class="p">(),</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<p>From the above example, we can see that:</p>
<ol class="arabic simple">
<li><p>If the memory type of a tensor is distributed across a parallel type, then
IterDomains with this parallel type are not allocated, regardless of
the tensor’s compute-at position.</p></li>
<li><p>If the memory type of a tensor is shared among a parallel type, then
IterDomains with this parallel type are always allocated, regardless of
the tensor’s compute-at position.</p></li>
<li><p>Except for the above two rules, the allocation of a tensor is determined by
its compute-at position. IterDomains on the right of the compute-at position
are allocated, while IterDomains on the left of the compute-at position are
not allocated.</p></li>
</ol>
<p>Tensor memory is similar to shared memory, that is, it is distributed across
<code class="docutils literal notranslate"><span class="pre">BIDx</span></code>, <code class="docutils literal notranslate"><span class="pre">BIDy</span></code>, <code class="docutils literal notranslate"><span class="pre">BIDz</span></code>, <code class="docutils literal notranslate"><span class="pre">DIDx</span></code>, <code class="docutils literal notranslate"><span class="pre">DIDy</span></code>, and <code class="docutils literal notranslate"><span class="pre">DIDz</span></code>, and shared across <code class="docutils literal notranslate"><span class="pre">TIDx</span></code>,
<code class="docutils literal notranslate"><span class="pre">TIDy</span></code>, and <code class="docutils literal notranslate"><span class="pre">TIDz</span></code>. So the above rules applies to tensor memory the same way
as they do to shared memory.</p>
</section>
<section id="tensor-memory">
<h2>Tensor memory<a class="headerlink" href="#tensor-memory" title="Link to this heading"></a></h2>
<p>NVIDIA’s official document for tensor memory can be found
<a class="reference external" href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#tensor-memory">here</a>.</p>
<p>Unlike any other memory type that we commonly see in computer architecture, the
addresses of tensor memory do not form a linear space. Instead, the addresses
of tensor memory are two-dimensional, and the two dimensions are called <code class="docutils literal notranslate"><span class="pre">row</span></code>
(or <code class="docutils literal notranslate"><span class="pre">lane</span></code>) and <code class="docutils literal notranslate"><span class="pre">column</span></code>.</p>
<p><img alt="Tensor-Memory-Layout" src="https://docs.nvidia.com/cuda/parallel-thread-execution/_images/tensor-memory-layout.png" /></p>
<p>In NVFuser, the allocation domain is the domain that specifies how tensor is
allocated in memory. For all other memory types, because these memory types are
linear spaces, the allocation domain is just a vector of <code class="docutils literal notranslate"><span class="pre">IterDomain</span></code>s. But for
tensor memory, because it is 2D, the allocation domain is a vector of
<code class="docutils literal notranslate"><span class="pre">IterDomain</span></code>s plus a position that splits the vector into two parts: the left
part specifies the lane, and the right part specifies the column. The position
is called <em>Tensor Memory Dimension Separator Position</em>, or <em>TMem DimSep
Position</em> in short.</p>
<p>In practice, to support some MMA shapes, we might want to allocate a tensor in
tensor memory like the figure below:</p>
<p><img alt="TMEM-Allocation-Example" src="../_images/alloc-example.svg" /></p>
<p>That is, the tensor is not a contiguous rectangle in tensor memory, but instead,
is strided in the row dimension. Because NVFuser already supports strided
allocation of global memory tensors, the concepts in that space easily extend to
tensor memory:</p>
<p>The TMem dimsep position does not only apply to the allocation domain, but also
applies to contiguity and stride. The contiguity and stride on the left of the
TMem dimsep position are for the lane, and the contiguity and stride on the
right are for the column.</p>
<p>In the above example, the allocation domain, contiguity, and stride of the
tensor could be:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">allocation</span> <span class="n">domain</span><span class="p">:</span> <span class="p">[</span> <span class="n">BIDx</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="o">|</span> <span class="p">,</span> <span class="n">BIDy</span><span class="p">,</span> <span class="mi">8</span> <span class="p">]</span>
       <span class="n">contiguity</span><span class="p">:</span> <span class="p">[</span>    <span class="err">?</span><span class="p">,</span>  <span class="n">F</span><span class="p">,</span>  <span class="n">T</span><span class="p">,</span> <span class="o">|</span> <span class="p">,</span>    <span class="err">?</span><span class="p">,</span> <span class="n">T</span> <span class="p">]</span>
           <span class="n">stride</span><span class="p">:</span> <span class="p">[</span>    <span class="err">?</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span> <span class="o">|</span> <span class="p">,</span>    <span class="err">?</span><span class="p">,</span> <span class="mi">1</span> <span class="p">]</span>
</pre></div>
</div>
<p>It worth noting that, because the tensor memory is distributed across <code class="docutils literal notranslate"><span class="pre">BIDx</span></code> and
<code class="docutils literal notranslate"><span class="pre">BIDy</span></code>, IterDomains with these parallel types are not allocated. So whether
these IterDomains are on the left or right of the TMem dimsep position does not
matter. The value of their contiguity and stride does not matter either (The <code class="docutils literal notranslate"><span class="pre">?</span></code>
in the above example).</p>
<p>Also, please note that the term “row”, “lane” and “column” when referring to the
memory layout of tensor memory are not related to the “row” and “column” of the
tensor itself. We can store an arbitrary part of the tensor in an arbitrary form
in tensor memory. That is, in the language of NVFuser, the logical domain of a
TMem TensorView and the allocation domain of that TensorView can have arbitrary
transformations between them. The important thing is not the transformation
between the logical domain and the allocation domain, but the transformation
between the allocation domain and the loop domain, which specifies how we access
tensor memory in the kernel.</p>
<p>Now let’s take a look at a few code examples of invalid tensor memory
allocation. Valid examples will be discussed in the next section. For all
valid and invalid examples, we will be looking at
gmem-&gt;register-&gt;tmem-&gt;register-&gt;gmem copy kernels. Note that there is no
data path between gmem and tmem, so we have to use registers as transfer
station.<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TEST_F</span><span class="p">(</span><span class="n">TMemTutorialC</span><span class="p">,</span><span class="w"> </span><span class="n">TooManyLanes</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">  </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="mi">11</span><span class="p">,</span><span class="w"> </span><span class="mi">13</span><span class="p">,</span><span class="w"> </span><span class="mi">17</span><span class="p">});</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv3</span><span class="p">);</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Tensor</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">StTMem</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv3</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">LdTMem</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">BIDx</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">BIDy</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDy</span><span class="p">);</span>
<span class="w">  </span><span class="n">scheduler_utils</span><span class="o">::</span><span class="n">parallelizeAllLike</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">inlineAllAt</span><span class="p">(</span><span class="n">tv4</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setTMemDimSepPos</span><span class="p">(</span><span class="mi">-2</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Tries to allocate (429, 17) for tv2.</span>
<span class="w">  </span><span class="n">EXPECT_THAT</span><span class="p">(</span>
<span class="w">      </span><span class="p">[</span><span class="o">&amp;</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">KernelExecutor</span><span class="p">().</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span><span class="w"> </span><span class="p">},</span>
<span class="w">      </span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">ThrowsMessage</span><span class="o">&lt;</span><span class="n">nvfError</span><span class="o">&gt;</span><span class="p">(</span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">HasSubstr</span><span class="p">(</span>
<span class="w">          </span><span class="s">&quot;Not enough tensor memory lanes: tried to allocate 429, but only 128 available.&quot;</span><span class="p">)));</span>
<span class="p">}</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<p>In the above example, the fusion is scheduled as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">BIDx</span><span class="p">{</span><span class="mi">2</span><span class="p">},</span> <span class="n">TIDx</span><span class="p">{</span><span class="mi">3</span><span class="p">},</span> <span class="mi">5</span><span class="p">,</span> <span class="p">(</span><span class="n">CA</span><span class="p">),</span> <span class="n">BIDy</span><span class="p">{</span><span class="mi">7</span><span class="p">},</span> <span class="n">TIDy</span><span class="p">{</span><span class="mi">11</span><span class="p">},</span> <span class="mi">13</span><span class="p">,</span> <span class="p">(</span><span class="n">DimSep</span><span class="p">),</span> <span class="mi">17</span><span class="p">]</span>
</pre></div>
</div>
<p>(Note that, when the allocation domain is not set, we assume it is the
<code class="docutils literal notranslate"><span class="pre">getMaybeAllocationDomain</span></code>)</p>
<p>Because 2 and 7 are parallelized on <code class="docutils literal notranslate"><span class="pre">BID</span></code>s, they are not allocated. Because 3
and 11 are parallelized on <code class="docutils literal notranslate"><span class="pre">TID</span></code>s, they are allocated. Because 5 is on the left
of the compute-at position, it is not allocated. Because 13 and 17 is on the right
of the compute-at position, it is allocated. So the total number of lanes required
for this tensor is: <code class="docutils literal notranslate"><span class="pre">3</span> <span class="pre">*</span> <span class="pre">11</span> <span class="pre">*</span> <span class="pre">13</span> <span class="pre">=</span> <span class="pre">429</span></code>, which is larger than the total available
lanes <code class="docutils literal notranslate"><span class="pre">128</span></code>.</p>
<p>Now let’s take a look at another example:<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TEST_F</span><span class="p">(</span><span class="n">TMemTutorialC</span><span class="p">,</span><span class="w"> </span><span class="n">TooManyCols</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">  </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="mi">11</span><span class="p">,</span><span class="w"> </span><span class="mi">13</span><span class="p">,</span><span class="w"> </span><span class="mi">17</span><span class="p">});</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv3</span><span class="p">);</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Tensor</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">StTMem</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv3</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">LdTMem</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">BIDx</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDy</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">BIDy</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDz</span><span class="p">);</span>
<span class="w">  </span><span class="n">scheduler_utils</span><span class="o">::</span><span class="n">parallelizeAllLike</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">inlineAllAt</span><span class="p">(</span><span class="n">tv4</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setTMemDimSepPos</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Tries to allocate (32, 1105) for tv2.</span>
<span class="w">  </span><span class="n">EXPECT_THAT</span><span class="p">(</span>
<span class="w">      </span><span class="p">[</span><span class="o">&amp;</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">KernelExecutor</span><span class="p">().</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span><span class="w"> </span><span class="p">},</span>
<span class="w">      </span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">ThrowsMessage</span><span class="o">&lt;</span><span class="n">nvfError</span><span class="o">&gt;</span><span class="p">(</span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">HasSubstr</span><span class="p">(</span>
<span class="w">          </span><span class="s">&quot;Not enough tensor memory columns: tried to allocate 1105, but only 512 available.&quot;</span><span class="p">)));</span>
<span class="p">}</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<p>In the above example, the fusion is scheduled as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">TIDx</span><span class="p">{</span><span class="mi">32</span><span class="p">},</span> <span class="p">(</span><span class="n">DimSep</span><span class="p">),</span> <span class="n">BIDx</span><span class="p">{</span><span class="mi">3</span><span class="p">},</span> <span class="n">TIDy</span><span class="p">{</span><span class="mi">5</span><span class="p">},</span> <span class="mi">7</span><span class="p">,</span> <span class="p">(</span><span class="n">CA</span><span class="p">),</span> <span class="n">BIDy</span><span class="p">{</span><span class="mi">11</span><span class="p">},</span> <span class="n">TIDz</span><span class="p">{</span><span class="mi">13</span><span class="p">},</span> <span class="mi">17</span><span class="p">]</span>
</pre></div>
</div>
<p>Because 3 and 11 are parallelized on <code class="docutils literal notranslate"><span class="pre">BID</span></code>s, they are not allocated. Because
32, 5, and 13 are parallelized on <code class="docutils literal notranslate"><span class="pre">TID</span></code>s, they are allocated. Because 7 is on
the left of the compute-at position, it is not allocated. Because 17 is on the
right of the compute-at position, it is allocated. So the total number of
columns required for this tensor is: <code class="docutils literal notranslate"><span class="pre">5</span> <span class="pre">*</span> <span class="pre">13</span> <span class="pre">*</span> <span class="pre">17</span> <span class="pre">=</span> <span class="pre">1105</span></code>, which is larger than
the total available columns <code class="docutils literal notranslate"><span class="pre">512</span></code>.</p>
</section>
<section id="the-loop-domain-of-tmem-load-and-store">
<h2>The loop domain of TMem load and store<a class="headerlink" href="#the-loop-domain-of-tmem-load-and-store" title="Link to this heading"></a></h2>
<p>In NVFuser, the loop structure, parallelization and compute-at strategy of an
expression is determined by the loop domain of the output of the expression.
Unlike shared memory, which allows threads to access it arbitrarily, tensor
memory must be accessed in a specific way. That is, the transformations between
the allocation domain of the TMem TensorView and the loop domain of the consumer
of the TMem accessing expression must satisfy specific patterns. That is, for
the case of a TMem load <code class="docutils literal notranslate"><span class="pre">T0_r</span> <span class="pre">-&gt;</span> <span class="pre">T1_t</span></code>, <code class="docutils literal notranslate"><span class="pre">T1_t</span></code>’s loop domain and allocation
domain must satisfy specific patterns. For the case of a TMem store <code class="docutils literal notranslate"><span class="pre">T0_t</span> <span class="pre">-&gt;</span> <span class="pre">T1_r</span></code>, the loop domain of <code class="docutils literal notranslate"><span class="pre">T1_r</span></code> and allocation domain of <code class="docutils literal notranslate"><span class="pre">T0_t</span></code> must satisfy
specific patterns.</p>
<p>The TMem&lt;-&gt;register transfer are warp-collective operations, and the threads in
a warp must access the tensor memory in a specific way.
The specific patterns of TMem&lt;-&gt;register transfer is specified in the
<a class="reference external" href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#tcgen05-memory-layout">PTX-documentation</a>.
These patterns are:</p>
<details>
<summary>32x32b:</summary>
<p><img alt="32x32b" src="https://docs.nvidia.com/cuda/parallel-thread-execution/_images/tcgen05-mma-fragment-3232b.png" /></p>
</details>
<details>
<summary>16x64b:</summary>
<p><img alt="16x64b" src="https://docs.nvidia.com/cuda/parallel-thread-execution/_images/tcgen05-mma-fragment-1664b.png" /></p>
</details>
<details>
<summary>16x128b:</summary>
<p><img alt="16x128b" src="https://docs.nvidia.com/cuda/parallel-thread-execution/_images/tcgen05-mma-fragment-16128b.png" /></p>
</details>
<details>
<summary>16x256b:</summary>
<p><img alt="16x256b" src="https://docs.nvidia.com/cuda/parallel-thread-execution/_images/tcgen05-mma-fragment-16256b.png" /></p>
</details>
<details>
<summary>16x32bx2:</summary>
<p><img alt="16x32bx2" src="https://docs.nvidia.com/cuda/parallel-thread-execution/_images/tcgen05-mma-fragment-1632b2.png" /></p>
</details>
<p>Besides threads in a warp must satisfy specific pattern, another restriction of
the TMem&lt;-&gt;register transfer is: not all warps can access all lanes of the tensor
memory. The entire 128 lanes of the tensor memory is divided into 4
subpartitions, each has 32 lanes. The warp <code class="docutils literal notranslate"><span class="pre">i</span></code> can only access the subpartition
<code class="docutils literal notranslate"><span class="pre">i</span> <span class="pre">%</span> <span class="pre">4</span></code>.</p>
<p>With the above restrictions in mind, let’s take a look at a few examples of how
NOT to schedule TMem load and store:<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TEST_F</span><span class="p">(</span><span class="n">TMemTutorialC</span><span class="p">,</span><span class="w"> </span><span class="n">NotWarpCollective</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">  </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">});</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv3</span><span class="p">);</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Tensor</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">StTMem</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv3</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">LdTMem</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>
<span class="w">  </span><span class="n">scheduler_utils</span><span class="o">::</span><span class="n">parallelizeAllLike</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setTMemDimSepPos</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>

<span class="w">  </span><span class="n">EXPECT_THAT</span><span class="p">(</span>
<span class="w">      </span><span class="p">[</span><span class="o">&amp;</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">KernelExecutor</span><span class="p">().</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span><span class="w"> </span><span class="p">},</span>
<span class="w">      </span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">ThrowsMessage</span><span class="o">&lt;</span><span class="n">nvfError</span><span class="o">&gt;</span><span class="p">(</span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">HasSubstr</span><span class="p">(</span>
<span class="w">          </span><span class="s">&quot;Invalid data access pattern in TMem load/store: &quot;</span>
<span class="w">          </span><span class="s">&quot;TMem load/store must be warp-collective, &quot;</span>
<span class="w">          </span><span class="s">&quot;but the innermost extent is not a multiple of 32.&quot;</span><span class="p">)));</span>
<span class="p">}</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<p>The above example is invalid because there are only 16 threads in the kernel.
Warp collective operations require at least a whole warp to run.<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TEST_F</span><span class="p">(</span><span class="n">TMemTutorialC</span><span class="p">,</span><span class="w"> </span><span class="n">NotContiguous</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">  </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">});</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv3</span><span class="p">);</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Tensor</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">StTMem</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv3</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">LdTMem</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDy</span><span class="p">);</span>
<span class="w">  </span><span class="n">scheduler_utils</span><span class="o">::</span><span class="n">parallelizeAllLike</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setTMemDimSepPos</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>

<span class="w">  </span><span class="n">EXPECT_THAT</span><span class="p">(</span>
<span class="w">      </span><span class="p">[</span><span class="o">&amp;</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">KernelExecutor</span><span class="p">().</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span><span class="w"> </span><span class="p">},</span>
<span class="w">      </span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">ThrowsMessage</span><span class="o">&lt;</span><span class="n">nvfError</span><span class="o">&gt;</span><span class="p">(</span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">HasSubstr</span><span class="p">(</span>
<span class="w">          </span><span class="s">&quot;Invalid data access pattern in TMem load/store: &quot;</span>
<span class="w">          </span><span class="s">&quot;Warp linearly accessing lanes, but not with stride 1.&quot;</span><span class="p">)));</span>
<span class="p">}</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<p>The above example is invalid because the tensor memory is not accessed in any of
the specified pattern. In the above example, because the lane allocation domain is
<code class="docutils literal notranslate"><span class="pre">[TIDx{64},</span> <span class="pre">TIDy{2}]</span></code>, where <code class="docutils literal notranslate"><span class="pre">TIDx</span></code> is not the innermost, threads in a warp access
lanes of the tensor memory in a stride-2 manner, while all the specified
patterns requires the warp to access a contiguous 32 or 16 lanes of data
.<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TEST_F</span><span class="p">(</span><span class="n">TMemTutorialC</span><span class="p">,</span><span class="w"> </span><span class="n">OneLane</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">  </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">});</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv3</span><span class="p">);</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Tensor</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">StTMem</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv3</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">LdTMem</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDy</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>
<span class="w">  </span><span class="n">scheduler_utils</span><span class="o">::</span><span class="n">parallelizeAllLike</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setTMemDimSepPos</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>

<span class="w">  </span><span class="n">EXPECT_THAT</span><span class="p">(</span>
<span class="w">      </span><span class="p">[</span><span class="o">&amp;</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">KernelExecutor</span><span class="p">().</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span><span class="w"> </span><span class="p">},</span>
<span class="w">      </span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">ThrowsMessage</span><span class="o">&lt;</span><span class="n">nvfError</span><span class="o">&gt;</span><span class="p">(</span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">HasSubstr</span><span class="p">(</span>
<span class="w">          </span><span class="s">&quot;Invalid data access pattern in TMem load/store:&quot;</span><span class="p">)));</span>
<span class="p">}</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<p>The above example is invalid because the tensor memory is not accessed in any of
the specified pattern. In the above example, each warp access one lane and 32
columns of the tensor memory, while all the specified patterns requires the warp
to access a contiguous 32 or 16 lanes of data.<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TEST_F</span><span class="p">(</span><span class="n">TMemTutorialC</span><span class="p">,</span><span class="w"> </span><span class="n">WrongSubpartition</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">  </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">});</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv3</span><span class="p">);</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Tensor</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">StTMem</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv3</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">LdTMem</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDy</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>
<span class="w">  </span><span class="n">scheduler_utils</span><span class="o">::</span><span class="n">parallelizeAllLike</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setTMemDimSepPos</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>

<span class="w">  </span><span class="n">EXPECT_THAT</span><span class="p">(</span>
<span class="w">      </span><span class="p">[</span><span class="o">&amp;</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">KernelExecutor</span><span class="p">().</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span><span class="w"> </span><span class="p">},</span>
<span class="w">      </span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">ThrowsMessage</span><span class="o">&lt;</span><span class="n">nvfError</span><span class="o">&gt;</span><span class="p">(</span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">HasSubstr</span><span class="p">(</span>
<span class="w">          </span><span class="s">&quot;Invalid data access pattern in TMem load/store: &quot;</span>
<span class="w">          </span><span class="s">&quot;Warps are not accessing the correct sub-partition.&quot;</span><span class="p">)));</span>
<span class="p">}</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<p>The above example is invalid because the warp accesses the wrong subpartition of
the tensor memory. In the above example, there are two warps, where warp 0
accesses the subpartition 0 and 1, and warp 1 accesses the subpartition 2 and 3.
However, warp 0 can only access subpartition 0, and warp 1 can only access
subpartition 1.<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TEST_F</span><span class="p">(</span><span class="n">TMemTutorialC</span><span class="p">,</span><span class="w"> </span><span class="n">WrongSubpartition2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">  </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">});</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv3</span><span class="p">);</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Tensor</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">StTMem</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv3</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">LdTMem</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDy</span><span class="p">);</span>
<span class="w">  </span><span class="n">scheduler_utils</span><span class="o">::</span><span class="n">parallelizeAllLike</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setTMemDimSepPos</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>

<span class="w">  </span><span class="n">EXPECT_THAT</span><span class="p">(</span>
<span class="w">      </span><span class="p">[</span><span class="o">&amp;</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">KernelExecutor</span><span class="p">().</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span><span class="w"> </span><span class="p">},</span>
<span class="w">      </span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">ThrowsMessage</span><span class="o">&lt;</span><span class="n">nvfError</span><span class="o">&gt;</span><span class="p">(</span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">HasSubstr</span><span class="p">(</span>
<span class="w">          </span><span class="s">&quot;Invalid data access pattern in TMem load/store: &quot;</span>
<span class="w">          </span><span class="s">&quot;Warps are not accessing the correct sub-partition.&quot;</span><span class="p">)));</span>
<span class="p">}</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<p>The above example is also invalid because the warp accesses the wrong subpartition.
In the above example, there are two warps, both accessing subpartition 0, which
is not allowed.</p>
<p>Now, let’s take a look at some valid examples:<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TEST_F</span><span class="p">(</span><span class="n">TMemTutorialR</span><span class="p">,</span><span class="w"> </span><span class="n">WarpXYZ</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">  </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">});</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv3</span><span class="p">);</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Tensor</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">StTMem</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv3</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">LdTMem</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDz</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDy</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>
<span class="w">  </span><span class="n">scheduler_utils</span><span class="o">::</span><span class="n">parallelizeAllLike</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setTMemDimSepPos</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>

<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">printKernel</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">KernelExecutor</span><span class="w"> </span><span class="n">ke</span><span class="p">;</span>
<span class="w">  </span><span class="n">ke</span><span class="p">.</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">t0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">rand</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ke</span><span class="p">.</span><span class="n">run</span><span class="p">({</span><span class="n">t0</span><span class="p">});</span>
<span class="w">  </span><span class="n">EXPECT_TRUE</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">equal</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">as</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(),</span><span class="w"> </span><span class="n">t0</span><span class="p">));</span>
<span class="p">}</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<p>In the above example, each CTA only has one warp, and this warp is split into
TIDz, TIDy, and TIDx. The above kernel uses a loop of 2, where each iteration
accesses a 32x1 box of the tensor memory. This is a valid 32x32b pattern
.<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TEST_F</span><span class="p">(</span><span class="n">TMemTutorialR</span><span class="p">,</span><span class="w"> </span><span class="n">WarpGroupXYZ</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">  </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">});</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv3</span><span class="p">);</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Tensor</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">StTMem</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv3</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">LdTMem</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDz</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDy</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>
<span class="w">  </span><span class="n">scheduler_utils</span><span class="o">::</span><span class="n">parallelizeAllLike</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setTMemDimSepPos</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>

<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">printKernel</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">KernelExecutor</span><span class="w"> </span><span class="n">ke</span><span class="p">;</span>
<span class="w">  </span><span class="n">ke</span><span class="p">.</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">t0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">rand</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ke</span><span class="p">.</span><span class="n">run</span><span class="p">({</span><span class="n">t0</span><span class="p">});</span>
<span class="w">  </span><span class="n">EXPECT_TRUE</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">equal</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">as</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(),</span><span class="w"> </span><span class="n">t0</span><span class="p">));</span>
<span class="p">}</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<p>In the above example, each CTA has one warp group (a group of 4 consecutive warps).
This entire warp group is accessing a whole column, with each warp accessing its
subpartition of 32 lanes. This is a valid 32x32b pattern.<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TEST_F</span><span class="p">(</span><span class="n">TMemTutorialR</span><span class="p">,</span><span class="w"> </span><span class="n">WarpGroupXYColZ</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">  </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">});</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv3</span><span class="p">);</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Tensor</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">StTMem</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv3</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">LdTMem</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDy</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDz</span><span class="p">);</span>
<span class="w">  </span><span class="n">scheduler_utils</span><span class="o">::</span><span class="n">parallelizeAllLike</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setTMemDimSepPos</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>

<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">printKernel</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">KernelExecutor</span><span class="w"> </span><span class="n">ke</span><span class="p">;</span>
<span class="w">  </span><span class="n">ke</span><span class="p">.</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">t0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">rand</span><span class="p">({</span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">},</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ke</span><span class="p">.</span><span class="n">run</span><span class="p">({</span><span class="n">t0</span><span class="p">});</span>
<span class="w">  </span><span class="n">EXPECT_TRUE</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">equal</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">as</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(),</span><span class="w"> </span><span class="n">t0</span><span class="p">));</span>
<span class="p">}</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<p>In the above example, each CTA has 8 warp groups, each warp group accesses a
whole column. Warp group <code class="docutils literal notranslate"><span class="pre">i</span></code> is accessing column <code class="docutils literal notranslate"><span class="pre">i</span></code>.
This is a valid 32x32b pattern.<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TEST_F</span><span class="p">(</span><span class="n">TMemTutorialR</span><span class="p">,</span><span class="w"> </span><span class="n">WarpGroupXColYZ</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">  </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">});</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv3</span><span class="p">);</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Tensor</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">StTMem</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv3</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">LdTMem</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDy</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDz</span><span class="p">);</span>
<span class="w">  </span><span class="n">scheduler_utils</span><span class="o">::</span><span class="n">parallelizeAllLike</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setTMemDimSepPos</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>

<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">printKernel</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">KernelExecutor</span><span class="w"> </span><span class="n">ke</span><span class="p">;</span>
<span class="w">  </span><span class="n">ke</span><span class="p">.</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">t0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">rand</span><span class="p">({</span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ke</span><span class="p">.</span><span class="n">run</span><span class="p">({</span><span class="n">t0</span><span class="p">});</span>
<span class="w">  </span><span class="n">EXPECT_TRUE</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">equal</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">as</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(),</span><span class="w"> </span><span class="n">t0</span><span class="p">));</span>
<span class="p">}</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<p>In the above example, each CTA has 4 warp groups, each warp group accesses a
whole column. The warp group id and the column each warp group accesses are
shown in the table below:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Warp Group</p></th>
<th class="head"><p>0</p></th>
<th class="head"><p>1</p></th>
<th class="head"><p>2</p></th>
<th class="head"><p>3</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Column</p></td>
<td><p>0</p></td>
<td><p>2</p></td>
<td><p>1</p></td>
<td><p>3</p></td>
</tr>
</tbody>
</table>
<p>This is a valid 32x32b pattern.<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TEST_F</span><span class="p">(</span><span class="n">TMemTutorialR</span><span class="p">,</span><span class="w"> </span><span class="n">X1WarpGroupYColZ</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">  </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">});</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv3</span><span class="p">);</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Tensor</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">StTMem</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv3</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">LdTMem</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDy</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDz</span><span class="p">);</span>
<span class="w">  </span><span class="n">scheduler_utils</span><span class="o">::</span><span class="n">parallelizeAllLike</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setTMemDimSepPos</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>

<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">printKernel</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">KernelExecutor</span><span class="w"> </span><span class="n">ke</span><span class="p">;</span>
<span class="w">  </span><span class="n">ke</span><span class="p">.</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">t0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">rand</span><span class="p">({</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ke</span><span class="p">.</span><span class="n">run</span><span class="p">({</span><span class="n">t0</span><span class="p">});</span>
<span class="w">  </span><span class="n">EXPECT_TRUE</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">equal</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">as</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(),</span><span class="w"> </span><span class="n">t0</span><span class="p">));</span>
<span class="p">}</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<p>In the above example, each CTA has 2 warp groups, each warp group accesses a
whole column. Warp group <code class="docutils literal notranslate"><span class="pre">i</span></code> is accessing column <code class="docutils literal notranslate"><span class="pre">i</span></code>. This is a valid 32x32b
pattern. Note that although the order of <code class="docutils literal notranslate"><span class="pre">TIDx</span></code> and <code class="docutils literal notranslate"><span class="pre">TIDy</span></code> seems wrong, it
does not matter because the size of <code class="docutils literal notranslate"><span class="pre">TIDx</span></code> is just 1.</p>
<p>Now, let’s take a look at a few more complicated examples that puts a lot
of what we have learned so far into practice.</p>
<p>First, to show that the logical domain and the allocation domain are independent,
we will XOR swizzle the logical domain in a very complicated fashion. What we
want to show is, the row and column of the tensor memory are not related to the
row and column of the tensor itself, and we have the freedom to choose to place
which items of the tensor to where of the tensor memory. In real applications,
it is unlikely that we will XOR swizzle the logical domain, but here we are just
showing that it is possible:<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Apply a fancy transformation to transform a [4096, 4096] tensor back to its</span>
<span class="c1">// original shape.</span>
<span class="kt">void</span><span class="w"> </span><span class="nf">fancyTransformations</span><span class="p">(</span><span class="n">TensorView</span><span class="o">*</span><span class="w"> </span><span class="n">tv</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">tv</span><span class="o">-&gt;</span><span class="n">swizzle</span><span class="p">(</span><span class="n">SwizzleType</span><span class="o">::</span><span class="n">XOR</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv</span><span class="o">-&gt;</span><span class="n">reorder</span><span class="p">({</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">});</span>
<span class="w">  </span><span class="n">tv</span><span class="o">-&gt;</span><span class="n">swizzle</span><span class="p">(</span><span class="n">SwizzleType</span><span class="o">::</span><span class="n">XOR</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv</span><span class="o">-&gt;</span><span class="n">swizzle</span><span class="p">(</span><span class="n">SwizzleType</span><span class="o">::</span><span class="n">XOR</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv</span><span class="o">-&gt;</span><span class="n">swizzle</span><span class="p">(</span><span class="n">SwizzleType</span><span class="o">::</span><span class="n">XOR</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv</span><span class="o">-&gt;</span><span class="n">reorder</span><span class="p">({</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">});</span>
<span class="w">  </span><span class="n">tv</span><span class="o">-&gt;</span><span class="n">merge</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv</span><span class="o">-&gt;</span><span class="n">merge</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="p">}</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<p>Second, let’s use the following function to check the allocation size of tensor
memory:<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">checkAllocationSize</span><span class="p">(</span><span class="n">KernelExecutor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">ke</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">expected_ncols</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">ke</span><span class="p">.</span><span class="n">registerLoweringHook</span><span class="p">([</span><span class="n">expected_ncols</span><span class="p">](</span><span class="n">GpuLower</span><span class="o">*</span><span class="w"> </span><span class="n">lower</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">check_pass</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="n">expected_ncols</span><span class="p">](</span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">Expr</span><span class="o">*&gt;&amp;</span><span class="w"> </span><span class="n">exprs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">regions</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">GpuLower</span><span class="o">::</span><span class="n">current</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">tmemInfo</span><span class="p">().</span><span class="n">allocation</span><span class="p">.</span><span class="n">regions</span><span class="p">;</span>
<span class="w">      </span><span class="p">[</span><span class="o">&amp;</span><span class="p">]</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">ASSERT_EQ</span><span class="p">(</span><span class="n">regions</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span><span class="w"> </span><span class="p">}();</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">region</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">regions</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
<span class="w">      </span><span class="p">[</span><span class="o">&amp;</span><span class="p">]</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">EXPECT_EQ</span><span class="p">(</span><span class="n">region</span><span class="p">.</span><span class="n">num_columns</span><span class="o">-&gt;</span><span class="n">evaluate</span><span class="p">(),</span><span class="w"> </span><span class="n">expected_ncols</span><span class="p">);</span><span class="w"> </span><span class="p">}();</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">exprs</span><span class="p">;</span>
<span class="w">    </span><span class="p">};</span>
<span class="w">    </span><span class="n">lower</span><span class="o">-&gt;</span><span class="n">passes</span><span class="p">().</span><span class="n">push_back</span><span class="p">({</span><span class="s">&quot;Check result&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">check_pass</span><span class="p">});</span>
<span class="w">  </span><span class="p">});</span>
<span class="p">}</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<p>Here comes the example 1:<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TEST_F</span><span class="p">(</span><span class="n">TMemTutorialR</span><span class="p">,</span><span class="w"> </span><span class="n">Complicated1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">  </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">4096</span><span class="p">,</span><span class="w"> </span><span class="mi">4096</span><span class="p">});</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv3</span><span class="p">);</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Tensor</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">StTMem</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv3</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">LdTMem</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// apply fancy transformations, shape is still [4096, 4096]</span>
<span class="w">  </span><span class="n">fancyTransformations</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// We want the first 4096 to go into lanes, and the second 4096 to go into</span>
<span class="w">  </span><span class="c1">// columns.</span>

<span class="w">  </span><span class="c1">// We want to split the second 4096 into [2, 4, 8, 64], where these dimensions</span>
<span class="w">  </span><span class="c1">// will eventually have the following properties:</span>
<span class="w">  </span><span class="c1">// -  2: serial,  left of CA (not allocated)</span>
<span class="w">  </span><span class="c1">// -  4:   BIDy,  left of CA (not allocated)</span>
<span class="w">  </span><span class="c1">// -  8:   BIDz, right of CA (not allocated)</span>
<span class="w">  </span><span class="c1">// - 64: serial, right of CA (allocated)</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// We want to split the first 4096 into [2, 16, 8, 2, 8, 1], where these</span>
<span class="w">  </span><span class="c1">// dimensions will eventually have the following properties:</span>
<span class="w">  </span><span class="c1">// -  2:   TIDz,  left of CA (allocated)</span>
<span class="w">  </span><span class="c1">// - 16: serial,  left of CA (not allocated)</span>
<span class="w">  </span><span class="c1">// -  8:   TIDy, right of CA (allocated)</span>
<span class="w">  </span><span class="c1">// -  2:   BIDx, right of CA (not allocated)</span>
<span class="w">  </span><span class="c1">// -  8:   TIDx, right of CA (allocated)</span>
<span class="w">  </span><span class="c1">// -  1: serial, right of CA (trivial allocated)</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Parallelize:</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDz</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDy</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">BIDx</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">BIDy</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">BIDz</span><span class="p">);</span>

<span class="w">  </span><span class="n">TransformPropagatorWithCheck</span><span class="w"> </span><span class="nf">propagator</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">MaxLogicalDomainInfoSpanningTree</span><span class="p">(</span><span class="n">tv4</span><span class="p">).</span><span class="n">traverse</span><span class="p">(</span><span class="o">&amp;</span><span class="n">propagator</span><span class="p">);</span>
<span class="w">  </span><span class="n">scheduler_utils</span><span class="o">::</span><span class="n">parallelizeAllLike</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Set the allocation domain of TMem tensor</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setAllocationDomain</span><span class="p">(</span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">getLoopDomain</span><span class="p">(),</span><span class="w"> </span><span class="nb">true</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setTMemDimSepPos</span><span class="p">(</span><span class="mi">6</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Reorder and inlining</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">reorder</span><span class="p">({{</span><span class="mi">6</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">}});</span>
<span class="w">  </span><span class="n">TransformPropagatorWithCheck</span><span class="w"> </span><span class="nf">propagator2</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">MaxLogicalDomainInfoSpanningTree</span><span class="p">(</span><span class="n">tv4</span><span class="p">).</span><span class="n">traverse</span><span class="p">(</span><span class="o">&amp;</span><span class="n">propagator2</span><span class="p">);</span>
<span class="w">  </span><span class="n">inlineAllAt</span><span class="p">(</span><span class="n">tv4</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">);</span>

<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">printKernel</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">KernelExecutor</span><span class="w"> </span><span class="n">ke</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Check that tv2 is allocated 64 columns.</span>
<span class="w">  </span><span class="n">checkAllocationSize</span><span class="p">(</span><span class="n">ke</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">);</span>

<span class="w">  </span><span class="n">ke</span><span class="p">.</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">t0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">rand</span><span class="p">({</span><span class="mi">4096</span><span class="p">,</span><span class="w"> </span><span class="mi">4096</span><span class="p">},</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ke</span><span class="p">.</span><span class="n">run</span><span class="p">({</span><span class="n">t0</span><span class="p">});</span>
<span class="w">  </span><span class="n">EXPECT_TRUE</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">equal</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">as</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(),</span><span class="w"> </span><span class="n">t0</span><span class="p">));</span>
<span class="p">}</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<p>Here comes the example 2:<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TEST_F</span><span class="p">(</span><span class="n">TMemTutorialR</span><span class="p">,</span><span class="w"> </span><span class="n">Complicated2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">  </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">4096</span><span class="p">,</span><span class="w"> </span><span class="mi">4096</span><span class="p">});</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv3</span><span class="p">);</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Tensor</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">StTMem</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv3</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">LdTMem</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// apply fancy transformations, shape is still [4096, 4096]</span>
<span class="w">  </span><span class="n">fancyTransformations</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// We want the first 4096 to go into lanes, and the second 4096 to go into</span>
<span class="w">  </span><span class="c1">// columns.</span>

<span class="w">  </span><span class="c1">// We want to split the second 4096 into [4, 8, 2, 16, 2, 2], where these</span>
<span class="w">  </span><span class="c1">// dimensions will eventually have the following properties:</span>
<span class="w">  </span><span class="c1">// -  4: serial,  left of CA (not allocated)</span>
<span class="w">  </span><span class="c1">// -  8:   BIDy,  left of CA (not allocated)</span>
<span class="w">  </span><span class="c1">// -  2:   TIDy,  left of CA (allocated)</span>
<span class="w">  </span><span class="c1">// - 16: serial, right of CA (allocated)</span>
<span class="w">  </span><span class="c1">// -  2:   BIDz, right of CA (not allocated)</span>
<span class="w">  </span><span class="c1">// -  2:   TIDz, right of CA (allocated)</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// We want to split the first 4096 into [32, 128], where these</span>
<span class="w">  </span><span class="c1">// dimensions will eventually have the following properties:</span>
<span class="w">  </span><span class="c1">// -  32: serial,   left of CA (not allocated)</span>
<span class="w">  </span><span class="c1">// - 128:   TIDx,  right of CA (allocated)</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Parallelize:</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">BIDy</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDy</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">BIDz</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDz</span><span class="p">);</span>

<span class="w">  </span><span class="n">TransformPropagatorWithCheck</span><span class="w"> </span><span class="nf">propagator</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">MaxLogicalDomainInfoSpanningTree</span><span class="p">(</span><span class="n">tv4</span><span class="p">).</span><span class="n">traverse</span><span class="p">(</span><span class="o">&amp;</span><span class="n">propagator</span><span class="p">);</span>
<span class="w">  </span><span class="n">scheduler_utils</span><span class="o">::</span><span class="n">parallelizeAllLike</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Set the allocation domain of TMem tensor</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setAllocationDomain</span><span class="p">(</span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">getLoopDomain</span><span class="p">(),</span><span class="w"> </span><span class="nb">true</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setTMemDimSepPos</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Reorder and inlining</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">reorder</span><span class="p">({{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">}});</span>
<span class="w">  </span><span class="n">TransformPropagatorWithCheck</span><span class="w"> </span><span class="nf">propagator2</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">MaxLogicalDomainInfoSpanningTree</span><span class="p">(</span><span class="n">tv4</span><span class="p">).</span><span class="n">traverse</span><span class="p">(</span><span class="o">&amp;</span><span class="n">propagator2</span><span class="p">);</span>
<span class="w">  </span><span class="n">inlineAllAt</span><span class="p">(</span><span class="n">tv4</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">);</span>

<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">printKernel</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">KernelExecutor</span><span class="w"> </span><span class="n">ke</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Check that tv2 is allocated 64 columns.</span>
<span class="w">  </span><span class="n">checkAllocationSize</span><span class="p">(</span><span class="n">ke</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">);</span>

<span class="w">  </span><span class="n">ke</span><span class="p">.</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">t0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">rand</span><span class="p">({</span><span class="mi">4096</span><span class="p">,</span><span class="w"> </span><span class="mi">4096</span><span class="p">},</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ke</span><span class="p">.</span><span class="n">run</span><span class="p">({</span><span class="n">t0</span><span class="p">});</span>
<span class="w">  </span><span class="n">EXPECT_TRUE</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">equal</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">as</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(),</span><span class="w"> </span><span class="n">t0</span><span class="p">));</span>
<span class="p">}</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<p>It also worth mentioning that the storing and loading of tensor memory is not
required to be scheduled in the same way. As long as both matches an allowed
pattern. The following example shows how to use tensor memory to do a transpose
:<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TEST_F</span><span class="p">(</span><span class="n">TMemTutorialR</span><span class="p">,</span><span class="w"> </span><span class="n">Transpose</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">  </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">});</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">transpose</span><span class="p">(</span><span class="n">tv2</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv3</span><span class="p">);</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Tensor</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">StTMem</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv3</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">LdTMem</span><span class="p">);</span>

<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tv</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">{</span><span class="n">tv1</span><span class="p">,</span><span class="w"> </span><span class="n">tv2</span><span class="p">,</span><span class="w"> </span><span class="n">tv3</span><span class="p">,</span><span class="w"> </span><span class="n">tv4</span><span class="p">})</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">tv</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>
<span class="w">    </span><span class="n">tv</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDy</span><span class="p">);</span>
<span class="w">    </span><span class="n">tv</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDz</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setTMemDimSepPos</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>

<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">printKernel</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">KernelExecutor</span><span class="w"> </span><span class="n">ke</span><span class="p">;</span>
<span class="w">  </span><span class="n">ke</span><span class="p">.</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">t0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">rand</span><span class="p">({</span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ke</span><span class="p">.</span><span class="n">run</span><span class="p">({</span><span class="n">t0</span><span class="p">});</span>
<span class="w">  </span><span class="n">EXPECT_TRUE</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">equal</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">as</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(),</span><span class="w"> </span><span class="n">t0</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)));</span>
<span class="p">}</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<p>In the above example, we store and load the tensor memory like the table below:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Warp Group</p></th>
<th class="head"><p>0</p></th>
<th class="head"><p>1</p></th>
<th class="head"><p>2</p></th>
<th class="head"><p>3</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Store Column</p></td>
<td><p>0</p></td>
<td><p>2</p></td>
<td><p>1</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-odd"><td><p>Load Column</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>2</p></td>
<td><p>3</p></td>
</tr>
</tbody>
</table>
</section>
<section id="vectorization-of-tmem-load-and-store">
<h2>Vectorization of TMem load and store<a class="headerlink" href="#vectorization-of-tmem-load-and-store" title="Link to this heading"></a></h2>
<p>Tensor memory load and store can be vectorized as a power of 2, from 4 bytes all the
way to 512 bytes:<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TEST_F</span><span class="p">(</span><span class="n">TMemTutorialR</span><span class="p">,</span><span class="w"> </span><span class="n">Vectorization</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">vec_factors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">64</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">};</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">st_vec</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">vec_factors</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">ld_vec</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">vec_factors</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">      </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">256</span><span class="p">});</span>
<span class="w">      </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">tv3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>
<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">tv4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv3</span><span class="p">);</span>
<span class="w">      </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">      </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Tensor</span><span class="p">);</span>
<span class="w">      </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">StTMem</span><span class="p">);</span>
<span class="w">      </span><span class="n">tv3</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">LdTMem</span><span class="p">);</span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tv</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">{</span><span class="n">tv1</span><span class="p">,</span><span class="w"> </span><span class="n">tv2</span><span class="p">,</span><span class="w"> </span><span class="n">tv3</span><span class="p">,</span><span class="w"> </span><span class="n">tv4</span><span class="p">})</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">tv</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>
<span class="w">      </span><span class="p">}</span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tv</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">{</span><span class="n">tv2</span><span class="p">,</span><span class="w"> </span><span class="n">tv1</span><span class="p">})</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">tv</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">st_vec</span><span class="p">);</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">      </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">-1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">Vectorize</span><span class="p">);</span>
<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tv</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">{</span><span class="n">tv3</span><span class="p">,</span><span class="w"> </span><span class="n">tv4</span><span class="p">})</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">tv</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">ld_vec</span><span class="p">);</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">      </span><span class="n">tv3</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">-1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">Vectorize</span><span class="p">);</span>

<span class="w">      </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setAllocationDomain</span><span class="p">(</span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">getLoopDomain</span><span class="p">(),</span><span class="w"> </span><span class="nb">true</span><span class="p">);</span>
<span class="w">      </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setTMemDimSepPos</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>

<span class="w">      </span><span class="n">inlineMost</span><span class="p">();</span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">fusion</span><span class="p">.</span><span class="n">printKernel</span><span class="p">();</span>
<span class="w">      </span><span class="p">}</span>

<span class="w">      </span><span class="n">KernelExecutor</span><span class="w"> </span><span class="n">ke</span><span class="p">;</span>

<span class="w">      </span><span class="c1">// Check the allocation size of tv2. When the load and store vectorization</span>
<span class="w">      </span><span class="c1">// factors are the same, the inlining position is one larger than the</span>
<span class="w">      </span><span class="c1">// case when they are different.</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">expected_ncols</span><span class="w"> </span><span class="o">=</span>
<span class="w">          </span><span class="n">st_vec</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">ld_vec</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">max</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">st_vec</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="mi">256</span><span class="p">;</span>
<span class="w">      </span><span class="n">checkAllocationSize</span><span class="p">(</span><span class="n">ke</span><span class="p">,</span><span class="w"> </span><span class="n">expected_ncols</span><span class="p">);</span>

<span class="w">      </span><span class="n">ke</span><span class="p">.</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">      </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">t0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">rand</span><span class="p">({</span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">256</span><span class="p">},</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">);</span>
<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ke</span><span class="p">.</span><span class="n">run</span><span class="p">({</span><span class="n">t0</span><span class="p">});</span>
<span class="w">      </span><span class="n">EXPECT_TRUE</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">equal</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">as</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(),</span><span class="w"> </span><span class="n">t0</span><span class="p">));</span>

<span class="w">      </span><span class="c1">// Check that vectorized PTX instructions are used</span>
<span class="w">      </span><span class="n">GpuLower</span><span class="w"> </span><span class="nf">gpulw</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>
<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">kernel_str</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">codegen</span><span class="o">::</span><span class="n">generateCudaKernel</span><span class="p">(</span><span class="n">gpulw</span><span class="p">.</span><span class="n">run</span><span class="p">());</span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">stringstream</span><span class="w"> </span><span class="n">expect_st</span><span class="p">,</span><span class="w"> </span><span class="n">expect_ld</span><span class="p">;</span>
<span class="w">      </span><span class="n">expect_st</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;tcgen05.st.sync.aligned.32x32b.x&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">st_vec</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;.b32&quot;</span><span class="p">;</span>
<span class="w">      </span><span class="n">expect_ld</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;tcgen05.ld.sync.aligned.32x32b.x&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">ld_vec</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;.b32&quot;</span><span class="p">;</span>
<span class="w">      </span><span class="n">EXPECT_THAT</span><span class="p">(</span><span class="n">kernel_str</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">HasSubstr</span><span class="p">(</span><span class="n">expect_st</span><span class="p">.</span><span class="n">str</span><span class="p">()));</span>
<span class="w">      </span><span class="n">EXPECT_THAT</span><span class="p">(</span><span class="n">kernel_str</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">HasSubstr</span><span class="p">(</span><span class="n">expect_ld</span><span class="p">.</span><span class="n">str</span><span class="p">()));</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<p>Note that the minimum unit of TMem load/store is 4 bytes, and the vectorization
factor used for TMem load/store must makes sure that the total size of the vector
is a multiple of 4 bytes. For example, char2, half1 are invalid vectorization
factors, but char4, half2 are valid:<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TEST_F</span><span class="p">(</span><span class="n">TMemTutorialR</span><span class="p">,</span><span class="w"> </span><span class="n">VectorizeMultipleOf4Bytes</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">run</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[](</span><span class="n">DataType</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">vec</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">    </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigConcreteTensor</span><span class="p">({</span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">256</span><span class="p">},</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">tv4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv3</span><span class="p">);</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">    </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Tensor</span><span class="p">);</span>
<span class="w">    </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">StTMem</span><span class="p">);</span>
<span class="w">    </span><span class="n">tv3</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">LdTMem</span><span class="p">);</span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tv</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">{</span><span class="n">tv1</span><span class="p">,</span><span class="w"> </span><span class="n">tv2</span><span class="p">,</span><span class="w"> </span><span class="n">tv3</span><span class="p">,</span><span class="w"> </span><span class="n">tv4</span><span class="p">})</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">tv</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">vec</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tv</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">{</span><span class="n">tv1</span><span class="p">,</span><span class="w"> </span><span class="n">tv2</span><span class="p">,</span><span class="w"> </span><span class="n">tv3</span><span class="p">,</span><span class="w"> </span><span class="n">tv4</span><span class="p">})</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">tv</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">vec</span><span class="p">);</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">      </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">-1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">Vectorize</span><span class="p">);</span>
<span class="w">      </span><span class="n">tv3</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">-1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">Vectorize</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setAllocationDomain</span><span class="p">(</span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">getLoopDomain</span><span class="p">(),</span><span class="w"> </span><span class="nb">true</span><span class="p">);</span>
<span class="w">    </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setTMemDimSepPos</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>

<span class="w">    </span><span class="n">inlineMost</span><span class="p">();</span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">fusion</span><span class="p">.</span><span class="n">printKernel</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">KernelExecutor</span><span class="w"> </span><span class="n">ke</span><span class="p">;</span>

<span class="w">    </span><span class="n">ke</span><span class="p">.</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">    </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="p">()</span>
<span class="w">                                    </span><span class="p">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">data_type_to_aten</span><span class="p">(</span><span class="n">dtype</span><span class="p">))</span>
<span class="w">                                    </span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="w">    </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">t0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dtype</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">DataType</span><span class="o">::</span><span class="n">Char</span>
<span class="w">        </span><span class="o">?</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">randint</span><span class="p">(</span><span class="mi">-128</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">256</span><span class="p">},</span><span class="w"> </span><span class="n">options</span><span class="p">)</span>
<span class="w">        </span><span class="o">:</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">rand</span><span class="p">({</span><span class="mi">128</span><span class="p">,</span><span class="w"> </span><span class="mi">256</span><span class="p">},</span><span class="w"> </span><span class="n">options</span><span class="p">);</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ke</span><span class="p">.</span><span class="n">run</span><span class="p">({</span><span class="n">t0</span><span class="p">});</span>
<span class="w">    </span><span class="n">EXPECT_TRUE</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">equal</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">as</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(),</span><span class="w"> </span><span class="n">t0</span><span class="p">));</span>
<span class="w">  </span><span class="p">};</span>

<span class="w">  </span><span class="n">EXPECT_THAT</span><span class="p">(</span>
<span class="w">      </span><span class="p">[</span><span class="o">&amp;</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">run</span><span class="p">(</span><span class="n">DataType</span><span class="o">::</span><span class="n">Char</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span><span class="w"> </span><span class="p">},</span>
<span class="w">      </span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">ThrowsMessage</span><span class="o">&lt;</span><span class="n">nvfuser</span><span class="o">::</span><span class="n">nvfError</span><span class="o">&gt;</span><span class="p">(</span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">HasSubstr</span><span class="p">(</span>
<span class="w">          </span><span class="s">&quot;Vectorize size is not a multiple of 4 bytes&quot;</span><span class="p">)));</span>
<span class="w">  </span><span class="n">EXPECT_THAT</span><span class="p">(</span>
<span class="w">      </span><span class="p">[</span><span class="o">&amp;</span><span class="p">]()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">run</span><span class="p">(</span><span class="n">DataType</span><span class="o">::</span><span class="n">Half</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span><span class="w"> </span><span class="p">},</span>
<span class="w">      </span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">ThrowsMessage</span><span class="o">&lt;</span><span class="n">nvfuser</span><span class="o">::</span><span class="n">nvfError</span><span class="o">&gt;</span><span class="p">(</span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">HasSubstr</span><span class="p">(</span>
<span class="w">          </span><span class="s">&quot;Vectorize size is not a multiple of 4 bytes&quot;</span><span class="p">)));</span>
<span class="w">  </span><span class="n">run</span><span class="p">(</span><span class="n">DataType</span><span class="o">::</span><span class="n">Char</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">);</span>
<span class="w">  </span><span class="n">run</span><span class="p">(</span><span class="n">DataType</span><span class="o">::</span><span class="n">Half</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>
<span class="p">}</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<p>When using tensor memory for non-matmul purposes, the allocation domain of the
tensor memory tensor is usually dictated by the global scheduling of the
problem, and the vectorization factor used for TMem load/store are the product
of the unroll factor and the vectorization factor of the global memory load/store.
The following example demonstrates a performant copy kernel
gmem -&gt; register -&gt; tmem -&gt; register -&gt; gmem with vectorization 4 and unroll
factor 2:<!-- */ //-->\</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TEST_F</span><span class="p">(</span><span class="n">TMemTutorialR</span><span class="p">,</span><span class="w"> </span><span class="n">PerformantVectorizedCopy</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">Fusion</span><span class="w"> </span><span class="n">fusion</span><span class="p">;</span>
<span class="w">  </span><span class="n">FusionGuard</span><span class="w"> </span><span class="nf">fg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">makeContigTensor</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addInput</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv0</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv1</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv2</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">tv4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">set</span><span class="p">(</span><span class="n">tv3</span><span class="p">);</span>
<span class="w">  </span><span class="n">fusion</span><span class="p">.</span><span class="n">addOutput</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setMemoryType</span><span class="p">(</span><span class="n">MemoryType</span><span class="o">::</span><span class="n">Tensor</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">StTMem</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv3</span><span class="o">-&gt;</span><span class="n">definition</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">as</span><span class="o">&lt;</span><span class="n">LoadStoreOp</span><span class="o">&gt;</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">setOpType</span><span class="p">(</span><span class="n">LoadStoreOpType</span><span class="o">::</span><span class="n">LdTMem</span><span class="p">);</span>

<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">Vectorize</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">128</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDy</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">split</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">Serial</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv4</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">BIDx</span><span class="p">);</span>

<span class="w">  </span><span class="n">TransformPropagatorWithCheck</span><span class="w"> </span><span class="nf">propagator</span><span class="p">(</span><span class="n">tv4</span><span class="p">);</span>
<span class="w">  </span><span class="n">MaxLogicalDomainInfoSpanningTree</span><span class="p">(</span><span class="n">tv4</span><span class="p">).</span><span class="n">traverse</span><span class="p">(</span><span class="o">&amp;</span><span class="n">propagator</span><span class="p">);</span>
<span class="w">  </span><span class="n">scheduler_utils</span><span class="o">::</span><span class="n">parallelizeAllLike</span><span class="p">(</span>
<span class="w">      </span><span class="n">tv4</span><span class="p">,</span><span class="w"> </span><span class="p">{</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDx</span><span class="p">,</span><span class="w"> </span><span class="n">ParallelType</span><span class="o">::</span><span class="n">TIDy</span><span class="p">,</span><span class="w"> </span><span class="n">ParallelType</span><span class="o">::</span><span class="n">BIDx</span><span class="p">});</span>
<span class="w">  </span><span class="n">tv1</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">-1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">Vectorize</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// [BIDx, Serial, TIDy, TIDx, Vec&#39;] -&gt;</span>
<span class="w">  </span><span class="c1">//   [BIDx, TIDx, |, TIDy, Vec{Serial * Vec&#39;}]</span>
<span class="w">  </span><span class="c1">// Where the Vec&#39; above are the vectorization dims of gmem access, not</span>
<span class="w">  </span><span class="c1">// the vectorization of the tmem access, and Vec is the the vectorization</span>
<span class="w">  </span><span class="c1">// of tmem access.</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="k">auto</span><span class="w"> </span><span class="n">tv</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="p">{</span><span class="n">tv2</span><span class="p">,</span><span class="w"> </span><span class="n">tv3</span><span class="p">})</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">tv</span><span class="o">-&gt;</span><span class="n">reorder</span><span class="p">({{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">}});</span>
<span class="w">    </span><span class="n">tv</span><span class="o">-&gt;</span><span class="n">merge</span><span class="p">(</span><span class="mi">-2</span><span class="p">);</span>
<span class="w">    </span><span class="n">tv</span><span class="o">-&gt;</span><span class="n">axis</span><span class="p">(</span><span class="mi">-1</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">parallelize</span><span class="p">(</span><span class="n">ParallelType</span><span class="o">::</span><span class="n">Vectorize</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setAllocationDomain</span><span class="p">(</span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">getLoopDomain</span><span class="p">(),</span><span class="w"> </span><span class="nb">true</span><span class="p">);</span>
<span class="w">  </span><span class="n">tv2</span><span class="o">-&gt;</span><span class="n">setTMemDimSepPos</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>

<span class="w">  </span><span class="n">inlineMost</span><span class="p">();</span>

<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="k">constexpr</span><span class="w"> </span><span class="p">(</span><span class="n">verbose</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">fusion</span><span class="p">.</span><span class="n">printKernel</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">KernelExecutor</span><span class="w"> </span><span class="n">ke</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Check that tv2 is allocated 32 columns. We actually only need</span>
<span class="w">  </span><span class="c1">// TIDy{2} * Serial{2} * Vec&#39;{4} = 16 columns, but 32 is the minimum</span>
<span class="w">  </span><span class="c1">// unit of allocation.</span>
<span class="w">  </span><span class="n">checkAllocationSize</span><span class="p">(</span><span class="n">ke</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">);</span>

<span class="w">  </span><span class="n">ke</span><span class="p">.</span><span class="n">compile</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>

<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">t0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">rand</span><span class="p">({</span><span class="mi">256</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">1024</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">1024</span><span class="p">},</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ke</span><span class="p">.</span><span class="n">run</span><span class="p">({</span><span class="n">t0</span><span class="p">});</span>
<span class="w">  </span><span class="n">EXPECT_TRUE</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">equal</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">as</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">(),</span><span class="w"> </span><span class="n">t0</span><span class="p">));</span>

<span class="w">  </span><span class="c1">// Check that vectorized PTX instructions are used</span>
<span class="w">  </span><span class="n">GpuLower</span><span class="w"> </span><span class="nf">gpulw</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fusion</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">kernel_str</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">codegen</span><span class="o">::</span><span class="n">generateCudaKernel</span><span class="p">(</span><span class="n">gpulw</span><span class="p">.</span><span class="n">run</span><span class="p">());</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">stringstream</span><span class="w"> </span><span class="n">expect_st</span><span class="p">,</span><span class="w"> </span><span class="n">expect_ld</span><span class="p">;</span>
<span class="w">  </span><span class="n">expect_st</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;tcgen05.st.sync.aligned.32x32b.x8.b32&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="n">expect_ld</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;tcgen05.ld.sync.aligned.32x32b.x8.b32&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="n">EXPECT_THAT</span><span class="p">(</span><span class="n">kernel_str</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">HasSubstr</span><span class="p">(</span><span class="n">expect_st</span><span class="p">.</span><span class="n">str</span><span class="p">()));</span>
<span class="w">  </span><span class="n">EXPECT_THAT</span><span class="p">(</span><span class="n">kernel_str</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">testing</span><span class="o">::</span><span class="n">HasSubstr</span><span class="p">(</span><span class="n">expect_ld</span><span class="p">.</span><span class="n">str</span><span class="p">()));</span>
<span class="p">}</span><span class="w"> </span><span class="cm">/*</span>
</pre></div>
</div>
<!--*/
} // namespace nvfuser
// \-->
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="tma.html" class="btn btn-neutral float-left" title="Introduction to TMA Support in NVFuser" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
<img src="../_static/NVIDIA-LogoBlack.svg"/>
<p class="notices">
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_blank">Privacy Policy</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/" target="_blank">Manage My Privacy</a>
|
<a href="https://www.nvidia.com/en-us/preferences/start/" target="_blank">Do Not Sell or Share My Data</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" target="_blank">Terms of Service</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" target="_blank">Accessibility</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" target="_blank">Corporate Policies</a>
|
<a href="https://www.nvidia.com/en-us/product-security/" target="_blank">Product Security</a>
|
<a href="https://www.nvidia.com/en-us/contact/" target="_blank">Contact</a>
</p>

    <p>&#169; Copyright 2023-2025, NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved..</p>

  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
        
        Version: 0.2.34
        
    </span>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>