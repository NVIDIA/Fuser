// clang-format off
/*
 * SPDX-FileCopyrightText: Copyright (c) 2025-present NVIDIA CORPORATION & AFFILIATES.
 * All rights reserved.
 * SPDX-License-Identifier: BSD-3-Clause
 */
// clang-format on

#include <cub/block/block_scan.cuh>

namespace nvf {
namespace scan {

// Block state constants following nvFuser conventions from fused_reduction.cu
// Scan Domain - TEMPLATE STATE 0
//   - Participating in the scan operation, has values coming in, scanned values
//   coming out
// Iteration Domain - TEMPLATE STATE 1
//   - Not participating in the scan operation, has values across the dimension
//   after scan
constexpr __device__ bool isScan(int STATE) {
  return STATE == 0;
}

constexpr __device__ bool isIter(int STATE) {
  return STATE == 1;
}

// Type utils for interoperability between our own half types and the
// CUDA standard types (identical to TopKOp implementation)
template <typename T>
struct CudaType {
  using type = T;

  __device__ inline static T get(const T& t) {
    return t;
  }
};

template <typename T>
struct NvFuserType {
  using type = T;

  __device__ inline static T get(const T& t) {
    return t;
  }
};

#ifdef __NVFUSER_HAS_HALF__
template <>
struct CudaType<__half> {
  using type = __nv_half;

  __device__ inline static type get(const __half& t) {
    return __ushort_as_half(__NVFUSER_HALF_TO_CUS(t));
  }
};
template <>
struct NvFuserType<__half> {
  __device__ inline static __half get(
      const typename CudaType<__half>::type& t) {
    return *(reinterpret_cast<const __half*>(&t));
  }
};
#endif // __NVFUSER_HAS_HALF__

#ifdef __NVFUSER_HAS_BFLOAT__
template <>
struct CudaType<__bfloat> {
  using type = __nv_bfloat16;

  __device__ inline static type get(const __bfloat& t) {
    return __ushort_as_bfloat16(__NVFUSER_BFLOAT_TO_CUS(t));
  }
};
template <>
struct NvFuserType<__bfloat> {
  __device__ inline static __bfloat get(
      const typename CudaType<__bfloat>::type& t) {
    return *(reinterpret_cast<const __bfloat*>(&t));
  }
};
#endif // __NVFUSER_HAS_BFLOAT__

// Binary operations for scan are now implemented as lambdas passed from outside
// following the pattern from genReductionOp in codegen.cpp

// Block-parallel scan using CUB BlockScan
// Following nvFuser dimensional template parameter pattern like TopKOp
//
// Template parameters (7 vs TopKOp's 9 - simplified for scan):
// - BLOCK_DIM_X/Y/Z: Block dimensions
// - BLOCK_STATE_X/Y/Z: Domain states (0=Scan, 1=Iter)
// - DataT: Data type
// - ITEMS_PER_THREAD: Items per thread
// - BlockDimT: Block dimension type
// - ScanOpT: Binary operation lambda type
//
// Usage example (lambda generated by codegen.cpp):
//   auto scan_op = [] (auto x, auto y) { return x + y; };  // For Add
//   blockScan<...>(scan_output, input_data, init_value, scan_op, block_dim);
template <
    int BLOCK_DIM_X,
    int BLOCK_DIM_Y,
    int BLOCK_DIM_Z,
    int BLOCK_STATE_X,
    int BLOCK_STATE_Y,
    int BLOCK_STATE_Z,
    typename DataT,
    int ITEMS_PER_THREAD,
    typename BlockDimT,
    typename ScanOpT>
__device__ void blockScan(
    DataT (&scan_output)[ITEMS_PER_THREAD],
    const DataT (&input_data)[ITEMS_PER_THREAD],
    const DataT& init_value,
    ScanOpT scan_op,
    BlockDimT block_dim) {
  // For now, only support all dimensions participating in scan (state=0)
  // This follows the same constraint as TopKOp implementation
  static_assert(
      (isScan(BLOCK_STATE_X) || BLOCK_DIM_X == 1) &&
          (isScan(BLOCK_STATE_Y) || BLOCK_DIM_Y == 1) &&
          (isScan(BLOCK_STATE_Z) || BLOCK_DIM_Z == 1),
      "For now, active TID dimensions must participate in scan");

  // Create temporary buffer for CUB operations since input_data is const
  typename CudaType<DataT>::type temp_data[ITEMS_PER_THREAD];
  typename CudaType<DataT>::type cuda_init = CudaType<DataT>::get(init_value);

  for (int i = 0; i < ITEMS_PER_THREAD; i++) {
    temp_data[i] = CudaType<DataT>::get(input_data[i]);
  }

  // CUB BlockScan setup - with proper multi-dimensional block support
  // CUB BlockScan template parameters are simpler than BlockRadixSort:
  // - Key type, Block dimensions, Items per thread, Algorithm (optional)
  using BlockScan = cub::BlockScan<
      typename CudaType<DataT>::type, // Data type
      BLOCK_DIM_X, // X dimension
      cub::BLOCK_SCAN_RAKING, // Algorithm (default for BlockScan)
      BLOCK_DIM_Y, // Y dimension
      BLOCK_DIM_Z // Z dimension
      >;

  // Allocate shared memory for CUB operations
  __shared__ typename BlockScan::TempStorage temp_storage;

  // Perform inclusive scan using the provided binary operation lambda
  BlockScan(temp_storage).InclusiveScan(temp_data, temp_data, scan_op);

  // Copy results back to nvFuser types
  for (int i = 0; i < ITEMS_PER_THREAD; i++) {
    scan_output[i] = NvFuserType<DataT>::get(temp_data[i]);
  }

  // IMPLEMENTATION NOTE: This implementation performs inclusive scan.
  // The init_value parameter is provided for API compatibility with nvFuser
  // ScanOp but is not used in the current inclusive scan implementation.
  // Future enhancement could support both inclusive and exclusive scans.
}

} // namespace scan
} // namespace nvf
