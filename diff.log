diff --git a/csrc/compute_at_map.cpp b/csrc/compute_at_map.cpp
index e5f4cd8..7ae6e2b 100644
--- a/csrc/compute_at_map.cpp
+++ b/csrc/compute_at_map.cpp
@@ -1128,6 +1128,21 @@ IterDomain* ComputeAtMap::computeConcreteId(
 }
 
 void ComputeAtMap::buildConcreteIds() {
+  // Since ScatterOp is an element wise-op based on indexTv, we need to replace
+  // the default iteration domain in some passes. Here we just save it.
+  for (auto expr : ir_utils::getScatterOps(fusion_)) {
+    auto output_ids = ir_utils::allIDsOf(expr->output(0)->as<TensorView>());
+    auto index_ids = ir_utils::allIDsOf(expr->indexTv());
+    auto src_ids = ir_utils::allIDsOf(expr->srcTv());
+    for (size_t i = 0; i < output_ids.size(); ++i) {
+      auto out_id = output_ids[i];
+      auto idx_id = index_ids[i];
+      auto src_id = src_ids[i];
+      scatter_id_map_[out_id] = idx_id;
+      scatter_id_map_[src_id] = idx_id;
+    }
+  }
+
   // For the exact map just select the first ID since they're all exactly the
   // same size, it doesn't matter which is selected. This should be run-to-run
   // deterministic but which ID gets selected her depends on the traversal order
diff --git a/csrc/compute_at_map.h b/csrc/compute_at_map.h
index 9463d05..1707984 100644
--- a/csrc/compute_at_map.h
+++ b/csrc/compute_at_map.h
@@ -297,6 +297,10 @@ class TORCH_CUDA_CU_API ComputeAtMap {
           std::shared_ptr<VectorOfUniqueEntries<IterDomain*>>>& exact_sets)
       const;
 
+  const std::unordered_map<IterDomain*, IterDomain*> scatterMap() const {
+    return scatter_id_map_;
+  }
+
  private:
   // Traverses through definitions of exact maps (unique_exact_definitions_) to
   // input ID's from provided ID. Returns all the exact map concrete IDs of the
@@ -363,6 +367,8 @@ class TORCH_CUDA_CU_API ComputeAtMap {
       DoubleBufferIndicesPtr>
       double_buffered_loop_index_variable_map_;
 
+  //! IterationDomain mapping for ScatterOp.
+  std::unordered_map<IterDomain*, IterDomain*> scatter_id_map_;
   // Shortcut to access the fusion this computeAt map was
   //  built from.
   Fusion* fusion_;
diff --git a/csrc/executor.cpp b/csrc/executor.cpp
index def03c9..f5ec537 100644
--- a/csrc/executor.cpp
+++ b/csrc/executor.cpp
@@ -41,6 +41,30 @@ bool shouldFillAllocationWithNan() {
   return fill_allocation_with_nan_;
 }
 
+// For scatter operator, we need initialize output tensor using self tensor.
+TensorView* getOutputTensorForFillWithInputTensor(Val* output) {
+  if (output->definition() && output->definition()->isA<ScatterOp>()) {
+    return output->definition()->as<ScatterOp>()->selfTv();
+  }
+  return nullptr;
+}
+
+at::Tensor getTensorForFillAnotherTensor(
+    TensorView* tv,
+    const KernelArgumentHolder& arg,
+    kir::Kernel* kernel) {
+  for (const auto i : c10::irange(kernel->inputs().size())) {
+    if (kernel->inputs()[i] == kernel->inputsOf(tv)[0]) {
+      return dynamic_cast<const TensorArgAbstract*>(arg[i])
+          ->getTensor()
+          .clone()
+          .detach();
+    }
+  }
+  TORCH_INTERNAL_ASSERT(
+      false, "can't select input tensor to initiallize output tensor");
+}
+
 void setFillAllocationWithNan(bool value) {
   fill_allocation_with_nan_ = value;
 }
@@ -845,7 +869,11 @@ std::vector<at::Tensor> FusionExecutor::allocOutputs(
           kernel->outputs()[out_i]->isA<TensorView>(),
           "Cannot allocate outputs that are not tensors.");
       auto output = kernel->outputs()[out_i]->as<TensorView>();
-      if (alias_indices.count((int)out_i) != 0) {
+      if (auto need_fill =
+              getOutputTensorForFillWithInputTensor(kernel->outputs()[out_i])) {
+        outputs.push_back(
+            getTensorForFillAnotherTensor(need_fill, args, kernel));
+      } else if (alias_indices.count(out_i) != 0) {
         // aliasing to inputs, no need to allocate real output, just push empty
         // tensor here.
         outputs.emplace_back();
diff --git a/csrc/index_compute.cpp b/csrc/index_compute.cpp
index 4e0c98e..f3c3d08 100644
--- a/csrc/index_compute.cpp
+++ b/csrc/index_compute.cpp
@@ -463,6 +463,13 @@ void IndexCompute::handle(Merge* merge) {
     return;
   }
 
+  // For ScatterOp, the extend of outputTv should be the extend of indexTv.
+  // The caMap.idMap store this info.
+  auto scatter_map = GpuLower::current()->caMap()->scatterMap();
+  if (scatter_map.find(inner_id) != scatter_map.end()) {
+    inner_id = scatter_map[inner_id];
+  }
+
   Val* inner_extent = getExtent(inner_id);
 
   // When the reference has halo extent for inner_id, that extent needs to
@@ -2906,6 +2913,17 @@ std::vector<RootPredicateInfo> Index::getReferenceRootPredicates(
     bool shift_padding) {
   FUSER_PERF_SCOPE("GpuLower::Lower::Index::getReferenceRootPredicates");
 
+  // For ScatterOp, the Predicates should keep the same as the index.
+  if (consumer_tv->definition() &&
+      consumer_tv->definition()->isA<ScatterOp>()) {
+    return getReferenceRootPredicates(
+        consumer_tv->definition()->as<ScatterOp>()->indexTv(),
+        loops,
+        rotated_loops,
+        unswitch_or_vec_loop,
+        shift_padding);
+  }
+
   const auto gpu_lower = GpuLower::current();
 
   const bool is_unswitch = unswitch_or_vec_loop != nullptr;
@@ -3043,6 +3061,10 @@ std::vector<RootPredicateInfo> Index::getReferenceRootPredicates(
     } else {
       auto offsetted_stop_index =
           SimplifyingIrBuilder::addExpr(stop_index, stop_offset);
+      auto scatter_map = gpu_lower->caMap()->scatterMap();
+      if (scatter_map.find(contig_id) != scatter_map.end()) {
+        contig_id = scatter_map[contig_id];
+      }
       auto stop_pred = SimplifyingIrBuilder::ltExpr(
                            offsetted_stop_index, contig_id->extent())
                            ->as<Bool>();
diff --git a/csrc/ir_utils.cpp b/csrc/ir_utils.cpp
index fa29f3e..59de6df 100644
--- a/csrc/ir_utils.cpp
+++ b/csrc/ir_utils.cpp
@@ -426,6 +426,18 @@ std::vector<TorchGatherOp*> getTorchGatherOps(Fusion* fusion) {
   return torch_gather_ops;
 }
 
+std::vector<ScatterOp*> getScatterOps(Fusion* fusion) {
+  std::vector<ScatterOp*> scatter_ops;
+
+  for (auto expr : fusion->exprs()) {
+    if (expr->isA<ScatterOp>()) {
+      scatter_ops.push_back(expr->as<ScatterOp>());
+    }
+  }
+
+  return scatter_ops;
+}
+
 std::vector<SelectOp*> getSelectOps(Fusion* fusion) {
   std::vector<SelectOp*> select_ops;
 
diff --git a/csrc/ir_utils.h b/csrc/ir_utils.h
index 9f037d8..1bde977 100644
--- a/csrc/ir_utils.h
+++ b/csrc/ir_utils.h
@@ -325,6 +325,8 @@ TORCH_CUDA_CU_API std::vector<IndexSelectOp*> getIndexSelectOps(Fusion* fusion);
 
 TORCH_CUDA_CU_API std::vector<TorchGatherOp*> getTorchGatherOps(Fusion* fusion);
 
+TORCH_CUDA_CU_API std::vector<ScatterOp*> getScatterOps(Fusion* fusion);
+
 TORCH_CUDA_CU_API std::vector<SelectOp*> getSelectOps(Fusion* fusion);
 
 // Returns the initialization value of tv or nullptr if not initialized.
diff --git a/csrc/lower_loops.cpp b/csrc/lower_loops.cpp
index fd7ca1d..4b43db0 100644
--- a/csrc/lower_loops.cpp
+++ b/csrc/lower_loops.cpp
@@ -173,6 +173,11 @@ void LoopNestGenerator::generate(const std::vector<Expr*>& exprs) {
       auto concrete_id =
           ca_map->getConcreteMappedID(tv_id, IdMappingMode::LOOP);
 
+      // For ScatterOp, the loop bound should be generated by indexTv.
+      auto scatter_map = ca_map->scatterMap();
+      if (scatter_map.find(concrete_id) != scatter_map.end()) {
+        concrete_id = scatter_map[concrete_id];
+      }
       if (concrete_id_dependencies.find(concrete_id) ==
           concrete_id_dependencies.end()) {
         concrete_id_dependencies[concrete_id] = dependencies;
@@ -242,6 +247,10 @@ void LoopNestGenerator::generate(const std::vector<Expr*>& exprs) {
 
     auto last_id_concrete = ca_map->getConcreteMappedID(
         tv->axis((int)(tv->nDims() - 1)), IdMappingMode::LOOP);
+    auto scatter_map = ca_map->scatterMap();
+    if (scatter_map.find(last_id_concrete) != scatter_map.end()) {
+      last_id_concrete = scatter_map[last_id_concrete];
+    }
     auto all_loops_it = concrete_id_dependencies.find(last_id_concrete);
     TORCH_INTERNAL_ASSERT(
         all_loops_it != concrete_id_dependencies.end(),
diff --git a/csrc/scheduler/vectorize_helper.cpp b/csrc/scheduler/vectorize_helper.cpp
index 460a994..2451815 100644
--- a/csrc/scheduler/vectorize_helper.cpp
+++ b/csrc/scheduler/vectorize_helper.cpp
@@ -154,11 +154,20 @@ Val* commonOrConstExtent(
     std::shared_ptr<const ComputeAtMap> ca_map,
     IterDomain* id) {
   auto disjoint_set = ca_map->idGraph().almostExactNodes().getDisjointSetOf(id);
+  // For ScatterOp, the extend of outputTv should be the extend of indexTv.
+  // The id_map store this mapping info.
+  auto scatter_map = ca_map->scatterMap();
   for (auto entry : disjoint_set) {
     if (entry->extent()->isConstScalar()) {
+       if (scatter_map.find(entry) != scatter_map.end()) {
+        return scatter_map[entry]->extent();
+      }
       return entry->extent();
     }
   }
+  if (scatter_map.find(id) != scatter_map.end()) {
+    return scatter_map[id]->extent();
+  }
   return ca_map->getConcreteMappedID(id, IdMappingMode::ALMOSTEXACT)->extent();
 }
 } // namespace
diff --git a/test/test_gpu_gather_ops.cpp b/test/test_gpu_gather_ops.cpp
index e7b32f3..231c9c6 100644
--- a/test/test_gpu_gather_ops.cpp
+++ b/test/test_gpu_gather_ops.cpp
@@ -116,6 +116,111 @@ TEST_F(NVFuserTest, FusionScatter1DIndexZerosSelfTvSameShape_CUDA) {
   }
 }
 
+TEST_F(NVFuserTest, FusionScatter2DZerosSelfTvFusion_CUDA) {
+  const std::vector<std::vector<int64_t>> input_dims = {
+      {4, 3}, {128, 22}, {128, 64}};
+
+  const std::vector<std::vector<int64_t>> src_dims = {
+      {3, 2}, {100, 14}, {64, 40}};
+
+  const std::vector<std::vector<int64_t>> idx_dims = {
+      {2, 2}, {32, 14}, {32, 40}};
+  auto options = at::TensorOptions().dtype(at::kFloat).device(at::kCUDA, 0);
+  auto options_i =
+      torch::TensorOptions().dtype(torch::kLong).device(at::kCUDA, 0);
+  at::manual_seed(0);
+  for (size_t test_id = 0; test_id < idx_dims.size(); ++test_id) {
+    auto fusion_ptr = std::make_unique<Fusion>();
+    Fusion& fusion = *fusion_ptr.get();
+    FusionGuard fg(&fusion);
+
+    TensorView* tv_input = makeContigTensor(2);
+    TensorView* tv_idx_1 = makeContigTensor(2, DataType::Int);
+    TensorView* tv_idx_2 = makeContigTensor(2, DataType::Int);
+    TensorView* tv_src = makeContigTensor(2);
+
+    fusion.addInput(tv_input);
+    fusion.addInput(tv_idx_1);
+    fusion.addInput(tv_idx_2);
+    fusion.addInput(tv_src);
+
+    auto tv_idx = add(tv_idx_1, tv_idx_2);
+    auto tv_out = scatter(tv_input, 0, tv_idx, tv_src);
+    fusion.addOutput(tv_out);
+
+    at::Tensor idx = generateScatter2DIndex(
+        0, idx_dims[test_id][1], idx_dims[test_id][0], 0);
+
+    at::Tensor idx_1 = at::randint(0, 1024, idx_dims[test_id], options_i);
+    at::Tensor idx_2 = idx - idx_1;
+    at::Tensor input = at::zeros(input_dims[test_id], options);
+    at::Tensor src = at::randn(src_dims[test_id], options);
+    auto t_index = at::add(idx_1, idx_2);
+    auto out_ref = at::scatter(input, 0, t_index, src);
+
+    std::vector<c10::IValue> aten_inputs = {input, idx_1, idx_2, src};
+
+    FusionExecutorCache executor_cache(std::move(fusion_ptr));
+    auto cg_outputs = executor_cache.runFusionWithInputs(aten_inputs);
+
+    testValidate(
+        &fusion, cg_outputs, aten_inputs, {out_ref}, __LINE__, __FILE__);
+  }
+}
+
+TEST_F(NVFuserTest, FusionScatterNoParallism_CUDA) {
+  const std::vector<std::vector<int64_t>> input_dims = {{6, 5}, {32, 28}};
+
+  const std::vector<std::vector<int64_t>> src_dims = {{4, 2}, {24, 16}};
+
+  const std::vector<std::vector<int64_t>> idx_dims = {{3, 2}, {18, 16}};
+  auto options = at::TensorOptions().dtype(at::kFloat).device(at::kCUDA, 0);
+  auto options_i =
+      torch::TensorOptions().dtype(torch::kLong).device(at::kCUDA, 0);
+  at::manual_seed(0);
+  for (size_t test_id = 0; test_id < idx_dims.size(); ++test_id) {
+    auto fusion_ptr = std::make_unique<Fusion>();
+    Fusion& fusion = *fusion_ptr.get();
+    FusionGuard fg(&fusion);
+
+    TensorView* tv_input = makeConcreteTensor(input_dims[test_id]);
+    TensorView* tv_idx_1 = makeConcreteTensor(idx_dims[test_id], DataType::Int);
+    TensorView* tv_idx_2 = makeConcreteTensor(idx_dims[test_id], DataType::Int);
+    TensorView* tv_src = makeConcreteTensor(src_dims[test_id]);
+
+    fusion.addInput(tv_input);
+    fusion.addInput(tv_idx_1);
+    fusion.addInput(tv_idx_2);
+    fusion.addInput(tv_src);
+
+    auto tv_idx = add(tv_idx_1, tv_idx_2);
+    auto tv_out = scatter(tv_input, 0, tv_idx, tv_src);
+    fusion.addOutput(tv_out);
+
+    tv_idx->computeAt(tv_out, 0);
+
+    at::Tensor idx = generateScatter2DIndex(
+        0, idx_dims[test_id][1], idx_dims[test_id][0], 0);
+
+    at::Tensor idx_1 = at::randint(0, 1024, idx_dims[test_id], options_i);
+    at::Tensor idx_2 = idx - idx_1;
+    at::Tensor input = at::zeros(input_dims[test_id], options);
+    at::Tensor src = at::randn(src_dims[test_id], options);
+
+    auto t_index = at::add(idx_1, idx_2);
+    auto out_ref = at::scatter(input, 0, t_index, src);
+
+    std::vector<c10::IValue> aten_inputs = {input, idx_1, idx_2, src};
+
+    FusionExecutor fe;
+    fe.compileFusion(&fusion, aten_inputs);
+    auto cg_outputs = fe.runFusion(aten_inputs);
+
+    testValidate(
+        &fusion, cg_outputs, aten_inputs, {out_ref}, __LINE__, __FILE__);
+  }
+}
+
 // all torch.gather test follow the FusionTorchGather* pattern
 
 // Test the correctness of gather operator in different dimensions and selcted
